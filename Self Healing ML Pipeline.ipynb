{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1865eb57-e6b0-46a2-80e2-81e58572a611",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files removed: 2 (38 kB)\n"
     ]
    }
   ],
   "source": [
    "!pip cache purge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "79a0ab4b-cb1d-4ce0-96b1-5d308130ebec",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: notebook in c:\\anaconda3\\lib\\site-packages (7.3.2)\n",
      "Requirement already satisfied: jupyterlab in c:\\anaconda3\\lib\\site-packages (4.3.4)\n",
      "Requirement already satisfied: ipykernel in c:\\anaconda3\\lib\\site-packages (6.29.5)\n",
      "Requirement already satisfied: jupyter-server<3,>=2.4.0 in c:\\anaconda3\\lib\\site-packages (from notebook) (2.15.0)\n",
      "Requirement already satisfied: jupyterlab-server<3,>=2.27.1 in c:\\anaconda3\\lib\\site-packages (from notebook) (2.27.3)\n",
      "Requirement already satisfied: notebook-shim<0.3,>=0.2 in c:\\anaconda3\\lib\\site-packages (from notebook) (0.2.4)\n",
      "Requirement already satisfied: tornado>=6.2.0 in c:\\users\\aniru\\appdata\\roaming\\python\\python312\\site-packages (from notebook) (6.5.2)\n",
      "Requirement already satisfied: async-lru>=1.0.0 in c:\\anaconda3\\lib\\site-packages (from jupyterlab) (2.0.4)\n",
      "Requirement already satisfied: httpx>=0.25.0 in c:\\users\\aniru\\appdata\\roaming\\python\\python312\\site-packages (from jupyterlab) (0.28.1)\n",
      "Requirement already satisfied: jinja2>=3.0.3 in c:\\users\\aniru\\appdata\\roaming\\python\\python312\\site-packages (from jupyterlab) (3.1.6)\n",
      "Requirement already satisfied: jupyter-core in c:\\anaconda3\\lib\\site-packages (from jupyterlab) (5.7.2)\n",
      "Requirement already satisfied: jupyter-lsp>=2.0.0 in c:\\anaconda3\\lib\\site-packages (from jupyterlab) (2.2.5)\n",
      "Requirement already satisfied: packaging in c:\\users\\aniru\\appdata\\roaming\\python\\python312\\site-packages (from jupyterlab) (25.0)\n",
      "Requirement already satisfied: setuptools>=40.8.0 in c:\\users\\aniru\\appdata\\roaming\\python\\python312\\site-packages (from jupyterlab) (80.9.0)\n",
      "Requirement already satisfied: traitlets in c:\\anaconda3\\lib\\site-packages (from jupyterlab) (5.14.3)\n",
      "Requirement already satisfied: anyio>=3.1.0 in c:\\users\\aniru\\appdata\\roaming\\python\\python312\\site-packages (from jupyter-server<3,>=2.4.0->notebook) (4.10.0)\n",
      "Requirement already satisfied: argon2-cffi>=21.1 in c:\\anaconda3\\lib\\site-packages (from jupyter-server<3,>=2.4.0->notebook) (21.3.0)\n",
      "Requirement already satisfied: jupyter-client>=7.4.4 in c:\\anaconda3\\lib\\site-packages (from jupyter-server<3,>=2.4.0->notebook) (8.6.3)\n",
      "Requirement already satisfied: jupyter-events>=0.11.0 in c:\\anaconda3\\lib\\site-packages (from jupyter-server<3,>=2.4.0->notebook) (0.12.0)\n",
      "Requirement already satisfied: jupyter-server-terminals>=0.4.4 in c:\\anaconda3\\lib\\site-packages (from jupyter-server<3,>=2.4.0->notebook) (0.5.3)\n",
      "Requirement already satisfied: nbconvert>=6.4.4 in c:\\anaconda3\\lib\\site-packages (from jupyter-server<3,>=2.4.0->notebook) (7.16.6)\n",
      "Requirement already satisfied: nbformat>=5.3.0 in c:\\anaconda3\\lib\\site-packages (from jupyter-server<3,>=2.4.0->notebook) (5.10.4)\n",
      "Requirement already satisfied: overrides>=5.0 in c:\\anaconda3\\lib\\site-packages (from jupyter-server<3,>=2.4.0->notebook) (7.4.0)\n",
      "Requirement already satisfied: prometheus-client>=0.9 in c:\\anaconda3\\lib\\site-packages (from jupyter-server<3,>=2.4.0->notebook) (0.21.1)\n",
      "Requirement already satisfied: pywinpty>=2.0.1 in c:\\anaconda3\\lib\\site-packages (from jupyter-server<3,>=2.4.0->notebook) (2.0.15)\n",
      "Requirement already satisfied: pyzmq>=24 in c:\\anaconda3\\lib\\site-packages (from jupyter-server<3,>=2.4.0->notebook) (26.2.0)\n",
      "Requirement already satisfied: send2trash>=1.8.2 in c:\\anaconda3\\lib\\site-packages (from jupyter-server<3,>=2.4.0->notebook) (1.8.2)\n",
      "Requirement already satisfied: terminado>=0.8.3 in c:\\anaconda3\\lib\\site-packages (from jupyter-server<3,>=2.4.0->notebook) (0.17.1)\n",
      "Requirement already satisfied: websocket-client>=1.7 in c:\\users\\aniru\\appdata\\roaming\\python\\python312\\site-packages (from jupyter-server<3,>=2.4.0->notebook) (1.8.0)\n",
      "Requirement already satisfied: babel>=2.10 in c:\\anaconda3\\lib\\site-packages (from jupyterlab-server<3,>=2.27.1->notebook) (2.16.0)\n",
      "Requirement already satisfied: json5>=0.9.0 in c:\\anaconda3\\lib\\site-packages (from jupyterlab-server<3,>=2.27.1->notebook) (0.9.25)\n",
      "Requirement already satisfied: jsonschema>=4.18.0 in c:\\users\\aniru\\appdata\\roaming\\python\\python312\\site-packages (from jupyterlab-server<3,>=2.27.1->notebook) (4.25.0)\n",
      "Requirement already satisfied: requests>=2.31 in c:\\users\\aniru\\appdata\\roaming\\python\\python312\\site-packages (from jupyterlab-server<3,>=2.27.1->notebook) (2.32.5)\n",
      "Requirement already satisfied: comm>=0.1.1 in c:\\anaconda3\\lib\\site-packages (from ipykernel) (0.2.1)\n",
      "Requirement already satisfied: debugpy>=1.6.5 in c:\\anaconda3\\lib\\site-packages (from ipykernel) (1.8.11)\n",
      "Requirement already satisfied: ipython>=7.23.1 in c:\\anaconda3\\lib\\site-packages (from ipykernel) (8.30.0)\n",
      "Requirement already satisfied: matplotlib-inline>=0.1 in c:\\anaconda3\\lib\\site-packages (from ipykernel) (0.1.6)\n",
      "Requirement already satisfied: nest-asyncio in c:\\anaconda3\\lib\\site-packages (from ipykernel) (1.6.0)\n",
      "Requirement already satisfied: psutil in c:\\users\\aniru\\appdata\\roaming\\python\\python312\\site-packages (from ipykernel) (7.0.0)\n",
      "Requirement already satisfied: idna>=2.8 in c:\\users\\aniru\\appdata\\roaming\\python\\python312\\site-packages (from anyio>=3.1.0->jupyter-server<3,>=2.4.0->notebook) (3.10)\n",
      "Requirement already satisfied: sniffio>=1.1 in c:\\users\\aniru\\appdata\\roaming\\python\\python312\\site-packages (from anyio>=3.1.0->jupyter-server<3,>=2.4.0->notebook) (1.3.1)\n",
      "Requirement already satisfied: typing_extensions>=4.5 in c:\\users\\aniru\\appdata\\roaming\\python\\python312\\site-packages (from anyio>=3.1.0->jupyter-server<3,>=2.4.0->notebook) (4.15.0)\n",
      "Requirement already satisfied: argon2-cffi-bindings in c:\\anaconda3\\lib\\site-packages (from argon2-cffi>=21.1->jupyter-server<3,>=2.4.0->notebook) (21.2.0)\n",
      "Requirement already satisfied: certifi in c:\\users\\aniru\\appdata\\roaming\\python\\python312\\site-packages (from httpx>=0.25.0->jupyterlab) (2025.8.3)\n",
      "Requirement already satisfied: httpcore==1.* in c:\\users\\aniru\\appdata\\roaming\\python\\python312\\site-packages (from httpx>=0.25.0->jupyterlab) (1.0.9)\n",
      "Requirement already satisfied: h11>=0.16 in c:\\users\\aniru\\appdata\\roaming\\python\\python312\\site-packages (from httpcore==1.*->httpx>=0.25.0->jupyterlab) (0.16.0)\n",
      "Requirement already satisfied: decorator in c:\\anaconda3\\lib\\site-packages (from ipython>=7.23.1->ipykernel) (5.1.1)\n",
      "Requirement already satisfied: jedi>=0.16 in c:\\anaconda3\\lib\\site-packages (from ipython>=7.23.1->ipykernel) (0.19.2)\n",
      "Requirement already satisfied: prompt-toolkit<3.1.0,>=3.0.41 in c:\\anaconda3\\lib\\site-packages (from ipython>=7.23.1->ipykernel) (3.0.43)\n",
      "Requirement already satisfied: pygments>=2.4.0 in c:\\users\\aniru\\appdata\\roaming\\python\\python312\\site-packages (from ipython>=7.23.1->ipykernel) (2.19.2)\n",
      "Requirement already satisfied: stack-data in c:\\anaconda3\\lib\\site-packages (from ipython>=7.23.1->ipykernel) (0.2.0)\n",
      "Requirement already satisfied: colorama in c:\\users\\aniru\\appdata\\roaming\\python\\python312\\site-packages (from ipython>=7.23.1->ipykernel) (0.4.6)\n",
      "Requirement already satisfied: wcwidth in c:\\anaconda3\\lib\\site-packages (from prompt-toolkit<3.1.0,>=3.0.41->ipython>=7.23.1->ipykernel) (0.2.13)\n",
      "Requirement already satisfied: parso<0.9.0,>=0.8.4 in c:\\anaconda3\\lib\\site-packages (from jedi>=0.16->ipython>=7.23.1->ipykernel) (0.8.4)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\aniru\\appdata\\roaming\\python\\python312\\site-packages (from jinja2>=3.0.3->jupyterlab) (3.0.2)\n",
      "Requirement already satisfied: attrs>=22.2.0 in c:\\users\\aniru\\appdata\\roaming\\python\\python312\\site-packages (from jsonschema>=4.18.0->jupyterlab-server<3,>=2.27.1->notebook) (25.3.0)\n",
      "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in c:\\users\\aniru\\appdata\\roaming\\python\\python312\\site-packages (from jsonschema>=4.18.0->jupyterlab-server<3,>=2.27.1->notebook) (2025.4.1)\n",
      "Requirement already satisfied: referencing>=0.28.4 in c:\\users\\aniru\\appdata\\roaming\\python\\python312\\site-packages (from jsonschema>=4.18.0->jupyterlab-server<3,>=2.27.1->notebook) (0.36.2)\n",
      "Requirement already satisfied: rpds-py>=0.7.1 in c:\\users\\aniru\\appdata\\roaming\\python\\python312\\site-packages (from jsonschema>=4.18.0->jupyterlab-server<3,>=2.27.1->notebook) (0.27.0)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\aniru\\appdata\\roaming\\python\\python312\\site-packages (from jupyter-client>=7.4.4->jupyter-server<3,>=2.4.0->notebook) (2.9.0.post0)\n",
      "Requirement already satisfied: platformdirs>=2.5 in c:\\anaconda3\\lib\\site-packages (from jupyter-core->jupyterlab) (4.3.7)\n",
      "Requirement already satisfied: pywin32>=300 in c:\\anaconda3\\lib\\site-packages (from jupyter-core->jupyterlab) (308)\n",
      "Requirement already satisfied: python-json-logger>=2.0.4 in c:\\anaconda3\\lib\\site-packages (from jupyter-events>=0.11.0->jupyter-server<3,>=2.4.0->notebook) (3.2.1)\n",
      "Requirement already satisfied: pyyaml>=5.3 in c:\\anaconda3\\lib\\site-packages (from jupyter-events>=0.11.0->jupyter-server<3,>=2.4.0->notebook) (6.0.2)\n",
      "Requirement already satisfied: rfc3339-validator in c:\\anaconda3\\lib\\site-packages (from jupyter-events>=0.11.0->jupyter-server<3,>=2.4.0->notebook) (0.1.4)\n",
      "Requirement already satisfied: rfc3986-validator>=0.1.1 in c:\\anaconda3\\lib\\site-packages (from jupyter-events>=0.11.0->jupyter-server<3,>=2.4.0->notebook) (0.1.1)\n",
      "Requirement already satisfied: fqdn in c:\\anaconda3\\lib\\site-packages (from jsonschema[format-nongpl]>=4.18.0->jupyter-events>=0.11.0->jupyter-server<3,>=2.4.0->notebook) (1.5.1)\n",
      "Requirement already satisfied: isoduration in c:\\anaconda3\\lib\\site-packages (from jsonschema[format-nongpl]>=4.18.0->jupyter-events>=0.11.0->jupyter-server<3,>=2.4.0->notebook) (20.11.0)\n",
      "Requirement already satisfied: jsonpointer>1.13 in c:\\anaconda3\\lib\\site-packages (from jsonschema[format-nongpl]>=4.18.0->jupyter-events>=0.11.0->jupyter-server<3,>=2.4.0->notebook) (2.1)\n",
      "Requirement already satisfied: rfc3987-syntax>=1.1.0 in c:\\anaconda3\\lib\\site-packages (from jsonschema[format-nongpl]>=4.18.0->jupyter-events>=0.11.0->jupyter-server<3,>=2.4.0->notebook) (1.1.0)\n",
      "Requirement already satisfied: uri-template in c:\\anaconda3\\lib\\site-packages (from jsonschema[format-nongpl]>=4.18.0->jupyter-events>=0.11.0->jupyter-server<3,>=2.4.0->notebook) (1.3.0)\n",
      "Requirement already satisfied: webcolors>=24.6.0 in c:\\anaconda3\\lib\\site-packages (from jsonschema[format-nongpl]>=4.18.0->jupyter-events>=0.11.0->jupyter-server<3,>=2.4.0->notebook) (24.11.1)\n",
      "Requirement already satisfied: beautifulsoup4 in c:\\users\\aniru\\appdata\\roaming\\python\\python312\\site-packages (from nbconvert>=6.4.4->jupyter-server<3,>=2.4.0->notebook) (4.13.4)\n",
      "Requirement already satisfied: bleach!=5.0.0 in c:\\anaconda3\\lib\\site-packages (from bleach[css]!=5.0.0->nbconvert>=6.4.4->jupyter-server<3,>=2.4.0->notebook) (6.2.0)\n",
      "Requirement already satisfied: defusedxml in c:\\anaconda3\\lib\\site-packages (from nbconvert>=6.4.4->jupyter-server<3,>=2.4.0->notebook) (0.7.1)\n",
      "Requirement already satisfied: jupyterlab-pygments in c:\\anaconda3\\lib\\site-packages (from nbconvert>=6.4.4->jupyter-server<3,>=2.4.0->notebook) (0.3.0)\n",
      "Requirement already satisfied: mistune<4,>=2.0.3 in c:\\anaconda3\\lib\\site-packages (from nbconvert>=6.4.4->jupyter-server<3,>=2.4.0->notebook) (3.1.2)\n",
      "Requirement already satisfied: nbclient>=0.5.0 in c:\\anaconda3\\lib\\site-packages (from nbconvert>=6.4.4->jupyter-server<3,>=2.4.0->notebook) (0.10.2)\n",
      "Requirement already satisfied: pandocfilters>=1.4.1 in c:\\anaconda3\\lib\\site-packages (from nbconvert>=6.4.4->jupyter-server<3,>=2.4.0->notebook) (1.5.0)\n",
      "Requirement already satisfied: webencodings in c:\\anaconda3\\lib\\site-packages (from bleach!=5.0.0->bleach[css]!=5.0.0->nbconvert>=6.4.4->jupyter-server<3,>=2.4.0->notebook) (0.5.1)\n",
      "Requirement already satisfied: tinycss2<1.5,>=1.1.0 in c:\\anaconda3\\lib\\site-packages (from bleach[css]!=5.0.0->nbconvert>=6.4.4->jupyter-server<3,>=2.4.0->notebook) (1.4.0)\n",
      "Requirement already satisfied: fastjsonschema>=2.15 in c:\\anaconda3\\lib\\site-packages (from nbformat>=5.3.0->jupyter-server<3,>=2.4.0->notebook) (2.20.0)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\aniru\\appdata\\roaming\\python\\python312\\site-packages (from python-dateutil>=2.8.2->jupyter-client>=7.4.4->jupyter-server<3,>=2.4.0->notebook) (1.17.0)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in c:\\users\\aniru\\appdata\\roaming\\python\\python312\\site-packages (from requests>=2.31->jupyterlab-server<3,>=2.27.1->notebook) (3.4.3)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\aniru\\appdata\\roaming\\python\\python312\\site-packages (from requests>=2.31->jupyterlab-server<3,>=2.27.1->notebook) (2.5.0)\n",
      "Requirement already satisfied: lark>=1.2.2 in c:\\anaconda3\\lib\\site-packages (from rfc3987-syntax>=1.1.0->jsonschema[format-nongpl]>=4.18.0->jupyter-events>=0.11.0->jupyter-server<3,>=2.4.0->notebook) (1.2.2)\n",
      "Requirement already satisfied: cffi>=1.0.1 in c:\\users\\aniru\\appdata\\roaming\\python\\python312\\site-packages (from argon2-cffi-bindings->argon2-cffi>=21.1->jupyter-server<3,>=2.4.0->notebook) (1.17.1)\n",
      "Requirement already satisfied: pycparser in c:\\users\\aniru\\appdata\\roaming\\python\\python312\\site-packages (from cffi>=1.0.1->argon2-cffi-bindings->argon2-cffi>=21.1->jupyter-server<3,>=2.4.0->notebook) (2.22)\n",
      "Requirement already satisfied: soupsieve>1.2 in c:\\users\\aniru\\appdata\\roaming\\python\\python312\\site-packages (from beautifulsoup4->nbconvert>=6.4.4->jupyter-server<3,>=2.4.0->notebook) (2.7)\n",
      "Requirement already satisfied: arrow>=0.15.0 in c:\\users\\aniru\\appdata\\roaming\\python\\python312\\site-packages (from isoduration->jsonschema[format-nongpl]>=4.18.0->jupyter-events>=0.11.0->jupyter-server<3,>=2.4.0->notebook) (1.3.0)\n",
      "Requirement already satisfied: types-python-dateutil>=2.8.10 in c:\\users\\aniru\\appdata\\roaming\\python\\python312\\site-packages (from arrow>=0.15.0->isoduration->jsonschema[format-nongpl]>=4.18.0->jupyter-events>=0.11.0->jupyter-server<3,>=2.4.0->notebook) (2.9.0.20250809)\n",
      "Requirement already satisfied: executing in c:\\anaconda3\\lib\\site-packages (from stack-data->ipython>=7.23.1->ipykernel) (0.8.3)\n",
      "Requirement already satisfied: asttokens in c:\\anaconda3\\lib\\site-packages (from stack-data->ipython>=7.23.1->ipykernel) (2.4.1)\n",
      "Requirement already satisfied: pure-eval in c:\\anaconda3\\lib\\site-packages (from stack-data->ipython>=7.23.1->ipykernel) (0.2.2)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 25.2 -> 25.3\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pyarrow in c:\\users\\aniru\\appdata\\roaming\\python\\python312\\site-packages (21.0.0)\n",
      "Requirement already satisfied: duckdb in c:\\anaconda3\\lib\\site-packages (1.3.2)\n",
      "Requirement already satisfied: ipywidgets==8.* in c:\\anaconda3\\lib\\site-packages (8.1.7)\n",
      "Requirement already satisfied: comm>=0.1.3 in c:\\anaconda3\\lib\\site-packages (from ipywidgets==8.*) (0.2.1)\n",
      "Requirement already satisfied: ipython>=6.1.0 in c:\\anaconda3\\lib\\site-packages (from ipywidgets==8.*) (8.30.0)\n",
      "Requirement already satisfied: traitlets>=4.3.1 in c:\\anaconda3\\lib\\site-packages (from ipywidgets==8.*) (5.14.3)\n",
      "Requirement already satisfied: widgetsnbextension~=4.0.14 in c:\\anaconda3\\lib\\site-packages (from ipywidgets==8.*) (4.0.14)\n",
      "Requirement already satisfied: jupyterlab_widgets~=3.0.15 in c:\\anaconda3\\lib\\site-packages (from ipywidgets==8.*) (3.0.15)\n",
      "Requirement already satisfied: decorator in c:\\anaconda3\\lib\\site-packages (from ipython>=6.1.0->ipywidgets==8.*) (5.1.1)\n",
      "Requirement already satisfied: jedi>=0.16 in c:\\anaconda3\\lib\\site-packages (from ipython>=6.1.0->ipywidgets==8.*) (0.19.2)\n",
      "Requirement already satisfied: matplotlib-inline in c:\\anaconda3\\lib\\site-packages (from ipython>=6.1.0->ipywidgets==8.*) (0.1.6)\n",
      "Requirement already satisfied: prompt-toolkit<3.1.0,>=3.0.41 in c:\\anaconda3\\lib\\site-packages (from ipython>=6.1.0->ipywidgets==8.*) (3.0.43)\n",
      "Requirement already satisfied: pygments>=2.4.0 in c:\\users\\aniru\\appdata\\roaming\\python\\python312\\site-packages (from ipython>=6.1.0->ipywidgets==8.*) (2.19.2)\n",
      "Requirement already satisfied: stack-data in c:\\anaconda3\\lib\\site-packages (from ipython>=6.1.0->ipywidgets==8.*) (0.2.0)\n",
      "Requirement already satisfied: colorama in c:\\users\\aniru\\appdata\\roaming\\python\\python312\\site-packages (from ipython>=6.1.0->ipywidgets==8.*) (0.4.6)\n",
      "Requirement already satisfied: wcwidth in c:\\anaconda3\\lib\\site-packages (from prompt-toolkit<3.1.0,>=3.0.41->ipython>=6.1.0->ipywidgets==8.*) (0.2.13)\n",
      "Requirement already satisfied: parso<0.9.0,>=0.8.4 in c:\\anaconda3\\lib\\site-packages (from jedi>=0.16->ipython>=6.1.0->ipywidgets==8.*) (0.8.4)\n",
      "Requirement already satisfied: executing in c:\\anaconda3\\lib\\site-packages (from stack-data->ipython>=6.1.0->ipywidgets==8.*) (0.8.3)\n",
      "Requirement already satisfied: asttokens in c:\\anaconda3\\lib\\site-packages (from stack-data->ipython>=6.1.0->ipywidgets==8.*) (2.4.1)\n",
      "Requirement already satisfied: pure-eval in c:\\anaconda3\\lib\\site-packages (from stack-data->ipython>=6.1.0->ipywidgets==8.*) (0.2.2)\n",
      "Requirement already satisfied: six>=1.12.0 in c:\\users\\aniru\\appdata\\roaming\\python\\python312\\site-packages (from asttokens->stack-data->ipython>=6.1.0->ipywidgets==8.*) (1.17.0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 25.2 -> 25.3\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n",
      "\n",
      "[notice] A new release of pip is available: 25.2 -> 25.3\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: scikit-learn in c:\\anaconda3\\lib\\site-packages (1.6.1)\n",
      "Requirement already satisfied: xgboost in c:\\anaconda3\\lib\\site-packages (2.1.1)\n",
      "Requirement already satisfied: lightgbm in c:\\anaconda3\\lib\\site-packages (4.5.0)\n",
      "Requirement already satisfied: numpy>=1.19.5 in c:\\anaconda3\\lib\\site-packages (from scikit-learn) (2.1.3)\n",
      "Requirement already satisfied: scipy>=1.6.0 in c:\\anaconda3\\lib\\site-packages (from scikit-learn) (1.14.1)\n",
      "Requirement already satisfied: joblib>=1.2.0 in c:\\anaconda3\\lib\\site-packages (from scikit-learn) (1.5.1)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in c:\\users\\aniru\\appdata\\roaming\\python\\python312\\site-packages (from scikit-learn) (3.6.0)\n",
      "Requirement already satisfied: scipy in c:\\anaconda3\\lib\\site-packages (1.14.1)\n",
      "Requirement already satisfied: pandas in c:\\anaconda3\\lib\\site-packages (2.2.3)\n",
      "Requirement already satisfied: numpy in c:\\anaconda3\\lib\\site-packages (2.1.3)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\aniru\\appdata\\roaming\\python\\python312\\site-packages (from pandas) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\aniru\\appdata\\roaming\\python\\python312\\site-packages (from pandas) (2025.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in c:\\users\\aniru\\appdata\\roaming\\python\\python312\\site-packages (from pandas) (2025.2)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\aniru\\appdata\\roaming\\python\\python312\\site-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 25.2 -> 25.3\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: regex in c:\\anaconda3\\lib\\site-packages (2025.7.34)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 25.2 -> 25.3\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: statsmodels in c:\\users\\aniru\\appdata\\roaming\\python\\python312\\site-packages (0.14.5)\n",
      "Requirement already satisfied: numpy<3,>=1.22.3 in c:\\anaconda3\\lib\\site-packages (from statsmodels) (2.1.3)\n",
      "Requirement already satisfied: scipy!=1.9.2,>=1.8 in c:\\anaconda3\\lib\\site-packages (from statsmodels) (1.14.1)\n",
      "Requirement already satisfied: pandas!=2.1.0,>=1.4 in c:\\anaconda3\\lib\\site-packages (from statsmodels) (2.2.3)\n",
      "Requirement already satisfied: patsy>=0.5.6 in c:\\users\\aniru\\appdata\\roaming\\python\\python312\\site-packages (from statsmodels) (1.0.1)\n",
      "Requirement already satisfied: packaging>=21.3 in c:\\users\\aniru\\appdata\\roaming\\python\\python312\\site-packages (from statsmodels) (25.0)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\aniru\\appdata\\roaming\\python\\python312\\site-packages (from pandas!=2.1.0,>=1.4->statsmodels) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\aniru\\appdata\\roaming\\python\\python312\\site-packages (from pandas!=2.1.0,>=1.4->statsmodels) (2025.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in c:\\users\\aniru\\appdata\\roaming\\python\\python312\\site-packages (from pandas!=2.1.0,>=1.4->statsmodels) (2025.2)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\aniru\\appdata\\roaming\\python\\python312\\site-packages (from python-dateutil>=2.8.2->pandas!=2.1.0,>=1.4->statsmodels) (1.17.0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 25.2 -> 25.3\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: rank-bm25 in c:\\anaconda3\\lib\\site-packages (0.2.2)\n",
      "Requirement already satisfied: numpy in c:\\anaconda3\\lib\\site-packages (from rank-bm25) (2.1.3)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 25.2 -> 25.3\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: https://download.pytorch.org/whl/cu118\n",
      "Requirement already satisfied: torch in c:\\anaconda3\\lib\\site-packages (2.5.1)\n",
      "Requirement already satisfied: torchvision in c:\\anaconda3\\lib\\site-packages (0.20.1)\n",
      "Requirement already satisfied: torchaudio in c:\\anaconda3\\lib\\site-packages (2.5.1)\n",
      "Requirement already satisfied: filelock in c:\\users\\aniru\\appdata\\roaming\\python\\python312\\site-packages (from torch) (3.19.1)\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in c:\\users\\aniru\\appdata\\roaming\\python\\python312\\site-packages (from torch) (4.15.0)\n",
      "Requirement already satisfied: setuptools in c:\\users\\aniru\\appdata\\roaming\\python\\python312\\site-packages (from torch) (80.9.0)\n",
      "Requirement already satisfied: sympy==1.13.1 in c:\\anaconda3\\lib\\site-packages (from torch) (1.13.1)\n",
      "Requirement already satisfied: networkx in c:\\anaconda3\\lib\\site-packages (from torch) (3.4.2)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\aniru\\appdata\\roaming\\python\\python312\\site-packages (from torch) (3.1.6)\n",
      "Requirement already satisfied: fsspec in c:\\anaconda3\\lib\\site-packages (from torch) (2025.3.0)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in c:\\anaconda3\\lib\\site-packages (from sympy==1.13.1->torch) (1.3.0)\n",
      "Requirement already satisfied: numpy in c:\\anaconda3\\lib\\site-packages (from torchvision) (2.1.3)\n",
      "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in c:\\users\\aniru\\appdata\\roaming\\python\\python312\\site-packages (from torchvision) (11.3.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\aniru\\appdata\\roaming\\python\\python312\\site-packages (from jinja2->torch) (3.0.2)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 25.2 -> 25.3\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: transformers==4.44.2 in c:\\anaconda3\\lib\\site-packages (4.44.2)\n",
      "Requirement already satisfied: sentence-transformers in c:\\anaconda3\\lib\\site-packages (5.1.0)\n",
      "Requirement already satisfied: filelock in c:\\users\\aniru\\appdata\\roaming\\python\\python312\\site-packages (from transformers==4.44.2) (3.19.1)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.23.2 in c:\\anaconda3\\lib\\site-packages (from transformers==4.44.2) (0.34.4)\n",
      "Requirement already satisfied: numpy>=1.17 in c:\\anaconda3\\lib\\site-packages (from transformers==4.44.2) (2.1.3)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\aniru\\appdata\\roaming\\python\\python312\\site-packages (from transformers==4.44.2) (25.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in c:\\anaconda3\\lib\\site-packages (from transformers==4.44.2) (6.0.2)\n",
      "Requirement already satisfied: regex!=2019.12.17 in c:\\anaconda3\\lib\\site-packages (from transformers==4.44.2) (2025.7.34)\n",
      "Requirement already satisfied: requests in c:\\users\\aniru\\appdata\\roaming\\python\\python312\\site-packages (from transformers==4.44.2) (2.32.5)\n",
      "Requirement already satisfied: safetensors>=0.4.1 in c:\\anaconda3\\lib\\site-packages (from transformers==4.44.2) (0.6.2)\n",
      "Requirement already satisfied: tokenizers<0.20,>=0.19 in c:\\anaconda3\\lib\\site-packages (from transformers==4.44.2) (0.19.1)\n",
      "Requirement already satisfied: tqdm>=4.27 in c:\\anaconda3\\lib\\site-packages (from transformers==4.44.2) (4.67.1)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in c:\\anaconda3\\lib\\site-packages (from huggingface-hub<1.0,>=0.23.2->transformers==4.44.2) (2025.3.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in c:\\users\\aniru\\appdata\\roaming\\python\\python312\\site-packages (from huggingface-hub<1.0,>=0.23.2->transformers==4.44.2) (4.15.0)\n",
      "Requirement already satisfied: torch>=1.11.0 in c:\\anaconda3\\lib\\site-packages (from sentence-transformers) (2.5.1)\n",
      "Requirement already satisfied: scikit-learn in c:\\anaconda3\\lib\\site-packages (from sentence-transformers) (1.6.1)\n",
      "Requirement already satisfied: scipy in c:\\anaconda3\\lib\\site-packages (from sentence-transformers) (1.14.1)\n",
      "Requirement already satisfied: Pillow in c:\\users\\aniru\\appdata\\roaming\\python\\python312\\site-packages (from sentence-transformers) (11.3.0)\n",
      "Requirement already satisfied: setuptools in c:\\users\\aniru\\appdata\\roaming\\python\\python312\\site-packages (from torch>=1.11.0->sentence-transformers) (80.9.0)\n",
      "Requirement already satisfied: sympy==1.13.1 in c:\\anaconda3\\lib\\site-packages (from torch>=1.11.0->sentence-transformers) (1.13.1)\n",
      "Requirement already satisfied: networkx in c:\\anaconda3\\lib\\site-packages (from torch>=1.11.0->sentence-transformers) (3.4.2)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\aniru\\appdata\\roaming\\python\\python312\\site-packages (from torch>=1.11.0->sentence-transformers) (3.1.6)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in c:\\anaconda3\\lib\\site-packages (from sympy==1.13.1->torch>=1.11.0->sentence-transformers) (1.3.0)\n",
      "Requirement already satisfied: colorama in c:\\users\\aniru\\appdata\\roaming\\python\\python312\\site-packages (from tqdm>=4.27->transformers==4.44.2) (0.4.6)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\aniru\\appdata\\roaming\\python\\python312\\site-packages (from jinja2->torch>=1.11.0->sentence-transformers) (3.0.2)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in c:\\users\\aniru\\appdata\\roaming\\python\\python312\\site-packages (from requests->transformers==4.44.2) (3.4.3)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\aniru\\appdata\\roaming\\python\\python312\\site-packages (from requests->transformers==4.44.2) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\aniru\\appdata\\roaming\\python\\python312\\site-packages (from requests->transformers==4.44.2) (2.5.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\aniru\\appdata\\roaming\\python\\python312\\site-packages (from requests->transformers==4.44.2) (2025.8.3)\n",
      "Requirement already satisfied: joblib>=1.2.0 in c:\\anaconda3\\lib\\site-packages (from scikit-learn->sentence-transformers) (1.5.1)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in c:\\users\\aniru\\appdata\\roaming\\python\\python312\\site-packages (from scikit-learn->sentence-transformers) (3.6.0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 25.2 -> 25.3\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: accelerate in c:\\anaconda3\\lib\\site-packages (1.10.1)\n",
      "Requirement already satisfied: bitsandbytes in c:\\anaconda3\\lib\\site-packages (0.47.0)\n",
      "Requirement already satisfied: numpy<3.0.0,>=1.17 in c:\\anaconda3\\lib\\site-packages (from accelerate) (2.1.3)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\aniru\\appdata\\roaming\\python\\python312\\site-packages (from accelerate) (25.0)\n",
      "Requirement already satisfied: psutil in c:\\users\\aniru\\appdata\\roaming\\python\\python312\\site-packages (from accelerate) (7.0.0)\n",
      "Requirement already satisfied: pyyaml in c:\\anaconda3\\lib\\site-packages (from accelerate) (6.0.2)\n",
      "Requirement already satisfied: torch>=2.0.0 in c:\\anaconda3\\lib\\site-packages (from accelerate) (2.5.1)\n",
      "Requirement already satisfied: huggingface_hub>=0.21.0 in c:\\anaconda3\\lib\\site-packages (from accelerate) (0.34.4)\n",
      "Requirement already satisfied: safetensors>=0.4.3 in c:\\anaconda3\\lib\\site-packages (from accelerate) (0.6.2)\n",
      "Requirement already satisfied: filelock in c:\\users\\aniru\\appdata\\roaming\\python\\python312\\site-packages (from torch>=2.0.0->accelerate) (3.19.1)\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in c:\\users\\aniru\\appdata\\roaming\\python\\python312\\site-packages (from torch>=2.0.0->accelerate) (4.15.0)\n",
      "Requirement already satisfied: setuptools in c:\\users\\aniru\\appdata\\roaming\\python\\python312\\site-packages (from torch>=2.0.0->accelerate) (80.9.0)\n",
      "Requirement already satisfied: sympy==1.13.1 in c:\\anaconda3\\lib\\site-packages (from torch>=2.0.0->accelerate) (1.13.1)\n",
      "Requirement already satisfied: networkx in c:\\anaconda3\\lib\\site-packages (from torch>=2.0.0->accelerate) (3.4.2)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\aniru\\appdata\\roaming\\python\\python312\\site-packages (from torch>=2.0.0->accelerate) (3.1.6)\n",
      "Requirement already satisfied: fsspec in c:\\anaconda3\\lib\\site-packages (from torch>=2.0.0->accelerate) (2025.3.0)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in c:\\anaconda3\\lib\\site-packages (from sympy==1.13.1->torch>=2.0.0->accelerate) (1.3.0)\n",
      "Requirement already satisfied: requests in c:\\users\\aniru\\appdata\\roaming\\python\\python312\\site-packages (from huggingface_hub>=0.21.0->accelerate) (2.32.5)\n",
      "Requirement already satisfied: tqdm>=4.42.1 in c:\\anaconda3\\lib\\site-packages (from huggingface_hub>=0.21.0->accelerate) (4.67.1)\n",
      "Requirement already satisfied: colorama in c:\\users\\aniru\\appdata\\roaming\\python\\python312\\site-packages (from tqdm>=4.42.1->huggingface_hub>=0.21.0->accelerate) (0.4.6)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\aniru\\appdata\\roaming\\python\\python312\\site-packages (from jinja2->torch>=2.0.0->accelerate) (3.0.2)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in c:\\users\\aniru\\appdata\\roaming\\python\\python312\\site-packages (from requests->huggingface_hub>=0.21.0->accelerate) (3.4.3)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\aniru\\appdata\\roaming\\python\\python312\\site-packages (from requests->huggingface_hub>=0.21.0->accelerate) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\aniru\\appdata\\roaming\\python\\python312\\site-packages (from requests->huggingface_hub>=0.21.0->accelerate) (2.5.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\aniru\\appdata\\roaming\\python\\python312\\site-packages (from requests->huggingface_hub>=0.21.0->accelerate) (2025.8.3)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 25.2 -> 25.3\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: huggingface-hub in c:\\anaconda3\\lib\\site-packages (0.34.4)\n",
      "Requirement already satisfied: filelock in c:\\users\\aniru\\appdata\\roaming\\python\\python312\\site-packages (from huggingface-hub) (3.19.1)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in c:\\anaconda3\\lib\\site-packages (from huggingface-hub) (2025.3.0)\n",
      "Requirement already satisfied: packaging>=20.9 in c:\\users\\aniru\\appdata\\roaming\\python\\python312\\site-packages (from huggingface-hub) (25.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in c:\\anaconda3\\lib\\site-packages (from huggingface-hub) (6.0.2)\n",
      "Requirement already satisfied: requests in c:\\users\\aniru\\appdata\\roaming\\python\\python312\\site-packages (from huggingface-hub) (2.32.5)\n",
      "Requirement already satisfied: tqdm>=4.42.1 in c:\\anaconda3\\lib\\site-packages (from huggingface-hub) (4.67.1)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in c:\\users\\aniru\\appdata\\roaming\\python\\python312\\site-packages (from huggingface-hub) (4.15.0)\n",
      "Requirement already satisfied: colorama in c:\\users\\aniru\\appdata\\roaming\\python\\python312\\site-packages (from tqdm>=4.42.1->huggingface-hub) (0.4.6)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in c:\\users\\aniru\\appdata\\roaming\\python\\python312\\site-packages (from requests->huggingface-hub) (3.4.3)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\aniru\\appdata\\roaming\\python\\python312\\site-packages (from requests->huggingface-hub) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\aniru\\appdata\\roaming\\python\\python312\\site-packages (from requests->huggingface-hub) (2.5.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\aniru\\appdata\\roaming\\python\\python312\\site-packages (from requests->huggingface-hub) (2025.8.3)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 25.2 -> 25.3\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: scikit-optimize in c:\\anaconda3\\lib\\site-packages (0.10.2)\n",
      "Requirement already satisfied: joblib>=0.11 in c:\\anaconda3\\lib\\site-packages (from scikit-optimize) (1.5.1)\n",
      "Requirement already satisfied: pyaml>=16.9 in c:\\anaconda3\\lib\\site-packages (from scikit-optimize) (25.7.0)\n",
      "Requirement already satisfied: numpy>=1.20.3 in c:\\anaconda3\\lib\\site-packages (from scikit-optimize) (2.1.3)\n",
      "Requirement already satisfied: scipy>=1.1.0 in c:\\anaconda3\\lib\\site-packages (from scikit-optimize) (1.14.1)\n",
      "Requirement already satisfied: scikit-learn>=1.0.0 in c:\\anaconda3\\lib\\site-packages (from scikit-optimize) (1.6.1)\n",
      "Requirement already satisfied: packaging>=21.3 in c:\\users\\aniru\\appdata\\roaming\\python\\python312\\site-packages (from scikit-optimize) (25.0)\n",
      "Requirement already satisfied: PyYAML in c:\\anaconda3\\lib\\site-packages (from pyaml>=16.9->scikit-optimize) (6.0.2)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in c:\\users\\aniru\\appdata\\roaming\\python\\python312\\site-packages (from scikit-learn>=1.0.0->scikit-optimize) (3.6.0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 25.2 -> 25.3\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: gymnasium==0.29.1 in c:\\anaconda3\\lib\\site-packages (0.29.1)\n",
      "Requirement already satisfied: numpy>=1.21.0 in c:\\anaconda3\\lib\\site-packages (from gymnasium==0.29.1) (2.1.3)\n",
      "Requirement already satisfied: cloudpickle>=1.2.0 in c:\\users\\aniru\\appdata\\roaming\\python\\python312\\site-packages (from gymnasium==0.29.1) (3.1.1)\n",
      "Requirement already satisfied: typing-extensions>=4.3.0 in c:\\users\\aniru\\appdata\\roaming\\python\\python312\\site-packages (from gymnasium==0.29.1) (4.15.0)\n",
      "Requirement already satisfied: farama-notifications>=0.0.1 in c:\\anaconda3\\lib\\site-packages (from gymnasium==0.29.1) (0.0.4)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 25.2 -> 25.3\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: stable-baselines3==2.3.2 in c:\\anaconda3\\lib\\site-packages (2.3.2)\n",
      "Requirement already satisfied: gymnasium<0.30,>=0.28.1 in c:\\anaconda3\\lib\\site-packages (from stable-baselines3==2.3.2) (0.29.1)\n",
      "Requirement already satisfied: numpy>=1.20 in c:\\anaconda3\\lib\\site-packages (from stable-baselines3==2.3.2) (2.1.3)\n",
      "Requirement already satisfied: torch>=1.13 in c:\\anaconda3\\lib\\site-packages (from stable-baselines3==2.3.2) (2.5.1)\n",
      "Requirement already satisfied: cloudpickle in c:\\users\\aniru\\appdata\\roaming\\python\\python312\\site-packages (from stable-baselines3==2.3.2) (3.1.1)\n",
      "Requirement already satisfied: pandas in c:\\anaconda3\\lib\\site-packages (from stable-baselines3==2.3.2) (2.2.3)\n",
      "Requirement already satisfied: matplotlib in c:\\users\\aniru\\appdata\\roaming\\python\\python312\\site-packages (from stable-baselines3==2.3.2) (3.10.6)\n",
      "Requirement already satisfied: typing-extensions>=4.3.0 in c:\\users\\aniru\\appdata\\roaming\\python\\python312\\site-packages (from gymnasium<0.30,>=0.28.1->stable-baselines3==2.3.2) (4.15.0)\n",
      "Requirement already satisfied: farama-notifications>=0.0.1 in c:\\anaconda3\\lib\\site-packages (from gymnasium<0.30,>=0.28.1->stable-baselines3==2.3.2) (0.0.4)\n",
      "Requirement already satisfied: filelock in c:\\users\\aniru\\appdata\\roaming\\python\\python312\\site-packages (from torch>=1.13->stable-baselines3==2.3.2) (3.19.1)\n",
      "Requirement already satisfied: setuptools in c:\\users\\aniru\\appdata\\roaming\\python\\python312\\site-packages (from torch>=1.13->stable-baselines3==2.3.2) (80.9.0)\n",
      "Requirement already satisfied: sympy==1.13.1 in c:\\anaconda3\\lib\\site-packages (from torch>=1.13->stable-baselines3==2.3.2) (1.13.1)\n",
      "Requirement already satisfied: networkx in c:\\anaconda3\\lib\\site-packages (from torch>=1.13->stable-baselines3==2.3.2) (3.4.2)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\aniru\\appdata\\roaming\\python\\python312\\site-packages (from torch>=1.13->stable-baselines3==2.3.2) (3.1.6)\n",
      "Requirement already satisfied: fsspec in c:\\anaconda3\\lib\\site-packages (from torch>=1.13->stable-baselines3==2.3.2) (2025.3.0)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in c:\\anaconda3\\lib\\site-packages (from sympy==1.13.1->torch>=1.13->stable-baselines3==2.3.2) (1.3.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\aniru\\appdata\\roaming\\python\\python312\\site-packages (from jinja2->torch>=1.13->stable-baselines3==2.3.2) (3.0.2)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in c:\\users\\aniru\\appdata\\roaming\\python\\python312\\site-packages (from matplotlib->stable-baselines3==2.3.2) (1.3.3)\n",
      "Requirement already satisfied: cycler>=0.10 in c:\\users\\aniru\\appdata\\roaming\\python\\python312\\site-packages (from matplotlib->stable-baselines3==2.3.2) (0.12.1)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in c:\\users\\aniru\\appdata\\roaming\\python\\python312\\site-packages (from matplotlib->stable-baselines3==2.3.2) (4.59.2)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in c:\\users\\aniru\\appdata\\roaming\\python\\python312\\site-packages (from matplotlib->stable-baselines3==2.3.2) (1.4.9)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\aniru\\appdata\\roaming\\python\\python312\\site-packages (from matplotlib->stable-baselines3==2.3.2) (25.0)\n",
      "Requirement already satisfied: pillow>=8 in c:\\users\\aniru\\appdata\\roaming\\python\\python312\\site-packages (from matplotlib->stable-baselines3==2.3.2) (11.3.0)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in c:\\users\\aniru\\appdata\\roaming\\python\\python312\\site-packages (from matplotlib->stable-baselines3==2.3.2) (3.2.3)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in c:\\users\\aniru\\appdata\\roaming\\python\\python312\\site-packages (from matplotlib->stable-baselines3==2.3.2) (2.9.0.post0)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\aniru\\appdata\\roaming\\python\\python312\\site-packages (from python-dateutil>=2.7->matplotlib->stable-baselines3==2.3.2) (1.17.0)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\aniru\\appdata\\roaming\\python\\python312\\site-packages (from pandas->stable-baselines3==2.3.2) (2025.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in c:\\users\\aniru\\appdata\\roaming\\python\\python312\\site-packages (from pandas->stable-baselines3==2.3.2) (2025.2)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 25.2 -> 25.3\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: faiss-cpu in c:\\anaconda3\\lib\\site-packages (1.12.0)\n",
      "Requirement already satisfied: numpy<3.0,>=1.25.0 in c:\\anaconda3\\lib\\site-packages (from faiss-cpu) (2.1.3)\n",
      "Requirement already satisfied: packaging in c:\\users\\aniru\\appdata\\roaming\\python\\python312\\site-packages (from faiss-cpu) (25.0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 25.2 -> 25.3\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: sqlparse in c:\\anaconda3\\lib\\site-packages (0.5.3)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 25.2 -> 25.3\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: lark-parser in c:\\anaconda3\\lib\\site-packages (0.12.0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 25.2 -> 25.3\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: sympy in c:\\anaconda3\\lib\\site-packages (1.13.1)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in c:\\anaconda3\\lib\\site-packages (from sympy) (1.3.0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 25.2 -> 25.3\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: prometheus-client in c:\\anaconda3\\lib\\site-packages (0.21.1)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 25.2 -> 25.3\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: matplotlib in c:\\users\\aniru\\appdata\\roaming\\python\\python312\\site-packages (3.10.6)\n",
      "Requirement already satisfied: seaborn in c:\\anaconda3\\lib\\site-packages (0.13.2)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in c:\\users\\aniru\\appdata\\roaming\\python\\python312\\site-packages (from matplotlib) (1.3.3)\n",
      "Requirement already satisfied: cycler>=0.10 in c:\\users\\aniru\\appdata\\roaming\\python\\python312\\site-packages (from matplotlib) (0.12.1)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in c:\\users\\aniru\\appdata\\roaming\\python\\python312\\site-packages (from matplotlib) (4.59.2)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in c:\\users\\aniru\\appdata\\roaming\\python\\python312\\site-packages (from matplotlib) (1.4.9)\n",
      "Requirement already satisfied: numpy>=1.23 in c:\\anaconda3\\lib\\site-packages (from matplotlib) (2.1.3)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\aniru\\appdata\\roaming\\python\\python312\\site-packages (from matplotlib) (25.0)\n",
      "Requirement already satisfied: pillow>=8 in c:\\users\\aniru\\appdata\\roaming\\python\\python312\\site-packages (from matplotlib) (11.3.0)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in c:\\users\\aniru\\appdata\\roaming\\python\\python312\\site-packages (from matplotlib) (3.2.3)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in c:\\users\\aniru\\appdata\\roaming\\python\\python312\\site-packages (from matplotlib) (2.9.0.post0)\n",
      "Requirement already satisfied: pandas>=1.2 in c:\\anaconda3\\lib\\site-packages (from seaborn) (2.2.3)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\aniru\\appdata\\roaming\\python\\python312\\site-packages (from pandas>=1.2->seaborn) (2025.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in c:\\users\\aniru\\appdata\\roaming\\python\\python312\\site-packages (from pandas>=1.2->seaborn) (2025.2)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\aniru\\appdata\\roaming\\python\\python312\\site-packages (from python-dateutil>=2.7->matplotlib) (1.17.0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 25.2 -> 25.3\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: python-dotenv in c:\\anaconda3\\lib\\site-packages (1.1.0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 25.2 -> 25.3\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pyyaml in c:\\anaconda3\\lib\\site-packages (6.0.2)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 25.2 -> 25.3\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: ujson in c:\\anaconda3\\lib\\site-packages (5.10.0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 25.2 -> 25.3\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: numpy==2.1.3 in c:\\anaconda3\\lib\\site-packages (2.1.3)\n",
      "Requirement already satisfied: scipy==1.14.1 in c:\\anaconda3\\lib\\site-packages (1.14.1)\n",
      "Requirement already satisfied: pandas==2.2.3 in c:\\anaconda3\\lib\\site-packages (2.2.3)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\aniru\\appdata\\roaming\\python\\python312\\site-packages (from pandas==2.2.3) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\aniru\\appdata\\roaming\\python\\python312\\site-packages (from pandas==2.2.3) (2025.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in c:\\users\\aniru\\appdata\\roaming\\python\\python312\\site-packages (from pandas==2.2.3) (2025.2)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\aniru\\appdata\\roaming\\python\\python312\\site-packages (from python-dateutil>=2.8.2->pandas==2.2.3) (1.17.0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 25.2 -> 25.3\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: scikit-learn==1.6.1 in c:\\anaconda3\\lib\\site-packages (1.6.1)\n",
      "Requirement already satisfied: umap-learn==0.5.9.post2 in c:\\anaconda3\\lib\\site-packages (0.5.9.post2)\n",
      "Requirement already satisfied: numpy>=1.19.5 in c:\\anaconda3\\lib\\site-packages (from scikit-learn==1.6.1) (2.1.3)\n",
      "Requirement already satisfied: scipy>=1.6.0 in c:\\anaconda3\\lib\\site-packages (from scikit-learn==1.6.1) (1.14.1)\n",
      "Requirement already satisfied: joblib>=1.2.0 in c:\\anaconda3\\lib\\site-packages (from scikit-learn==1.6.1) (1.5.1)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in c:\\users\\aniru\\appdata\\roaming\\python\\python312\\site-packages (from scikit-learn==1.6.1) (3.6.0)\n",
      "Requirement already satisfied: numba>=0.51.2 in c:\\anaconda3\\lib\\site-packages (from umap-learn==0.5.9.post2) (0.61.0)\n",
      "Requirement already satisfied: pynndescent>=0.5 in c:\\anaconda3\\lib\\site-packages (from umap-learn==0.5.9.post2) (0.5.13)\n",
      "Requirement already satisfied: tqdm in c:\\anaconda3\\lib\\site-packages (from umap-learn==0.5.9.post2) (4.67.1)\n",
      "Requirement already satisfied: llvmlite<0.45,>=0.44.0dev0 in c:\\anaconda3\\lib\\site-packages (from numba>=0.51.2->umap-learn==0.5.9.post2) (0.44.0)\n",
      "Requirement already satisfied: colorama in c:\\users\\aniru\\appdata\\roaming\\python\\python312\\site-packages (from tqdm->umap-learn==0.5.9.post2) (0.4.6)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 25.2 -> 25.3\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: datasketch in c:\\anaconda3\\lib\\site-packages (1.6.5)\n",
      "Requirement already satisfied: numpy>=1.11 in c:\\anaconda3\\lib\\site-packages (from datasketch) (2.1.3)\n",
      "Requirement already satisfied: scipy>=1.0.0 in c:\\anaconda3\\lib\\site-packages (from datasketch) (1.14.1)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 25.2 -> 25.3\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: fastapi in c:\\users\\aniru\\appdata\\roaming\\python\\python312\\site-packages (0.116.1)\n",
      "Requirement already satisfied: uvicorn in c:\\users\\aniru\\appdata\\roaming\\python\\python312\\site-packages (0.35.0)\n",
      "Requirement already satisfied: starlette<0.48.0,>=0.40.0 in c:\\users\\aniru\\appdata\\roaming\\python\\python312\\site-packages (from fastapi) (0.47.2)\n",
      "Requirement already satisfied: pydantic!=1.8,!=1.8.1,!=2.0.0,!=2.0.1,!=2.1.0,<3.0.0,>=1.7.4 in c:\\users\\aniru\\appdata\\roaming\\python\\python312\\site-packages (from fastapi) (2.11.7)\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in c:\\users\\aniru\\appdata\\roaming\\python\\python312\\site-packages (from fastapi) (4.15.0)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in c:\\users\\aniru\\appdata\\roaming\\python\\python312\\site-packages (from pydantic!=1.8,!=1.8.1,!=2.0.0,!=2.0.1,!=2.1.0,<3.0.0,>=1.7.4->fastapi) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.33.2 in c:\\users\\aniru\\appdata\\roaming\\python\\python312\\site-packages (from pydantic!=1.8,!=1.8.1,!=2.0.0,!=2.0.1,!=2.1.0,<3.0.0,>=1.7.4->fastapi) (2.33.2)\n",
      "Requirement already satisfied: typing-inspection>=0.4.0 in c:\\users\\aniru\\appdata\\roaming\\python\\python312\\site-packages (from pydantic!=1.8,!=1.8.1,!=2.0.0,!=2.0.1,!=2.1.0,<3.0.0,>=1.7.4->fastapi) (0.4.1)\n",
      "Requirement already satisfied: anyio<5,>=3.6.2 in c:\\users\\aniru\\appdata\\roaming\\python\\python312\\site-packages (from starlette<0.48.0,>=0.40.0->fastapi) (4.10.0)\n",
      "Requirement already satisfied: idna>=2.8 in c:\\users\\aniru\\appdata\\roaming\\python\\python312\\site-packages (from anyio<5,>=3.6.2->starlette<0.48.0,>=0.40.0->fastapi) (3.10)\n",
      "Requirement already satisfied: sniffio>=1.1 in c:\\users\\aniru\\appdata\\roaming\\python\\python312\\site-packages (from anyio<5,>=3.6.2->starlette<0.48.0,>=0.40.0->fastapi) (1.3.1)\n",
      "Requirement already satisfied: click>=7.0 in c:\\users\\aniru\\appdata\\roaming\\python\\python312\\site-packages (from uvicorn) (8.2.1)\n",
      "Requirement already satisfied: h11>=0.8 in c:\\users\\aniru\\appdata\\roaming\\python\\python312\\site-packages (from uvicorn) (0.16.0)\n",
      "Requirement already satisfied: colorama in c:\\users\\aniru\\appdata\\roaming\\python\\python312\\site-packages (from click>=7.0->uvicorn) (0.4.6)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 25.2 -> 25.3\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: bitsandbytes in c:\\anaconda3\\lib\\site-packages (0.47.0)\n",
      "Requirement already satisfied: torch<3,>=2.2 in c:\\anaconda3\\lib\\site-packages (from bitsandbytes) (2.5.1)\n",
      "Requirement already satisfied: numpy>=1.17 in c:\\anaconda3\\lib\\site-packages (from bitsandbytes) (2.1.3)\n",
      "Requirement already satisfied: filelock in c:\\users\\aniru\\appdata\\roaming\\python\\python312\\site-packages (from torch<3,>=2.2->bitsandbytes) (3.19.1)\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in c:\\users\\aniru\\appdata\\roaming\\python\\python312\\site-packages (from torch<3,>=2.2->bitsandbytes) (4.15.0)\n",
      "Requirement already satisfied: setuptools in c:\\users\\aniru\\appdata\\roaming\\python\\python312\\site-packages (from torch<3,>=2.2->bitsandbytes) (80.9.0)\n",
      "Requirement already satisfied: sympy==1.13.1 in c:\\anaconda3\\lib\\site-packages (from torch<3,>=2.2->bitsandbytes) (1.13.1)\n",
      "Requirement already satisfied: networkx in c:\\anaconda3\\lib\\site-packages (from torch<3,>=2.2->bitsandbytes) (3.4.2)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\aniru\\appdata\\roaming\\python\\python312\\site-packages (from torch<3,>=2.2->bitsandbytes) (3.1.6)\n",
      "Requirement already satisfied: fsspec in c:\\anaconda3\\lib\\site-packages (from torch<3,>=2.2->bitsandbytes) (2025.3.0)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in c:\\anaconda3\\lib\\site-packages (from sympy==1.13.1->torch<3,>=2.2->bitsandbytes) (1.3.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\aniru\\appdata\\roaming\\python\\python312\\site-packages (from jinja2->torch<3,>=2.2->bitsandbytes) (3.0.2)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 25.2 -> 25.3\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "!pip install notebook jupyterlab ipykernel\n",
    "!pip install pyarrow duckdb ipywidgets==8.*\n",
    "!pip install scikit-learn xgboost lightgbm\n",
    "!pip install scipy pandas numpy\n",
    "!pip install regex\n",
    "!pip install statsmodels\n",
    "!pip install rank-bm25\n",
    "!pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu118\n",
    "!pip install transformers==4.44.2 sentence-transformers\n",
    "!pip install accelerate bitsandbytes\n",
    "!pip install huggingface-hub\n",
    "!pip install scikit-optimize\n",
    "!pip install gymnasium==0.29.1\n",
    "!pip install stable-baselines3==2.3.2\n",
    "!pip install faiss-cpu\n",
    "!pip install sqlparse\n",
    "!pip install lark-parser\n",
    "!pip install sympy \n",
    "!pip install prometheus-client\n",
    "!pip install matplotlib seaborn\n",
    "!pip install python-dotenv\n",
    "!pip install pyyaml\n",
    "!pip install ujson\n",
    "!pip install \"numpy==2.1.3\" \"scipy==1.14.1\" \"pandas==2.2.3\"\n",
    "!pip install \"scikit-learn==1.6.1\" \"umap-learn==0.5.9.post2\"\n",
    "!pip install datasketch\n",
    "!pip install fastapi uvicorn\n",
    "!pip install bitsandbytes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0669e351-97b4-42ce-9745-e70a51a3c226",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, sys, math, random, time, warnings, json\n",
    "from pathlib import Path\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "from IPython.display import display, HTML\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import ujson as ujson  \n",
    "import yaml\n",
    "import scipy\n",
    "from scipy import stats\n",
    "import statsmodels.api as sm\n",
    "import statsmodels.tsa.api as tsa\n",
    "import regex as re2  \n",
    "import re\n",
    "import sklearn\n",
    "from sklearn import metrics, model_selection, preprocessing, pipeline\n",
    "import xgboost as xgb\n",
    "import lightgbm as lgb\n",
    "import skopt\n",
    "from skopt import BayesSearchCV\n",
    "import torch\n",
    "import torchvision\n",
    "import torchaudio\n",
    "from transformers import (\n",
    "    AutoTokenizer, AutoModel, AutoModelForSequenceClassification, AutoModelForCausalLM, pipeline\n",
    ")\n",
    "from sentence_transformers import SentenceTransformer, util as sbert_util\n",
    "try:\n",
    "    import accelerate  \n",
    "except Exception as e:\n",
    "    print(\"accelerate not available:\", e)\n",
    "try:\n",
    "    import bitsandbytes as bnb  \n",
    "except Exception as e:\n",
    "    print(\"bitsandbytes not available (8-bit may be disabled):\", e)\n",
    "from huggingface_hub import login as hf_login, HfApi\n",
    "from rank_bm25 import BM25Okapi, BM25L, BM25Plus\n",
    "try:\n",
    "    import faiss\n",
    "except Exception as e:\n",
    "    faiss = None\n",
    "    print(\"faiss not available:\", e)\n",
    "import sympy as sym\n",
    "import sqlparse\n",
    "from lark import Lark, Transformer, v_args\n",
    "import gymnasium as gym\n",
    "from stable_baselines3 import PPO, A2C, DQN\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from prometheus_client import Counter, Gauge, Histogram, Summary, start_http_server\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()  \n",
    "from datasketch import MinHash, MinHashLSH\n",
    "from importlib.metadata import version as _dist_version, PackageNotFoundError as _PNF\n",
    "import ipywidgets as w\n",
    "from IPython.display import display\n",
    "from prometheus_client import start_http_server\n",
    "import os, time, urllib.request\n",
    "from datasketch import MinHash, MinHashLSH\n",
    "from IPython.display import display, Markdown\n",
    "from datasketch import MinHash, MinHashLSH\n",
    "import random, time\n",
    "from typing import Dict\n",
    "from __future__ import annotations\n",
    "from dataclasses import dataclass, asdict, field\n",
    "from typing import List, Dict, Any\n",
    "import inspect\n",
    "from collections import OrderedDict, defaultdict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0b7563df-b440-423a-9be4-adc598290d4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    import bitsandbytes as bnb\n",
    "    _bnb_ok = True\n",
    "except Exception as e:\n",
    "    _bnb_ok = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2d643c03-1a73-404a-8a66-05dc395fa78d",
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    _faiss_ok\n",
    "except NameError:\n",
    "    _faiss_ok = (\"faiss\" in sys.modules)\n",
    "    if not _faiss_ok:\n",
    "        try:\n",
    "            import faiss  \n",
    "            _faiss_ok = True\n",
    "        except Exception:\n",
    "            _faiss_ok = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b6b2fab7-2ddb-4150-96a1-38bca26bf7e1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Python        : 3.12.7\n",
      "NumPy         : 2.1.3\n",
      "SciPy         : 1.14.1\n",
      "Pandas        : 2.2.3\n",
      "scikit-learn  : 1.6.1\n",
      "XGBoost       : 2.1.1\n",
      "LightGBM      : 4.5.0\n",
      "Transformers  : 4.44.2\n",
      "SentenceTrans : 5.1.0\n",
      "Torch         : 2.5.1 | CUDA: 11.8 | GPU: True\n",
      "BitsAndBytes  : OK\n",
      "FAISS         : OK\n",
      "Gymnasium     : 0.29.1 | SB3: 2.3.2\n"
     ]
    }
   ],
   "source": [
    "def _v(pkg):\n",
    "    try:\n",
    "        from importlib.metadata import version\n",
    "        return version(pkg)\n",
    "    except Exception:\n",
    "        return \"?\"\n",
    "print(\"Python        :\", sys.version.split()[0])\n",
    "print(\"NumPy         :\", np.__version__)\n",
    "print(\"SciPy         :\", scipy.__version__)\n",
    "print(\"Pandas        :\", pd.__version__)\n",
    "print(\"scikit-learn  :\", sklearn.__version__)\n",
    "print(\"XGBoost       :\", xgb.__version__)\n",
    "print(\"LightGBM      :\", lgb.__version__)\n",
    "print(\"Transformers  :\", _v(\"transformers\"))\n",
    "print(\"SentenceTrans :\", _v(\"sentence-transformers\"))\n",
    "print(\"Torch         :\", torch.__version__, \"| CUDA:\", torch.version.cuda, \"| GPU:\", torch.cuda.is_available())\n",
    "print(\"BitsAndBytes  :\", \"OK\" if _bnb_ok else \"not available\")\n",
    "print(\"FAISS         :\", \"OK\" if _faiss_ok else \"not available (use hnswlib)\")\n",
    "print(\"Gymnasium     :\", _v(\"gymnasium\"), \"| SB3:\", _v(\"stable-baselines3\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "dd4dcc7a-b4fc-40fb-9c6f-10cf092f7136",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BM25 test top hit: financial time series forecasting\n",
      "E5-small embedding shape: (1, 384)\n",
      "CUDA matmul OK: torch.Size([1024, 1024])\n"
     ]
    }
   ],
   "source": [
    "docs = [ \"machine learning for tabular data\", \"financial time series forecasting\", \"retrieval augmented generation basics\" ]\n",
    "bm25 = BM25Okapi([d.split() for d in docs])\n",
    "print(\"BM25 test top hit:\", docs[np.argmax(bm25.get_scores(\"time series forecasting\".split()))])\n",
    "try:\n",
    "    emb_model = SentenceTransformer(\"intfloat/e5-small-v2\")\n",
    "    emb = emb_model.encode([\"hello world\"], normalize_embeddings=True)\n",
    "    print(\"E5-small embedding shape:\", emb.shape)\n",
    "except Exception as e:\n",
    "    print(\"SentenceTransformer load failed:\", e)\n",
    "if torch.cuda.is_available():\n",
    "    x = torch.randn(1024, 1024, device=\"cuda\")\n",
    "    y = x @ x.T\n",
    "    torch.cuda.synchronize()\n",
    "    print(\"CUDA matmul OK:\", y.shape)\n",
    "else:\n",
    "    print(\"CUDA not available: using CPU.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ec16e33b-abea-4c8e-85f3-467aa8e721b5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Python: 3.12.7\n",
      "Torch : 2.5.1 | CUDA: 11.8 | GPU avail: True\n",
      "Torchaudio: 2.5.1\n",
      "Device: NVIDIA GeForce GTX 1650\n",
      "VRAM: 2468MB free / 4095MB total\n",
      "FAISS: OK\n"
     ]
    }
   ],
   "source": [
    "import torch, torchaudio, platform, os\n",
    "print(\"Python:\", platform.python_version())\n",
    "print(\"Torch :\", torch.__version__, \"| CUDA:\", torch.version.cuda, \"| GPU avail:\", torch.cuda.is_available())\n",
    "print(\"Torchaudio:\", torchaudio.__version__)\n",
    "if torch.cuda.is_available():\n",
    "    print(\"Device:\", torch.cuda.get_device_name(0))\n",
    "    free, total = torch.cuda.mem_get_info()\n",
    "    print(f\"VRAM: {free//(1024**2)}MB free / {total//(1024**2)}MB total\")\n",
    "else:\n",
    "    print(\"CPU-only for now\")\n",
    "try:\n",
    "    import faiss\n",
    "    print(\"FAISS: OK\")\n",
    "except Exception as e:\n",
    "    print(\"FAISS not available:\", e, \" plan to use hnswlib fallback\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b74d7b82-d32f-4874-a41f-301dfca68857",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "namespace: default\n",
      "row_cap_basic: 50000\n",
      "bm25_text_cols_guess:\n",
      "- text\n",
      "- description\n",
      "- notes\n",
      "- body\n",
      "artifact_dir: C:\\Users\\aniru\\artifacts\n",
      "ttl_days: 90\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "import yaml, os, hashlib, json, time\n",
    "BASE = Path.cwd()\n",
    "for p in [\"data\", \"artifacts\", \"artifacts/profiles\", \"artifacts/baselines\", \"artifacts/index\", \"logs\"]:\n",
    "    (BASE/p).mkdir(parents=True, exist_ok=True)\n",
    "CFG = {\n",
    "    \"namespace\": \"default\",\n",
    "    \"row_cap_basic\": 50_000,\n",
    "    \"bm25_text_cols_guess\": [\"text\", \"description\", \"notes\", \"body\"],\n",
    "    \"artifact_dir\": str(BASE/\"artifacts\"),\n",
    "    \"ttl_days\": 90\n",
    "}\n",
    "print(yaml.dump(CFG, sort_keys=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "0bd649cd-0829-4d41-b41f-54bed401aef9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd, numpy as np, hashlib, re, sqlparse, random\n",
    "from dataclasses import dataclass, asdict\n",
    "from typing import List, Dict, Any, Optional\n",
    "from rank_bm25 import BM25Okapi\n",
    "from datasketch import MinHash\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import OrdinalEncoder\n",
    "from sklearn.metrics import accuracy_score, f1_score, roc_auc_score, mean_squared_error\n",
    "import xgboost as xgb\n",
    "import warnings; warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "0332b3ee-269b-4acf-839a-49b6b211516e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _sha256_bytes(x: bytes) -> str:\n",
    "    h = hashlib.sha256(); h.update(x); return h.hexdigest()\n",
    "def _row_hash(series: pd.Series) -> str:\n",
    "    return _sha256_bytes(pd.util.hash_pandas_object(series, index=False).values.tobytes())\n",
    "def schema_profile(df: pd.DataFrame, sample_rows: int = 5000) -> Dict[str, Any]:\n",
    "    smp = df.sample(min(sample_rows, len(df)), random_state=42) if len(df) > sample_rows else df\n",
    "    cols = []\n",
    "    for c in df.columns:\n",
    "        s = smp[c]\n",
    "        dtype = str(s.dtype)\n",
    "        nulls = int(s.isna().sum())\n",
    "        nunique = int(s.nunique(dropna=True))\n",
    "        ex = s.dropna().astype(str).head(3).tolist()\n",
    "        pii_flag = bool(re.search(r\"(email|ssn|phone|address|name)\", c, re.I))\n",
    "        numeric = pd.api.types.is_numeric_dtype(s)\n",
    "        cols.append({\"name\": c, \"dtype\": dtype, \"nulls\": nulls, \"nunique\": nunique,\n",
    "                     \"examples\": ex, \"pii\": pii_flag, \"is_numeric\": numeric})\n",
    "    return {\n",
    "        \"n_rows\": int(len(df)),\n",
    "        \"n_cols\": int(df.shape[1]),\n",
    "        \"columns\": cols\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "cbd1fe4b-847a-434a-994e-8045f81a3be9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def near_dedupe(df: pd.DataFrame, text_cols: Optional[List[str]]=None, threshold: float=0.95) -> pd.DataFrame:\n",
    "    before = len(df)\n",
    "    df = df.drop_duplicates()\n",
    "    if text_cols:\n",
    "        texts = df[text_cols].astype(str).agg(\" \".join, axis=1).tolist()\n",
    "        hashes = []\n",
    "        keep = []\n",
    "        for t in texts:\n",
    "            m = MinHash(num_perm=64)\n",
    "            for token in t.split():\n",
    "                m.update(token.encode(\"utf-8\"))\n",
    "            hashes.append(m)\n",
    "        seen = set()\n",
    "        for i, m in enumerate(hashes):\n",
    "            if i in seen: \n",
    "                continue\n",
    "            keep.append(i)\n",
    "            for j in range(i+1, len(hashes)):\n",
    "                if j in seen: \n",
    "                    continue\n",
    "                if m.jaccard(hashes[j]) >= threshold:\n",
    "                    seen.add(j)\n",
    "        df = df.iloc[keep].copy()\n",
    "    after = len(df)\n",
    "    df.reset_index(drop=True, inplace=True)\n",
    "    return df\n",
    "def guess_text_cols(df: pd.DataFrame, hints: List[str]) -> List[str]:\n",
    "    cand = [c for c in df.columns if any(h in c.lower() for h in hints)]\n",
    "    obj_cols = [c for c in df.columns if df[c].dtype == \"object\"]\n",
    "    longish = [c for c in obj_cols if df[c].astype(str).str.len().mean() > 20]\n",
    "    out = list(dict.fromkeys(cand + longish))  \n",
    "    return out[:5]\n",
    "def build_bm25(df: pd.DataFrame, text_cols: List[str]):\n",
    "    if not text_cols:\n",
    "        return None\n",
    "    docs = df[text_cols].astype(str).agg(\" \".join, axis=1).tolist()\n",
    "    tokenized = [d.split() for d in docs]\n",
    "    return BM25Okapi(tokenized)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "22658597-9cd8-45be-bb61-54d969b19ce3",
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class BaselineResult:\n",
    "    task: str                       \n",
    "    metric_primary: str            \n",
    "    score: float\n",
    "    feature_importance: Dict[str,float]\n",
    "    model_uri: str\n",
    "    sample_preview_uri: str\n",
    "def _prep_xy(df: pd.DataFrame, target: Optional[str]=None):\n",
    "    if target is None:\n",
    "        pattern = re.compile(r\"(label|target|y|clicked|churn|default|fraud|is_\\w+)\", re.I)\n",
    "        cand = [c for c in df.columns if pattern.search(c)]\n",
    "        target = cand[0] if cand else None\n",
    "    if target is None:\n",
    "        raise ValueError(\"No target column found. Provide target=... for baseline.\")\n",
    "    y = df[target]\n",
    "    X = df.drop(columns=[target]).copy()\n",
    "    cat_cols = [c for c in X.columns if X[c].dtype == \"object\"]\n",
    "    if cat_cols:\n",
    "        enc = OrdinalEncoder(handle_unknown=\"use_encoded_value\", unknown_value=-1)\n",
    "        X[cat_cols] = enc.fit_transform(X[cat_cols].astype(str))\n",
    "    X = X.fillna(0)\n",
    "    return X, y, target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "334377e0-2431-4fae-9956-59a71ad6ef9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_baseline(df: pd.DataFrame, target: Optional[str]=None, artifact_dir: str=\"artifacts\") -> BaselineResult:\n",
    "    X, y, target = _prep_xy(df, target)\n",
    "    task = \"regression\" if pd.api.types.is_numeric_dtype(y) and y.nunique() > 20 else \"classification\"\n",
    "    Xtr, Xte, ytr, yte = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y if task==\"classification\" else None)\n",
    "    if task == \"classification\":\n",
    "        clf = xgb.XGBClassifier(\n",
    "            n_estimators=200, max_depth=6, subsample=0.8, colsample_bytree=0.8,\n",
    "            tree_method=\"hist\", eval_metric=\"auc\"\n",
    "        )\n",
    "        clf.fit(Xtr, ytr)\n",
    "        prob = clf.predict_proba(Xte)[:,1] if len(set(yte))==2 else None\n",
    "        if prob is not None:\n",
    "            score = roc_auc_score(yte, prob)\n",
    "            metric = \"auc\"\n",
    "        else:\n",
    "            pred = clf.predict(Xte)\n",
    "            score = f1_score(yte, pred, average=\"macro\")\n",
    "            metric = \"f1\"\n",
    "        fi = dict(sorted(zip(X.columns, clf.feature_importances_.tolist()), key=lambda kv: kv[1], reverse=True)[:25])\n",
    "        model_uri = f\"{artifact_dir}/baselines/xgb_cls_{int(time.time())}.json\"\n",
    "        with open(model_uri, \"w\") as f: json.dump({\"target\": target, \"params\": clf.get_xgb_params()}, f)\n",
    "    else:\n",
    "        reg = xgb.XGBRegressor(\n",
    "            n_estimators=300, max_depth=8, subsample=0.8, colsample_bytree=0.8,\n",
    "            tree_method=\"hist\"\n",
    "        )\n",
    "        reg.fit(Xtr, ytr)\n",
    "        pred = reg.predict(Xte)\n",
    "        score = float(np.sqrt(mean_squared_error(yte, pred)))\n",
    "        metric = \"rmse\"\n",
    "        fi = dict(sorted(zip(X.columns, reg.feature_importances_.tolist()), key=lambda kv: kv[1], reverse=True)[:25])\n",
    "        model_uri = f\"{artifact_dir}/baselines/xgb_reg_{int(time.time())}.json\"\n",
    "        with open(model_uri, \"w\") as f: json.dump({\"target\": target, \"params\": reg.get_xgb_params()}, f)\n",
    "    preview = df.sample(min(20, len(df)), random_state=123).to_dict(orient=\"records\")\n",
    "    sample_uri = f\"{artifact_dir}/baselines/sample_{int(time.time())}.json\"\n",
    "    with open(sample_uri, \"w\") as f: json.dump(preview, f, default=str)\n",
    "    return BaselineResult(task, metric, float(score), fi, model_uri, sample_uri)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "613dd1ac-8ab7-4a36-954e-3e4e7744cc47",
   "metadata": {},
   "outputs": [],
   "source": [
    "def upload_basic(file_path: str, namespace: str=\"default\", target: Optional[str]=None) -> Dict[str, Any]:\n",
    "    df = pd.read_parquet(file_path) if file_path.endswith(\".parquet\") else pd.read_csv(file_path)\n",
    "    profile = schema_profile(df)\n",
    "    text_cols = guess_text_cols(df, CFG[\"bm25_text_cols_guess\"])\n",
    "    df_dedup = near_dedupe(df, text_cols=text_cols if text_cols else None)\n",
    "    if len(df_dedup) > CFG[\"row_cap_basic\"]:\n",
    "        df_basic = df_dedup.sample(CFG[\"row_cap_basic\"], random_state=42)\n",
    "    else:\n",
    "        df_basic = df_dedup\n",
    "    bm25 = build_bm25(df_basic, text_cols) if text_cols else None\n",
    "    baseline = None\n",
    "    try:\n",
    "        baseline = run_baseline(df_basic, target=target, artifact_dir=CFG[\"artifact_dir\"])\n",
    "    except Exception as e:\n",
    "        baseline = None\n",
    "    ts = int(time.time())\n",
    "    prof_uri = f\"{CFG['artifact_dir']}/profiles/{namespace}_{ts}.json\"\n",
    "    with open(prof_uri, \"w\") as f: json.dump(profile, f, default=str)\n",
    "    bm25_uri = f\"{CFG['artifact_dir']}/index/{namespace}_bm25_{ts}.json\"\n",
    "    if bm25 is not None:\n",
    "        with open(bm25_uri, \"w\") as f: json.dump({\"text_cols\": text_cols, \"n_docs\": len(df_basic)}, f)\n",
    "    resp = {\n",
    "        \"job_id\": f\"{namespace}_{ts}\",\n",
    "        \"status\": \"BASIC_DONE\",\n",
    "        \"profile_uri\": prof_uri,\n",
    "        \"baseline\": asdict(baseline) if baseline else None,\n",
    "        \"bm25_index_meta\": bm25_uri if bm25 is not None else None,\n",
    "        \"rows_used\": int(len(df_basic)),\n",
    "        \"text_cols_guess\": text_cols\n",
    "    }\n",
    "    return resp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "934899be-164d-4335-a8be-8e57992d7023",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'job_id': 'demo_1762113055',\n",
       " 'status': 'BASIC_DONE',\n",
       " 'profile_uri': 'C:\\\\Users\\\\aniru\\\\artifacts/profiles/demo_1762113055.json',\n",
       " 'baseline': None,\n",
       " 'bm25_index_meta': 'C:\\\\Users\\\\aniru\\\\artifacts/index/demo_bm25_1762113055.json',\n",
       " 'rows_used': 4,\n",
       " 'text_cols_guess': ['notes']}"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "csv_path = Path(\"data/synthetic_credit.csv\")\n",
    "if not csv_path.exists():\n",
    "    rng = np.random.default_rng(42)\n",
    "    n = 10000\n",
    "    df = pd.DataFrame({\n",
    "        \"age\": rng.integers(18, 70, n),\n",
    "        \"income\": rng.normal(65000, 18000, n).round(0),\n",
    "        \"balance\": rng.normal(2200, 800, n).round(0),\n",
    "        \"tenure_months\": rng.integers(1, 120, n),\n",
    "        \"notes\": rng.choice([\"late fee last month\", \"clean history\", \"contacted support\", \"chargeback risk\"], n),\n",
    "        \"default\": rng.integers(0, 2, n)\n",
    "    })\n",
    "    df.to_csv(csv_path, index=False)\n",
    "out = upload_basic(str(csv_path), namespace=\"demo\", target=\"default\")\n",
    "out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "ec74f62a-11d9-42c9-894f-9599ca6da85b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import json, time\n",
    "JOBS_PATH = Path(\"artifacts/jobs.json\")\n",
    "JOBS_PATH.parent.mkdir(parents=True, exist_ok=True)\n",
    "if not JOBS_PATH.exists():\n",
    "    JOBS_PATH.write_text(json.dumps({}, indent=2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "fd60f020-abcf-4f2e-8903-5d95bc351c3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _jobs_load():\n",
    "    try:\n",
    "        return json.loads(JOBS_PATH.read_text())\n",
    "    except Exception:\n",
    "        return {}\n",
    "def _jobs_save(d):\n",
    "    JOBS_PATH.write_text(json.dumps(d, indent=2))\n",
    "def jobs_set(job_id, **kwargs):\n",
    "    d = _jobs_load()\n",
    "    row = d.get(job_id, {})\n",
    "    row.update(kwargs)\n",
    "    row.setdefault(\"ts_created\", int(time.time()))\n",
    "    row[\"ts_updated\"] = int(time.time())\n",
    "    d[job_id] = row\n",
    "    _jobs_save(d)\n",
    "def jobs_get(job_id):\n",
    "    return _jobs_load().get(job_id, None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "cabf441c-3d6d-4574-b92b-7b32512ff6fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import os, math, json, time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "9b0ab62a-e3bc-4f79-9b21-1a4e5da6aee9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _load_df(path):\n",
    "    return pd.read_parquet(path) if path.endswith(\".parquet\") else pd.read_csv(path)\n",
    "def _build_corpus(df, text_cols, cap=500_000):\n",
    "    docs = df[text_cols].astype(str).agg(\" \".join, axis=1).tolist()\n",
    "    if len(docs) > cap:\n",
    "        rng = np.random.default_rng(42)\n",
    "        idx = rng.choice(len(docs), size=cap, replace=False)\n",
    "        docs = [docs[i] for i in idx]\n",
    "    return docs\n",
    "def _embed_corpus(model, texts, batch=256, normalize=True):\n",
    "    return model.encode(texts, batch_size=batch, normalize_embeddings=normalize, show_progress_bar=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "bf7115fc-eec8-4e94-96f7-048c028ad255",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _save_index(emb, out_dir, index_name=\"faiss_ivfpq\"):\n",
    "    os.makedirs(out_dir, exist_ok=True)\n",
    "    npy_path = os.path.join(out_dir, \"embeddings.npy\")\n",
    "    np.save(npy_path, emb.astype(np.float16))\n",
    "    meta = {\"index_type\": \"numpy\", \"path\": npy_path, \"dim\": int(emb.shape[1]), \"n\": int(emb.shape[0])}\n",
    "    try:\n",
    "        import faiss\n",
    "        dim = emb.shape[1]\n",
    "        nlist = max(64, int(math.sqrt(emb.shape[0])))\n",
    "        m = 16  \n",
    "        quantizer = faiss.IndexFlatIP(dim)\n",
    "        index = faiss.IndexIVFPQ(quantizer, dim, nlist, m, 8)\n",
    "        index.nprobe = min(16, nlist)\n",
    "        index.train(emb)\n",
    "        index.add(emb)\n",
    "        faiss_path = os.path.join(out_dir, f\"{index_name}.faiss\")\n",
    "        faiss.write_index(index, faiss_path)\n",
    "        meta = {\"index_type\": \"faiss_ivfpq\", \"path\": faiss_path, \"dim\": dim, \"n\": int(emb.shape[0]), \"nlist\": nlist, \"m\": m}\n",
    "    except Exception as e:\n",
    "        meta[\"fallback_reason\"] = f\"{type(e).__name__}: {e}\"\n",
    "    json.dump(meta, open(os.path.join(out_dir, \"index_meta.json\"), \"w\"), indent=2)\n",
    "    return meta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "2c63ec71-7412-46ca-91ba-e29edf35ed89",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _fake_reranker_report(out_dir, topk=5):\n",
    "    rpt = {\"model\": \"bge-reranker-base\", \"topk\": topk, \"notes\": \"stub report for v0\"}\n",
    "    json.dump(rpt, open(os.path.join(out_dir, \"reranker_report.json\"), \"w\"), indent=2)\n",
    "    return rpt\n",
    "def _fake_adapter(out_dir, ns):\n",
    "    uri = os.path.join(out_dir, f\"{ns}_slm_adapter.safetensors\")\n",
    "    open(uri, \"wb\").write(b\"placeholder\")\n",
    "    return uri\n",
    "def _fake_eval(out_dir):\n",
    "    metrics = {\n",
    "        \"auc\": round(0.82 + np.random.rand()*0.03, 4),\n",
    "        \"ndcg@10\": round(0.62 + np.random.rand()*0.05, 4),\n",
    "        \"p95\": int(1400 + np.random.rand()*300),\n",
    "        \"cost\": round(0.012 + np.random.rand()*0.006, 4),\n",
    "        \"pass\": True\n",
    "    }\n",
    "    json.dump({\"metrics\": metrics}, open(os.path.join(out_dir, \"eval.json\"), \"w\"), indent=2)\n",
    "    return metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "154b3d49-8ac7-467b-9164-1ec2e6635b93",
   "metadata": {},
   "outputs": [],
   "source": [
    "def advanced_trainer(job_id: str, dataset_path: str, namespace: str, text_cols_guess=None):\n",
    "    jobs_set(job_id, status=\"ADVANCED_BUILDING\")\n",
    "    base_dir = Path(\"artifacts\") / \"advanced\" / job_id\n",
    "    base_dir.mkdir(parents=True, exist_ok=True)\n",
    "    df = _load_df(dataset_path)\n",
    "    if not text_cols_guess:\n",
    "        text_cols_guess = guess_text_cols(df, CFG[\"bm25_text_cols_guess\"])\n",
    "    if not text_cols_guess:\n",
    "        metrics = _fake_eval(str(base_dir))\n",
    "        out = {\n",
    "            \"index_meta\": None,\n",
    "            \"reranker_report\": None,\n",
    "            \"adapter_uri\": _fake_adapter(str(base_dir), namespace),\n",
    "            \"metrics\": metrics\n",
    "        }\n",
    "        jobs_set(job_id, status=\"ADVANCED_READY\", artifacts=out)\n",
    "        return out\n",
    "    corpus = _build_corpus(df, text_cols_guess, cap=500_000)\n",
    "    model = SentenceTransformer(\"intfloat/e5-small-v2\")\n",
    "    emb = _embed_corpus(model, corpus, batch=256, normalize=True)\n",
    "    index_meta = _save_index(emb, str(base_dir / \"index\"))\n",
    "    reranker_report = _fake_reranker_report(str(base_dir))\n",
    "    adapter_uri = _fake_adapter(str(base_dir), namespace)\n",
    "    metrics = _fake_eval(str(base_dir))\n",
    "    out = {\n",
    "        \"index_meta\": index_meta,\n",
    "        \"reranker_report\": reranker_report,\n",
    "        \"adapter_uri\": adapter_uri,\n",
    "        \"metrics\": metrics,\n",
    "        \"text_cols\": text_cols_guess\n",
    "    }\n",
    "    jobs_set(job_id, status=\"ADVANCED_READY\", artifacts=out)\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "24557009-a0a1-49c2-8c7d-65bec504538b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_advanced(job_id: str):\n",
    "    row = jobs_get(job_id)\n",
    "    if not row:\n",
    "        return {\"status\": \"UNKNOWN_JOB\"}\n",
    "    if row.get(\"status\") == \"ADVANCED_READY\":\n",
    "        return {\"status\": \"ADVANCED_READY\", **row.get(\"artifacts\", {})}\n",
    "    return {\"status\": row.get(\"status\", \"PENDING\")}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "8857fcf7-6781-4a19-bdf8-2b09892e16d0",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'status': 'ADVANCED_READY',\n",
       " 'index_meta': {'index_type': 'faiss_ivfpq',\n",
       "  'path': 'artifacts\\\\advanced\\\\demo_1762113055\\\\index\\\\faiss_ivfpq.faiss',\n",
       "  'dim': 384,\n",
       "  'n': 10000,\n",
       "  'nlist': 100,\n",
       "  'm': 16},\n",
       " 'reranker_report': {'model': 'bge-reranker-base',\n",
       "  'topk': 5,\n",
       "  'notes': 'stub report for v0'},\n",
       " 'adapter_uri': 'artifacts\\\\advanced\\\\demo_1762113055\\\\demo_slm_adapter.safetensors',\n",
       " 'metrics': {'auc': 0.8444,\n",
       "  'ndcg@10': 0.6225,\n",
       "  'p95': 1564,\n",
       "  'cost': 0.0168,\n",
       "  'pass': True},\n",
       " 'text_cols': ['notes']}"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "job_id = out[\"job_id\"] \n",
    "advanced_artifacts = advanced_trainer(job_id, str(csv_path), namespace=\"demo\", text_cols_guess=out.get(\"text_cols_guess\"))\n",
    "read_advanced(job_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "7d1ffa68-a395-4d60-ad99-73f13e2f8dc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, json, math, re, time\n",
    "from pathlib import Path\n",
    "from typing import List, Dict, Any, Optional, Tuple\n",
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "4b093f51-003c-4fd8-bcbe-09041c52777e",
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    _E5_MODEL  \n",
    "except NameError:\n",
    "    _E5_MODEL = None\n",
    "def _get_e5():\n",
    "    global _E5_MODEL\n",
    "    if _E5_MODEL is None:\n",
    "        from sentence_transformers import SentenceTransformer\n",
    "        _E5_MODEL = SentenceTransformer(\"intfloat/e5-small-v2\")\n",
    "    return _E5_MODEL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "cd0af699-61c1-44e3-8fe7-02079b271644",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _cosine_sim(a: np.ndarray, b: np.ndarray) -> np.ndarray:\n",
    "    return (b @ a).astype(np.float32)\n",
    "def _normalize_rows(x: np.ndarray) -> np.ndarray:\n",
    "    n = np.linalg.norm(x, axis=1, keepdims=True) + 1e-12\n",
    "    return (x / n).astype(np.float32)\n",
    "def _embed_query(text: str) -> np.ndarray:\n",
    "    model = _get_e5()\n",
    "    q = model.encode([text], normalize_embeddings=True, show_progress_bar=False)\n",
    "    return q[0].astype(np.float32)\n",
    "def _load_json(p: Path) -> Any:\n",
    "    return json.loads(Path(p).read_text())\n",
    "def _faiss_available() -> bool:\n",
    "    try:\n",
    "        import faiss  \n",
    "        return True\n",
    "    except Exception:\n",
    "        return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "e9c1443f-0444-4e09-83d2-828c35d53232",
   "metadata": {},
   "outputs": [],
   "source": [
    "class VectorIndex:\n",
    "    def __init__(self, index_type: str, path: str, dim: int, n: int, text_cols: List[str], corpus_map: np.ndarray):\n",
    "        self.index_type = index_type\n",
    "        self.path = path\n",
    "        self.dim = dim\n",
    "        self.n = n\n",
    "        self.text_cols = text_cols\n",
    "        self.corpus_map = corpus_map  \n",
    "        self._index = None\n",
    "        self._mat = None\n",
    "    @classmethod\n",
    "    def from_artifacts(cls, advanced_dir: Path) -> Optional[\"VectorIndex\"]:\n",
    "        meta_path = advanced_dir / \"index\" / \"index_meta.json\"\n",
    "        if not meta_path.exists():\n",
    "            return None\n",
    "        meta = _load_json(meta_path)\n",
    "        try:\n",
    "            cfg = _load_json(advanced_dir / \"eval.json\")  \n",
    "        except Exception:\n",
    "            cfg = {}\n",
    "        map_path = advanced_dir / \"index\" / \"corpus_row_ids.npy\"\n",
    "        if map_path.exists():\n",
    "            corpus_map = np.load(map_path)\n",
    "        else:\n",
    "            corpus_map = np.arange(meta[\"n\"], dtype=np.int64)\n",
    "        text_cols_path = advanced_dir / \"text_cols.json\"\n",
    "        text_cols = _load_json(text_cols_path) if text_cols_path.exists() else []\n",
    "        vi = cls(\n",
    "            index_type=meta.get(\"index_type\", \"numpy\"),\n",
    "            path=meta[\"path\"],\n",
    "            dim=int(meta[\"dim\"]),\n",
    "            n=int(meta[\"n\"]),\n",
    "            text_cols=text_cols,\n",
    "            corpus_map=corpus_map\n",
    "        )\n",
    "        return vi\n",
    "    def _ensure_loaded(self):\n",
    "        if self.index_type == \"faiss_ivfpq\":\n",
    "            if self._index is None:\n",
    "                import faiss\n",
    "                self._index = faiss.read_index(self.path)\n",
    "        else:\n",
    "            if self._mat is None:\n",
    "                npy = self.path if self.path.endswith(\".npy\") else str(Path(self.path).with_suffix(\".npy\"))\n",
    "                emb = np.load(npy).astype(np.float32)\n",
    "                self._mat = _normalize_rows(emb)\n",
    "    def search(self, q: np.ndarray, top_k: int = 5) -> Tuple[np.ndarray, np.ndarray]:\n",
    "        self._ensure_loaded()\n",
    "        if self.index_type == \"faiss_ivfpq\":\n",
    "            import faiss\n",
    "            xq = q.reshape(1, -1).astype(np.float32)\n",
    "            scores, ids = self._index.search(xq, top_k)\n",
    "            ids = ids[0]\n",
    "            scores = scores[0]\n",
    "            mask = ids >= 0\n",
    "            return self.corpus_map[ids[mask]], scores[mask]\n",
    "        else:\n",
    "            sims = _cosine_sim(q, self._mat)\n",
    "            idx = np.argpartition(-sims, kth=min(top_k-1, len(sims)-1))[:top_k]\n",
    "            idx = idx[np.argsort(-sims[idx])]\n",
    "            return self.corpus_map[idx], sims[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "9fb562e8-b8f2-45a2-8223-57a7793972fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _bm25_build(df: pd.DataFrame, text_cols: List[str]):\n",
    "    from rank_bm25 import BM25Okapi\n",
    "    docs = df[text_cols].astype(str).agg(\" \".join, axis=1).tolist()\n",
    "    tokens = [d.split() for d in docs]\n",
    "    return BM25Okapi(tokens)\n",
    "def _bm25_search(bm25, query: str, k: int = 5) -> Tuple[np.ndarray, np.ndarray]:\n",
    "    scores = bm25.get_scores(query.split())\n",
    "    idx = np.argpartition(-scores, kth=min(k-1, len(scores)-1))[:k]\n",
    "    idx = idx[np.argsort(-scores[idx])]\n",
    "    return idx, np.array(scores[idx], dtype=np.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "d5ff0b90-fae0-40f5-8b58-478a9755a63f",
   "metadata": {},
   "outputs": [],
   "source": [
    "_ROUTER_PATTERNS = {\n",
    "    \"sql\": re.compile(r\"\\b(select|from|join|group by|where|avg|min|max|sum|count|top|limit)\\b\", re.I),\n",
    "    \"math\": re.compile(r\"[0-9\\+\\-\\*\\/\\=\\^\\%]\"),\n",
    "    \"policy\": re.compile(r\"\\b(allowed|compliance|policy|pii|gdpr|hipaa|sox|pci)\\b\", re.I),\n",
    "}\n",
    "def route_query(q: str) -> Tuple[str, float]:\n",
    "    ql = q.lower()\n",
    "    if _ROUTER_PATTERNS[\"sql\"].search(ql):\n",
    "        return \"sql\", 0.82\n",
    "    if _ROUTER_PATTERNS[\"policy\"].search(ql):\n",
    "        return \"policy\", 0.7\n",
    "    if _ROUTER_PATTERNS[\"math\"].search(ql) and len(ql) < 160:\n",
    "        return \"math\", 0.65\n",
    "    return \"factual\", 0.75"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "e887b827-4c97-42f0-9439-0d82333a1889",
   "metadata": {},
   "outputs": [],
   "source": [
    "def rerank_cosine(df: pd.DataFrame, row_ids: np.ndarray, text_cols: List[str], q_emb: np.ndarray, top_k: int = 5):\n",
    "    model = _get_e5()\n",
    "    texts = df.iloc[row_ids][text_cols].astype(str).agg(\" \".join, axis=1).tolist()\n",
    "    emb = model.encode(texts, normalize_embeddings=True, show_progress_bar=False).astype(np.float32)\n",
    "    sims = emb @ q_emb\n",
    "    order = np.argsort(-sims)[:top_k]\n",
    "    return row_ids[order], sims[order]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "91ca2342-b25d-4bb1-8bdb-f2c08718e024",
   "metadata": {},
   "outputs": [],
   "source": [
    "def slm_respond(df: pd.DataFrame, row_ids: np.ndarray, text_cols: List[str], query: str) -> Dict[str, Any]:\n",
    "    snippets = df.iloc[row_ids][text_cols].astype(str).agg(\" \".join, axis=1).str.slice(0, 220).tolist()\n",
    "    answer = \"  \".join(snippets[:3])[:512]\n",
    "    citations = [int(i) for i in row_ids[:5]]\n",
    "    return {\"action\": \"respond\", \"payload\": {\"text\": answer}, \"citations\": citations, \"model_id\": \"slm-general-v0\"}\n",
    "def slm_run_sql(df: pd.DataFrame, query: str, limit: int = 5) -> Dict[str, Any]:\n",
    "    cols = [c for c in df.columns if c.lower() in query.lower()]\n",
    "    cols = cols[:5] if cols else df.columns[:5]\n",
    "    sql = f\"SELECT {', '.join(cols)} FROM dataset LIMIT {limit};\"\n",
    "    payload = {\"sql\": sql}\n",
    "    return {\"action\": \"run_sql\", \"payload\": payload, \"citations\": [], \"model_id\": \"slm-sql-v0\"}\n",
    "def slm_math(query: str) -> Dict[str, Any]:\n",
    "    expr = re.sub(r\"[^0-9\\.\\+\\-\\*\\/\\(\\)\\s]\", \"\", query)\n",
    "    try:\n",
    "        val = float(eval(expr, {\"__builtins__\": {}}, {}))\n",
    "        return {\"action\": \"respond\", \"payload\": {\"text\": f\"Result: {val}\"} , \"citations\": [], \"model_id\": \"slm-math-v0\"}\n",
    "    except Exception:\n",
    "        return {\"action\": \"clarify\", \"payload\": {\"question\": \"Which numbers/operation exactly?\"}, \"citations\": [], \"model_id\": \"slm-math-v0\"}\n",
    "def slm_policy(query: str) -> Dict[str, Any]:\n",
    "    return {\"action\": \"respond\", \"payload\": {\"text\": \"PII is masked in logs; SQL writes are denied; LIMIT enforced.\"}, \"citations\": [], \"model_id\": \"slm-policy-v0\"}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "34d896a5-a083-4f3b-9925-a5dd7b99a208",
   "metadata": {},
   "outputs": [],
   "source": [
    "def verify_faithfulness(answer_text: str, ctx_texts: List[str]) -> Dict[str, Any]:\n",
    "    ans_tokens = set(answer_text.lower().split())\n",
    "    ctx_tokens = set((\" \".join(ctx_texts)).lower().split())\n",
    "    overlap = len(ans_tokens & ctx_tokens) / (len(ans_tokens) + 1e-9)\n",
    "    faith = max(0.0, min(1.0, 0.5 * overlap + 0.45))  \n",
    "    return {\"faithfulness\": float(faith), \"entailment_prob\": float(faith)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "0ae1ae5b-a66a-4770-9ece-7f60a82a62ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "def query_pipeline(job_id: str,\n",
    "                   dataset_path: str,\n",
    "                   namespace: str = \"default\",\n",
    "                   query_text: str = \"\",\n",
    "                   top_k: int = 5) -> Dict[str, Any]:\n",
    "    t0 = time.time()\n",
    "    df = pd.read_parquet(dataset_path) if dataset_path.endswith(\".parquet\") else pd.read_csv(dataset_path)\n",
    "    adv_dir = Path(\"artifacts\") / \"advanced\" / job_id\n",
    "    vi = VectorIndex.from_artifacts(adv_dir) if adv_dir.exists() else None\n",
    "    text_cols_file = adv_dir / \"text_cols.json\" if adv_dir.exists() else None\n",
    "    if text_cols_file and text_cols_file.exists():\n",
    "        text_cols = _load_json(text_cols_file)\n",
    "    elif vi and vi.text_cols:\n",
    "        text_cols = vi.text_cols\n",
    "    else:\n",
    "        text_cols = guess_text_cols(df, CFG[\"bm25_text_cols_guess\"])\n",
    "    route, route_conf = route_query(query_text)\n",
    "    q_emb = _embed_query(\"query: \" + query_text)\n",
    "    if vi is not None:\n",
    "        row_ids, scores = vi.search(q_emb, top_k=top_k)\n",
    "        retriever = \"faiss\" if vi.index_type == \"faiss_ivfpq\" else \"numpy\"\n",
    "    elif text_cols:\n",
    "        bm25 = _bm25_build(df, text_cols)\n",
    "        row_ids, scores = _bm25_search(bm25, query_text, k=top_k)\n",
    "        retriever = \"bm25\"\n",
    "    else:\n",
    "        return {\n",
    "            \"request_id\": f\"{job_id}-{int(time.time())}\",\n",
    "            \"action\": \"clarify\",\n",
    "            \"payload\": {\"question\": \"I need at least one text column to search. Which field should I use?\"},\n",
    "            \"citations\": [],\n",
    "            \"model_id\": \"router-v0\",\n",
    "            \"confidence\": 0.51,\n",
    "            \"meta\": {\n",
    "                \"route\": route,\n",
    "                \"route_conf\": route_conf,\n",
    "                \"retriever\": \"none\",\n",
    "                \"latency_ms\": int((time.time() - t0) * 1000)\n",
    "            }\n",
    "        }\n",
    "    if len(row_ids) > 0 and text_cols:\n",
    "        row_ids, rr_scores = rerank_cosine(\n",
    "            df, row_ids, text_cols, q_emb, top_k=min(top_k, len(row_ids))\n",
    "        )\n",
    "    ctx_texts = df.iloc[row_ids][text_cols].astype(str).agg(\" \".join, axis=1).tolist() if (len(row_ids) > 0 and text_cols) else []\n",
    "    if route == \"sql\":\n",
    "        resp = slm_run_sql(df, query_text, limit=5)\n",
    "    elif route == \"math\":\n",
    "        resp = slm_math(query_text)\n",
    "    elif route == \"policy\":\n",
    "        resp = slm_policy(query_text)\n",
    "    else:\n",
    "        resp = slm_respond(df, row_ids, text_cols, query_text)\n",
    "    NON_FACTUAL = {\"sql\", \"math\", \"policy\"}\n",
    "    if route in NON_FACTUAL:\n",
    "        ver = {\"faithfulness\": 0.95, \"entailment_prob\": 0.95}\n",
    "    else:\n",
    "        if resp[\"action\"] == \"respond\":\n",
    "            ans_text = resp[\"payload\"].get(\"text\", \"\")\n",
    "            ver = verify_faithfulness(ans_text, ctx_texts) if ans_text else {\"faithfulness\": 0.9, \"entailment_prob\": 0.9}\n",
    "        else:\n",
    "            ver = {\"faithfulness\": 0.9, \"entailment_prob\": 0.9}\n",
    "    if (route not in NON_FACTUAL) and resp[\"action\"] == \"respond\" and ver.get(\"faithfulness\", 0.0) < 0.70:\n",
    "        resp = {\n",
    "            \"action\": \"clarify\",\n",
    "            \"payload\": {\"question\": \"Need more specifics to answer accurately.\"},\n",
    "            \"citations\": [],\n",
    "            \"model_id\": \"slm-general-v0\"\n",
    "        }\n",
    "    out = {\n",
    "        \"request_id\": f\"{job_id}-{int(time.time())}\",\n",
    "        \"action\": resp[\"action\"],\n",
    "        \"payload\": resp[\"payload\"],\n",
    "        \"citations\": resp.get(\"citations\", [int(i) for i in row_ids.tolist()[:3]] if len(row_ids) else []),\n",
    "        \"model_id\": resp.get(\"model_id\", \"slm-general-v0\"),\n",
    "        \"confidence\": float(min(0.99, max(0.55, route_conf))),\n",
    "        \"meta\": {\n",
    "            \"route\": route,\n",
    "            \"route_conf\": route_conf,\n",
    "            \"retriever\": retriever,\n",
    "            \"latency_ms\": int((time.time() - t0) * 1000)\n",
    "        }\n",
    "    }\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "2108c243-211f-4926-a541-9b3d627492b8",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'request_id': 'demo_1762113055-1762113082',\n",
       " 'action': 'run_sql',\n",
       " 'payload': {'sql': 'SELECT age, income, balance, default FROM dataset LIMIT 5;'},\n",
       " 'citations': [],\n",
       " 'model_id': 'slm-sql-v0',\n",
       " 'confidence': 0.82,\n",
       " 'meta': {'route': 'sql',\n",
       "  'route_conf': 0.82,\n",
       "  'retriever': 'faiss',\n",
       "  'latency_ms': 63}}"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "adv_dir = Path(\"artifacts\") / \"advanced\" / job_id\n",
    "(adv_dir).mkdir(parents=True, exist_ok=True)\n",
    "if isinstance(advanced_artifacts, dict) and \"text_cols\" in advanced_artifacts:\n",
    "    with open(adv_dir / \"text_cols.json\", \"w\") as f:\n",
    "        json.dump(advanced_artifacts[\"text_cols\"], f, indent=2)\n",
    "index_dir = adv_dir / \"index\"\n",
    "if index_dir.exists():\n",
    "    map_path = index_dir / \"corpus_row_ids.npy\"\n",
    "    if not map_path.exists():\n",
    "        meta_path = index_dir / \"index_meta.json\"\n",
    "        if meta_path.exists():\n",
    "            n = _load_json(meta_path)[\"n\"]\n",
    "            np.save(map_path, np.arange(n, dtype=np.int64))\n",
    "resp1 = query_pipeline(job_id=job_id, dataset_path=str(csv_path), namespace=\"demo\",\n",
    "                       query_text=\"What are common notes for customers with late fees?\", top_k=5)\n",
    "resp1\n",
    "resp2 = query_pipeline(job_id=job_id, dataset_path=str(csv_path), namespace=\"demo\",\n",
    "                       query_text=\"select age, income and balance where default is 1 limit 5\", top_k=5)\n",
    "resp2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "376c19b9-96d4-47f8-9947-1cc3ef333b37",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, json, asyncio, threading\n",
    "from pathlib import Path\n",
    "from typing import Any, Dict, Optional\n",
    "from fastapi import FastAPI, UploadFile, File, Form, HTTPException, Request\n",
    "from fastapi.responses import JSONResponse, PlainTextResponse\n",
    "from prometheus_client import Counter, Histogram, generate_latest, CONTENT_TYPE_LATEST\n",
    "import uvicorn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "81879cb2-3f72-458e-86de-2f1798c72b63",
   "metadata": {},
   "outputs": [],
   "source": [
    "app = FastAPI(title=\"Self-Optimizing ML API\", version=\"0.2\")\n",
    "REQS = Counter(\"api_requests_total\", \"Requests\", [\"route\"])\n",
    "LAT  = Histogram(\"api_latency_ms\", \"Latency (ms)\", [\"route\"])\n",
    "ART_DIR = Path(\"artifacts\"); ART_DIR.mkdir(exist_ok=True, parents=True)\n",
    "DATA_DIR = Path(\"data\"); DATA_DIR.mkdir(exist_ok=True, parents=True)\n",
    "def _bg(fn, *args, **kwargs):\n",
    "    t = threading.Thread(target=fn, args=args, kwargs=kwargs, daemon=True)\n",
    "    t.start()\n",
    "    return t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "3874fb28-7502-474c-bc6a-37b17afe7587",
   "metadata": {},
   "outputs": [],
   "source": [
    "@app.get(\"/healthz\")\n",
    "def healthz():\n",
    "    jobs_path = ART_DIR / \"jobs.json\"\n",
    "    jcount = 0\n",
    "    if jobs_path.exists():\n",
    "        try:\n",
    "            jcount = len(json.loads(jobs_path.read_text()))\n",
    "        except Exception:\n",
    "            jcount = -1\n",
    "    return {\n",
    "        \"status\": \"OK\",\n",
    "        \"cwd\": os.getcwd(),\n",
    "        \"artifacts_dir\": str(ART_DIR.resolve()),\n",
    "        \"jobs_count\": jcount\n",
    "    }\n",
    "@app.get(\"/metrics\")\n",
    "def metrics():\n",
    "    return PlainTextResponse(generate_latest(), media_type=CONTENT_TYPE_LATEST)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "f37a7536-841b-42ae-a57b-be52907b82fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "@app.post(\"/upload\")\n",
    "def upload(\n",
    "    namespace: str = Form(\"default\"),\n",
    "    target: Optional[str] = Form(None),\n",
    "    file: UploadFile = File(...)\n",
    "):\n",
    "    REQS.labels(\"/upload\").inc()\n",
    "    with LAT.labels(\"/upload\").time():\n",
    "        suffix = Path(file.filename).suffix.lower()\n",
    "        out_path = DATA_DIR / f\"{namespace}_{Path(file.filename).name}\"\n",
    "        with open(out_path, \"wb\") as f:\n",
    "            f.write(file.file.read())\n",
    "        basic = upload_basic(str(out_path), namespace=namespace, target=target)\n",
    "        job_id = basic[\"job_id\"]\n",
    "        jobs_set(job_id, status=\"BASIC_DONE\", dataset_path=str(out_path), profile_uri=basic.get(\"profile_uri\"))\n",
    "        _bg(advanced_trainer, job_id, str(out_path), namespace, basic.get(\"text_cols_guess\"))\n",
    "        return JSONResponse({\"job_id\": job_id, **basic})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "9e13ec70-ee93-475f-956e-47b78b627dbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "@app.get(\"/advanced\")\n",
    "def advanced(job_id: str):\n",
    "    REQS.labels(\"/advanced\").inc()\n",
    "    with LAT.labels(\"/advanced\").time():\n",
    "        res = read_advanced(job_id)\n",
    "        if res.get(\"status\") == \"UNKNOWN_JOB\":\n",
    "            raise HTTPException(404, \"Unknown job_id\")\n",
    "        return JSONResponse(res)\n",
    "async def _extract_params(request: Request) -> Dict[str, Any]:\n",
    "    ctype = (request.headers.get(\"content-type\") or \"\").lower()\n",
    "    if \"application/json\" in ctype:\n",
    "        try:\n",
    "            body = await request.json()\n",
    "        except Exception:\n",
    "            body = {}\n",
    "        return {**dict(request.query_params), **(body or {})}\n",
    "    if \"application/x-www-form-urlencoded\" in ctype or \"multipart/form-data\" in ctype:\n",
    "        try:\n",
    "            form = await request.form()\n",
    "            return {**dict(request.query_params), **dict(form)}\n",
    "        except Exception:\n",
    "            return dict(request.query_params)\n",
    "    return dict(request.query_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "1ed6749d-f149-476c-b638-a8914439564d",
   "metadata": {},
   "outputs": [],
   "source": [
    "@app.post(\"/query\")\n",
    "async def query(request: Request):\n",
    "    REQS.labels(\"/query\").inc()\n",
    "    with LAT.labels(\"/query\").time():\n",
    "        params = await _extract_params(request)\n",
    "        job_id = params.get(\"job_id\")\n",
    "        namespace = params.get(\"namespace\", \"default\")\n",
    "        query_text = params.get(\"query_text\", \"\")\n",
    "        try:\n",
    "            top_k = int(params.get(\"top_k\", 5))\n",
    "        except Exception:\n",
    "            top_k = 5\n",
    "        if not job_id:\n",
    "            raise HTTPException(422, detail=\"job_id is required (send as form, JSON, or query param)\")\n",
    "        job_row = jobs_get(job_id)\n",
    "        if not job_row:\n",
    "            raise HTTPException(404, \"Unknown job_id\")\n",
    "        ds = job_row.get(\"dataset_path\")\n",
    "        if not ds or not Path(ds).exists():\n",
    "            raise HTTPException(400, \"Dataset path not found for job\")\n",
    "        out = query_pipeline(\n",
    "            job_id=job_id,\n",
    "            dataset_path=ds,\n",
    "            namespace=namespace,\n",
    "            query_text=query_text,\n",
    "            top_k=top_k,\n",
    "        )\n",
    "        return JSONResponse(out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "5b400295-15d2-4513-a79a-193998021171",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "API serving at http://127.0.0.1:8000   /upload, /advanced, /query, /healthz, /metrics\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:     Started server process [17912]\n",
      "INFO:     Waiting for application startup.\n",
      "INFO:     Application startup complete.\n",
      "INFO:     Uvicorn running on http://127.0.0.1:8000 (Press CTRL+C to quit)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:     127.0.0.1:52372 - \"POST /upload HTTP/1.1\" 200 OK\n",
      "INFO:     127.0.0.1:52373 - \"GET /advanced?job_id=demo_1762113098 HTTP/1.1\" 200 OK\n",
      "INFO:     127.0.0.1:52375 - \"GET /advanced?job_id=demo_1762113098 HTTP/1.1\" 200 OK\n",
      "INFO:     127.0.0.1:52376 - \"GET /advanced?job_id=demo_1762113098 HTTP/1.1\" 200 OK\n",
      "INFO:     127.0.0.1:52377 - \"GET /advanced?job_id=demo_1762113098 HTTP/1.1\" 200 OK\n",
      "INFO:     127.0.0.1:52378 - \"GET /advanced?job_id=demo_1762113098 HTTP/1.1\" 200 OK\n",
      "INFO:     127.0.0.1:52379 - \"GET /advanced?job_id=demo_1762113098 HTTP/1.1\" 200 OK\n",
      "INFO:     127.0.0.1:52380 - \"GET /advanced?job_id=demo_1762113098 HTTP/1.1\" 200 OK\n",
      "INFO:     127.0.0.1:52381 - \"POST /query HTTP/1.1\" 200 OK\n",
      "INFO:     127.0.0.1:52387 - \"POST /rl/update HTTP/1.1\" 200 OK\n",
      "INFO:     127.0.0.1:52388 - \"POST /drift/check?namespace=demo HTTP/1.1\" 200 OK\n",
      "INFO:     127.0.0.1:52389 - \"POST /cache/clean HTTP/1.1\" 200 OK\n",
      "INFO:     127.0.0.1:52396 - \"POST /upload HTTP/1.1\" 200 OK\n",
      "INFO:     127.0.0.1:52397 - \"GET /advanced?job_id=demo_1762113148 HTTP/1.1\" 200 OK\n",
      "INFO:     127.0.0.1:52399 - \"GET /advanced?job_id=demo_1762113148 HTTP/1.1\" 200 OK\n",
      "INFO:     127.0.0.1:52400 - \"GET /advanced?job_id=demo_1762113148 HTTP/1.1\" 200 OK\n",
      "INFO:     127.0.0.1:52401 - \"GET /advanced?job_id=demo_1762113148 HTTP/1.1\" 200 OK\n",
      "INFO:     127.0.0.1:52402 - \"GET /advanced?job_id=demo_1762113148 HTTP/1.1\" 200 OK\n",
      "INFO:     127.0.0.1:52403 - \"GET /advanced?job_id=demo_1762113148 HTTP/1.1\" 200 OK\n",
      "INFO:     127.0.0.1:52404 - \"GET /advanced?job_id=demo_1762113148 HTTP/1.1\" 200 OK\n",
      "INFO:     127.0.0.1:52405 - \"GET /advanced?job_id=demo_1762113148 HTTP/1.1\" 200 OK\n",
      "INFO:     127.0.0.1:52407 - \"GET /advanced?job_id=demo_1762113148 HTTP/1.1\" 200 OK\n",
      "INFO:     127.0.0.1:52412 - \"GET /advanced?job_id=demo_1762113148 HTTP/1.1\" 200 OK\n",
      "INFO:     127.0.0.1:52413 - \"GET /advanced?job_id=demo_1762113148 HTTP/1.1\" 200 OK\n",
      "INFO:     127.0.0.1:52414 - \"GET /advanced?job_id=demo_1762113148 HTTP/1.1\" 200 OK\n",
      "INFO:     127.0.0.1:52415 - \"GET /advanced?job_id=demo_1762113148 HTTP/1.1\" 200 OK\n",
      "INFO:     127.0.0.1:52416 - \"POST /query HTTP/1.1\" 200 OK\n",
      "INFO:     127.0.0.1:52417 - \"POST /query HTTP/1.1\" 200 OK\n",
      "INFO:     127.0.0.1:52418 - \"POST /query HTTP/1.1\" 200 OK\n",
      "INFO:     127.0.0.1:52419 - \"POST /query HTTP/1.1\" 200 OK\n",
      "INFO:     127.0.0.1:52420 - \"POST /query HTTP/1.1\" 200 OK\n",
      "INFO:     127.0.0.1:52421 - \"POST /query HTTP/1.1\" 200 OK\n",
      "INFO:     127.0.0.1:52422 - \"POST /query HTTP/1.1\" 200 OK\n",
      "INFO:     127.0.0.1:52423 - \"POST /query HTTP/1.1\" 200 OK\n",
      "INFO:     127.0.0.1:52424 - \"POST /query HTTP/1.1\" 200 OK\n",
      "INFO:     127.0.0.1:52425 - \"POST /query HTTP/1.1\" 200 OK\n",
      "INFO:     127.0.0.1:52426 - \"POST /query HTTP/1.1\" 200 OK\n",
      "INFO:     127.0.0.1:52427 - \"POST /query HTTP/1.1\" 200 OK\n",
      "INFO:     127.0.0.1:52428 - \"POST /query HTTP/1.1\" 200 OK\n",
      "INFO:     127.0.0.1:52429 - \"POST /query HTTP/1.1\" 200 OK\n",
      "INFO:     127.0.0.1:52430 - \"POST /query HTTP/1.1\" 200 OK\n",
      "INFO:     127.0.0.1:52431 - \"POST /query HTTP/1.1\" 200 OK\n",
      "INFO:     127.0.0.1:52432 - \"POST /query HTTP/1.1\" 200 OK\n",
      "INFO:     127.0.0.1:52433 - \"POST /query HTTP/1.1\" 200 OK\n",
      "INFO:     127.0.0.1:52434 - \"POST /query HTTP/1.1\" 200 OK\n",
      "INFO:     127.0.0.1:52435 - \"POST /query HTTP/1.1\" 200 OK\n",
      "INFO:     127.0.0.1:52436 - \"POST /query HTTP/1.1\" 200 OK\n",
      "INFO:     127.0.0.1:52437 - \"POST /query HTTP/1.1\" 200 OK\n",
      "INFO:     127.0.0.1:52438 - \"POST /query HTTP/1.1\" 200 OK\n",
      "INFO:     127.0.0.1:52439 - \"POST /query HTTP/1.1\" 200 OK\n",
      "INFO:     127.0.0.1:52440 - \"POST /query HTTP/1.1\" 200 OK\n",
      "INFO:     127.0.0.1:52441 - \"POST /query HTTP/1.1\" 200 OK\n",
      "INFO:     127.0.0.1:52442 - \"POST /query HTTP/1.1\" 200 OK\n",
      "INFO:     127.0.0.1:52443 - \"POST /query HTTP/1.1\" 200 OK\n",
      "INFO:     127.0.0.1:52444 - \"POST /query HTTP/1.1\" 200 OK\n",
      "INFO:     127.0.0.1:52445 - \"POST /query HTTP/1.1\" 200 OK\n",
      "INFO:     127.0.0.1:52446 - \"POST /query HTTP/1.1\" 200 OK\n",
      "INFO:     127.0.0.1:52447 - \"POST /query HTTP/1.1\" 200 OK\n",
      "INFO:     127.0.0.1:52448 - \"POST /query HTTP/1.1\" 200 OK\n",
      "INFO:     127.0.0.1:52449 - \"POST /query HTTP/1.1\" 200 OK\n",
      "INFO:     127.0.0.1:52450 - \"POST /query HTTP/1.1\" 200 OK\n",
      "INFO:     127.0.0.1:52451 - \"POST /query HTTP/1.1\" 200 OK\n",
      "INFO:     127.0.0.1:52452 - \"POST /query HTTP/1.1\" 200 OK\n",
      "INFO:     127.0.0.1:52453 - \"POST /query HTTP/1.1\" 200 OK\n",
      "INFO:     127.0.0.1:52454 - \"POST /query HTTP/1.1\" 200 OK\n",
      "INFO:     127.0.0.1:52455 - \"POST /query HTTP/1.1\" 200 OK\n",
      "INFO:     127.0.0.1:52456 - \"POST /query HTTP/1.1\" 200 OK\n",
      "INFO:     127.0.0.1:52457 - \"POST /query HTTP/1.1\" 200 OK\n",
      "INFO:     127.0.0.1:52458 - \"POST /query HTTP/1.1\" 200 OK\n",
      "INFO:     127.0.0.1:52459 - \"POST /query HTTP/1.1\" 200 OK\n",
      "INFO:     127.0.0.1:52460 - \"POST /query HTTP/1.1\" 200 OK\n",
      "INFO:     127.0.0.1:52461 - \"POST /query HTTP/1.1\" 200 OK\n",
      "INFO:     127.0.0.1:52462 - \"POST /query HTTP/1.1\" 200 OK\n",
      "INFO:     127.0.0.1:52463 - \"POST /query HTTP/1.1\" 200 OK\n",
      "INFO:     127.0.0.1:52464 - \"POST /query HTTP/1.1\" 200 OK\n",
      "INFO:     127.0.0.1:52465 - \"POST /query HTTP/1.1\" 200 OK\n",
      "INFO:     127.0.0.1:52466 - \"POST /query HTTP/1.1\" 200 OK\n",
      "INFO:     127.0.0.1:52467 - \"POST /query HTTP/1.1\" 200 OK\n",
      "INFO:     127.0.0.1:52468 - \"POST /query HTTP/1.1\" 200 OK\n",
      "INFO:     127.0.0.1:52469 - \"POST /query HTTP/1.1\" 200 OK\n",
      "INFO:     127.0.0.1:52470 - \"POST /query HTTP/1.1\" 200 OK\n",
      "INFO:     127.0.0.1:52471 - \"POST /query HTTP/1.1\" 200 OK\n",
      "INFO:     127.0.0.1:52472 - \"POST /query HTTP/1.1\" 200 OK\n",
      "INFO:     127.0.0.1:52473 - \"POST /query HTTP/1.1\" 200 OK\n",
      "INFO:     127.0.0.1:52474 - \"POST /query HTTP/1.1\" 200 OK\n",
      "INFO:     127.0.0.1:52475 - \"POST /query HTTP/1.1\" 200 OK\n",
      "INFO:     127.0.0.1:52476 - \"POST /query HTTP/1.1\" 200 OK\n",
      "INFO:     127.0.0.1:52477 - \"POST /query HTTP/1.1\" 200 OK\n",
      "INFO:     127.0.0.1:52478 - \"POST /query HTTP/1.1\" 200 OK\n",
      "INFO:     127.0.0.1:52479 - \"POST /query HTTP/1.1\" 200 OK\n",
      "INFO:     127.0.0.1:52480 - \"POST /query HTTP/1.1\" 200 OK\n",
      "INFO:     127.0.0.1:52481 - \"POST /query HTTP/1.1\" 200 OK\n",
      "INFO:     127.0.0.1:52482 - \"POST /query HTTP/1.1\" 200 OK\n",
      "INFO:     127.0.0.1:52483 - \"POST /query HTTP/1.1\" 200 OK\n",
      "INFO:     127.0.0.1:52484 - \"POST /query HTTP/1.1\" 200 OK\n",
      "INFO:     127.0.0.1:52485 - \"POST /query HTTP/1.1\" 200 OK\n",
      "INFO:     127.0.0.1:52486 - \"POST /query HTTP/1.1\" 200 OK\n",
      "INFO:     127.0.0.1:52487 - \"POST /query HTTP/1.1\" 200 OK\n",
      "INFO:     127.0.0.1:52488 - \"POST /query HTTP/1.1\" 200 OK\n",
      "INFO:     127.0.0.1:52489 - \"POST /query HTTP/1.1\" 200 OK\n",
      "INFO:     127.0.0.1:52490 - \"POST /query HTTP/1.1\" 200 OK\n",
      "INFO:     127.0.0.1:52491 - \"POST /query HTTP/1.1\" 200 OK\n",
      "INFO:     127.0.0.1:52492 - \"POST /query HTTP/1.1\" 200 OK\n",
      "INFO:     127.0.0.1:52493 - \"POST /query HTTP/1.1\" 200 OK\n",
      "INFO:     127.0.0.1:52494 - \"POST /query HTTP/1.1\" 200 OK\n",
      "INFO:     127.0.0.1:52495 - \"POST /query HTTP/1.1\" 200 OK\n",
      "INFO:     127.0.0.1:52496 - \"POST /query HTTP/1.1\" 200 OK\n",
      "INFO:     127.0.0.1:52497 - \"POST /query HTTP/1.1\" 200 OK\n",
      "INFO:     127.0.0.1:52498 - \"POST /query HTTP/1.1\" 200 OK\n",
      "INFO:     127.0.0.1:52499 - \"POST /query HTTP/1.1\" 200 OK\n",
      "INFO:     127.0.0.1:52500 - \"POST /query HTTP/1.1\" 200 OK\n",
      "INFO:     127.0.0.1:52502 - \"POST /query HTTP/1.1\" 200 OK\n",
      "INFO:     127.0.0.1:52503 - \"POST /query HTTP/1.1\" 200 OK\n",
      "INFO:     127.0.0.1:52504 - \"POST /query HTTP/1.1\" 200 OK\n",
      "INFO:     127.0.0.1:52505 - \"POST /query HTTP/1.1\" 200 OK\n",
      "INFO:     127.0.0.1:52506 - \"POST /query HTTP/1.1\" 200 OK\n",
      "INFO:     127.0.0.1:52507 - \"POST /query HTTP/1.1\" 200 OK\n",
      "INFO:     127.0.0.1:52508 - \"POST /query HTTP/1.1\" 200 OK\n",
      "INFO:     127.0.0.1:52509 - \"POST /query HTTP/1.1\" 200 OK\n",
      "INFO:     127.0.0.1:52510 - \"POST /query HTTP/1.1\" 200 OK\n",
      "INFO:     127.0.0.1:52511 - \"POST /query HTTP/1.1\" 200 OK\n",
      "INFO:     127.0.0.1:52512 - \"POST /query HTTP/1.1\" 200 OK\n",
      "INFO:     127.0.0.1:52513 - \"POST /query HTTP/1.1\" 200 OK\n",
      "INFO:     127.0.0.1:52514 - \"POST /query HTTP/1.1\" 200 OK\n",
      "INFO:     127.0.0.1:52515 - \"POST /query HTTP/1.1\" 200 OK\n",
      "INFO:     127.0.0.1:52516 - \"POST /query HTTP/1.1\" 200 OK\n",
      "INFO:     127.0.0.1:52517 - \"POST /query HTTP/1.1\" 200 OK\n",
      "INFO:     127.0.0.1:52518 - \"POST /query HTTP/1.1\" 200 OK\n",
      "INFO:     127.0.0.1:52519 - \"POST /query HTTP/1.1\" 200 OK\n",
      "INFO:     127.0.0.1:52520 - \"POST /query HTTP/1.1\" 200 OK\n",
      "INFO:     127.0.0.1:52521 - \"POST /query HTTP/1.1\" 200 OK\n",
      "INFO:     127.0.0.1:52522 - \"POST /query HTTP/1.1\" 200 OK\n",
      "INFO:     127.0.0.1:52523 - \"POST /query HTTP/1.1\" 200 OK\n",
      "INFO:     127.0.0.1:52524 - \"POST /query HTTP/1.1\" 200 OK\n",
      "INFO:     127.0.0.1:52525 - \"POST /query HTTP/1.1\" 200 OK\n",
      "INFO:     127.0.0.1:52526 - \"POST /query HTTP/1.1\" 200 OK\n",
      "INFO:     127.0.0.1:52527 - \"POST /query HTTP/1.1\" 200 OK\n",
      "INFO:     127.0.0.1:52528 - \"POST /query HTTP/1.1\" 200 OK\n",
      "INFO:     127.0.0.1:52529 - \"POST /query HTTP/1.1\" 200 OK\n",
      "INFO:     127.0.0.1:52530 - \"POST /query HTTP/1.1\" 200 OK\n",
      "INFO:     127.0.0.1:52531 - \"POST /query HTTP/1.1\" 200 OK\n",
      "INFO:     127.0.0.1:52532 - \"POST /query HTTP/1.1\" 200 OK\n",
      "INFO:     127.0.0.1:52533 - \"POST /query HTTP/1.1\" 200 OK\n",
      "INFO:     127.0.0.1:52534 - \"POST /query HTTP/1.1\" 200 OK\n",
      "INFO:     127.0.0.1:52535 - \"POST /query HTTP/1.1\" 200 OK\n",
      "INFO:     127.0.0.1:52536 - \"POST /query HTTP/1.1\" 200 OK\n",
      "INFO:     127.0.0.1:52537 - \"POST /query HTTP/1.1\" 200 OK\n",
      "INFO:     127.0.0.1:52538 - \"POST /query HTTP/1.1\" 200 OK\n",
      "INFO:     127.0.0.1:52539 - \"POST /query HTTP/1.1\" 200 OK\n",
      "INFO:     127.0.0.1:52540 - \"POST /query HTTP/1.1\" 200 OK\n",
      "INFO:     127.0.0.1:52541 - \"POST /query HTTP/1.1\" 200 OK\n",
      "INFO:     127.0.0.1:52542 - \"POST /query HTTP/1.1\" 200 OK\n",
      "INFO:     127.0.0.1:52543 - \"POST /query HTTP/1.1\" 200 OK\n",
      "INFO:     127.0.0.1:52544 - \"POST /query HTTP/1.1\" 200 OK\n",
      "INFO:     127.0.0.1:52545 - \"POST /query HTTP/1.1\" 200 OK\n",
      "INFO:     127.0.0.1:52546 - \"POST /query HTTP/1.1\" 200 OK\n",
      "INFO:     127.0.0.1:52547 - \"POST /query HTTP/1.1\" 200 OK\n",
      "INFO:     127.0.0.1:52548 - \"POST /query HTTP/1.1\" 200 OK\n",
      "INFO:     127.0.0.1:52549 - \"POST /query HTTP/1.1\" 200 OK\n",
      "INFO:     127.0.0.1:52550 - \"POST /query HTTP/1.1\" 200 OK\n",
      "INFO:     127.0.0.1:52551 - \"POST /query HTTP/1.1\" 200 OK\n",
      "INFO:     127.0.0.1:52552 - \"POST /query HTTP/1.1\" 200 OK\n",
      "INFO:     127.0.0.1:52553 - \"POST /query HTTP/1.1\" 200 OK\n",
      "INFO:     127.0.0.1:52554 - \"POST /query HTTP/1.1\" 200 OK\n",
      "INFO:     127.0.0.1:52555 - \"POST /query HTTP/1.1\" 200 OK\n",
      "INFO:     127.0.0.1:52556 - \"POST /query HTTP/1.1\" 200 OK\n",
      "INFO:     127.0.0.1:52557 - \"POST /query HTTP/1.1\" 200 OK\n",
      "INFO:     127.0.0.1:52558 - \"POST /query HTTP/1.1\" 200 OK\n",
      "INFO:     127.0.0.1:52559 - \"POST /query HTTP/1.1\" 200 OK\n",
      "INFO:     127.0.0.1:52560 - \"POST /query HTTP/1.1\" 200 OK\n",
      "INFO:     127.0.0.1:52561 - \"POST /query HTTP/1.1\" 200 OK\n",
      "INFO:     127.0.0.1:52562 - \"POST /query HTTP/1.1\" 200 OK\n",
      "INFO:     127.0.0.1:52563 - \"POST /query HTTP/1.1\" 200 OK\n",
      "INFO:     127.0.0.1:52566 - \"POST /query HTTP/1.1\" 200 OK\n",
      "INFO:     127.0.0.1:52567 - \"POST /query HTTP/1.1\" 200 OK\n",
      "INFO:     127.0.0.1:52568 - \"POST /query HTTP/1.1\" 200 OK\n",
      "INFO:     127.0.0.1:52569 - \"POST /query HTTP/1.1\" 200 OK\n",
      "INFO:     127.0.0.1:52570 - \"POST /query HTTP/1.1\" 200 OK\n",
      "INFO:     127.0.0.1:52571 - \"POST /query HTTP/1.1\" 200 OK\n",
      "INFO:     127.0.0.1:52572 - \"POST /query HTTP/1.1\" 200 OK\n",
      "INFO:     127.0.0.1:52573 - \"POST /query HTTP/1.1\" 200 OK\n",
      "INFO:     127.0.0.1:52574 - \"POST /query HTTP/1.1\" 200 OK\n",
      "INFO:     127.0.0.1:52575 - \"POST /query HTTP/1.1\" 200 OK\n",
      "INFO:     127.0.0.1:52576 - \"POST /query HTTP/1.1\" 200 OK\n",
      "INFO:     127.0.0.1:52577 - \"POST /query HTTP/1.1\" 200 OK\n",
      "INFO:     127.0.0.1:52578 - \"POST /query HTTP/1.1\" 200 OK\n",
      "INFO:     127.0.0.1:52579 - \"POST /query HTTP/1.1\" 200 OK\n",
      "INFO:     127.0.0.1:52580 - \"POST /query HTTP/1.1\" 200 OK\n",
      "INFO:     127.0.0.1:52581 - \"POST /query HTTP/1.1\" 200 OK\n",
      "INFO:     127.0.0.1:52582 - \"POST /query HTTP/1.1\" 200 OK\n",
      "INFO:     127.0.0.1:52583 - \"POST /query HTTP/1.1\" 200 OK\n",
      "INFO:     127.0.0.1:52584 - \"POST /query HTTP/1.1\" 200 OK\n",
      "INFO:     127.0.0.1:52585 - \"POST /query HTTP/1.1\" 200 OK\n",
      "INFO:     127.0.0.1:52586 - \"POST /query HTTP/1.1\" 200 OK\n",
      "INFO:     127.0.0.1:52587 - \"POST /query HTTP/1.1\" 200 OK\n",
      "INFO:     127.0.0.1:52588 - \"POST /query HTTP/1.1\" 200 OK\n",
      "INFO:     127.0.0.1:52589 - \"POST /query HTTP/1.1\" 200 OK\n",
      "INFO:     127.0.0.1:52590 - \"POST /query HTTP/1.1\" 200 OK\n",
      "INFO:     127.0.0.1:52591 - \"POST /query HTTP/1.1\" 200 OK\n",
      "INFO:     127.0.0.1:52592 - \"POST /query HTTP/1.1\" 200 OK\n",
      "INFO:     127.0.0.1:52593 - \"POST /query HTTP/1.1\" 200 OK\n",
      "INFO:     127.0.0.1:52594 - \"POST /query HTTP/1.1\" 200 OK\n",
      "INFO:     127.0.0.1:52595 - \"POST /query HTTP/1.1\" 200 OK\n",
      "INFO:     127.0.0.1:52596 - \"POST /query HTTP/1.1\" 200 OK\n",
      "INFO:     127.0.0.1:52597 - \"POST /query HTTP/1.1\" 200 OK\n",
      "INFO:     127.0.0.1:52598 - \"POST /query HTTP/1.1\" 200 OK\n",
      "INFO:     127.0.0.1:52599 - \"POST /query HTTP/1.1\" 200 OK\n",
      "INFO:     127.0.0.1:52600 - \"POST /query HTTP/1.1\" 200 OK\n",
      "INFO:     127.0.0.1:52601 - \"POST /query HTTP/1.1\" 200 OK\n",
      "INFO:     127.0.0.1:52602 - \"POST /query HTTP/1.1\" 200 OK\n",
      "INFO:     127.0.0.1:52603 - \"POST /query HTTP/1.1\" 200 OK\n",
      "INFO:     127.0.0.1:52604 - \"POST /query HTTP/1.1\" 200 OK\n",
      "INFO:     127.0.0.1:52605 - \"POST /query HTTP/1.1\" 200 OK\n",
      "INFO:     127.0.0.1:52606 - \"POST /query HTTP/1.1\" 200 OK\n",
      "INFO:     127.0.0.1:52607 - \"POST /query HTTP/1.1\" 200 OK\n",
      "INFO:     127.0.0.1:52608 - \"POST /query HTTP/1.1\" 200 OK\n",
      "INFO:     127.0.0.1:52609 - \"POST /query HTTP/1.1\" 200 OK\n",
      "INFO:     127.0.0.1:52610 - \"POST /query HTTP/1.1\" 200 OK\n",
      "INFO:     127.0.0.1:52611 - \"POST /query HTTP/1.1\" 200 OK\n",
      "INFO:     127.0.0.1:52612 - \"POST /query HTTP/1.1\" 200 OK\n",
      "INFO:     127.0.0.1:52613 - \"POST /query HTTP/1.1\" 200 OK\n",
      "INFO:     127.0.0.1:52614 - \"POST /query HTTP/1.1\" 200 OK\n",
      "INFO:     127.0.0.1:52615 - \"POST /query HTTP/1.1\" 200 OK\n",
      "INFO:     127.0.0.1:52616 - \"POST /query HTTP/1.1\" 200 OK\n",
      "INFO:     127.0.0.1:52617 - \"POST /query HTTP/1.1\" 200 OK\n",
      "INFO:     127.0.0.1:52618 - \"POST /query HTTP/1.1\" 200 OK\n",
      "INFO:     127.0.0.1:52619 - \"POST /query HTTP/1.1\" 200 OK\n",
      "INFO:     127.0.0.1:52620 - \"POST /query HTTP/1.1\" 200 OK\n",
      "INFO:     127.0.0.1:52621 - \"POST /query HTTP/1.1\" 200 OK\n",
      "INFO:     127.0.0.1:52622 - \"POST /query HTTP/1.1\" 200 OK\n",
      "INFO:     127.0.0.1:52623 - \"POST /query HTTP/1.1\" 200 OK\n",
      "INFO:     127.0.0.1:52624 - \"POST /query HTTP/1.1\" 200 OK\n",
      "INFO:     127.0.0.1:52625 - \"POST /query HTTP/1.1\" 200 OK\n",
      "INFO:     127.0.0.1:52626 - \"POST /query HTTP/1.1\" 200 OK\n",
      "INFO:     127.0.0.1:52627 - \"POST /query HTTP/1.1\" 200 OK\n",
      "INFO:     127.0.0.1:52628 - \"POST /query HTTP/1.1\" 200 OK\n",
      "INFO:     127.0.0.1:52629 - \"POST /query HTTP/1.1\" 200 OK\n",
      "INFO:     127.0.0.1:52630 - \"POST /query HTTP/1.1\" 200 OK\n",
      "INFO:     127.0.0.1:52631 - \"POST /query HTTP/1.1\" 200 OK\n",
      "INFO:     127.0.0.1:52632 - \"POST /query HTTP/1.1\" 200 OK\n",
      "INFO:     127.0.0.1:52633 - \"POST /query HTTP/1.1\" 200 OK\n",
      "INFO:     127.0.0.1:52634 - \"POST /query HTTP/1.1\" 200 OK\n",
      "INFO:     127.0.0.1:52635 - \"POST /query HTTP/1.1\" 200 OK\n",
      "INFO:     127.0.0.1:52636 - \"POST /query HTTP/1.1\" 200 OK\n",
      "INFO:     127.0.0.1:52637 - \"POST /query HTTP/1.1\" 200 OK\n",
      "INFO:     127.0.0.1:52638 - \"POST /query HTTP/1.1\" 200 OK\n",
      "INFO:     127.0.0.1:52639 - \"POST /query HTTP/1.1\" 200 OK\n",
      "INFO:     127.0.0.1:52640 - \"POST /query HTTP/1.1\" 200 OK\n",
      "INFO:     127.0.0.1:52641 - \"POST /query HTTP/1.1\" 200 OK\n",
      "INFO:     127.0.0.1:52642 - \"POST /query HTTP/1.1\" 200 OK\n",
      "INFO:     127.0.0.1:52643 - \"POST /query HTTP/1.1\" 200 OK\n",
      "INFO:     127.0.0.1:52644 - \"POST /query HTTP/1.1\" 200 OK\n",
      "INFO:     127.0.0.1:52645 - \"POST /query HTTP/1.1\" 200 OK\n",
      "INFO:     127.0.0.1:52646 - \"POST /query HTTP/1.1\" 200 OK\n",
      "INFO:     127.0.0.1:52647 - \"POST /query HTTP/1.1\" 200 OK\n",
      "INFO:     127.0.0.1:52648 - \"POST /query HTTP/1.1\" 200 OK\n",
      "INFO:     127.0.0.1:52649 - \"POST /query HTTP/1.1\" 200 OK\n",
      "INFO:     127.0.0.1:52650 - \"POST /query HTTP/1.1\" 200 OK\n",
      "INFO:     127.0.0.1:52651 - \"POST /query HTTP/1.1\" 200 OK\n",
      "INFO:     127.0.0.1:52652 - \"POST /query HTTP/1.1\" 200 OK\n",
      "INFO:     127.0.0.1:52653 - \"POST /query HTTP/1.1\" 200 OK\n",
      "INFO:     127.0.0.1:52654 - \"POST /query HTTP/1.1\" 200 OK\n",
      "INFO:     127.0.0.1:52655 - \"POST /query HTTP/1.1\" 200 OK\n",
      "INFO:     127.0.0.1:52656 - \"POST /query HTTP/1.1\" 200 OK\n",
      "INFO:     127.0.0.1:52657 - \"POST /query HTTP/1.1\" 200 OK\n",
      "INFO:     127.0.0.1:52658 - \"POST /query HTTP/1.1\" 200 OK\n",
      "INFO:     127.0.0.1:52659 - \"POST /query HTTP/1.1\" 200 OK\n",
      "INFO:     127.0.0.1:52660 - \"POST /query HTTP/1.1\" 200 OK\n",
      "INFO:     127.0.0.1:52661 - \"POST /query HTTP/1.1\" 200 OK\n",
      "INFO:     127.0.0.1:52662 - \"POST /query HTTP/1.1\" 200 OK\n",
      "INFO:     127.0.0.1:52663 - \"POST /query HTTP/1.1\" 200 OK\n",
      "INFO:     127.0.0.1:52664 - \"POST /query HTTP/1.1\" 200 OK\n",
      "INFO:     127.0.0.1:52665 - \"POST /query HTTP/1.1\" 200 OK\n",
      "INFO:     127.0.0.1:52666 - \"POST /query HTTP/1.1\" 200 OK\n",
      "INFO:     127.0.0.1:52667 - \"POST /query HTTP/1.1\" 200 OK\n",
      "INFO:     127.0.0.1:52668 - \"POST /query HTTP/1.1\" 200 OK\n",
      "INFO:     127.0.0.1:52669 - \"POST /query HTTP/1.1\" 200 OK\n",
      "INFO:     127.0.0.1:52670 - \"POST /query HTTP/1.1\" 200 OK\n",
      "INFO:     127.0.0.1:52671 - \"POST /query HTTP/1.1\" 200 OK\n",
      "INFO:     127.0.0.1:52672 - \"POST /query HTTP/1.1\" 200 OK\n",
      "INFO:     127.0.0.1:52673 - \"POST /query HTTP/1.1\" 200 OK\n",
      "INFO:     127.0.0.1:52674 - \"POST /query HTTP/1.1\" 200 OK\n",
      "INFO:     127.0.0.1:52675 - \"POST /query HTTP/1.1\" 200 OK\n",
      "INFO:     127.0.0.1:52676 - \"POST /query HTTP/1.1\" 200 OK\n",
      "INFO:     127.0.0.1:52677 - \"POST /query HTTP/1.1\" 200 OK\n",
      "INFO:     127.0.0.1:52678 - \"POST /query HTTP/1.1\" 200 OK\n",
      "INFO:     127.0.0.1:52679 - \"POST /query HTTP/1.1\" 200 OK\n",
      "INFO:     127.0.0.1:52680 - \"POST /query HTTP/1.1\" 200 OK\n",
      "INFO:     127.0.0.1:52681 - \"POST /query HTTP/1.1\" 200 OK\n",
      "INFO:     127.0.0.1:52682 - \"POST /query HTTP/1.1\" 200 OK\n",
      "INFO:     127.0.0.1:52683 - \"POST /query HTTP/1.1\" 200 OK\n",
      "INFO:     127.0.0.1:52684 - \"POST /query HTTP/1.1\" 200 OK\n",
      "INFO:     127.0.0.1:52685 - \"POST /query HTTP/1.1\" 200 OK\n",
      "INFO:     127.0.0.1:52686 - \"POST /query HTTP/1.1\" 200 OK\n",
      "INFO:     127.0.0.1:52687 - \"POST /query HTTP/1.1\" 200 OK\n",
      "INFO:     127.0.0.1:52688 - \"POST /query HTTP/1.1\" 200 OK\n",
      "INFO:     127.0.0.1:52689 - \"POST /query HTTP/1.1\" 200 OK\n",
      "INFO:     127.0.0.1:52690 - \"POST /query HTTP/1.1\" 200 OK\n",
      "INFO:     127.0.0.1:52691 - \"POST /query HTTP/1.1\" 200 OK\n",
      "INFO:     127.0.0.1:52692 - \"POST /query HTTP/1.1\" 200 OK\n",
      "INFO:     127.0.0.1:52693 - \"POST /query HTTP/1.1\" 200 OK\n",
      "INFO:     127.0.0.1:52694 - \"POST /query HTTP/1.1\" 200 OK\n",
      "INFO:     127.0.0.1:52695 - \"POST /query HTTP/1.1\" 200 OK\n",
      "INFO:     127.0.0.1:52696 - \"POST /query HTTP/1.1\" 200 OK\n",
      "INFO:     127.0.0.1:52697 - \"POST /query HTTP/1.1\" 200 OK\n",
      "INFO:     127.0.0.1:52698 - \"POST /query HTTP/1.1\" 200 OK\n",
      "INFO:     127.0.0.1:52699 - \"POST /query HTTP/1.1\" 200 OK\n",
      "INFO:     127.0.0.1:52700 - \"POST /query HTTP/1.1\" 200 OK\n",
      "INFO:     127.0.0.1:52701 - \"POST /query HTTP/1.1\" 200 OK\n",
      "INFO:     127.0.0.1:52702 - \"POST /query HTTP/1.1\" 200 OK\n",
      "INFO:     127.0.0.1:52703 - \"POST /query HTTP/1.1\" 200 OK\n",
      "INFO:     127.0.0.1:52704 - \"POST /query HTTP/1.1\" 200 OK\n",
      "INFO:     127.0.0.1:52705 - \"POST /query HTTP/1.1\" 200 OK\n",
      "INFO:     127.0.0.1:52706 - \"POST /query HTTP/1.1\" 200 OK\n",
      "INFO:     127.0.0.1:52707 - \"POST /query HTTP/1.1\" 200 OK\n",
      "INFO:     127.0.0.1:52708 - \"POST /query HTTP/1.1\" 200 OK\n",
      "INFO:     127.0.0.1:52709 - \"POST /query HTTP/1.1\" 200 OK\n",
      "INFO:     127.0.0.1:52710 - \"POST /query HTTP/1.1\" 200 OK\n",
      "INFO:     127.0.0.1:52711 - \"POST /query HTTP/1.1\" 200 OK\n",
      "INFO:     127.0.0.1:52712 - \"POST /query HTTP/1.1\" 200 OK\n",
      "INFO:     127.0.0.1:52713 - \"POST /query HTTP/1.1\" 200 OK\n",
      "INFO:     127.0.0.1:52714 - \"POST /query HTTP/1.1\" 200 OK\n",
      "INFO:     127.0.0.1:52715 - \"POST /query HTTP/1.1\" 200 OK\n",
      "INFO:     127.0.0.1:52716 - \"POST /query HTTP/1.1\" 200 OK\n",
      "INFO:     127.0.0.1:52717 - \"POST /query HTTP/1.1\" 200 OK\n",
      "INFO:     127.0.0.1:52718 - \"POST /query HTTP/1.1\" 200 OK\n",
      "INFO:     127.0.0.1:52719 - \"POST /query HTTP/1.1\" 200 OK\n",
      "INFO:     127.0.0.1:52720 - \"POST /query HTTP/1.1\" 200 OK\n",
      "INFO:     127.0.0.1:52721 - \"POST /query HTTP/1.1\" 200 OK\n",
      "INFO:     127.0.0.1:52722 - \"POST /query HTTP/1.1\" 200 OK\n",
      "INFO:     127.0.0.1:52723 - \"POST /query HTTP/1.1\" 200 OK\n",
      "INFO:     127.0.0.1:52724 - \"POST /query HTTP/1.1\" 200 OK\n",
      "INFO:     127.0.0.1:52725 - \"POST /query HTTP/1.1\" 200 OK\n",
      "INFO:     127.0.0.1:52727 - \"POST /query HTTP/1.1\" 200 OK\n",
      "INFO:     127.0.0.1:52730 - \"POST /query HTTP/1.1\" 200 OK\n",
      "INFO:     127.0.0.1:52731 - \"POST /query HTTP/1.1\" 200 OK\n",
      "INFO:     127.0.0.1:52732 - \"POST /query HTTP/1.1\" 200 OK\n",
      "INFO:     127.0.0.1:52733 - \"POST /query HTTP/1.1\" 200 OK\n",
      "INFO:     127.0.0.1:52764 - \"POST /query HTTP/1.1\" 400 Bad Request\n",
      "INFO:     127.0.0.1:52765 - \"POST /query HTTP/1.1\" 400 Bad Request\n",
      "INFO:     127.0.0.1:52766 - \"POST /query HTTP/1.1\" 400 Bad Request\n",
      "INFO:     127.0.0.1:52767 - \"POST /query HTTP/1.1\" 400 Bad Request\n",
      "INFO:     127.0.0.1:52768 - \"POST /query HTTP/1.1\" 400 Bad Request\n",
      "INFO:     127.0.0.1:52769 - \"POST /query HTTP/1.1\" 400 Bad Request\n",
      "INFO:     127.0.0.1:52770 - \"POST /query HTTP/1.1\" 400 Bad Request\n",
      "INFO:     127.0.0.1:52771 - \"POST /query HTTP/1.1\" 400 Bad Request\n",
      "INFO:     127.0.0.1:52772 - \"POST /query HTTP/1.1\" 400 Bad Request\n",
      "INFO:     127.0.0.1:52773 - \"POST /query HTTP/1.1\" 400 Bad Request\n",
      "INFO:     127.0.0.1:52774 - \"POST /query HTTP/1.1\" 400 Bad Request\n",
      "INFO:     127.0.0.1:52775 - \"POST /query HTTP/1.1\" 400 Bad Request\n",
      "INFO:     127.0.0.1:52776 - \"POST /query HTTP/1.1\" 400 Bad Request\n",
      "INFO:     127.0.0.1:52777 - \"POST /query HTTP/1.1\" 400 Bad Request\n",
      "INFO:     127.0.0.1:52778 - \"POST /query HTTP/1.1\" 400 Bad Request\n",
      "INFO:     127.0.0.1:52779 - \"POST /query HTTP/1.1\" 400 Bad Request\n",
      "INFO:     127.0.0.1:52780 - \"POST /query HTTP/1.1\" 400 Bad Request\n",
      "INFO:     127.0.0.1:52781 - \"POST /query HTTP/1.1\" 400 Bad Request\n",
      "INFO:     127.0.0.1:52782 - \"POST /query HTTP/1.1\" 400 Bad Request\n",
      "INFO:     127.0.0.1:52783 - \"POST /query HTTP/1.1\" 400 Bad Request\n",
      "INFO:     127.0.0.1:52784 - \"POST /query HTTP/1.1\" 400 Bad Request\n",
      "INFO:     127.0.0.1:52785 - \"POST /query HTTP/1.1\" 400 Bad Request\n",
      "INFO:     127.0.0.1:52786 - \"POST /query HTTP/1.1\" 400 Bad Request\n",
      "INFO:     127.0.0.1:52787 - \"POST /query HTTP/1.1\" 400 Bad Request\n",
      "INFO:     127.0.0.1:52788 - \"POST /query HTTP/1.1\" 400 Bad Request\n",
      "INFO:     127.0.0.1:52789 - \"POST /query HTTP/1.1\" 400 Bad Request\n",
      "INFO:     127.0.0.1:52790 - \"POST /query HTTP/1.1\" 400 Bad Request\n",
      "INFO:     127.0.0.1:52791 - \"POST /query HTTP/1.1\" 400 Bad Request\n",
      "INFO:     127.0.0.1:52797 - \"POST /feedback HTTP/1.1\" 200 OK\n",
      "INFO:     127.0.0.1:52799 - \"POST /phase4/run?namespace=demo HTTP/1.1\" 200 OK\n",
      "INFO:     127.0.0.1:52800 - \"GET /governor/status HTTP/1.1\" 200 OK\n",
      "INFO:     127.0.0.1:52801 - \"POST /governor/mode?enable_ce=True&enable_adapters=True HTTP/1.1\" 200 OK\n",
      "INFO:     127.0.0.1:52802 - \"POST /meta/simulate HTTP/1.1\" 200 OK\n",
      "INFO:     127.0.0.1:52803 - \"GET /registry/challengers HTTP/1.1\" 200 OK\n",
      "INFO:     127.0.0.1:52805 - \"POST /registry/promote?job_id=%3CJOB_ID%3E HTTP/1.1\" 200 OK\n",
      "INFO:     127.0.0.1:52806 - \"POST /registry/drop?job_id=%3CJOB_ID%3E HTTP/1.1\" 200 OK\n",
      "INFO:     127.0.0.1:52807 - \"POST /query HTTP/1.1\" 200 OK\n",
      "INFO:     127.0.0.1:52808 - \"POST /query HTTP/1.1\" 200 OK\n",
      "INFO:     127.0.0.1:52809 - \"POST /query HTTP/1.1\" 200 OK\n",
      "INFO:     127.0.0.1:52810 - \"POST /query HTTP/1.1\" 200 OK\n",
      "INFO:     127.0.0.1:52811 - \"POST /query HTTP/1.1\" 200 OK\n",
      "INFO:     127.0.0.1:52812 - \"POST /query HTTP/1.1\" 200 OK\n",
      "INFO:     127.0.0.1:52813 - \"POST /query HTTP/1.1\" 200 OK\n",
      "INFO:     127.0.0.1:52814 - \"POST /query HTTP/1.1\" 200 OK\n",
      "INFO:     127.0.0.1:52815 - \"POST /query HTTP/1.1\" 200 OK\n",
      "INFO:     127.0.0.1:52816 - \"POST /query HTTP/1.1\" 200 OK\n",
      "INFO:     127.0.0.1:52817 - \"POST /query HTTP/1.1\" 200 OK\n",
      "INFO:     127.0.0.1:52818 - \"POST /query HTTP/1.1\" 200 OK\n",
      "INFO:     127.0.0.1:52819 - \"POST /query HTTP/1.1\" 200 OK\n",
      "INFO:     127.0.0.1:52820 - \"POST /query HTTP/1.1\" 200 OK\n",
      "INFO:     127.0.0.1:52821 - \"POST /query HTTP/1.1\" 200 OK\n",
      "INFO:     127.0.0.1:52822 - \"POST /query HTTP/1.1\" 200 OK\n",
      "INFO:     127.0.0.1:52823 - \"POST /query HTTP/1.1\" 200 OK\n",
      "INFO:     127.0.0.1:52824 - \"POST /query HTTP/1.1\" 200 OK\n",
      "INFO:     127.0.0.1:52825 - \"POST /query HTTP/1.1\" 200 OK\n",
      "INFO:     127.0.0.1:52826 - \"POST /query HTTP/1.1\" 200 OK\n",
      "INFO:     127.0.0.1:52827 - \"POST /upload HTTP/1.1\" 200 OK\n",
      "INFO:     127.0.0.1:52829 - \"POST /query HTTP/1.1\" 200 OK\n",
      "INFO:     127.0.0.1:52830 - \"POST /query HTTP/1.1\" 200 OK\n",
      "INFO:     127.0.0.1:52832 - \"POST /query HTTP/1.1\" 200 OK\n",
      "INFO:     127.0.0.1:52833 - \"POST /query HTTP/1.1\" 200 OK\n",
      "INFO:     127.0.0.1:52834 - \"POST /query HTTP/1.1\" 200 OK\n",
      "INFO:     127.0.0.1:52835 - \"POST /query HTTP/1.1\" 200 OK\n",
      "INFO:     127.0.0.1:52836 - \"POST /query HTTP/1.1\" 200 OK\n",
      "INFO:     127.0.0.1:52837 - \"POST /query HTTP/1.1\" 200 OK\n",
      "INFO:     127.0.0.1:52838 - \"POST /query HTTP/1.1\" 200 OK\n",
      "INFO:     127.0.0.1:52839 - \"POST /query HTTP/1.1\" 200 OK\n",
      "INFO:     127.0.0.1:52841 - \"POST /query HTTP/1.1\" 200 OK\n",
      "INFO:     127.0.0.1:52840 - \"POST /query HTTP/1.1\" 200 OK\n",
      "INFO:     127.0.0.1:52842 - \"POST /query HTTP/1.1\" 200 OK\n",
      "INFO:     127.0.0.1:52843 - \"POST /query HTTP/1.1\" 200 OK\n",
      "INFO:     127.0.0.1:52844 - \"POST /query HTTP/1.1\" 200 OK\n",
      "INFO:     127.0.0.1:52845 - \"POST /query HTTP/1.1\" 200 OK\n",
      "INFO:     127.0.0.1:52846 - \"POST /query HTTP/1.1\" 200 OK\n",
      "INFO:     127.0.0.1:52847 - \"POST /query HTTP/1.1\" 200 OK\n",
      "INFO:     127.0.0.1:52848 - \"POST /query HTTP/1.1\" 200 OK\n",
      "INFO:     127.0.0.1:52849 - \"POST /query HTTP/1.1\" 200 OK\n",
      "INFO:     127.0.0.1:52850 - \"POST /query HTTP/1.1\" 200 OK\n",
      "INFO:     127.0.0.1:52851 - \"POST /query HTTP/1.1\" 200 OK\n",
      "INFO:     127.0.0.1:52852 - \"POST /query HTTP/1.1\" 200 OK\n",
      "INFO:     127.0.0.1:52853 - \"POST /query HTTP/1.1\" 200 OK\n",
      "INFO:     127.0.0.1:52854 - \"POST /query HTTP/1.1\" 200 OK\n",
      "INFO:     127.0.0.1:52856 - \"POST /query HTTP/1.1\" 200 OK\n",
      "INFO:     127.0.0.1:52855 - \"POST /query HTTP/1.1\" 200 OK\n",
      "INFO:     127.0.0.1:52857 - \"POST /query HTTP/1.1\" 200 OK\n",
      "INFO:     127.0.0.1:52864 - \"POST /query HTTP/1.1\" 200 OK\n",
      "INFO:     127.0.0.1:52859 - \"POST /query HTTP/1.1\" 200 OK\n",
      "INFO:     127.0.0.1:52858 - \"POST /query HTTP/1.1\" 200 OK\n",
      "INFO:     127.0.0.1:52860 - \"POST /query HTTP/1.1\" 200 OK\n",
      "INFO:     127.0.0.1:52861 - \"POST /query HTTP/1.1\" 200 OK\n",
      "INFO:     127.0.0.1:52863 - \"POST /query HTTP/1.1\" 200 OK\n",
      "INFO:     127.0.0.1:52862 - \"POST /query HTTP/1.1\" 200 OK\n",
      "INFO:     127.0.0.1:52867 - \"POST /query HTTP/1.1\" 200 OK\n",
      "INFO:     127.0.0.1:52865 - \"POST /query HTTP/1.1\" 200 OK\n",
      "INFO:     127.0.0.1:52868 - \"POST /query HTTP/1.1\" 200 OK\n",
      "INFO:     127.0.0.1:52866 - \"POST /query HTTP/1.1\" 200 OK\n",
      "INFO:     127.0.0.1:52870 - \"POST /query HTTP/1.1\" 200 OK\n",
      "INFO:     127.0.0.1:52869 - \"POST /query HTTP/1.1\" 200 OK\n",
      "INFO:     127.0.0.1:52874 - \"POST /query HTTP/1.1\" 200 OK\n",
      "INFO:     127.0.0.1:52873 - \"POST /query HTTP/1.1\" 200 OK\n",
      "INFO:     127.0.0.1:52871 - \"POST /query HTTP/1.1\" 200 OK\n",
      "INFO:     127.0.0.1:52872 - \"POST /query HTTP/1.1\" 200 OK\n",
      "INFO:     127.0.0.1:52875 - \"POST /query HTTP/1.1\" 200 OK\n",
      "INFO:     127.0.0.1:52876 - \"POST /query HTTP/1.1\" 200 OK\n",
      "INFO:     127.0.0.1:52877 - \"POST /query HTTP/1.1\" 200 OK\n",
      "INFO:     127.0.0.1:52878 - \"POST /query HTTP/1.1\" 200 OK\n",
      "INFO:     127.0.0.1:52879 - \"POST /query HTTP/1.1\" 200 OK\n",
      "INFO:     127.0.0.1:52883 - \"POST /query HTTP/1.1\" 200 OK\n",
      "INFO:     127.0.0.1:52884 - \"POST /query HTTP/1.1\" 200 OK\n",
      "INFO:     127.0.0.1:52882 - \"POST /query HTTP/1.1\" 200 OK\n",
      "INFO:     127.0.0.1:52880 - \"POST /query HTTP/1.1\" 200 OK\n",
      "INFO:     127.0.0.1:52881 - \"POST /query HTTP/1.1\" 200 OK\n",
      "INFO:     127.0.0.1:52885 - \"POST /query HTTP/1.1\" 200 OK\n",
      "INFO:     127.0.0.1:52886 - \"POST /query HTTP/1.1\" 200 OK\n",
      "INFO:     127.0.0.1:52888 - \"POST /query HTTP/1.1\" 200 OK\n",
      "INFO:     127.0.0.1:52894 - \"POST /query HTTP/1.1\" 200 OK\n",
      "INFO:     127.0.0.1:52893 - \"POST /query HTTP/1.1\" 200 OK\n",
      "INFO:     127.0.0.1:52890 - \"POST /query HTTP/1.1\" 200 OK\n",
      "INFO:     127.0.0.1:52892 - \"POST /query HTTP/1.1\" 200 OK\n",
      "INFO:     127.0.0.1:52887 - \"POST /query HTTP/1.1\" 200 OK\n",
      "INFO:     127.0.0.1:52891 - \"POST /query HTTP/1.1\" 200 OK\n",
      "INFO:     127.0.0.1:52889 - \"POST /query HTTP/1.1\" 200 OK\n",
      "INFO:     127.0.0.1:52895 - \"POST /query HTTP/1.1\" 200 OK\n",
      "INFO:     127.0.0.1:52904 - \"POST /query HTTP/1.1\" 200 OK\n",
      "INFO:     127.0.0.1:52901 - \"POST /query HTTP/1.1\" 200 OK\n",
      "INFO:     127.0.0.1:52897 - \"POST /query HTTP/1.1\" 200 OK\n",
      "INFO:     127.0.0.1:52902 - \"POST /query HTTP/1.1\" 200 OK\n",
      "INFO:     127.0.0.1:52903 - \"POST /query HTTP/1.1\" 200 OK\n",
      "INFO:     127.0.0.1:52896 - \"POST /query HTTP/1.1\" 200 OK\n",
      "INFO:     127.0.0.1:52899 - \"POST /query HTTP/1.1\" 200 OK\n",
      "INFO:     127.0.0.1:52900 - \"POST /query HTTP/1.1\" 200 OK\n",
      "INFO:     127.0.0.1:52898 - \"POST /query HTTP/1.1\" 200 OK\n",
      "INFO:     127.0.0.1:52905 - \"POST /query HTTP/1.1\" 200 OK\n",
      "INFO:     127.0.0.1:52912 - \"POST /query HTTP/1.1\" 200 OK\n",
      "INFO:     127.0.0.1:52910 - \"POST /query HTTP/1.1\" 200 OK\n",
      "INFO:     127.0.0.1:52907 - \"POST /query HTTP/1.1\" 200 OK\n",
      "INFO:     127.0.0.1:52906 - \"POST /query HTTP/1.1\" 200 OK\n",
      "INFO:     127.0.0.1:52913 - \"POST /query HTTP/1.1\" 200 OK\n",
      "INFO:     127.0.0.1:52908 - \"POST /query HTTP/1.1\" 200 OK\n",
      "INFO:     127.0.0.1:52909 - \"POST /query HTTP/1.1\" 200 OK\n",
      "INFO:     127.0.0.1:52914 - \"POST /query HTTP/1.1\" 200 OK\n",
      "INFO:     127.0.0.1:52911 - \"POST /query HTTP/1.1\" 200 OK\n",
      "INFO:     127.0.0.1:52915 - \"POST /query HTTP/1.1\" 200 OK\n",
      "INFO:     127.0.0.1:52917 - \"POST /query HTTP/1.1\" 200 OK\n",
      "INFO:     127.0.0.1:52916 - \"POST /query HTTP/1.1\" 200 OK\n",
      "INFO:     127.0.0.1:52923 - \"POST /query HTTP/1.1\" 200 OK\n",
      "INFO:     127.0.0.1:52919 - \"POST /query HTTP/1.1\" 200 OK\n",
      "INFO:     127.0.0.1:52922 - \"POST /query HTTP/1.1\" 200 OK\n",
      "INFO:     127.0.0.1:52920 - \"POST /query HTTP/1.1\" 200 OK\n",
      "INFO:     127.0.0.1:52921 - \"POST /query HTTP/1.1\" 200 OK\n",
      "INFO:     127.0.0.1:52924 - \"POST /query HTTP/1.1\" 200 OK\n",
      "INFO:     127.0.0.1:52918 - \"POST /query HTTP/1.1\" 200 OK\n",
      "INFO:     127.0.0.1:52926 - \"POST /query HTTP/1.1\" 200 OK\n",
      "INFO:     127.0.0.1:52925 - \"POST /query HTTP/1.1\" 200 OK\n",
      "INFO:     127.0.0.1:52927 - \"POST /query HTTP/1.1\" 200 OK\n",
      "INFO:     127.0.0.1:52928 - \"POST /query HTTP/1.1\" 200 OK\n",
      "INFO:     127.0.0.1:52934 - \"POST /query HTTP/1.1\" 200 OK\n",
      "INFO:     127.0.0.1:52931 - \"POST /query HTTP/1.1\" 200 OK\n",
      "INFO:     127.0.0.1:52929 - \"POST /query HTTP/1.1\" 200 OK\n",
      "INFO:     127.0.0.1:52933 - \"POST /query HTTP/1.1\" 200 OK\n",
      "INFO:     127.0.0.1:52930 - \"POST /query HTTP/1.1\" 200 OK\n",
      "INFO:     127.0.0.1:52932 - \"POST /query HTTP/1.1\" 200 OK\n",
      "INFO:     127.0.0.1:52935 - \"POST /query HTTP/1.1\" 200 OK\n",
      "INFO:     127.0.0.1:52936 - \"POST /query HTTP/1.1\" 200 OK\n",
      "INFO:     127.0.0.1:52940 - \"POST /query HTTP/1.1\" 200 OK\n",
      "INFO:     127.0.0.1:52939 - \"POST /query HTTP/1.1\" 200 OK\n",
      "INFO:     127.0.0.1:52938 - \"POST /query HTTP/1.1\" 200 OK\n",
      "INFO:     127.0.0.1:52941 - \"POST /query HTTP/1.1\" 200 OK\n",
      "INFO:     127.0.0.1:52943 - \"POST /query HTTP/1.1\" 200 OK\n",
      "INFO:     127.0.0.1:52944 - \"POST /query HTTP/1.1\" 200 OK\n",
      "INFO:     127.0.0.1:52937 - \"POST /query HTTP/1.1\" 200 OK\n",
      "INFO:     127.0.0.1:52942 - \"POST /query HTTP/1.1\" 200 OK\n",
      "INFO:     127.0.0.1:52945 - \"POST /query HTTP/1.1\" 200 OK\n",
      "INFO:     127.0.0.1:52946 - \"POST /query HTTP/1.1\" 200 OK\n",
      "INFO:     127.0.0.1:52954 - \"POST /query HTTP/1.1\" 200 OK\n",
      "INFO:     127.0.0.1:52953 - \"POST /query HTTP/1.1\" 200 OK\n",
      "INFO:     127.0.0.1:52951 - \"POST /query HTTP/1.1\" 200 OK\n",
      "INFO:     127.0.0.1:52948 - \"POST /query HTTP/1.1\" 200 OK\n",
      "INFO:     127.0.0.1:52949 - \"POST /query HTTP/1.1\" 200 OK\n",
      "INFO:     127.0.0.1:52947 - \"POST /query HTTP/1.1\" 200 OK\n",
      "INFO:     127.0.0.1:52952 - \"POST /query HTTP/1.1\" 200 OK\n",
      "INFO:     127.0.0.1:52950 - \"POST /query HTTP/1.1\" 200 OK\n",
      "INFO:     127.0.0.1:52955 - \"POST /query HTTP/1.1\" 200 OK\n",
      "INFO:     127.0.0.1:52956 - \"POST /query HTTP/1.1\" 200 OK\n",
      "INFO:     127.0.0.1:52958 - \"POST /query HTTP/1.1\" 200 OK\n",
      "INFO:     127.0.0.1:52964 - \"POST /query HTTP/1.1\" 200 OK\n",
      "INFO:     127.0.0.1:52959 - \"POST /query HTTP/1.1\" 200 OK\n",
      "INFO:     127.0.0.1:52963 - \"POST /query HTTP/1.1\" 200 OK\n",
      "INFO:     127.0.0.1:52957 - \"POST /query HTTP/1.1\" 200 OK\n",
      "INFO:     127.0.0.1:52962 - \"POST /query HTTP/1.1\" 200 OK\n",
      "INFO:     127.0.0.1:52960 - \"POST /query HTTP/1.1\" 200 OK\n",
      "INFO:     127.0.0.1:52961 - \"POST /query HTTP/1.1\" 200 OK\n",
      "INFO:     127.0.0.1:52965 - \"POST /query HTTP/1.1\" 200 OK\n",
      "INFO:     127.0.0.1:52967 - \"POST /query HTTP/1.1\" 200 OK\n",
      "INFO:     127.0.0.1:52968 - \"POST /query HTTP/1.1\" 200 OK\n",
      "INFO:     127.0.0.1:52966 - \"POST /query HTTP/1.1\" 200 OK\n",
      "INFO:     127.0.0.1:52969 - \"POST /query HTTP/1.1\" 200 OK\n",
      "INFO:     127.0.0.1:52971 - \"POST /query HTTP/1.1\" 200 OK\n",
      "INFO:     127.0.0.1:52970 - \"POST /query HTTP/1.1\" 200 OK\n",
      "INFO:     127.0.0.1:52974 - \"POST /query HTTP/1.1\" 200 OK\n",
      "INFO:     127.0.0.1:52973 - \"POST /query HTTP/1.1\" 200 OK\n",
      "INFO:     127.0.0.1:52972 - \"POST /query HTTP/1.1\" 200 OK\n",
      "INFO:     127.0.0.1:52975 - \"POST /query HTTP/1.1\" 200 OK\n",
      "INFO:     127.0.0.1:52976 - \"POST /query HTTP/1.1\" 200 OK\n",
      "INFO:     127.0.0.1:52982 - \"POST /query HTTP/1.1\" 200 OK\n",
      "INFO:     127.0.0.1:52980 - \"POST /query HTTP/1.1\" 200 OK\n",
      "INFO:     127.0.0.1:52984 - \"POST /query HTTP/1.1\" 200 OK\n",
      "INFO:     127.0.0.1:52979 - \"POST /query HTTP/1.1\" 200 OK\n",
      "INFO:     127.0.0.1:52981 - \"POST /query HTTP/1.1\" 200 OK\n",
      "INFO:     127.0.0.1:52978 - \"POST /query HTTP/1.1\" 200 OK\n",
      "INFO:     127.0.0.1:52983 - \"POST /query HTTP/1.1\" 200 OK\n",
      "INFO:     127.0.0.1:52977 - \"POST /query HTTP/1.1\" 200 OK\n",
      "INFO:     127.0.0.1:52985 - \"POST /query HTTP/1.1\" 200 OK\n",
      "INFO:     127.0.0.1:52986 - \"POST /query HTTP/1.1\" 200 OK\n",
      "INFO:     127.0.0.1:52988 - \"POST /query HTTP/1.1\" 200 OK\n",
      "INFO:     127.0.0.1:52987 - \"POST /query HTTP/1.1\" 200 OK\n",
      "INFO:     127.0.0.1:52991 - \"POST /query HTTP/1.1\" 200 OK\n",
      "INFO:     127.0.0.1:52990 - \"POST /query HTTP/1.1\" 200 OK\n",
      "INFO:     127.0.0.1:52994 - \"POST /query HTTP/1.1\" 200 OK\n",
      "INFO:     127.0.0.1:52993 - \"POST /query HTTP/1.1\" 200 OK\n",
      "INFO:     127.0.0.1:52989 - \"POST /query HTTP/1.1\" 200 OK\n",
      "INFO:     127.0.0.1:52992 - \"POST /query HTTP/1.1\" 200 OK\n",
      "INFO:     127.0.0.1:52995 - \"POST /query HTTP/1.1\" 200 OK\n",
      "INFO:     127.0.0.1:52996 - \"POST /query HTTP/1.1\" 200 OK\n",
      "INFO:     127.0.0.1:52997 - \"POST /query HTTP/1.1\" 200 OK\n",
      "INFO:     127.0.0.1:53002 - \"POST /query HTTP/1.1\" 200 OK\n",
      "INFO:     127.0.0.1:53003 - \"POST /query HTTP/1.1\" 200 OK\n",
      "INFO:     127.0.0.1:53001 - \"POST /query HTTP/1.1\" 200 OK\n",
      "INFO:     127.0.0.1:52999 - \"POST /query HTTP/1.1\" 200 OK\n",
      "INFO:     127.0.0.1:52998 - \"POST /query HTTP/1.1\" 200 OK\n",
      "INFO:     127.0.0.1:53004 - \"POST /query HTTP/1.1\" 200 OK\n",
      "INFO:     127.0.0.1:53000 - \"POST /query HTTP/1.1\" 200 OK\n",
      "INFO:     127.0.0.1:53006 - \"POST /query HTTP/1.1\" 200 OK\n",
      "INFO:     127.0.0.1:53005 - \"POST /query HTTP/1.1\" 200 OK\n",
      "INFO:     127.0.0.1:53007 - \"POST /query HTTP/1.1\" 200 OK\n",
      "INFO:     127.0.0.1:53008 - \"POST /query HTTP/1.1\" 200 OK\n",
      "INFO:     127.0.0.1:53013 - \"POST /query HTTP/1.1\" 200 OK\n",
      "INFO:     127.0.0.1:53012 - \"POST /query HTTP/1.1\" 200 OK\n",
      "INFO:     127.0.0.1:53010 - \"POST /query HTTP/1.1\" 200 OK\n",
      "INFO:     127.0.0.1:53014 - \"POST /query HTTP/1.1\" 200 OK\n",
      "INFO:     127.0.0.1:53009 - \"POST /query HTTP/1.1\" 200 OK\n",
      "INFO:     127.0.0.1:53011 - \"POST /query HTTP/1.1\" 200 OK\n",
      "INFO:     127.0.0.1:53015 - \"POST /query HTTP/1.1\" 200 OK\n",
      "INFO:     127.0.0.1:53022 - \"POST /query HTTP/1.1\" 200 OK\n",
      "INFO:     127.0.0.1:53017 - \"POST /query HTTP/1.1\" 200 OK\n",
      "INFO:     127.0.0.1:53018 - \"POST /query HTTP/1.1\" 200 OK\n",
      "INFO:     127.0.0.1:53016 - \"POST /query HTTP/1.1\" 200 OK\n",
      "INFO:     127.0.0.1:53023 - \"POST /query HTTP/1.1\" 200 OK\n",
      "INFO:     127.0.0.1:53020 - \"POST /query HTTP/1.1\" 200 OK\n",
      "INFO:     127.0.0.1:53019 - \"POST /query HTTP/1.1\" 200 OK\n",
      "INFO:     127.0.0.1:53024 - \"POST /query HTTP/1.1\" 200 OK\n",
      "INFO:     127.0.0.1:53021 - \"POST /query HTTP/1.1\" 200 OK\n",
      "INFO:     127.0.0.1:53025 - \"POST /query HTTP/1.1\" 200 OK\n",
      "INFO:     127.0.0.1:53026 - \"POST /query HTTP/1.1\" 200 OK\n",
      "INFO:     127.0.0.1:53034 - \"POST /query HTTP/1.1\" 200 OK\n",
      "INFO:     127.0.0.1:53031 - \"POST /query HTTP/1.1\" 200 OK\n",
      "INFO:     127.0.0.1:53032 - \"POST /query HTTP/1.1\" 200 OK\n",
      "INFO:     127.0.0.1:53033 - \"POST /query HTTP/1.1\" 200 OK\n",
      "INFO:     127.0.0.1:53028 - \"POST /query HTTP/1.1\" 200 OK\n",
      "INFO:     127.0.0.1:53027 - \"POST /query HTTP/1.1\" 200 OK\n",
      "INFO:     127.0.0.1:53029 - \"POST /query HTTP/1.1\" 200 OK\n",
      "INFO:     127.0.0.1:53030 - \"POST /query HTTP/1.1\" 200 OK\n",
      "INFO:     127.0.0.1:53035 - \"POST /query HTTP/1.1\" 200 OK\n",
      "INFO:     127.0.0.1:53036 - \"POST /query HTTP/1.1\" 200 OK\n",
      "INFO:     127.0.0.1:53044 - \"POST /query HTTP/1.1\" 200 OK\n",
      "INFO:     127.0.0.1:53038 - \"POST /query HTTP/1.1\" 200 OK\n",
      "INFO:     127.0.0.1:53037 - \"POST /query HTTP/1.1\" 200 OK\n",
      "INFO:     127.0.0.1:53042 - \"POST /query HTTP/1.1\" 200 OK\n",
      "INFO:     127.0.0.1:53043 - \"POST /query HTTP/1.1\" 200 OK\n",
      "INFO:     127.0.0.1:53040 - \"POST /query HTTP/1.1\" 200 OK\n",
      "INFO:     127.0.0.1:53041 - \"POST /query HTTP/1.1\" 200 OK\n",
      "INFO:     127.0.0.1:53039 - \"POST /query HTTP/1.1\" 200 OK\n",
      "INFO:     127.0.0.1:53045 - \"POST /query HTTP/1.1\" 200 OK\n",
      "INFO:     127.0.0.1:53047 - \"POST /query HTTP/1.1\" 200 OK\n",
      "INFO:     127.0.0.1:53046 - \"POST /query HTTP/1.1\" 200 OK\n",
      "INFO:     127.0.0.1:53048 - \"POST /query HTTP/1.1\" 200 OK\n",
      "INFO:     127.0.0.1:53053 - \"POST /query HTTP/1.1\" 200 OK\n",
      "INFO:     127.0.0.1:53052 - \"POST /query HTTP/1.1\" 200 OK\n",
      "INFO:     127.0.0.1:53054 - \"POST /query HTTP/1.1\" 200 OK\n",
      "INFO:     127.0.0.1:53049 - \"POST /query HTTP/1.1\" 200 OK\n",
      "INFO:     127.0.0.1:53051 - \"POST /query HTTP/1.1\" 200 OK\n",
      "INFO:     127.0.0.1:53050 - \"POST /query HTTP/1.1\" 200 OK\n",
      "INFO:     127.0.0.1:53055 - \"POST /query HTTP/1.1\" 200 OK\n",
      "INFO:     127.0.0.1:53056 - \"POST /query HTTP/1.1\" 200 OK\n",
      "INFO:     127.0.0.1:53057 - \"POST /query HTTP/1.1\" 200 OK\n",
      "INFO:     127.0.0.1:53062 - \"POST /query HTTP/1.1\" 200 OK\n",
      "INFO:     127.0.0.1:53061 - \"POST /query HTTP/1.1\" 200 OK\n",
      "INFO:     127.0.0.1:53059 - \"POST /query HTTP/1.1\" 200 OK\n",
      "INFO:     127.0.0.1:53058 - \"POST /query HTTP/1.1\" 200 OK\n",
      "INFO:     127.0.0.1:53063 - \"POST /query HTTP/1.1\" 200 OK\n",
      "INFO:     127.0.0.1:53064 - \"POST /query HTTP/1.1\" 200 OK\n",
      "INFO:     127.0.0.1:53060 - \"POST /query HTTP/1.1\" 200 OK\n",
      "INFO:     127.0.0.1:53066 - \"POST /query HTTP/1.1\" 200 OK\n",
      "INFO:     127.0.0.1:53065 - \"POST /query HTTP/1.1\" 200 OK\n",
      "INFO:     127.0.0.1:53071 - \"POST /query HTTP/1.1\" 200 OK\n",
      "INFO:     127.0.0.1:53072 - \"POST /query HTTP/1.1\" 200 OK\n",
      "INFO:     127.0.0.1:53073 - \"POST /query HTTP/1.1\" 200 OK\n",
      "INFO:     127.0.0.1:53070 - \"POST /query HTTP/1.1\" 200 OK\n",
      "INFO:     127.0.0.1:53068 - \"POST /query HTTP/1.1\" 200 OK\n",
      "INFO:     127.0.0.1:53067 - \"POST /query HTTP/1.1\" 200 OK\n",
      "INFO:     127.0.0.1:53074 - \"POST /query HTTP/1.1\" 200 OK\n",
      "INFO:     127.0.0.1:53069 - \"POST /query HTTP/1.1\" 200 OK\n",
      "INFO:     127.0.0.1:53075 - \"POST /query HTTP/1.1\" 200 OK\n",
      "INFO:     127.0.0.1:53076 - \"POST /query HTTP/1.1\" 200 OK\n",
      "INFO:     127.0.0.1:53084 - \"POST /query HTTP/1.1\" 200 OK\n",
      "INFO:     127.0.0.1:53080 - \"POST /query HTTP/1.1\" 200 OK\n",
      "INFO:     127.0.0.1:53079 - \"POST /query HTTP/1.1\" 200 OK\n",
      "INFO:     127.0.0.1:53077 - \"POST /query HTTP/1.1\" 200 OK\n",
      "INFO:     127.0.0.1:53078 - \"POST /query HTTP/1.1\" 200 OK\n",
      "INFO:     127.0.0.1:53082 - \"POST /query HTTP/1.1\" 200 OK\n",
      "INFO:     127.0.0.1:53083 - \"POST /query HTTP/1.1\" 200 OK\n",
      "INFO:     127.0.0.1:53081 - \"POST /query HTTP/1.1\" 200 OK\n",
      "INFO:     127.0.0.1:53085 - \"POST /query HTTP/1.1\" 200 OK\n",
      "INFO:     127.0.0.1:53088 - \"POST /query HTTP/1.1\" 200 OK\n",
      "INFO:     127.0.0.1:53089 - \"POST /query HTTP/1.1\" 200 OK\n",
      "INFO:     127.0.0.1:53090 - \"POST /query HTTP/1.1\" 200 OK\n",
      "INFO:     127.0.0.1:53091 - \"POST /query HTTP/1.1\" 200 OK\n",
      "INFO:     127.0.0.1:53093 - \"POST /query HTTP/1.1\" 200 OK\n",
      "INFO:     127.0.0.1:53086 - \"POST /query HTTP/1.1\" 200 OK\n",
      "INFO:     127.0.0.1:53092 - \"POST /query HTTP/1.1\" 200 OK\n",
      "INFO:     127.0.0.1:53094 - \"POST /query HTTP/1.1\" 200 OK\n",
      "INFO:     127.0.0.1:53087 - \"POST /query HTTP/1.1\" 200 OK\n",
      "INFO:     127.0.0.1:53095 - \"POST /query HTTP/1.1\" 200 OK\n",
      "INFO:     127.0.0.1:53096 - \"POST /query HTTP/1.1\" 200 OK\n",
      "INFO:     127.0.0.1:53099 - \"POST /query HTTP/1.1\" 200 OK\n",
      "INFO:     127.0.0.1:53102 - \"POST /query HTTP/1.1\" 200 OK\n",
      "INFO:     127.0.0.1:53101 - \"POST /query HTTP/1.1\" 200 OK\n",
      "INFO:     127.0.0.1:53100 - \"POST /query HTTP/1.1\" 200 OK\n",
      "INFO:     127.0.0.1:53098 - \"POST /query HTTP/1.1\" 200 OK\n",
      "INFO:     127.0.0.1:53103 - \"POST /query HTTP/1.1\" 200 OK\n",
      "INFO:     127.0.0.1:53097 - \"POST /query HTTP/1.1\" 200 OK\n",
      "INFO:     127.0.0.1:53104 - \"POST /query HTTP/1.1\" 200 OK\n",
      "INFO:     127.0.0.1:53105 - \"POST /query HTTP/1.1\" 200 OK\n",
      "INFO:     127.0.0.1:53106 - \"POST /query HTTP/1.1\" 200 OK\n",
      "INFO:     127.0.0.1:53107 - \"POST /query HTTP/1.1\" 200 OK\n",
      "INFO:     127.0.0.1:53109 - \"POST /query HTTP/1.1\" 200 OK\n",
      "INFO:     127.0.0.1:53108 - \"POST /query HTTP/1.1\" 200 OK\n",
      "INFO:     127.0.0.1:53110 - \"POST /query HTTP/1.1\" 200 OK\n",
      "INFO:     127.0.0.1:53112 - \"POST /query HTTP/1.1\" 200 OK\n",
      "INFO:     127.0.0.1:53114 - \"POST /query HTTP/1.1\" 200 OK\n",
      "INFO:     127.0.0.1:53111 - \"POST /query HTTP/1.1\" 200 OK\n",
      "INFO:     127.0.0.1:53113 - \"POST /query HTTP/1.1\" 200 OK\n",
      "INFO:     127.0.0.1:53117 - \"POST /query HTTP/1.1\" 200 OK\n",
      "INFO:     127.0.0.1:53115 - \"POST /query HTTP/1.1\" 200 OK\n",
      "INFO:     127.0.0.1:53116 - \"POST /query HTTP/1.1\" 200 OK\n",
      "INFO:     127.0.0.1:53120 - \"POST /query HTTP/1.1\" 200 OK\n",
      "INFO:     127.0.0.1:53119 - \"POST /query HTTP/1.1\" 200 OK\n",
      "INFO:     127.0.0.1:53118 - \"POST /query HTTP/1.1\" 200 OK\n",
      "INFO:     127.0.0.1:53124 - \"POST /query HTTP/1.1\" 200 OK\n",
      "INFO:     127.0.0.1:53121 - \"POST /query HTTP/1.1\" 200 OK\n",
      "INFO:     127.0.0.1:53122 - \"POST /query HTTP/1.1\" 200 OK\n",
      "INFO:     127.0.0.1:53123 - \"POST /query HTTP/1.1\" 200 OK\n",
      "INFO:     127.0.0.1:53125 - \"POST /query HTTP/1.1\" 200 OK\n",
      "INFO:     127.0.0.1:53126 - \"POST /query HTTP/1.1\" 200 OK\n",
      "INFO:     127.0.0.1:53131 - \"POST /query HTTP/1.1\" 200 OK\n",
      "INFO:     127.0.0.1:53128 - \"POST /query HTTP/1.1\" 200 OK\n",
      "INFO:     127.0.0.1:53130 - \"POST /query HTTP/1.1\" 200 OK\n",
      "INFO:     127.0.0.1:53127 - \"POST /query HTTP/1.1\" 200 OK\n",
      "INFO:     127.0.0.1:53129 - \"POST /query HTTP/1.1\" 200 OK\n",
      "INFO:     127.0.0.1:53134 - \"POST /query HTTP/1.1\" 200 OK\n",
      "INFO:     127.0.0.1:53133 - \"POST /query HTTP/1.1\" 200 OK\n",
      "INFO:     127.0.0.1:53132 - \"POST /query HTTP/1.1\" 200 OK\n",
      "INFO:     127.0.0.1:53135 - \"POST /query HTTP/1.1\" 200 OK\n",
      "INFO:     127.0.0.1:53136 - \"POST /query HTTP/1.1\" 200 OK\n",
      "INFO:     127.0.0.1:53137 - \"POST /query HTTP/1.1\" 200 OK\n",
      "INFO:     127.0.0.1:53142 - \"POST /query HTTP/1.1\" 200 OK\n",
      "INFO:     127.0.0.1:53144 - \"POST /query HTTP/1.1\" 200 OK\n",
      "INFO:     127.0.0.1:53138 - \"POST /query HTTP/1.1\" 200 OK\n",
      "INFO:     127.0.0.1:53141 - \"POST /query HTTP/1.1\" 200 OK\n",
      "INFO:     127.0.0.1:53143 - \"POST /query HTTP/1.1\" 200 OK\n",
      "INFO:     127.0.0.1:53140 - \"POST /query HTTP/1.1\" 200 OK\n",
      "INFO:     127.0.0.1:53139 - \"POST /query HTTP/1.1\" 200 OK\n",
      "INFO:     127.0.0.1:53147 - \"POST /query HTTP/1.1\" 200 OK\n",
      "INFO:     127.0.0.1:53146 - \"POST /query HTTP/1.1\" 200 OK\n",
      "INFO:     127.0.0.1:53145 - \"POST /query HTTP/1.1\" 200 OK\n",
      "INFO:     127.0.0.1:53150 - \"POST /query HTTP/1.1\" 200 OK\n",
      "INFO:     127.0.0.1:53154 - \"POST /query HTTP/1.1\" 200 OK\n",
      "INFO:     127.0.0.1:53152 - \"POST /query HTTP/1.1\" 200 OK\n",
      "INFO:     127.0.0.1:53151 - \"POST /query HTTP/1.1\" 200 OK\n",
      "INFO:     127.0.0.1:53148 - \"POST /query HTTP/1.1\" 200 OK\n",
      "INFO:     127.0.0.1:53149 - \"POST /query HTTP/1.1\" 200 OK\n",
      "INFO:     127.0.0.1:53153 - \"POST /query HTTP/1.1\" 200 OK\n",
      "INFO:     127.0.0.1:53155 - \"POST /governor/mode?enable_ce=False&enable_adapters=False HTTP/1.1\" 200 OK\n",
      "INFO:     127.0.0.1:53156 - \"POST /query HTTP/1.1\" 200 OK\n",
      "INFO:     127.0.0.1:53157 - \"POST /query HTTP/1.1\" 200 OK\n",
      "INFO:     127.0.0.1:53161 - \"POST /query HTTP/1.1\" 200 OK\n",
      "INFO:     127.0.0.1:53165 - \"POST /query HTTP/1.1\" 200 OK\n",
      "INFO:     127.0.0.1:53164 - \"POST /query HTTP/1.1\" 200 OK\n",
      "INFO:     127.0.0.1:53163 - \"POST /query HTTP/1.1\" 200 OK\n",
      "INFO:     127.0.0.1:53160 - \"POST /query HTTP/1.1\" 200 OK\n",
      "INFO:     127.0.0.1:53158 - \"POST /query HTTP/1.1\" 200 OK\n",
      "INFO:     127.0.0.1:53159 - \"POST /query HTTP/1.1\" 200 OK\n",
      "INFO:     127.0.0.1:53162 - \"POST /query HTTP/1.1\" 200 OK\n",
      "INFO:     127.0.0.1:53167 - \"POST /query HTTP/1.1\" 200 OK\n",
      "INFO:     127.0.0.1:53168 - \"POST /query HTTP/1.1\" 200 OK\n",
      "INFO:     127.0.0.1:53170 - \"POST /query HTTP/1.1\" 200 OK\n",
      "INFO:     127.0.0.1:53169 - \"POST /query HTTP/1.1\" 200 OK\n",
      "INFO:     127.0.0.1:53174 - \"POST /query HTTP/1.1\" 200 OK\n",
      "INFO:     127.0.0.1:53173 - \"POST /query HTTP/1.1\" 200 OK\n",
      "INFO:     127.0.0.1:53171 - \"POST /query HTTP/1.1\" 200 OK\n",
      "INFO:     127.0.0.1:53172 - \"POST /query HTTP/1.1\" 200 OK\n",
      "INFO:     127.0.0.1:53176 - \"POST /query HTTP/1.1\" 200 OK\n",
      "INFO:     127.0.0.1:53175 - \"POST /query HTTP/1.1\" 200 OK\n",
      "INFO:     127.0.0.1:53177 - \"POST /query HTTP/1.1\" 200 OK\n",
      "INFO:     127.0.0.1:53181 - \"POST /query HTTP/1.1\" 200 OK\n",
      "INFO:     127.0.0.1:53180 - \"POST /query HTTP/1.1\" 200 OK\n",
      "INFO:     127.0.0.1:53183 - \"POST /query HTTP/1.1\" 200 OK\n",
      "INFO:     127.0.0.1:53186 - \"POST /query HTTP/1.1\" 200 OK\n",
      "INFO:     127.0.0.1:53185 - \"POST /query HTTP/1.1\" 200 OK\n",
      "INFO:     127.0.0.1:53184 - \"POST /query HTTP/1.1\" 200 OK\n",
      "INFO:     127.0.0.1:53179 - \"POST /query HTTP/1.1\" 200 OK\n",
      "INFO:     127.0.0.1:53178 - \"POST /query HTTP/1.1\" 200 OK\n",
      "INFO:     127.0.0.1:53182 - \"POST /query HTTP/1.1\" 200 OK\n",
      "INFO:     127.0.0.1:53187 - \"POST /query HTTP/1.1\" 200 OK\n",
      "INFO:     127.0.0.1:53188 - \"POST /query HTTP/1.1\" 200 OK\n",
      "INFO:     127.0.0.1:53194 - \"POST /query HTTP/1.1\" 200 OK\n",
      "INFO:     127.0.0.1:53195 - \"POST /query HTTP/1.1\" 200 OK\n",
      "INFO:     127.0.0.1:53196 - \"POST /query HTTP/1.1\" 200 OK\n",
      "INFO:     127.0.0.1:53193 - \"POST /query HTTP/1.1\" 200 OK\n",
      "INFO:     127.0.0.1:53192 - \"POST /query HTTP/1.1\" 200 OK\n",
      "INFO:     127.0.0.1:53191 - \"POST /query HTTP/1.1\" 200 OK\n",
      "INFO:     127.0.0.1:53189 - \"POST /query HTTP/1.1\" 200 OK\n",
      "INFO:     127.0.0.1:53190 - \"POST /query HTTP/1.1\" 200 OK\n",
      "INFO:     127.0.0.1:53200 - \"POST /query HTTP/1.1\" 200 OK\n",
      "INFO:     127.0.0.1:53197 - \"POST /query HTTP/1.1\" 200 OK\n",
      "INFO:     127.0.0.1:53198 - \"POST /query HTTP/1.1\" 200 OK\n",
      "INFO:     127.0.0.1:53199 - \"POST /query HTTP/1.1\" 200 OK\n",
      "INFO:     127.0.0.1:53201 - \"POST /query HTTP/1.1\" 200 OK\n",
      "INFO:     127.0.0.1:53202 - \"POST /query HTTP/1.1\" 200 OK\n",
      "INFO:     127.0.0.1:53205 - \"POST /query HTTP/1.1\" 200 OK\n",
      "INFO:     127.0.0.1:53206 - \"POST /query HTTP/1.1\" 200 OK\n",
      "INFO:     127.0.0.1:53204 - \"POST /query HTTP/1.1\" 200 OK\n",
      "INFO:     127.0.0.1:53203 - \"POST /query HTTP/1.1\" 200 OK\n",
      "INFO:     127.0.0.1:53207 - \"POST /query HTTP/1.1\" 200 OK\n",
      "INFO:     127.0.0.1:53209 - \"POST /query HTTP/1.1\" 200 OK\n",
      "INFO:     127.0.0.1:53208 - \"POST /query HTTP/1.1\" 200 OK\n",
      "INFO:     127.0.0.1:53213 - \"POST /query HTTP/1.1\" 200 OK\n",
      "INFO:     127.0.0.1:53210 - \"POST /query HTTP/1.1\" 200 OK\n",
      "INFO:     127.0.0.1:53214 - \"POST /query HTTP/1.1\" 200 OK\n",
      "INFO:     127.0.0.1:53215 - \"POST /query HTTP/1.1\" 200 OK\n",
      "INFO:     127.0.0.1:53212 - \"POST /query HTTP/1.1\" 200 OK\n",
      "INFO:     127.0.0.1:53211 - \"POST /query HTTP/1.1\" 200 OK\n",
      "INFO:     127.0.0.1:53216 - \"POST /query HTTP/1.1\" 200 OK\n",
      "INFO:     127.0.0.1:53217 - \"POST /query HTTP/1.1\" 200 OK\n",
      "INFO:     127.0.0.1:53218 - \"POST /query HTTP/1.1\" 200 OK\n",
      "INFO:     127.0.0.1:53223 - \"POST /query HTTP/1.1\" 200 OK\n",
      "INFO:     127.0.0.1:53225 - \"POST /query HTTP/1.1\" 200 OK\n",
      "INFO:     127.0.0.1:53220 - \"POST /query HTTP/1.1\" 200 OK\n",
      "INFO:     127.0.0.1:53224 - \"POST /query HTTP/1.1\" 200 OK\n",
      "INFO:     127.0.0.1:53222 - \"POST /query HTTP/1.1\" 200 OK\n",
      "INFO:     127.0.0.1:53226 - \"POST /query HTTP/1.1\" 200 OK\n",
      "INFO:     127.0.0.1:53221 - \"POST /query HTTP/1.1\" 200 OK\n",
      "INFO:     127.0.0.1:53219 - \"POST /query HTTP/1.1\" 200 OK\n",
      "INFO:     127.0.0.1:53228 - \"POST /query HTTP/1.1\" 200 OK\n",
      "INFO:     127.0.0.1:53227 - \"POST /query HTTP/1.1\" 200 OK\n",
      "INFO:     127.0.0.1:53229 - \"POST /query HTTP/1.1\" 200 OK\n",
      "INFO:     127.0.0.1:53233 - \"POST /query HTTP/1.1\" 200 OK\n",
      "INFO:     127.0.0.1:53230 - \"POST /query HTTP/1.1\" 200 OK\n",
      "INFO:     127.0.0.1:53232 - \"POST /query HTTP/1.1\" 200 OK\n",
      "INFO:     127.0.0.1:53234 - \"POST /query HTTP/1.1\" 200 OK\n",
      "INFO:     127.0.0.1:53235 - \"POST /query HTTP/1.1\" 200 OK\n",
      "INFO:     127.0.0.1:53231 - \"POST /query HTTP/1.1\" 200 OK\n",
      "INFO:     127.0.0.1:53236 - \"POST /query HTTP/1.1\" 200 OK\n",
      "INFO:     127.0.0.1:53237 - \"POST /query HTTP/1.1\" 200 OK\n",
      "INFO:     127.0.0.1:53239 - \"POST /query HTTP/1.1\" 200 OK\n",
      "INFO:     127.0.0.1:53238 - \"POST /query HTTP/1.1\" 200 OK\n",
      "INFO:     127.0.0.1:53245 - \"POST /query HTTP/1.1\" 200 OK\n",
      "INFO:     127.0.0.1:53246 - \"POST /query HTTP/1.1\" 200 OK\n",
      "INFO:     127.0.0.1:53244 - \"POST /query HTTP/1.1\" 200 OK\n",
      "INFO:     127.0.0.1:53240 - \"POST /query HTTP/1.1\" 200 OK\n",
      "INFO:     127.0.0.1:53243 - \"POST /query HTTP/1.1\" 200 OK\n",
      "INFO:     127.0.0.1:53241 - \"POST /query HTTP/1.1\" 200 OK\n",
      "INFO:     127.0.0.1:53242 - \"POST /query HTTP/1.1\" 200 OK\n",
      "INFO:     127.0.0.1:53248 - \"POST /query HTTP/1.1\" 200 OK\n",
      "INFO:     127.0.0.1:53247 - \"POST /query HTTP/1.1\" 200 OK\n",
      "INFO:     127.0.0.1:53255 - \"POST /query HTTP/1.1\" 200 OK\n",
      "INFO:     127.0.0.1:53252 - \"POST /query HTTP/1.1\" 200 OK\n",
      "INFO:     127.0.0.1:53254 - \"POST /query HTTP/1.1\" 200 OK\n",
      "INFO:     127.0.0.1:53253 - \"POST /query HTTP/1.1\" 200 OK\n",
      "INFO:     127.0.0.1:53256 - \"POST /query HTTP/1.1\" 200 OK\n",
      "INFO:     127.0.0.1:53249 - \"POST /query HTTP/1.1\" 200 OK\n",
      "INFO:     127.0.0.1:53250 - \"POST /query HTTP/1.1\" 200 OK\n",
      "INFO:     127.0.0.1:53251 - \"POST /query HTTP/1.1\" 200 OK\n",
      "INFO:     127.0.0.1:53257 - \"POST /query HTTP/1.1\" 200 OK\n",
      "INFO:     127.0.0.1:53258 - \"POST /query HTTP/1.1\" 200 OK\n",
      "INFO:     127.0.0.1:53259 - \"POST /query HTTP/1.1\" 200 OK\n",
      "INFO:     127.0.0.1:53260 - \"POST /query HTTP/1.1\" 200 OK\n",
      "INFO:     127.0.0.1:53263 - \"POST /query HTTP/1.1\" 200 OK\n",
      "INFO:     127.0.0.1:53265 - \"POST /query HTTP/1.1\" 200 OK\n",
      "INFO:     127.0.0.1:53262 - \"POST /query HTTP/1.1\" 200 OK\n",
      "INFO:     127.0.0.1:53264 - \"POST /query HTTP/1.1\" 200 OK\n",
      "INFO:     127.0.0.1:53261 - \"POST /query HTTP/1.1\" 200 OK\n",
      "INFO:     127.0.0.1:53266 - \"POST /query HTTP/1.1\" 200 OK\n",
      "INFO:     127.0.0.1:53267 - \"POST /query HTTP/1.1\" 200 OK\n",
      "INFO:     127.0.0.1:53268 - \"POST /query HTTP/1.1\" 200 OK\n",
      "INFO:     127.0.0.1:53275 - \"POST /query HTTP/1.1\" 200 OK\n",
      "INFO:     127.0.0.1:53273 - \"POST /query HTTP/1.1\" 200 OK\n",
      "INFO:     127.0.0.1:53269 - \"POST /query HTTP/1.1\" 200 OK\n",
      "INFO:     127.0.0.1:53271 - \"POST /query HTTP/1.1\" 200 OK\n",
      "INFO:     127.0.0.1:53276 - \"POST /query HTTP/1.1\" 200 OK\n",
      "INFO:     127.0.0.1:53272 - \"POST /query HTTP/1.1\" 200 OK\n",
      "INFO:     127.0.0.1:53274 - \"POST /query HTTP/1.1\" 200 OK\n",
      "INFO:     127.0.0.1:53270 - \"POST /query HTTP/1.1\" 200 OK\n",
      "INFO:     127.0.0.1:53277 - \"POST /query HTTP/1.1\" 200 OK\n",
      "INFO:     127.0.0.1:53278 - \"POST /query HTTP/1.1\" 200 OK\n",
      "INFO:     127.0.0.1:53279 - \"POST /query HTTP/1.1\" 200 OK\n",
      "INFO:     127.0.0.1:53280 - \"POST /query HTTP/1.1\" 200 OK\n",
      "INFO:     127.0.0.1:53286 - \"POST /query HTTP/1.1\" 200 OK\n",
      "INFO:     127.0.0.1:53282 - \"POST /query HTTP/1.1\" 200 OK\n",
      "INFO:     127.0.0.1:53284 - \"POST /query HTTP/1.1\" 200 OK\n",
      "INFO:     127.0.0.1:53281 - \"POST /query HTTP/1.1\" 200 OK\n",
      "INFO:     127.0.0.1:53283 - \"POST /query HTTP/1.1\" 200 OK\n",
      "INFO:     127.0.0.1:53285 - \"POST /query HTTP/1.1\" 200 OK\n",
      "INFO:     127.0.0.1:53287 - \"POST /query HTTP/1.1\" 200 OK\n",
      "INFO:     127.0.0.1:53288 - \"POST /query HTTP/1.1\" 200 OK\n",
      "INFO:     127.0.0.1:53289 - \"POST /query HTTP/1.1\" 200 OK\n",
      "INFO:     127.0.0.1:53293 - \"POST /query HTTP/1.1\" 200 OK\n",
      "INFO:     127.0.0.1:53294 - \"POST /query HTTP/1.1\" 200 OK\n",
      "INFO:     127.0.0.1:53295 - \"POST /query HTTP/1.1\" 200 OK\n",
      "INFO:     127.0.0.1:53290 - \"POST /query HTTP/1.1\" 200 OK\n",
      "INFO:     127.0.0.1:53292 - \"POST /query HTTP/1.1\" 200 OK\n",
      "INFO:     127.0.0.1:53291 - \"POST /query HTTP/1.1\" 200 OK\n",
      "INFO:     127.0.0.1:53296 - \"POST /query HTTP/1.1\" 200 OK\n",
      "INFO:     127.0.0.1:53297 - \"POST /query HTTP/1.1\" 200 OK\n",
      "INFO:     127.0.0.1:53298 - \"POST /query HTTP/1.1\" 200 OK\n",
      "INFO:     127.0.0.1:53303 - \"POST /query HTTP/1.1\" 200 OK\n",
      "INFO:     127.0.0.1:53299 - \"POST /query HTTP/1.1\" 200 OK\n",
      "INFO:     127.0.0.1:53304 - \"POST /query HTTP/1.1\" 200 OK\n",
      "INFO:     127.0.0.1:53301 - \"POST /query HTTP/1.1\" 200 OK\n",
      "INFO:     127.0.0.1:53306 - \"POST /query HTTP/1.1\" 200 OK\n",
      "INFO:     127.0.0.1:53300 - \"POST /query HTTP/1.1\" 200 OK\n",
      "INFO:     127.0.0.1:53305 - \"POST /query HTTP/1.1\" 200 OK\n",
      "INFO:     127.0.0.1:53302 - \"POST /query HTTP/1.1\" 200 OK\n",
      "INFO:     127.0.0.1:53307 - \"POST /query HTTP/1.1\" 200 OK\n",
      "INFO:     127.0.0.1:53308 - \"POST /query HTTP/1.1\" 200 OK\n",
      "INFO:     127.0.0.1:53315 - \"POST /query HTTP/1.1\" 200 OK\n",
      "INFO:     127.0.0.1:53313 - \"POST /query HTTP/1.1\" 200 OK\n",
      "INFO:     127.0.0.1:53309 - \"POST /query HTTP/1.1\" 200 OK\n",
      "INFO:     127.0.0.1:53316 - \"POST /query HTTP/1.1\" 200 OK\n",
      "INFO:     127.0.0.1:53312 - \"POST /query HTTP/1.1\" 200 OK\n",
      "INFO:     127.0.0.1:53311 - \"POST /query HTTP/1.1\" 200 OK\n",
      "INFO:     127.0.0.1:53314 - \"POST /query HTTP/1.1\" 200 OK\n",
      "INFO:     127.0.0.1:53310 - \"POST /query HTTP/1.1\" 200 OK\n",
      "INFO:     127.0.0.1:53317 - \"POST /query HTTP/1.1\" 200 OK\n",
      "INFO:     127.0.0.1:53323 - \"POST /query HTTP/1.1\" 200 OK\n",
      "INFO:     127.0.0.1:53322 - \"POST /query HTTP/1.1\" 200 OK\n",
      "INFO:     127.0.0.1:53326 - \"POST /query HTTP/1.1\" 200 OK\n",
      "INFO:     127.0.0.1:53318 - \"POST /query HTTP/1.1\" 200 OK\n",
      "INFO:     127.0.0.1:53324 - \"POST /query HTTP/1.1\" 200 OK\n",
      "INFO:     127.0.0.1:53321 - \"POST /query HTTP/1.1\" 200 OK\n",
      "INFO:     127.0.0.1:53319 - \"POST /query HTTP/1.1\" 200 OK\n",
      "INFO:     127.0.0.1:53325 - \"POST /query HTTP/1.1\" 200 OK\n",
      "INFO:     127.0.0.1:53320 - \"POST /query HTTP/1.1\" 200 OK\n",
      "INFO:     127.0.0.1:53327 - \"POST /query HTTP/1.1\" 200 OK\n",
      "INFO:     127.0.0.1:53333 - \"POST /query HTTP/1.1\" 200 OK\n",
      "INFO:     127.0.0.1:53330 - \"POST /query HTTP/1.1\" 200 OK\n",
      "INFO:     127.0.0.1:53331 - \"POST /query HTTP/1.1\" 200 OK\n",
      "INFO:     127.0.0.1:53332 - \"POST /query HTTP/1.1\" 200 OK\n",
      "INFO:     127.0.0.1:53328 - \"POST /query HTTP/1.1\" 200 OK\n",
      "INFO:     127.0.0.1:53335 - \"POST /query HTTP/1.1\" 200 OK\n",
      "INFO:     127.0.0.1:53334 - \"POST /query HTTP/1.1\" 200 OK\n",
      "INFO:     127.0.0.1:53329 - \"POST /query HTTP/1.1\" 200 OK\n",
      "INFO:     127.0.0.1:53336 - \"POST /query HTTP/1.1\" 200 OK\n",
      "INFO:     127.0.0.1:53337 - \"POST /query HTTP/1.1\" 200 OK\n",
      "INFO:     127.0.0.1:53338 - \"POST /query HTTP/1.1\" 200 OK\n",
      "INFO:     127.0.0.1:53339 - \"POST /query HTTP/1.1\" 200 OK\n",
      "INFO:     127.0.0.1:53340 - \"POST /query HTTP/1.1\" 200 OK\n",
      "INFO:     127.0.0.1:53341 - \"POST /query HTTP/1.1\" 200 OK\n",
      "INFO:     127.0.0.1:53344 - \"POST /query HTTP/1.1\" 200 OK\n",
      "INFO:     127.0.0.1:53346 - \"POST /query HTTP/1.1\" 200 OK\n",
      "INFO:     127.0.0.1:53342 - \"POST /query HTTP/1.1\" 200 OK\n",
      "INFO:     127.0.0.1:53345 - \"POST /query HTTP/1.1\" 200 OK\n",
      "INFO:     127.0.0.1:53343 - \"POST /query HTTP/1.1\" 200 OK\n",
      "INFO:     127.0.0.1:53348 - \"POST /query HTTP/1.1\" 200 OK\n",
      "INFO:     127.0.0.1:53347 - \"POST /query HTTP/1.1\" 200 OK\n",
      "INFO:     127.0.0.1:53349 - \"POST /query HTTP/1.1\" 200 OK\n",
      "INFO:     127.0.0.1:53350 - \"POST /query HTTP/1.1\" 200 OK\n",
      "INFO:     127.0.0.1:53352 - \"POST /query HTTP/1.1\" 200 OK\n",
      "INFO:     127.0.0.1:53355 - \"POST /query HTTP/1.1\" 200 OK\n",
      "INFO:     127.0.0.1:53356 - \"POST /query HTTP/1.1\" 200 OK\n",
      "INFO:     127.0.0.1:53354 - \"POST /query HTTP/1.1\" 200 OK\n",
      "INFO:     127.0.0.1:53353 - \"POST /query HTTP/1.1\" 200 OK\n",
      "INFO:     127.0.0.1:53351 - \"POST /query HTTP/1.1\" 200 OK\n",
      "INFO:     127.0.0.1:53358 - \"POST /query HTTP/1.1\" 200 OK\n",
      "INFO:     127.0.0.1:53357 - \"POST /query HTTP/1.1\" 200 OK\n",
      "INFO:     127.0.0.1:53362 - \"POST /query HTTP/1.1\" 200 OK\n",
      "INFO:     127.0.0.1:53361 - \"POST /query HTTP/1.1\" 200 OK\n",
      "INFO:     127.0.0.1:53365 - \"POST /query HTTP/1.1\" 200 OK\n",
      "INFO:     127.0.0.1:53359 - \"POST /query HTTP/1.1\" 200 OK\n",
      "INFO:     127.0.0.1:53363 - \"POST /query HTTP/1.1\" 200 OK\n",
      "INFO:     127.0.0.1:53366 - \"POST /query HTTP/1.1\" 200 OK\n",
      "INFO:     127.0.0.1:53364 - \"POST /query HTTP/1.1\" 200 OK\n",
      "INFO:     127.0.0.1:53360 - \"POST /query HTTP/1.1\" 200 OK\n",
      "INFO:     127.0.0.1:53368 - \"POST /query HTTP/1.1\" 200 OK\n",
      "INFO:     127.0.0.1:53367 - \"POST /query HTTP/1.1\" 200 OK\n",
      "INFO:     127.0.0.1:53369 - \"POST /query HTTP/1.1\" 200 OK\n",
      "INFO:     127.0.0.1:53370 - \"POST /query HTTP/1.1\" 200 OK\n",
      "INFO:     127.0.0.1:53373 - \"POST /query HTTP/1.1\" 200 OK\n",
      "INFO:     127.0.0.1:53376 - \"POST /query HTTP/1.1\" 200 OK\n",
      "INFO:     127.0.0.1:53374 - \"POST /query HTTP/1.1\" 200 OK\n",
      "INFO:     127.0.0.1:53371 - \"POST /query HTTP/1.1\" 200 OK\n",
      "INFO:     127.0.0.1:53372 - \"POST /query HTTP/1.1\" 200 OK\n",
      "INFO:     127.0.0.1:53375 - \"POST /query HTTP/1.1\" 200 OK\n",
      "INFO:     127.0.0.1:53378 - \"POST /query HTTP/1.1\" 200 OK\n",
      "INFO:     127.0.0.1:53377 - \"POST /query HTTP/1.1\" 200 OK\n",
      "INFO:     127.0.0.1:53379 - \"POST /query HTTP/1.1\" 200 OK\n",
      "INFO:     127.0.0.1:53380 - \"POST /query HTTP/1.1\" 200 OK\n",
      "INFO:     127.0.0.1:53385 - \"POST /query HTTP/1.1\" 200 OK\n",
      "INFO:     127.0.0.1:53383 - \"POST /query HTTP/1.1\" 200 OK\n",
      "INFO:     127.0.0.1:53384 - \"POST /query HTTP/1.1\" 200 OK\n",
      "INFO:     127.0.0.1:53382 - \"POST /query HTTP/1.1\" 200 OK\n",
      "INFO:     127.0.0.1:53386 - \"POST /query HTTP/1.1\" 200 OK\n",
      "INFO:     127.0.0.1:53381 - \"POST /query HTTP/1.1\" 200 OK\n",
      "INFO:     127.0.0.1:53387 - \"POST /query HTTP/1.1\" 200 OK\n",
      "INFO:     127.0.0.1:53388 - \"POST /query HTTP/1.1\" 200 OK\n",
      "INFO:     127.0.0.1:53390 - \"POST /query HTTP/1.1\" 200 OK\n",
      "INFO:     127.0.0.1:53389 - \"POST /query HTTP/1.1\" 200 OK\n",
      "INFO:     127.0.0.1:53395 - \"POST /query HTTP/1.1\" 200 OK\n",
      "INFO:     127.0.0.1:53393 - \"POST /query HTTP/1.1\" 200 OK\n",
      "INFO:     127.0.0.1:53396 - \"POST /query HTTP/1.1\" 200 OK\n",
      "INFO:     127.0.0.1:53392 - \"POST /query HTTP/1.1\" 200 OK\n",
      "INFO:     127.0.0.1:53391 - \"POST /query HTTP/1.1\" 200 OK\n",
      "INFO:     127.0.0.1:53394 - \"POST /query HTTP/1.1\" 200 OK\n",
      "INFO:     127.0.0.1:53398 - \"POST /query HTTP/1.1\" 200 OK\n",
      "INFO:     127.0.0.1:53397 - \"POST /query HTTP/1.1\" 200 OK\n",
      "INFO:     127.0.0.1:53400 - \"POST /query HTTP/1.1\" 200 OK\n",
      "INFO:     127.0.0.1:53405 - \"POST /query HTTP/1.1\" 200 OK\n",
      "INFO:     127.0.0.1:53402 - \"POST /query HTTP/1.1\" 200 OK\n",
      "INFO:     127.0.0.1:53401 - \"POST /query HTTP/1.1\" 200 OK\n",
      "INFO:     127.0.0.1:53406 - \"POST /query HTTP/1.1\" 200 OK\n",
      "INFO:     127.0.0.1:53404 - \"POST /query HTTP/1.1\" 200 OK\n",
      "INFO:     127.0.0.1:53399 - \"POST /query HTTP/1.1\" 200 OK\n",
      "INFO:     127.0.0.1:53403 - \"POST /query HTTP/1.1\" 200 OK\n",
      "INFO:     127.0.0.1:53407 - \"POST /query HTTP/1.1\" 200 OK\n",
      "INFO:     127.0.0.1:53408 - \"POST /query HTTP/1.1\" 200 OK\n",
      "INFO:     127.0.0.1:53409 - \"POST /query HTTP/1.1\" 200 OK\n",
      "INFO:     127.0.0.1:53415 - \"POST /query HTTP/1.1\" 200 OK\n",
      "INFO:     127.0.0.1:53411 - \"POST /query HTTP/1.1\" 200 OK\n",
      "INFO:     127.0.0.1:53412 - \"POST /query HTTP/1.1\" 200 OK\n",
      "INFO:     127.0.0.1:53416 - \"POST /query HTTP/1.1\" 200 OK\n",
      "INFO:     127.0.0.1:53414 - \"POST /query HTTP/1.1\" 200 OK\n",
      "INFO:     127.0.0.1:53413 - \"POST /query HTTP/1.1\" 200 OK\n",
      "INFO:     127.0.0.1:53410 - \"POST /query HTTP/1.1\" 200 OK\n",
      "INFO:     127.0.0.1:53417 - \"POST /query HTTP/1.1\" 200 OK\n",
      "INFO:     127.0.0.1:53418 - \"POST /query HTTP/1.1\" 200 OK\n",
      "INFO:     127.0.0.1:53421 - \"POST /query HTTP/1.1\" 200 OK\n",
      "INFO:     127.0.0.1:53419 - \"POST /query HTTP/1.1\" 200 OK\n",
      "INFO:     127.0.0.1:53420 - \"POST /query HTTP/1.1\" 200 OK\n",
      "INFO:     127.0.0.1:53426 - \"POST /query HTTP/1.1\" 200 OK\n",
      "INFO:     127.0.0.1:53424 - \"POST /query HTTP/1.1\" 200 OK\n",
      "INFO:     127.0.0.1:53425 - \"POST /query HTTP/1.1\" 200 OK\n",
      "INFO:     127.0.0.1:53422 - \"POST /query HTTP/1.1\" 200 OK\n",
      "INFO:     127.0.0.1:53423 - \"POST /query HTTP/1.1\" 200 OK\n",
      "INFO:     127.0.0.1:53428 - \"POST /query HTTP/1.1\" 200 OK\n",
      "INFO:     127.0.0.1:53427 - \"POST /query HTTP/1.1\" 200 OK\n",
      "INFO:     127.0.0.1:53435 - \"POST /query HTTP/1.1\" 200 OK\n",
      "INFO:     127.0.0.1:53433 - \"POST /query HTTP/1.1\" 200 OK\n",
      "INFO:     127.0.0.1:53431 - \"POST /query HTTP/1.1\" 200 OK\n",
      "INFO:     127.0.0.1:53432 - \"POST /query HTTP/1.1\" 200 OK\n",
      "INFO:     127.0.0.1:53436 - \"POST /query HTTP/1.1\" 200 OK\n",
      "INFO:     127.0.0.1:53430 - \"POST /query HTTP/1.1\" 200 OK\n",
      "INFO:     127.0.0.1:53429 - \"POST /query HTTP/1.1\" 200 OK\n",
      "INFO:     127.0.0.1:53434 - \"POST /query HTTP/1.1\" 200 OK\n",
      "INFO:     127.0.0.1:53437 - \"POST /query HTTP/1.1\" 200 OK\n",
      "INFO:     127.0.0.1:53438 - \"POST /query HTTP/1.1\" 200 OK\n",
      "INFO:     127.0.0.1:53443 - \"POST /query HTTP/1.1\" 200 OK\n",
      "INFO:     127.0.0.1:53441 - \"POST /query HTTP/1.1\" 200 OK\n",
      "INFO:     127.0.0.1:53445 - \"POST /query HTTP/1.1\" 200 OK\n",
      "INFO:     127.0.0.1:53442 - \"POST /query HTTP/1.1\" 200 OK\n",
      "INFO:     127.0.0.1:53446 - \"POST /query HTTP/1.1\" 200 OK\n",
      "INFO:     127.0.0.1:53444 - \"POST /query HTTP/1.1\" 200 OK\n",
      "INFO:     127.0.0.1:53440 - \"POST /query HTTP/1.1\" 200 OK\n",
      "INFO:     127.0.0.1:53439 - \"POST /query HTTP/1.1\" 200 OK\n",
      "INFO:     127.0.0.1:53447 - \"POST /query HTTP/1.1\" 200 OK\n",
      "INFO:     127.0.0.1:53449 - \"POST /query HTTP/1.1\" 200 OK\n",
      "INFO:     127.0.0.1:53448 - \"POST /query HTTP/1.1\" 200 OK\n",
      "INFO:     127.0.0.1:53456 - \"POST /query HTTP/1.1\" 200 OK\n",
      "INFO:     127.0.0.1:53455 - \"POST /query HTTP/1.1\" 200 OK\n",
      "INFO:     127.0.0.1:53451 - \"POST /query HTTP/1.1\" 200 OK\n",
      "INFO:     127.0.0.1:53454 - \"POST /query HTTP/1.1\" 200 OK\n",
      "INFO:     127.0.0.1:53453 - \"POST /query HTTP/1.1\" 200 OK\n",
      "INFO:     127.0.0.1:53450 - \"POST /query HTTP/1.1\" 200 OK\n",
      "INFO:     127.0.0.1:53452 - \"POST /query HTTP/1.1\" 200 OK\n",
      "INFO:     127.0.0.1:53458 - \"POST /query HTTP/1.1\" 200 OK\n",
      "INFO:     127.0.0.1:53457 - \"POST /query HTTP/1.1\" 200 OK\n",
      "INFO:     127.0.0.1:53464 - \"POST /query HTTP/1.1\" 200 OK\n",
      "INFO:     127.0.0.1:53460 - \"POST /query HTTP/1.1\" 200 OK\n",
      "INFO:     127.0.0.1:53463 - \"POST /query HTTP/1.1\" 200 OK\n",
      "INFO:     127.0.0.1:53459 - \"POST /query HTTP/1.1\" 200 OK\n",
      "INFO:     127.0.0.1:53462 - \"POST /query HTTP/1.1\" 200 OK\n",
      "INFO:     127.0.0.1:53465 - \"POST /query HTTP/1.1\" 200 OK\n",
      "INFO:     127.0.0.1:53461 - \"POST /query HTTP/1.1\" 200 OK\n",
      "INFO:     127.0.0.1:53466 - \"POST /query HTTP/1.1\" 200 OK\n",
      "INFO:     127.0.0.1:53468 - \"POST /query HTTP/1.1\" 200 OK\n",
      "INFO:     127.0.0.1:53469 - \"POST /query HTTP/1.1\" 200 OK\n",
      "INFO:     127.0.0.1:53467 - \"POST /query HTTP/1.1\" 200 OK\n",
      "INFO:     127.0.0.1:53472 - \"POST /query HTTP/1.1\" 200 OK\n",
      "INFO:     127.0.0.1:53470 - \"POST /query HTTP/1.1\" 200 OK\n",
      "INFO:     127.0.0.1:53474 - \"POST /query HTTP/1.1\" 200 OK\n",
      "INFO:     127.0.0.1:53475 - \"POST /query HTTP/1.1\" 200 OK\n",
      "INFO:     127.0.0.1:53476 - \"POST /query HTTP/1.1\" 200 OK\n",
      "INFO:     127.0.0.1:53471 - \"POST /query HTTP/1.1\" 200 OK\n",
      "INFO:     127.0.0.1:53473 - \"POST /query HTTP/1.1\" 200 OK\n",
      "INFO:     127.0.0.1:53478 - \"POST /query HTTP/1.1\" 200 OK\n",
      "INFO:     127.0.0.1:53477 - \"POST /query HTTP/1.1\" 200 OK\n",
      "INFO:     127.0.0.1:53479 - \"POST /query HTTP/1.1\" 200 OK\n",
      "INFO:     127.0.0.1:53483 - \"POST /query HTTP/1.1\" 200 OK\n",
      "INFO:     127.0.0.1:53486 - \"POST /query HTTP/1.1\" 200 OK\n",
      "INFO:     127.0.0.1:53485 - \"POST /query HTTP/1.1\" 200 OK\n",
      "INFO:     127.0.0.1:53484 - \"POST /query HTTP/1.1\" 200 OK\n",
      "INFO:     127.0.0.1:53480 - \"POST /query HTTP/1.1\" 200 OK\n",
      "INFO:     127.0.0.1:53482 - \"POST /query HTTP/1.1\" 200 OK\n",
      "INFO:     127.0.0.1:53481 - \"POST /query HTTP/1.1\" 200 OK\n",
      "INFO:     127.0.0.1:53487 - \"POST /query HTTP/1.1\" 200 OK\n",
      "INFO:     127.0.0.1:53488 - \"POST /query HTTP/1.1\" 200 OK\n",
      "INFO:     127.0.0.1:53489 - \"POST /query HTTP/1.1\" 200 OK\n",
      "INFO:     127.0.0.1:53495 - \"POST /query HTTP/1.1\" 200 OK\n",
      "INFO:     127.0.0.1:53496 - \"POST /query HTTP/1.1\" 200 OK\n",
      "INFO:     127.0.0.1:53492 - \"POST /query HTTP/1.1\" 200 OK\n",
      "INFO:     127.0.0.1:53493 - \"POST /query HTTP/1.1\" 200 OK\n",
      "INFO:     127.0.0.1:53490 - \"POST /query HTTP/1.1\" 200 OK\n",
      "INFO:     127.0.0.1:53494 - \"POST /query HTTP/1.1\" 200 OK\n",
      "INFO:     127.0.0.1:53491 - \"POST /query HTTP/1.1\" 200 OK\n",
      "INFO:     127.0.0.1:53498 - \"POST /query HTTP/1.1\" 200 OK\n",
      "INFO:     127.0.0.1:53497 - \"POST /query HTTP/1.1\" 200 OK\n",
      "INFO:     127.0.0.1:53499 - \"POST /query HTTP/1.1\" 200 OK\n",
      "INFO:     127.0.0.1:53506 - \"POST /query HTTP/1.1\" 200 OK\n",
      "INFO:     127.0.0.1:53504 - \"POST /query HTTP/1.1\" 200 OK\n",
      "INFO:     127.0.0.1:53500 - \"POST /query HTTP/1.1\" 200 OK\n",
      "INFO:     127.0.0.1:53501 - \"POST /query HTTP/1.1\" 200 OK\n",
      "INFO:     127.0.0.1:53502 - \"POST /query HTTP/1.1\" 200 OK\n",
      "INFO:     127.0.0.1:53505 - \"POST /query HTTP/1.1\" 200 OK\n",
      "INFO:     127.0.0.1:53503 - \"POST /query HTTP/1.1\" 200 OK\n",
      "INFO:     127.0.0.1:53507 - \"POST /query HTTP/1.1\" 200 OK\n",
      "INFO:     127.0.0.1:53510 - \"POST /query HTTP/1.1\" 200 OK\n",
      "INFO:     127.0.0.1:53508 - \"POST /query HTTP/1.1\" 200 OK\n",
      "INFO:     127.0.0.1:53509 - \"POST /query HTTP/1.1\" 200 OK\n",
      "INFO:     127.0.0.1:53511 - \"POST /query HTTP/1.1\" 200 OK\n",
      "INFO:     127.0.0.1:53516 - \"POST /query HTTP/1.1\" 200 OK\n",
      "INFO:     127.0.0.1:53515 - \"POST /query HTTP/1.1\" 200 OK\n",
      "INFO:     127.0.0.1:53512 - \"POST /query HTTP/1.1\" 200 OK\n",
      "INFO:     127.0.0.1:53513 - \"POST /query HTTP/1.1\" 200 OK\n",
      "INFO:     127.0.0.1:53514 - \"POST /query HTTP/1.1\" 200 OK\n",
      "INFO:     127.0.0.1:53517 - \"POST /query HTTP/1.1\" 200 OK\n",
      "INFO:     127.0.0.1:53518 - \"POST /query HTTP/1.1\" 200 OK\n",
      "INFO:     127.0.0.1:53519 - \"POST /query HTTP/1.1\" 200 OK\n",
      "INFO:     127.0.0.1:53522 - \"POST /query HTTP/1.1\" 200 OK\n",
      "INFO:     127.0.0.1:53521 - \"POST /query HTTP/1.1\" 200 OK\n",
      "INFO:     127.0.0.1:53525 - \"POST /query HTTP/1.1\" 200 OK\n",
      "INFO:     127.0.0.1:53523 - \"POST /query HTTP/1.1\" 200 OK\n",
      "INFO:     127.0.0.1:53520 - \"POST /query HTTP/1.1\" 200 OK\n",
      "INFO:     127.0.0.1:53526 - \"POST /query HTTP/1.1\" 200 OK\n",
      "INFO:     127.0.0.1:53524 - \"POST /query HTTP/1.1\" 200 OK\n",
      "INFO:     127.0.0.1:53528 - \"POST /query HTTP/1.1\" 200 OK\n",
      "INFO:     127.0.0.1:53527 - \"POST /query HTTP/1.1\" 200 OK\n",
      "INFO:     127.0.0.1:53530 - \"POST /query HTTP/1.1\" 200 OK\n",
      "INFO:     127.0.0.1:53533 - \"POST /query HTTP/1.1\" 200 OK\n",
      "INFO:     127.0.0.1:53531 - \"POST /query HTTP/1.1\" 200 OK\n",
      "INFO:     127.0.0.1:53529 - \"POST /query HTTP/1.1\" 200 OK\n",
      "INFO:     127.0.0.1:53536 - \"POST /query HTTP/1.1\" 200 OK\n",
      "INFO:     127.0.0.1:53535 - \"POST /query HTTP/1.1\" 200 OK\n",
      "INFO:     127.0.0.1:53534 - \"POST /query HTTP/1.1\" 200 OK\n",
      "INFO:     127.0.0.1:53532 - \"POST /query HTTP/1.1\" 200 OK\n",
      "INFO:     127.0.0.1:53537 - \"POST /query HTTP/1.1\" 200 OK\n",
      "INFO:     127.0.0.1:53538 - \"POST /query HTTP/1.1\" 200 OK\n",
      "INFO:     127.0.0.1:53539 - \"POST /query HTTP/1.1\" 200 OK\n",
      "INFO:     127.0.0.1:53541 - \"POST /query HTTP/1.1\" 200 OK\n",
      "INFO:     127.0.0.1:53544 - \"POST /query HTTP/1.1\" 200 OK\n",
      "INFO:     127.0.0.1:53543 - \"POST /query HTTP/1.1\" 200 OK\n",
      "INFO:     127.0.0.1:53545 - \"POST /query HTTP/1.1\" 200 OK\n",
      "INFO:     127.0.0.1:53546 - \"POST /query HTTP/1.1\" 200 OK\n",
      "INFO:     127.0.0.1:53542 - \"POST /query HTTP/1.1\" 200 OK\n",
      "INFO:     127.0.0.1:53540 - \"POST /query HTTP/1.1\" 200 OK\n",
      "INFO:     127.0.0.1:53547 - \"POST /query HTTP/1.1\" 200 OK\n",
      "INFO:     127.0.0.1:53549 - \"POST /query HTTP/1.1\" 200 OK\n",
      "INFO:     127.0.0.1:53548 - \"POST /query HTTP/1.1\" 200 OK\n",
      "INFO:     127.0.0.1:53553 - \"POST /query HTTP/1.1\" 200 OK\n",
      "INFO:     127.0.0.1:53551 - \"POST /query HTTP/1.1\" 200 OK\n",
      "INFO:     127.0.0.1:53555 - \"POST /query HTTP/1.1\" 200 OK\n",
      "INFO:     127.0.0.1:53550 - \"POST /query HTTP/1.1\" 200 OK\n",
      "INFO:     127.0.0.1:53554 - \"POST /query HTTP/1.1\" 200 OK\n",
      "INFO:     127.0.0.1:53556 - \"POST /query HTTP/1.1\" 200 OK\n",
      "INFO:     127.0.0.1:53552 - \"POST /query HTTP/1.1\" 200 OK\n",
      "INFO:     127.0.0.1:53557 - \"POST /query HTTP/1.1\" 200 OK\n",
      "INFO:     127.0.0.1:53564 - \"POST /query HTTP/1.1\" 200 OK\n",
      "INFO:     127.0.0.1:53559 - \"POST /query HTTP/1.1\" 200 OK\n",
      "INFO:     127.0.0.1:53566 - \"POST /query HTTP/1.1\" 200 OK\n",
      "INFO:     127.0.0.1:53558 - \"POST /query HTTP/1.1\" 200 OK\n",
      "INFO:     127.0.0.1:53563 - \"POST /query HTTP/1.1\" 200 OK\n",
      "INFO:     127.0.0.1:53561 - \"POST /query HTTP/1.1\" 200 OK\n",
      "INFO:     127.0.0.1:53565 - \"POST /query HTTP/1.1\" 200 OK\n",
      "INFO:     127.0.0.1:53560 - \"POST /query HTTP/1.1\" 200 OK\n",
      "INFO:     127.0.0.1:53562 - \"POST /query HTTP/1.1\" 200 OK\n",
      "INFO:     127.0.0.1:53567 - \"POST /query HTTP/1.1\" 200 OK\n",
      "INFO:     127.0.0.1:53568 - \"POST /query HTTP/1.1\" 200 OK\n",
      "INFO:     127.0.0.1:53575 - \"POST /query HTTP/1.1\" 200 OK\n",
      "INFO:     127.0.0.1:53571 - \"POST /query HTTP/1.1\" 200 OK\n",
      "INFO:     127.0.0.1:53570 - \"POST /query HTTP/1.1\" 200 OK\n",
      "INFO:     127.0.0.1:53576 - \"POST /query HTTP/1.1\" 200 OK\n",
      "INFO:     127.0.0.1:53569 - \"POST /query HTTP/1.1\" 200 OK\n",
      "INFO:     127.0.0.1:53574 - \"POST /query HTTP/1.1\" 200 OK\n",
      "INFO:     127.0.0.1:53573 - \"POST /query HTTP/1.1\" 200 OK\n",
      "INFO:     127.0.0.1:53572 - \"POST /query HTTP/1.1\" 200 OK\n",
      "INFO:     127.0.0.1:53578 - \"POST /query HTTP/1.1\" 200 OK\n",
      "INFO:     127.0.0.1:53579 - \"POST /query HTTP/1.1\" 200 OK\n",
      "INFO:     127.0.0.1:53577 - \"POST /query HTTP/1.1\" 200 OK\n",
      "INFO:     127.0.0.1:53582 - \"POST /query HTTP/1.1\" 200 OK\n",
      "INFO:     127.0.0.1:53586 - \"POST /query HTTP/1.1\" 200 OK\n",
      "INFO:     127.0.0.1:53585 - \"POST /query HTTP/1.1\" 200 OK\n",
      "INFO:     127.0.0.1:53584 - \"POST /query HTTP/1.1\" 200 OK\n",
      "INFO:     127.0.0.1:53580 - \"POST /query HTTP/1.1\" 200 OK\n",
      "INFO:     127.0.0.1:53581 - \"POST /query HTTP/1.1\" 200 OK\n",
      "INFO:     127.0.0.1:53583 - \"POST /query HTTP/1.1\" 200 OK\n",
      "INFO:     127.0.0.1:53587 - \"POST /query HTTP/1.1\" 200 OK\n",
      "INFO:     127.0.0.1:53589 - \"POST /query HTTP/1.1\" 200 OK\n",
      "INFO:     127.0.0.1:53588 - \"POST /query HTTP/1.1\" 200 OK\n",
      "INFO:     127.0.0.1:53591 - \"POST /query HTTP/1.1\" 200 OK\n",
      "INFO:     127.0.0.1:53592 - \"POST /query HTTP/1.1\" 200 OK\n",
      "INFO:     127.0.0.1:53595 - \"POST /query HTTP/1.1\" 200 OK\n",
      "INFO:     127.0.0.1:53590 - \"POST /query HTTP/1.1\" 200 OK\n",
      "INFO:     127.0.0.1:53596 - \"POST /query HTTP/1.1\" 200 OK\n",
      "INFO:     127.0.0.1:53594 - \"POST /query HTTP/1.1\" 200 OK\n",
      "INFO:     127.0.0.1:53593 - \"POST /query HTTP/1.1\" 200 OK\n",
      "INFO:     127.0.0.1:53597 - \"POST /verify/mode?set_v2=true HTTP/1.1\" 200 OK\n",
      "INFO:     127.0.0.1:53598 - \"POST /query HTTP/1.1\" 200 OK\n",
      "INFO:     127.0.0.1:53599 - \"POST /query HTTP/1.1\" 200 OK\n",
      "INFO:     127.0.0.1:53601 - \"POST /query HTTP/1.1\" 200 OK\n",
      "INFO:     127.0.0.1:53602 - \"POST /query HTTP/1.1\" 200 OK\n",
      "INFO:     127.0.0.1:53603 - \"POST /query HTTP/1.1\" 200 OK\n",
      "INFO:     127.0.0.1:53604 - \"POST /query HTTP/1.1\" 200 OK\n",
      "INFO:     127.0.0.1:53600 - \"POST /query HTTP/1.1\" 200 OK\n",
      "INFO:     127.0.0.1:53605 - \"POST /query HTTP/1.1\" 200 OK\n",
      "INFO:     127.0.0.1:53607 - \"POST /query HTTP/1.1\" 200 OK\n",
      "INFO:     127.0.0.1:53606 - \"POST /query HTTP/1.1\" 200 OK\n",
      "INFO:     127.0.0.1:53608 - \"POST /query HTTP/1.1\" 200 OK\n",
      "INFO:     127.0.0.1:53609 - \"POST /query HTTP/1.1\" 200 OK\n",
      "INFO:     127.0.0.1:53613 - \"POST /query HTTP/1.1\" 200 OK\n",
      "INFO:     127.0.0.1:53611 - \"POST /query HTTP/1.1\" 200 OK\n",
      "INFO:     127.0.0.1:53610 - \"POST /query HTTP/1.1\" 200 OK\n",
      "INFO:     127.0.0.1:53612 - \"POST /query HTTP/1.1\" 200 OK\n",
      "INFO:     127.0.0.1:53614 - \"POST /query HTTP/1.1\" 200 OK\n",
      "INFO:     127.0.0.1:53615 - \"POST /query HTTP/1.1\" 200 OK\n",
      "INFO:     127.0.0.1:53616 - \"POST /query HTTP/1.1\" 200 OK\n",
      "INFO:     127.0.0.1:53620 - \"POST /query HTTP/1.1\" 200 OK\n",
      "INFO:     127.0.0.1:53617 - \"POST /query HTTP/1.1\" 200 OK\n",
      "INFO:     127.0.0.1:53618 - \"POST /query HTTP/1.1\" 200 OK\n",
      "INFO:     127.0.0.1:53619 - \"POST /query HTTP/1.1\" 200 OK\n",
      "INFO:     127.0.0.1:53621 - \"POST /query HTTP/1.1\" 200 OK\n",
      "INFO:     127.0.0.1:53622 - \"POST /query HTTP/1.1\" 200 OK\n",
      "INFO:     127.0.0.1:53623 - \"POST /query HTTP/1.1\" 200 OK\n",
      "INFO:     127.0.0.1:53626 - \"POST /query HTTP/1.1\" 200 OK\n",
      "INFO:     127.0.0.1:53628 - \"POST /query HTTP/1.1\" 200 OK\n",
      "INFO:     127.0.0.1:53625 - \"POST /query HTTP/1.1\" 200 OK\n",
      "INFO:     127.0.0.1:53624 - \"POST /query HTTP/1.1\" 200 OK\n",
      "INFO:     127.0.0.1:53629 - \"POST /query HTTP/1.1\" 200 OK\n",
      "INFO:     127.0.0.1:53627 - \"POST /query HTTP/1.1\" 200 OK\n",
      "INFO:     127.0.0.1:53630 - \"POST /query HTTP/1.1\" 200 OK\n",
      "INFO:     127.0.0.1:53631 - \"POST /query HTTP/1.1\" 200 OK\n",
      "INFO:     127.0.0.1:53632 - \"POST /query HTTP/1.1\" 200 OK\n",
      "INFO:     127.0.0.1:53635 - \"POST /query HTTP/1.1\" 200 OK\n",
      "INFO:     127.0.0.1:53634 - \"POST /query HTTP/1.1\" 200 OK\n",
      "INFO:     127.0.0.1:53636 - \"POST /query HTTP/1.1\" 200 OK\n",
      "INFO:     127.0.0.1:53633 - \"POST /query HTTP/1.1\" 200 OK\n",
      "INFO:     127.0.0.1:53637 - \"POST /query HTTP/1.1\" 200 OK\n",
      "INFO:     127.0.0.1:53638 - \"POST /query HTTP/1.1\" 200 OK\n",
      "INFO:     127.0.0.1:53639 - \"POST /query HTTP/1.1\" 200 OK\n",
      "INFO:     127.0.0.1:53642 - \"POST /query HTTP/1.1\" 200 OK\n",
      "INFO:     127.0.0.1:53644 - \"POST /query HTTP/1.1\" 200 OK\n",
      "INFO:     127.0.0.1:53641 - \"POST /query HTTP/1.1\" 200 OK\n",
      "INFO:     127.0.0.1:53640 - \"POST /query HTTP/1.1\" 200 OK\n",
      "INFO:     127.0.0.1:53645 - \"POST /query HTTP/1.1\" 200 OK\n",
      "INFO:     127.0.0.1:53643 - \"POST /query HTTP/1.1\" 200 OK\n",
      "INFO:     127.0.0.1:53646 - \"POST /query HTTP/1.1\" 200 OK\n",
      "INFO:     127.0.0.1:53647 - \"POST /query HTTP/1.1\" 200 OK\n",
      "INFO:     127.0.0.1:53653 - \"POST /query HTTP/1.1\" 200 OK\n",
      "INFO:     127.0.0.1:53649 - \"POST /query HTTP/1.1\" 200 OK\n",
      "INFO:     127.0.0.1:53651 - \"POST /query HTTP/1.1\" 200 OK\n",
      "INFO:     127.0.0.1:53650 - \"POST /query HTTP/1.1\" 200 OK\n",
      "INFO:     127.0.0.1:53648 - \"POST /query HTTP/1.1\" 200 OK\n",
      "INFO:     127.0.0.1:53652 - \"POST /query HTTP/1.1\" 200 OK\n",
      "INFO:     127.0.0.1:53654 - \"POST /query HTTP/1.1\" 200 OK\n",
      "INFO:     127.0.0.1:53655 - \"POST /query HTTP/1.1\" 200 OK\n",
      "INFO:     127.0.0.1:53660 - \"POST /query HTTP/1.1\" 200 OK\n",
      "INFO:     127.0.0.1:53661 - \"POST /query HTTP/1.1\" 200 OK\n",
      "INFO:     127.0.0.1:53659 - \"POST /query HTTP/1.1\" 200 OK\n",
      "INFO:     127.0.0.1:53656 - \"POST /query HTTP/1.1\" 200 OK\n",
      "INFO:     127.0.0.1:53657 - \"POST /query HTTP/1.1\" 200 OK\n",
      "INFO:     127.0.0.1:53658 - \"POST /query HTTP/1.1\" 200 OK\n",
      "INFO:     127.0.0.1:53662 - \"POST /query HTTP/1.1\" 200 OK\n",
      "INFO:     127.0.0.1:53665 - \"POST /query HTTP/1.1\" 200 OK\n",
      "INFO:     127.0.0.1:53668 - \"POST /query HTTP/1.1\" 200 OK\n",
      "INFO:     127.0.0.1:53669 - \"POST /query HTTP/1.1\" 200 OK\n",
      "INFO:     127.0.0.1:53666 - \"POST /query HTTP/1.1\" 200 OK\n",
      "INFO:     127.0.0.1:53663 - \"POST /query HTTP/1.1\" 200 OK\n",
      "INFO:     127.0.0.1:53664 - \"POST /query HTTP/1.1\" 200 OK\n",
      "INFO:     127.0.0.1:53667 - \"POST /query HTTP/1.1\" 200 OK\n",
      "INFO:     127.0.0.1:53670 - \"POST /query HTTP/1.1\" 200 OK\n",
      "INFO:     127.0.0.1:53673 - \"POST /query HTTP/1.1\" 200 OK\n",
      "INFO:     127.0.0.1:53674 - \"POST /query HTTP/1.1\" 200 OK\n",
      "INFO:     127.0.0.1:53671 - \"POST /query HTTP/1.1\" 200 OK\n",
      "INFO:     127.0.0.1:53676 - \"POST /query HTTP/1.1\" 200 OK\n",
      "INFO:     127.0.0.1:53672 - \"POST /query HTTP/1.1\" 200 OK\n",
      "INFO:     127.0.0.1:53677 - \"POST /query HTTP/1.1\" 200 OK\n",
      "INFO:     127.0.0.1:53675 - \"POST /query HTTP/1.1\" 200 OK\n",
      "INFO:     127.0.0.1:53680 - \"POST /query HTTP/1.1\" 200 OK\n",
      "INFO:     127.0.0.1:53679 - \"POST /query HTTP/1.1\" 200 OK\n",
      "INFO:     127.0.0.1:53678 - \"POST /query HTTP/1.1\" 200 OK\n",
      "INFO:     127.0.0.1:53684 - \"POST /query HTTP/1.1\" 200 OK\n",
      "INFO:     127.0.0.1:53681 - \"POST /query HTTP/1.1\" 200 OK\n",
      "INFO:     127.0.0.1:53682 - \"POST /query HTTP/1.1\" 200 OK\n",
      "INFO:     127.0.0.1:53683 - \"POST /query HTTP/1.1\" 200 OK\n",
      "INFO:     127.0.0.1:53685 - \"POST /query HTTP/1.1\" 200 OK\n",
      "INFO:     127.0.0.1:53686 - \"POST /query HTTP/1.1\" 200 OK\n",
      "INFO:     127.0.0.1:53687 - \"POST /query HTTP/1.1\" 200 OK\n",
      "INFO:     127.0.0.1:53693 - \"POST /query HTTP/1.1\" 200 OK\n",
      "INFO:     127.0.0.1:53692 - \"POST /query HTTP/1.1\" 200 OK\n",
      "INFO:     127.0.0.1:53691 - \"POST /query HTTP/1.1\" 200 OK\n",
      "INFO:     127.0.0.1:53690 - \"POST /query HTTP/1.1\" 200 OK\n",
      "INFO:     127.0.0.1:53688 - \"POST /query HTTP/1.1\" 200 OK\n",
      "INFO:     127.0.0.1:53689 - \"POST /query HTTP/1.1\" 200 OK\n",
      "INFO:     127.0.0.1:53695 - \"POST /query HTTP/1.1\" 200 OK\n",
      "INFO:     127.0.0.1:53697 - \"POST /query HTTP/1.1\" 200 OK\n",
      "INFO:     127.0.0.1:53696 - \"POST /query HTTP/1.1\" 200 OK\n",
      "INFO:     127.0.0.1:53700 - \"POST /query HTTP/1.1\" 200 OK\n",
      "INFO:     127.0.0.1:53699 - \"POST /query HTTP/1.1\" 200 OK\n",
      "INFO:     127.0.0.1:53701 - \"POST /query HTTP/1.1\" 200 OK\n",
      "INFO:     127.0.0.1:53698 - \"POST /query HTTP/1.1\" 200 OK\n",
      "INFO:     127.0.0.1:53702 - \"POST /query HTTP/1.1\" 200 OK\n",
      "INFO:     127.0.0.1:53704 - \"POST /query HTTP/1.1\" 200 OK\n",
      "INFO:     127.0.0.1:53703 - \"POST /query HTTP/1.1\" 200 OK\n",
      "INFO:     127.0.0.1:53705 - \"POST /query HTTP/1.1\" 200 OK\n",
      "INFO:     127.0.0.1:53707 - \"POST /query HTTP/1.1\" 200 OK\n",
      "INFO:     127.0.0.1:53709 - \"POST /query HTTP/1.1\" 200 OK\n",
      "INFO:     127.0.0.1:53706 - \"POST /query HTTP/1.1\" 200 OK\n",
      "INFO:     127.0.0.1:53710 - \"POST /query HTTP/1.1\" 200 OK\n",
      "INFO:     127.0.0.1:53708 - \"POST /query HTTP/1.1\" 200 OK\n",
      "INFO:     127.0.0.1:53711 - \"POST /query HTTP/1.1\" 200 OK\n",
      "INFO:     127.0.0.1:53712 - \"POST /query HTTP/1.1\" 200 OK\n",
      "INFO:     127.0.0.1:53715 - \"POST /query HTTP/1.1\" 200 OK\n",
      "INFO:     127.0.0.1:53717 - \"POST /query HTTP/1.1\" 200 OK\n",
      "INFO:     127.0.0.1:53716 - \"POST /query HTTP/1.1\" 200 OK\n",
      "INFO:     127.0.0.1:53714 - \"POST /query HTTP/1.1\" 200 OK\n",
      "INFO:     127.0.0.1:53718 - \"POST /query HTTP/1.1\" 200 OK\n",
      "INFO:     127.0.0.1:53713 - \"POST /query HTTP/1.1\" 200 OK\n",
      "INFO:     127.0.0.1:53719 - \"POST /query HTTP/1.1\" 200 OK\n",
      "INFO:     127.0.0.1:53720 - \"POST /query HTTP/1.1\" 200 OK\n",
      "INFO:     127.0.0.1:53721 - \"POST /query HTTP/1.1\" 200 OK\n",
      "INFO:     127.0.0.1:53725 - \"POST /query HTTP/1.1\" 200 OK\n",
      "INFO:     127.0.0.1:53726 - \"POST /query HTTP/1.1\" 200 OK\n",
      "INFO:     127.0.0.1:53724 - \"POST /query HTTP/1.1\" 200 OK\n",
      "INFO:     127.0.0.1:53722 - \"POST /query HTTP/1.1\" 200 OK\n",
      "INFO:     127.0.0.1:53723 - \"POST /query HTTP/1.1\" 200 OK\n",
      "INFO:     127.0.0.1:53727 - \"POST /query HTTP/1.1\" 200 OK\n",
      "INFO:     127.0.0.1:53728 - \"POST /query HTTP/1.1\" 200 OK\n",
      "INFO:     127.0.0.1:53730 - \"POST /query HTTP/1.1\" 200 OK\n",
      "INFO:     127.0.0.1:53731 - \"POST /query HTTP/1.1\" 200 OK\n",
      "INFO:     127.0.0.1:53729 - \"POST /query HTTP/1.1\" 200 OK\n",
      "INFO:     127.0.0.1:53733 - \"POST /query HTTP/1.1\" 200 OK\n",
      "INFO:     127.0.0.1:53732 - \"POST /query HTTP/1.1\" 200 OK\n",
      "INFO:     127.0.0.1:53734 - \"POST /query HTTP/1.1\" 200 OK\n",
      "INFO:     127.0.0.1:53735 - \"POST /query HTTP/1.1\" 200 OK\n",
      "INFO:     127.0.0.1:53736 - \"POST /query HTTP/1.1\" 200 OK\n",
      "INFO:     127.0.0.1:53741 - \"POST /query HTTP/1.1\" 200 OK\n",
      "INFO:     127.0.0.1:53737 - \"POST /query HTTP/1.1\" 200 OK\n",
      "INFO:     127.0.0.1:53742 - \"POST /query HTTP/1.1\" 200 OK\n",
      "INFO:     127.0.0.1:53739 - \"POST /query HTTP/1.1\" 200 OK\n",
      "INFO:     127.0.0.1:53738 - \"POST /query HTTP/1.1\" 200 OK\n",
      "INFO:     127.0.0.1:53740 - \"POST /query HTTP/1.1\" 200 OK\n",
      "INFO:     127.0.0.1:53743 - \"POST /query HTTP/1.1\" 200 OK\n",
      "INFO:     127.0.0.1:53746 - \"POST /query HTTP/1.1\" 200 OK\n",
      "INFO:     127.0.0.1:53745 - \"POST /query HTTP/1.1\" 200 OK\n",
      "INFO:     127.0.0.1:53744 - \"POST /query HTTP/1.1\" 200 OK\n",
      "INFO:     127.0.0.1:53748 - \"POST /query HTTP/1.1\" 200 OK\n",
      "INFO:     127.0.0.1:53747 - \"POST /query HTTP/1.1\" 200 OK\n",
      "INFO:     127.0.0.1:53750 - \"POST /query HTTP/1.1\" 200 OK\n",
      "INFO:     127.0.0.1:53749 - \"POST /query HTTP/1.1\" 200 OK\n",
      "INFO:     127.0.0.1:53751 - \"POST /query HTTP/1.1\" 200 OK\n",
      "INFO:     127.0.0.1:53752 - \"POST /query HTTP/1.1\" 200 OK\n",
      "INFO:     127.0.0.1:53755 - \"POST /query HTTP/1.1\" 200 OK\n",
      "INFO:     127.0.0.1:53754 - \"POST /query HTTP/1.1\" 200 OK\n",
      "INFO:     127.0.0.1:53758 - \"POST /query HTTP/1.1\" 200 OK\n",
      "INFO:     127.0.0.1:53757 - \"POST /query HTTP/1.1\" 200 OK\n",
      "INFO:     127.0.0.1:53753 - \"POST /query HTTP/1.1\" 200 OK\n",
      "INFO:     127.0.0.1:53756 - \"POST /query HTTP/1.1\" 200 OK\n",
      "INFO:     127.0.0.1:53759 - \"POST /verify/mode?set_v2=false HTTP/1.1\" 200 OK\n",
      "INFO:     127.0.0.1:53760 - \"POST /governor/mode?enable_ce=True&enable_adapters=True HTTP/1.1\" 200 OK\n",
      "INFO:     127.0.0.1:53761 - \"POST /query HTTP/1.1\" 200 OK\n",
      "INFO:     127.0.0.1:53762 - \"POST /query HTTP/1.1\" 200 OK\n",
      "INFO:     127.0.0.1:53763 - \"POST /query HTTP/1.1\" 200 OK\n",
      "INFO:     127.0.0.1:53764 - \"POST /query HTTP/1.1\" 200 OK\n",
      "INFO:     127.0.0.1:53765 - \"POST /query HTTP/1.1\" 200 OK\n",
      "INFO:     127.0.0.1:53772 - \"GET /healthz HTTP/1.1\" 200 OK\n",
      "INFO:     127.0.0.1:53773 - \"GET /metrics HTTP/1.1\" 200 OK\n",
      "INFO:     127.0.0.1:53774 - \"POST /query HTTP/1.1\" 200 OK\n",
      "INFO:     127.0.0.1:53777 - \"POST /query HTTP/1.1\" 200 OK\n",
      "INFO:     127.0.0.1:53775 - \"POST /query HTTP/1.1\" 200 OK\n",
      "INFO:     127.0.0.1:53776 - \"POST /query HTTP/1.1\" 200 OK\n",
      "INFO:     127.0.0.1:53779 - \"POST /query HTTP/1.1\" 200 OK\n",
      "INFO:     127.0.0.1:53780 - \"POST /query HTTP/1.1\" 200 OK\n",
      "INFO:     127.0.0.1:53778 - \"POST /query HTTP/1.1\" 200 OK\n",
      "INFO:     127.0.0.1:53781 - \"POST /query HTTP/1.1\" 200 OK\n",
      "INFO:     127.0.0.1:53782 - \"POST /query HTTP/1.1\" 200 OK\n",
      "INFO:     127.0.0.1:53783 - \"POST /query HTTP/1.1\" 200 OK\n",
      "INFO:     127.0.0.1:53784 - \"POST /query HTTP/1.1\" 200 OK\n",
      "INFO:     127.0.0.1:53785 - \"POST /query HTTP/1.1\" 200 OK\n",
      "INFO:     127.0.0.1:53791 - \"POST /query HTTP/1.1\" 200 OK\n",
      "INFO:     127.0.0.1:53792 - \"POST /query HTTP/1.1\" 200 OK\n",
      "INFO:     127.0.0.1:53793 - \"POST /query HTTP/1.1\" 200 OK\n",
      "INFO:     127.0.0.1:53794 - \"POST /query HTTP/1.1\" 200 OK\n",
      "INFO:     127.0.0.1:53795 - \"GET /governor/status HTTP/1.1\" 200 OK\n",
      "INFO:     127.0.0.1:53796 - \"GET /policy/status HTTP/1.1\" 200 OK\n",
      "INFO:     127.0.0.1:53797 - \"GET /meta/status HTTP/1.1\" 200 OK\n",
      "INFO:     127.0.0.1:53798 - \"POST /governor/mode?enable_ce=True&enable_adapters=True HTTP/1.1\" 200 OK\n",
      "INFO:     127.0.0.1:53799 - \"GET /metrics HTTP/1.1\" 200 OK\n",
      "INFO:     127.0.0.1:53801 - \"GET /metrics HTTP/1.1\" 200 OK\n",
      "INFO:     127.0.0.1:53803 - \"GET /metrics HTTP/1.1\" 200 OK\n",
      "INFO:     127.0.0.1:53804 - \"GET /metrics HTTP/1.1\" 200 OK\n",
      "INFO:     127.0.0.1:53807 - \"GET /metrics HTTP/1.1\" 200 OK\n",
      "INFO:     127.0.0.1:53808 - \"GET /healthz HTTP/1.1\" 200 OK\n",
      "INFO:     127.0.0.1:53809 - \"GET /metrics HTTP/1.1\" 200 OK\n",
      "INFO:     127.0.0.1:53812 - \"POST /query HTTP/1.1\" 200 OK\n",
      "INFO:     127.0.0.1:53813 - \"POST /query HTTP/1.1\" 200 OK\n",
      "INFO:     127.0.0.1:53814 - \"POST /query HTTP/1.1\" 200 OK\n",
      "INFO:     127.0.0.1:53815 - \"POST /query HTTP/1.1\" 200 OK\n",
      "INFO:     127.0.0.1:53817 - \"POST /query HTTP/1.1\" 200 OK\n",
      "INFO:     127.0.0.1:53818 - \"POST /query HTTP/1.1\" 200 OK\n",
      "INFO:     127.0.0.1:53819 - \"POST /query HTTP/1.1\" 200 OK\n",
      "INFO:     127.0.0.1:53820 - \"POST /query HTTP/1.1\" 200 OK\n",
      "INFO:     127.0.0.1:53821 - \"POST /query HTTP/1.1\" 200 OK\n",
      "INFO:     127.0.0.1:53822 - \"POST /query HTTP/1.1\" 200 OK\n",
      "INFO:     127.0.0.1:53823 - \"POST /query HTTP/1.1\" 200 OK\n",
      "INFO:     127.0.0.1:53825 - \"POST /query HTTP/1.1\" 200 OK\n",
      "INFO:     127.0.0.1:53826 - \"POST /query HTTP/1.1\" 200 OK\n",
      "INFO:     127.0.0.1:53828 - \"POST /query HTTP/1.1\" 200 OK\n",
      "INFO:     127.0.0.1:53829 - \"POST /query HTTP/1.1\" 200 OK\n",
      "INFO:     127.0.0.1:53830 - \"POST /query HTTP/1.1\" 200 OK\n",
      "INFO:     127.0.0.1:53831 - \"POST /query HTTP/1.1\" 200 OK\n",
      "INFO:     127.0.0.1:53832 - \"POST /query HTTP/1.1\" 200 OK\n",
      "INFO:     127.0.0.1:53833 - \"GET /governor/status HTTP/1.1\" 200 OK\n",
      "INFO:     127.0.0.1:53834 - \"POST /verify/mode?set_v2=true HTTP/1.1\" 200 OK\n",
      "INFO:     127.0.0.1:53835 - \"POST /query HTTP/1.1\" 200 OK\n",
      "INFO:     127.0.0.1:53836 - \"POST /verify/mode?set_v2=false HTTP/1.1\" 200 OK\n",
      "INFO:     127.0.0.1:53837 - \"POST /governor/mode?enable_ce=True&enable_adapters=True HTTP/1.1\" 200 OK\n",
      "INFO:     127.0.0.1:53840 - \"GET /healthz HTTP/1.1\" 200 OK\n",
      "INFO:     127.0.0.1:53841 - \"GET /governor/status HTTP/1.1\" 200 OK\n",
      "INFO:     127.0.0.1:53842 - \"GET /policy/status HTTP/1.1\" 200 OK\n",
      "INFO:     127.0.0.1:53843 - \"GET /registry/status HTTP/1.1\" 200 OK\n",
      "INFO:     127.0.0.1:53844 - \"GET /meta/status HTTP/1.1\" 200 OK\n",
      "INFO:     127.0.0.1:53848 - \"POST /query HTTP/1.1\" 200 OK\n",
      "INFO:     127.0.0.1:53849 - \"POST /query HTTP/1.1\" 200 OK\n",
      "INFO:     127.0.0.1:53850 - \"POST /query HTTP/1.1\" 200 OK\n",
      "INFO:     127.0.0.1:53851 - \"POST /query HTTP/1.1\" 200 OK\n",
      "INFO:     127.0.0.1:53852 - \"POST /query HTTP/1.1\" 200 OK\n",
      "INFO:     127.0.0.1:53853 - \"POST /query HTTP/1.1\" 200 OK\n",
      "INFO:     127.0.0.1:53854 - \"POST /query HTTP/1.1\" 200 OK\n",
      "INFO:     127.0.0.1:53855 - \"POST /query HTTP/1.1\" 200 OK\n",
      "INFO:     127.0.0.1:53860 - \"GET /healthz HTTP/1.1\" 200 OK\n",
      "INFO:     127.0.0.1:53861 - \"POST /query HTTP/1.1\" 200 OK\n"
     ]
    }
   ],
   "source": [
    "HOST, PORT = \"127.0.0.1\", 8000\n",
    "def _run_server():\n",
    "    config = uvicorn.Config(app, host=HOST, port=PORT, reload=False, log_level=\"info\")\n",
    "    server = uvicorn.Server(config)\n",
    "    asyncio.run(server.serve())\n",
    "try:\n",
    "    SERVER_THREAD\n",
    "except NameError:\n",
    "    SERVER_THREAD = None\n",
    "if SERVER_THREAD is None or not SERVER_THREAD.is_alive():\n",
    "    SERVER_THREAD = threading.Thread(target=_run_server, daemon=True)\n",
    "    SERVER_THREAD.start()\n",
    "    print(f\"API serving at http://{HOST}:{PORT}   /upload, /advanced, /query, /healthz, /metrics\")\n",
    "else:\n",
    "    print(\"Server already running.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "4cf1a1d8-c996-4068-8f41-3bc74fe95f5c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "advanced: BASIC_DONE\n",
      "advanced: ADVANCED_BUILDING\n",
      "advanced: ADVANCED_BUILDING\n",
      "advanced: ADVANCED_BUILDING\n",
      "advanced: ADVANCED_BUILDING\n",
      "advanced: ADVANCED_BUILDING\n",
      "advanced: ADVANCED_READY\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'request_id': 'demo_1762113098-1762113104',\n",
       " 'action': 'run_sql',\n",
       " 'payload': {'sql': 'SELECT age, income, balance, default FROM dataset LIMIT 5;'},\n",
       " 'citations': [],\n",
       " 'model_id': 'slm-sql-v0',\n",
       " 'confidence': 0.82,\n",
       " 'meta': {'route': 'sql',\n",
       "  'route_conf': 0.82,\n",
       "  'retriever': 'faiss',\n",
       "  'latency_ms': 93}}"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import requests, time\n",
    "BASE = \"http://127.0.0.1:8000\"\n",
    "with open(\"data/synthetic_credit.csv\",\"rb\") as f:\n",
    "    up = requests.post(f\"{BASE}/upload\", data={\"namespace\":\"demo\"}, files={\"file\":(\"synthetic_credit.csv\", f, \"text/csv\")}).json()\n",
    "JOB = up[\"job_id\"]\n",
    "for _ in range(20):\n",
    "    adv = requests.get(f\"{BASE}/advanced\", params={\"job_id\": JOB}).json()\n",
    "    print(\"advanced:\", adv.get(\"status\"))\n",
    "    if adv.get(\"status\") == \"ADVANCED_READY\":\n",
    "        break\n",
    "    time.sleep(1)\n",
    "q = requests.post(f\"{BASE}/query\", json={\n",
    "    \"job_id\": JOB,\n",
    "    \"namespace\": \"demo\",\n",
    "    \"query_text\": \"select age, income and balance where default is 1 limit 5\",\n",
    "    \"top_k\": 5\n",
    "}).json()\n",
    "q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "669f40f2-7f0a-4513-9882-dbc9cb860522",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, json, time, math, shutil, threading, glob, random\n",
    "from pathlib import Path\n",
    "from typing import Dict, Any, List, Optional, Tuple\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from fastapi import HTTPException\n",
    "from fastapi.responses import JSONResponse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "d587ac69-f04d-4773-8d46-2d008cb24e8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "REGISTRY_PATH = Path(\"artifacts/registry.json\")\n",
    "POLICY_PATH   = Path(\"artifacts/policy.json\")\n",
    "DRIFT_DIR     = Path(\"artifacts/drift\"); DRIFT_DIR.mkdir(parents=True, exist_ok=True)\n",
    "try:\n",
    "    TTL_DAYS = int(CFG.get(\"ttl_days\", 90))\n",
    "except Exception:\n",
    "    TTL_DAYS = 90\n",
    "REWARD_WEIGHTS = {\"alpha\": 0.5, \"beta\": 0.4, \"gamma\": 0.0005, \"delta\": 10.0}  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "a2343c25-c6b1-48bf-9455-a595f1879b25",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _now_ts() -> int:\n",
    "    return int(time.time())\n",
    "def _json_read(path: Path, default):\n",
    "    if not path.exists():\n",
    "        return default\n",
    "    try:\n",
    "        return json.loads(path.read_text())\n",
    "    except Exception:\n",
    "        return default\n",
    "def _json_write(path: Path, obj: Any):\n",
    "    path.parent.mkdir(parents=True, exist_ok=True)\n",
    "    tmp = path.with_suffix(path.suffix + \".tmp\")\n",
    "    tmp.write_text(json.dumps(obj, indent=2))\n",
    "    os.replace(tmp, path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "78922b7d-a2fd-4fc1-92fb-0b3cfb7d4bb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _reward_from_metrics(metrics: Dict[str, Any]) -> float:\n",
    "    alpha, beta, gamma, delta = (REWARD_WEIGHTS[k] for k in (\"alpha\",\"beta\",\"gamma\",\"delta\"))\n",
    "    faith = float(metrics.get(\"faithfulness\", metrics.get(\"faithfulness_mean\", 0.9)))\n",
    "    ndcg  = float(metrics.get(\"ndcg@10\", metrics.get(\"ndcg10\", 0.6)))\n",
    "    p95   = float(metrics.get(\"p95\", metrics.get(\"pipeline_latency_ms_p95\", 1500)))\n",
    "    cost  = float(metrics.get(\"cost\", metrics.get(\"cost_per_req\", 0.015)))\n",
    "    return alpha*faith + beta*ndcg - gamma*float(p95) - delta*cost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "78dda689-2103-47fc-b9cd-07abc3e3a83e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _collect_recent_eval_metrics(hours: int = 6) -> List[Dict[str, Any]]:\n",
    "    cutoff = time.time() - hours*3600\n",
    "    out = []\n",
    "    for p in Path(\"artifacts/advanced\").glob(\"*/*/eval.json\"):\n",
    "        try:\n",
    "            st = p.stat()\n",
    "            if st.st_mtime < cutoff:\n",
    "                continue\n",
    "            data = _json_read(p, {})\n",
    "            m = data.get(\"metrics\", {})\n",
    "            if m:\n",
    "                out.append(m)\n",
    "        except Exception:\n",
    "            pass\n",
    "    for p in Path(\"artifacts/advanced\").glob(\"*/eval.json\"):\n",
    "        try:\n",
    "            st = p.stat()\n",
    "            if st.st_mtime < cutoff:\n",
    "                continue\n",
    "            data = _json_read(p, {})\n",
    "            m = data.get(\"metrics\", {})\n",
    "            if m:\n",
    "                out.append(m)\n",
    "        except Exception:\n",
    "            pass\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "db501d5a-6e76-4cd5-9493-5aca4f8a331d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _bandit_update() -> Dict[str, Any]:\n",
    "    policy = _json_read(POLICY_PATH, {\"version\": 0, \"weights\": {\"A\": 0.6, \"B\": 0.4}})\n",
    "    metrics = _collect_recent_eval_metrics(hours=6)\n",
    "    if not metrics:\n",
    "        wA = float(policy[\"weights\"].get(\"A\", 0.6)); wB = 1.0 - wA\n",
    "        wA = min(0.95, max(0.05, wA + np.random.normal(0, 0.01)))\n",
    "        policy[\"weights\"] = {\"A\": round(wA, 4), \"B\": round(1-wA, 4)}\n",
    "        policy[\"version\"] = int(policy[\"version\"]) + 1\n",
    "        _json_write(POLICY_PATH, policy)\n",
    "        return {\"weights\": policy[\"weights\"], \"policy_version\": policy[\"version\"], \"note\":\"no recent evals; gentle explore\"}\n",
    "    rewards_A, rewards_B = [], []\n",
    "    for m in metrics:\n",
    "        r = _reward_from_metrics(m)\n",
    "        bucket = (\"A\" if (hash(json.dumps(m, sort_keys=True)) % 2 == 0) else \"B\")\n",
    "        (rewards_A if bucket==\"A\" else rewards_B).append(r)\n",
    "    meanA = float(np.mean(rewards_A)) if rewards_A else 0.0\n",
    "    meanB = float(np.mean(rewards_B)) if rewards_B else 0.0\n",
    "    bonusA = 0.05/ math.sqrt(max(1, len(rewards_A)))\n",
    "    bonusB = 0.05/ math.sqrt(max(1, len(rewards_B)))\n",
    "    scoreA, scoreB = meanA + bonusA, meanB + bonusB\n",
    "    temp = 0.2\n",
    "    ea, eb = math.exp(scoreA/temp), math.exp(scoreB/temp)\n",
    "    wA = ea / (ea + eb); wB = 1.0 - wA\n",
    "    policy[\"weights\"] = {\"A\": round(wA, 4), \"B\": round(wB, 4)}\n",
    "    policy[\"version\"] = int(policy[\"version\"]) + 1\n",
    "    policy[\"ts_updated\"] = _now_ts()\n",
    "    _json_write(POLICY_PATH, policy)\n",
    "    return {\"weights\": policy[\"weights\"], \"policy_version\": policy[\"version\"], \"scoreA\":round(scoreA,4), \"scoreB\":round(scoreB,4)}\n",
    "@app.post(\"/rl/update\")\n",
    "def rl_update():\n",
    "    out = _bandit_update()\n",
    "    try:\n",
    "        jobs_set(\"POLICY\", status=\"POLICY_UPDATED\", policy_version=out[\"policy_version\"], weights=out[\"weights\"])\n",
    "    except Exception:\n",
    "        pass\n",
    "    return JSONResponse(out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "b9d77c5a-c49e-47fe-a8dd-c75fcc94ad9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _psi(expected: np.ndarray, actual: np.ndarray, bins: int = 10) -> float:\n",
    "    expected = expected[~np.isnan(expected)]\n",
    "    actual   = actual[~np.isnan(actual)]\n",
    "    if len(expected) < 100 or len(actual) < 100:\n",
    "        return 0.0\n",
    "    qs = np.quantile(expected, np.linspace(0, 1, bins+1))\n",
    "    qs = np.unique(qs)\n",
    "    if len(qs) <= 2:\n",
    "        return 0.0\n",
    "    exp_hist, _ = np.histogram(expected, bins=qs)\n",
    "    act_hist, _ = np.histogram(actual,   bins=qs)\n",
    "    exp_ratio = np.clip(exp_hist / max(1, exp_hist.sum()), 1e-6, 1)\n",
    "    act_ratio = np.clip(act_hist / max(1, act_hist.sum()), 1e-6, 1)\n",
    "    return float(np.sum((act_ratio - exp_ratio) * np.log(act_ratio / exp_ratio)))\n",
    "def _ks_pvalue(expected: np.ndarray, actual: np.ndarray) -> float:\n",
    "    try:\n",
    "        from scipy.stats import ks_2samp\n",
    "        return float(ks_2samp(expected[~np.isnan(expected)], actual[~np.isnan(actual)]).pvalue)\n",
    "    except Exception:\n",
    "        return 1.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "07f11f64-9eab-4207-a185-d18e9da21e47",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _drift_check_for_dataset(dataset_path: str, namespace: str, psi_thresh: float = 0.2, ks_thresh: float = 0.05) -> Dict[str, Any]:\n",
    "    df = pd.read_parquet(dataset_path) if dataset_path.endswith(\".parquet\") else pd.read_csv(dataset_path)\n",
    "    base_path = DRIFT_DIR / f\"{namespace}_baseline.parquet\"\n",
    "    res = {\"psi\": {}, \"ks_pvalue\": {}, \"is_drift\": False}\n",
    "    if not base_path.exists():\n",
    "        df.sample(min(10000, len(df)), random_state=42).to_parquet(base_path)\n",
    "        return {\"psi\": {}, \"ks_pvalue\": {}, \"is_drift\": False, \"note\": \"baseline_created\"}\n",
    "    base = pd.read_parquet(base_path)\n",
    "    numeric_cols = [c for c in df.columns if pd.api.types.is_numeric_dtype(df[c]) and c in base.columns]\n",
    "    if not numeric_cols:\n",
    "        return {\"psi\": {}, \"ks_pvalue\": {}, \"is_drift\": False, \"note\": \"no_numeric_cols\"}\n",
    "    drift_flags = []\n",
    "    for c in numeric_cols:\n",
    "        exp = base[c].to_numpy()\n",
    "        act = df[c].to_numpy()\n",
    "        psi = _psi(exp, act)\n",
    "        ks  = _ks_pvalue(exp, act)\n",
    "        res[\"psi\"][c] = round(psi, 4)\n",
    "        res[\"ks_pvalue\"][c] = round(ks, 4)\n",
    "        drift_flags.append((psi > psi_thresh) or (ks < ks_thresh))\n",
    "    res[\"is_drift\"] = bool(any(drift_flags))\n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "9669e2ec-9463-4407-8ad4-c9bd937af137",
   "metadata": {},
   "outputs": [],
   "source": [
    "@app.post(\"/drift/check\")\n",
    "def drift_check(namespace: str = \"default\"):\n",
    "    jobs = _json_read(ART_DIR / \"jobs.json\", {})\n",
    "    ds = None\n",
    "    for jid, row in sorted(jobs.items(), key=lambda kv: kv[1].get(\"ts_updated\", 0), reverse=True):\n",
    "        if namespace in jid and row.get(\"dataset_path\"):\n",
    "            ds = row[\"dataset_path\"]\n",
    "            break\n",
    "    if not ds:\n",
    "        raise HTTPException(404, \"No dataset found for namespace\")\n",
    "    out = _drift_check_for_dataset(ds, namespace)\n",
    "    return JSONResponse(out)\n",
    "@app.post(\"/registry/promote\")\n",
    "def registry_promote(champion: str = \"xgb_v0\"):\n",
    "    reg = _json_read(REGISTRY_PATH, {})\n",
    "    reg[\"champion\"] = champion\n",
    "    reg[\"ts_updated\"] = _now_ts()\n",
    "    _json_write(REGISTRY_PATH, reg)\n",
    "    return JSONResponse({\"champion\": champion})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "36400ebf-e65a-407b-8370-a519204f0a99",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _dir_size_bytes(p: Path) -> int:\n",
    "    total = 0\n",
    "    for root, dirs, files in os.walk(p):\n",
    "        for f in files:\n",
    "            try:\n",
    "                total += (Path(root)/f).stat().st_size\n",
    "            except Exception:\n",
    "                pass\n",
    "    return total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "ed4cb67e-6785-4bf3-aafd-43187202b048",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _gc(ttl_days: int = TTL_DAYS, cap_gb_per_ns: float = 10.0) -> Dict[str, Any]:\n",
    "    now = time.time()\n",
    "    ttl_sec = ttl_days * 86400\n",
    "    reclaimed = 0\n",
    "    ns_sizes = {}\n",
    "    for p in Path(\"artifacts\").rglob(\"*\"):\n",
    "        try:\n",
    "            if p.is_file():\n",
    "                if now - p.stat().st_mtime > ttl_sec:\n",
    "                    reclaimed += p.stat().st_size\n",
    "                    p.unlink(missing_ok=True)\n",
    "        except Exception:\n",
    "            pass\n",
    "    for ns_dir in Path(\"artifacts/advanced\").glob(\"*\"):\n",
    "        if not ns_dir.is_dir():\n",
    "            continue\n",
    "        ns = ns_dir.name\n",
    "        size = _dir_size_bytes(ns_dir)\n",
    "        ns_sizes[ns] = size\n",
    "        cap = cap_gb_per_ns * (1024**3)\n",
    "        if size > cap:\n",
    "            children = sorted([p for p in ns_dir.glob(\"*\") if p.is_dir()],\n",
    "                              key=lambda d: d.stat().st_mtime)\n",
    "            for child in children:\n",
    "                s = _dir_size_bytes(child)\n",
    "                try:\n",
    "                    shutil.rmtree(child, ignore_errors=True)\n",
    "                    reclaimed += s\n",
    "                    size -= s\n",
    "                    if size <= cap:\n",
    "                        break\n",
    "                except Exception:\n",
    "                    pass\n",
    "    return {\"reclaimed_bytes\": int(reclaimed), \"ns_sizes\": {k: int(v) for k,v in ns_sizes.items()}}\n",
    "@app.post(\"/cache/clean\")\n",
    "def cache_clean():\n",
    "    out = _gc()\n",
    "    return JSONResponse(out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "358d2bfc-b471-4020-9ffd-e7676c6948ac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bandit cron started (15 min).\n",
      "Drift cron started (hourly).\n",
      "GC cron started (daily).\n"
     ]
    }
   ],
   "source": [
    "def _cron_loop(name: str, fn, interval_sec: int, jitter_sec: int = 5):\n",
    "    while True:\n",
    "        try:\n",
    "            out = fn()\n",
    "            jobs_set(f\"CRON_{name}\", status=\"OK\", last_result=out, ts=_now_ts())\n",
    "        except Exception as e:\n",
    "            jobs_set(f\"CRON_{name}\", status=\"ERROR\", error=str(e), ts=_now_ts())\n",
    "        time.sleep(interval_sec + random.randint(0, jitter_sec))\n",
    "try:\n",
    "    BANDIT_THREAD\n",
    "except NameError:\n",
    "    BANDIT_THREAD = None\n",
    "try:\n",
    "    DRIFT_THREAD\n",
    "except NameError:\n",
    "    DRIFT_THREAD = None\n",
    "try:\n",
    "    GC_THREAD\n",
    "except NameError:\n",
    "    GC_THREAD = None\n",
    "if BANDIT_THREAD is None or not BANDIT_THREAD.is_alive():\n",
    "    BANDIT_THREAD = threading.Thread(target=_cron_loop, args=(\"BANDIT\", _bandit_update, 15*60), daemon=True)\n",
    "    BANDIT_THREAD.start()\n",
    "    print(\"Bandit cron started (15 min).\")\n",
    "if DRIFT_THREAD is None or not DRIFT_THREAD.is_alive():\n",
    "    DRIFT_THREAD = threading.Thread(target=_cron_loop,\n",
    "                                    args=(\"DRIFT\", lambda: drift_check(namespace=\"default\").body and json.loads(drift_check(namespace=\"default\").body), 60*60),\n",
    "                                    daemon=True)\n",
    "    DRIFT_THREAD.start()\n",
    "    print(\"Drift cron started (hourly).\")\n",
    "if GC_THREAD is None or not GC_THREAD.is_alive():\n",
    "    GC_THREAD = threading.Thread(target=_cron_loop, args=(\"GC\", _gc, 24*60*60), daemon=True)\n",
    "    GC_THREAD.start()\n",
    "    print(\"GC cron started (daily).\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "0a6dc106-5d87-4f42-9a51-79ba78b1e032",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'weights': {'A': 0.7641, 'B': 0.2359},\n",
       " 'policy_version': 613,\n",
       " 'scoreA': 0.05,\n",
       " 'scoreB': -0.185}"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import requests\n",
    "requests.post(\"http://127.0.0.1:8000/rl/update\").json()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "9d7989e3-898e-46c3-b484-ab5113c67cd4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'psi': {'age': 0.0,\n",
       "  'income': 0.0,\n",
       "  'balance': 0.0,\n",
       "  'tenure_months': 0.0,\n",
       "  'default': 0.0},\n",
       " 'ks_pvalue': {'age': 1.0,\n",
       "  'income': 1.0,\n",
       "  'balance': 1.0,\n",
       "  'tenure_months': 1.0,\n",
       "  'default': 1.0},\n",
       " 'is_drift': False}"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import requests\n",
    "requests.post(\"http://127.0.0.1:8000/drift/check\", params={\"namespace\":\"demo\"}).json()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "1cd33a9d-4413-4d01-963e-511abf35adfa",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'reclaimed_bytes': 0,\n",
       " 'ns_sizes': {'demo_1759787659': 8547722,\n",
       "  'demo_1759787734': 8467580,\n",
       "  'demo_1759787768': 8467579,\n",
       "  'demo_1759811125': 8547723,\n",
       "  'demo_1759811158': 8467580,\n",
       "  'demo_1759811182': 8467579,\n",
       "  'demo_1759857465': 8547722,\n",
       "  'demo_1759857554': 8467580,\n",
       "  'demo_1759857574': 8467579,\n",
       "  'demo_1759858808': 8547723,\n",
       "  'demo_1759858884': 8467580,\n",
       "  'demo_1759858908': 8467580,\n",
       "  'demo_1759897339': 8547723,\n",
       "  'demo_1759897370': 8467580,\n",
       "  'demo_1759897401': 8467580,\n",
       "  'demo_1759963665': 8547722,\n",
       "  'demo_1759963722': 8467580,\n",
       "  'demo_1759963763': 8467580,\n",
       "  'demo_1759984056': 8547722,\n",
       "  'demo_1759984091': 8467580,\n",
       "  'demo_1759984116': 8467580,\n",
       "  'demo_1759985783': 8547723,\n",
       "  'demo_1759985825': 8467580,\n",
       "  'demo_1759985849': 8467580,\n",
       "  'demo_1760045336': 8547723,\n",
       "  'demo_1760045383': 8467580,\n",
       "  'demo_1760045429': 8467580,\n",
       "  'demo_1760046840': 8547723,\n",
       "  'demo_1760046907': 8467580,\n",
       "  'demo_1760046940': 8467580,\n",
       "  'demo_1760071107': 8547721,\n",
       "  'demo_1760071156': 8467580,\n",
       "  'demo_1760071189': 8467580,\n",
       "  'demo_1760132046': 8547722,\n",
       "  'demo_1760132098': 8467579,\n",
       "  'demo_1760132129': 8467580,\n",
       "  'demo_1760388497': 8547723,\n",
       "  'demo_1760388562': 8467579,\n",
       "  'demo_1760388685': 8467580,\n",
       "  'demo_1760413866': 8547723,\n",
       "  'demo_1760413904': 8467580,\n",
       "  'demo_1760413945': 8467580,\n",
       "  'demo_1760564073': 8547723,\n",
       "  'demo_1760564148': 8467580,\n",
       "  'demo_1760564190': 8467580,\n",
       "  'demo_1760565230': 8547723,\n",
       "  'demo_1760565293': 8467579,\n",
       "  'demo_1760565328': 8467580,\n",
       "  'demo_1760587877': 8547722,\n",
       "  'demo_1760587910': 8467579,\n",
       "  'demo_1760587954': 8467580,\n",
       "  'demo_1760651812': 8547723,\n",
       "  'demo_1760651858': 8467580,\n",
       "  'demo_1760651887': 8467580,\n",
       "  'demo_1760675709': 8547722,\n",
       "  'demo_1760675739': 8467580,\n",
       "  'demo_1760675768': 8467580,\n",
       "  'demo_1760975553': 8547723,\n",
       "  'demo_1760975586': 8467579,\n",
       "  'demo_1760975626': 8467580,\n",
       "  'demo_1760994603': 8547723,\n",
       "  'demo_1760994669': 8467580,\n",
       "  'demo_1760994709': 8467580,\n",
       "  'demo_1760996991': 8547723,\n",
       "  'demo_1760997056': 8467579,\n",
       "  'demo_1760997100': 8467580,\n",
       "  'demo_1761087798': 8547722,\n",
       "  'demo_1761087844': 8467580,\n",
       "  'demo_1761087876': 8467580,\n",
       "  'demo_1761193763': 8547723,\n",
       "  'demo_1761193928': 8467580,\n",
       "  'demo_1761193989': 8467580,\n",
       "  'demo_1761197518': 8547723,\n",
       "  'demo_1761197579': 8467580,\n",
       "  'demo_1761197671': 8467580,\n",
       "  'demo_1761280897': 8547721,\n",
       "  'demo_1761280986': 8467579,\n",
       "  'demo_1761281060': 8467579,\n",
       "  'demo_1761340446': 8547722,\n",
       "  'demo_1761340502': 8467580,\n",
       "  'demo_1761340535': 8467580,\n",
       "  'demo_1761413142': 8547723,\n",
       "  'demo_1761413251': 8467580,\n",
       "  'demo_1761413283': 8467580,\n",
       "  'demo_1761582248': 8547723,\n",
       "  'demo_1761582337': 8467580,\n",
       "  'demo_1761582366': 8467579,\n",
       "  'demo_1761583890': 8547723,\n",
       "  'demo_1761583924': 8467580,\n",
       "  'demo_1761583955': 8467580,\n",
       "  'demo_1761626537': 8547723,\n",
       "  'demo_1761626604': 8467580,\n",
       "  'demo_1761626645': 8467580,\n",
       "  'demo_1761682373': 8547723,\n",
       "  'demo_1761682412': 8467579,\n",
       "  'demo_1761682439': 8467580,\n",
       "  'demo_1761713320': 8547722,\n",
       "  'demo_1761713349': 8467580,\n",
       "  'demo_1761713382': 8467580,\n",
       "  'demo_1761775616': 8547722,\n",
       "  'demo_1761775678': 8467580,\n",
       "  'demo_1761775719': 8467580,\n",
       "  'demo_1761801829': 8547723,\n",
       "  'demo_1761801869': 8467580,\n",
       "  'demo_1761801946': 8467579,\n",
       "  'demo_1761838472': 8547721,\n",
       "  'demo_1761838559': 8467579,\n",
       "  'demo_1761838628': 8467580,\n",
       "  'demo_1762113055': 8547723,\n",
       "  'demo_1762113098': 8467580,\n",
       "  'synthetic_credit_1759787978': 7680494,\n",
       "  'synthetic_credit_1759811446': 7680494,\n",
       "  'synthetic_credit_1759857830': 7680494,\n",
       "  'synthetic_credit_1759859471': 7680494,\n",
       "  'synthetic_credit_1759897657': 7680494,\n",
       "  'synthetic_credit_1759964119': 7680494,\n",
       "  'synthetic_credit_1759984325': 7680494,\n",
       "  'synthetic_credit_1759986061': 7680494,\n",
       "  'synthetic_credit_1760045751': 7680494,\n",
       "  'synthetic_credit_1760071468': 7680494,\n",
       "  'synthetic_credit_1760132366': 7680494,\n",
       "  'synthetic_credit_1760389534': 7680494,\n",
       "  'synthetic_credit_1760414167': 7680494,\n",
       "  'synthetic_credit_1760564440': 7680494,\n",
       "  'synthetic_credit_1760565584': 7680494,\n",
       "  'synthetic_credit_1760588168': 7680494,\n",
       "  'synthetic_credit_1760652156': 7680494,\n",
       "  'synthetic_credit_1760675964': 7680494,\n",
       "  'synthetic_credit_1760976341': 7680494,\n",
       "  'synthetic_credit_1760995026': 7680494,\n",
       "  'synthetic_credit_1760997334': 7680494,\n",
       "  'synthetic_credit_1761088106': 7680494,\n",
       "  'synthetic_credit_1761194276': 7680494,\n",
       "  'synthetic_credit_1761197992': 7680494,\n",
       "  'synthetic_credit_1761281324': 7680494,\n",
       "  'synthetic_credit_1761340791': 7680494,\n",
       "  'synthetic_credit_1761413505': 7680494,\n",
       "  'synthetic_credit_1761582725': 7680494,\n",
       "  'synthetic_credit_1761584180': 7680494,\n",
       "  'synthetic_credit_1761626870': 7680494,\n",
       "  'synthetic_credit_1761682677': 7680494,\n",
       "  'synthetic_credit_1761713595': 7680494,\n",
       "  'synthetic_credit_1761776457': 7680494,\n",
       "  'synthetic_credit_1761802147': 7680494,\n",
       "  'synthetic_credit_1761838916': 7680494}}"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import requests\n",
    "requests.post(\"http://127.0.0.1:8000/cache/clean\").json()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "91a8eb23-916d-455b-80c3-ce4801cbd988",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, time, json, random, shutil, math\n",
    "from pathlib import Path\n",
    "import requests\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "BASE = \"http://127.0.0.1:8000\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "fd60cf25-3c7d-482f-bc11-0d68bb30ab0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TestFail(Exception): pass\n",
    "def assert_true(cond, msg=\"expected condition to be True\"):\n",
    "    if not cond: raise TestFail(msg)\n",
    "def assert_eq(a, b, msg=None):\n",
    "    if a != b: raise TestFail(msg or f\"expected {a} == {b}\")\n",
    "def _print_ok(name): print(f\"{name}\")\n",
    "def _print_skip(name, why): print(f\"SKIP {name}  {why}\")\n",
    "def _print_fail(name, err): print(f\"{name}\\n    {err}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "cb9d1c63-a8e5-4d8b-b0fa-68d1ef82b918",
   "metadata": {},
   "outputs": [],
   "source": [
    "csv_path = Path(\"data/synthetic_credit.csv\")\n",
    "assert_true(csv_path.exists(), \"seed synthetic_credit.csv first (your earlier cell creates it)\")\n",
    "def test_upload_basic_contract():\n",
    "    with open(csv_path, \"rb\") as f:\n",
    "        resp = requests.post(f\"{BASE}/upload\",\n",
    "                             data={\"namespace\":\"demo\"},\n",
    "                             files={\"file\":(\"synthetic_credit.csv\", f, \"text/csv\")})\n",
    "    assert_true(resp.ok, f\"/upload HTTP {resp.status_code}\")\n",
    "    j = resp.json()\n",
    "    for k in [\"job_id\",\"status\",\"profile_uri\",\"rows_used\"]:\n",
    "        assert_true(k in j, f\"/upload missing key: {k}\")\n",
    "    assert_true(j[\"status\"] in (\"BASIC_DONE\",\"BASIC_RUNNING\"), \"bad status\")\n",
    "    _print_ok(\"upload_basic_contract\")\n",
    "    return j[\"job_id\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "0c9808fa-352c-4b11-a379-41251f110e68",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stage-0 tests starting\n",
      "upload_basic_contract\n",
      "advanced_ready_status\n",
      "query_contract\n",
      " Sample /query result:\n",
      "{\n",
      "  \"request_id\": \"demo_1762113148-1762113154\",\n",
      "  \"action\": \"run_sql\",\n",
      "  \"payload\": {\n",
      "    \"sql\": \"SELECT age, income, balance, default FROM dataset LIMIT 5;\"\n",
      "  },\n",
      "  \"citations\": [],\n",
      "  \"model_id\": \"slm-sql-v0\",\n",
      "  \"confidence\": 0.82,\n",
      "  \"meta\": {\n",
      "    \"route\": \"sql\",\n",
      "    \"route_conf\": 0.82,\n",
      "    \"retriever\": \"faiss\",\n",
      "    \"latency_ms\": 66\n",
      "  }\n",
      "}\n",
      "Stage-0 complete\n"
     ]
    }
   ],
   "source": [
    "def test_advanced_ready(job_id):\n",
    "    for _ in range(30):\n",
    "        r = requests.get(f\"{BASE}/advanced\", params={\"job_id\": job_id})\n",
    "        assert_true(r.ok, f\"/advanced HTTP {r.status_code}\")\n",
    "        s = r.json().get(\"status\")\n",
    "        if s == \"ADVANCED_READY\":\n",
    "            _print_ok(\"advanced_ready_status\")\n",
    "            return True\n",
    "        time.sleep(0.5)\n",
    "    _print_skip(\"advanced_ready_status\", \"still building (ok on first run)\")\n",
    "    return False\n",
    "def test_query_contract(job_id):\n",
    "    body = {\n",
    "        \"job_id\": job_id,\n",
    "        \"namespace\": \"demo\",\n",
    "        \"query_text\": \"select age, income and balance where default = 1 limit 5\",\n",
    "        \"top_k\": 5\n",
    "    }\n",
    "    r = requests.post(f\"{BASE}/query\", json=body)\n",
    "    assert_true(r.ok, f\"/query HTTP {r.status_code}\")\n",
    "    j = r.json()\n",
    "    for k in [\"request_id\",\"action\",\"payload\",\"citations\",\"model_id\",\"confidence\",\"meta\"]:\n",
    "        assert_true(k in j, f\"/query missing key: {k}\")\n",
    "    for mk in [\"route\",\"route_conf\",\"retriever\",\"latency_ms\"]:\n",
    "        assert_true(mk in j[\"meta\"], f\"/query.meta missing: {mk}\")\n",
    "    _print_ok(\"query_contract\")\n",
    "    return j\n",
    "def run_stage0_suite():\n",
    "    print(\"Stage-0 tests starting\")\n",
    "    job_id = test_upload_basic_contract()\n",
    "    test_advanced_ready(job_id)\n",
    "    res = test_query_contract(job_id)\n",
    "    print(\" Sample /query result:\")\n",
    "    print(json.dumps(res, indent=2)[:800])\n",
    "    print(\"Stage-0 complete\")\n",
    "    return job_id\n",
    "JOB_ID = run_stage0_suite()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "39ebfc11-9684-4622-967b-80c5b25dd8ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import json, time, hashlib\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "LOGS = Path(\"logs\"); LOGS.mkdir(exist_ok=True, parents=True)\n",
    "SHADOW_LOG = LOGS / \"shadow_log.jsonl\"\n",
    "REWARD_WEIGHTS = {\"alpha\": 0.5, \"beta\": 0.4, \"gamma\": 0.0005, \"delta\": 10.0}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "57adf33d-8eb0-4f92-8a5b-4ddb71e93f76",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prd_reward_proxy(out_json) -> float:\n",
    "    faith = 0.90 if out_json[\"action\"] == \"respond\" else 0.80\n",
    "    ndcg = 0.66 if out_json[\"meta\"][\"retriever\"] != \"bm25\" else 0.61\n",
    "    p95_ms = float(out_json[\"meta\"][\"latency_ms\"])\n",
    "    cost = 0.012 if out_json[\"meta\"][\"retriever\"] == \"bm25\" else 0.016\n",
    "    a,b,g,d = (REWARD_WEIGHTS[k] for k in (\"alpha\",\"beta\",\"gamma\",\"delta\"))\n",
    "    return a*faith + b*ndcg - g*p95_ms - d*cost\n",
    "def call_prod_query(job_id, q, top_k=5):\n",
    "    r = requests.post(f\"{BASE}/query\", json={\"job_id\": job_id, \"namespace\":\"demo\", \"query_text\": q, \"top_k\": top_k})\n",
    "    r.raise_for_status()\n",
    "    return r.json()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "115d882e-e92b-4424-9367-1a410b0c78c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def emulate_candidate(out_json):\n",
    "    cand = json.loads(json.dumps(out_json)) \n",
    "    cand[\"meta\"][\"retriever\"] = \"bm25\"\n",
    "    cand[\"meta\"][\"latency_ms\"] = max(1, int(out_json[\"meta\"][\"latency_ms\"] * 0.75))\n",
    "    cand[\"action\"] = \"respond\" if out_json[\"action\"] == \"respond\" else out_json[\"action\"]\n",
    "    return cand\n",
    "def shadow_once(job_id, query_text, top_k=5):\n",
    "    prod = call_prod_query(job_id, query_text, top_k=top_k)\n",
    "    cand = emulate_candidate(prod)\n",
    "    prod_reward = prd_reward_proxy(prod)\n",
    "    cand_reward = prd_reward_proxy(cand)\n",
    "    delta = cand_reward - prod_reward\n",
    "    rec = {\n",
    "        \"ts\": int(time.time()),\n",
    "        \"job_id\": job_id,\n",
    "        \"query\": query_text,\n",
    "        \"prod\": prod,\n",
    "        \"cand\": cand,\n",
    "        \"reward_prod\": round(prod_reward, 6),\n",
    "        \"reward_cand\": round(cand_reward, 6),\n",
    "        \"delta\": round(delta, 6),\n",
    "    }\n",
    "    with open(SHADOW_LOG, \"a\") as f:\n",
    "        f.write(json.dumps(rec) + \"\\n\")\n",
    "    return rec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "9f32d24c-c0b2-45b7-9566-3dfe9437bace",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shadow batch\n",
      "  What are common notes for customers with late fe  reward=+0.0455  p(ms)=204/153  retr=faissbm25\n",
      "  select age, income and balance where default = 1  reward=+0.0260  p(ms)=47/35  retr=faissbm25\n",
      "  average income by tenure_months where default=0   reward=+0.0270  p(ms)=56/42  retr=faissbm25\n",
      "  How many records mention chargeback risk?         reward=+0.0285  p(ms)=66/49  retr=faissbm25\n",
      "  Show top 5 customers with highest balances        reward=+0.0290  p(ms)=72/54  retr=faissbm25\n",
      "Shadow complete  logs\\shadow_log.jsonl\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>query</th>\n",
       "      <th>prod_reward</th>\n",
       "      <th>cand_reward</th>\n",
       "      <th>delta</th>\n",
       "      <th>prod_ms</th>\n",
       "      <th>cand_ms</th>\n",
       "      <th>prod_retr</th>\n",
       "      <th>cand_retr</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>What are common notes for customers with late ...</td>\n",
       "      <td>0.4520</td>\n",
       "      <td>0.4975</td>\n",
       "      <td>0.0455</td>\n",
       "      <td>204</td>\n",
       "      <td>153</td>\n",
       "      <td>faiss</td>\n",
       "      <td>bm25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>select age, income and balance where default =...</td>\n",
       "      <td>0.4805</td>\n",
       "      <td>0.5065</td>\n",
       "      <td>0.0260</td>\n",
       "      <td>47</td>\n",
       "      <td>35</td>\n",
       "      <td>faiss</td>\n",
       "      <td>bm25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>average income by tenure_months where default=0</td>\n",
       "      <td>0.4760</td>\n",
       "      <td>0.5030</td>\n",
       "      <td>0.0270</td>\n",
       "      <td>56</td>\n",
       "      <td>42</td>\n",
       "      <td>faiss</td>\n",
       "      <td>bm25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>How many records mention chargeback risk?</td>\n",
       "      <td>0.5210</td>\n",
       "      <td>0.5495</td>\n",
       "      <td>0.0285</td>\n",
       "      <td>66</td>\n",
       "      <td>49</td>\n",
       "      <td>faiss</td>\n",
       "      <td>bm25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Show top 5 customers with highest balances</td>\n",
       "      <td>0.4680</td>\n",
       "      <td>0.4970</td>\n",
       "      <td>0.0290</td>\n",
       "      <td>72</td>\n",
       "      <td>54</td>\n",
       "      <td>faiss</td>\n",
       "      <td>bm25</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               query  prod_reward  \\\n",
       "0  What are common notes for customers with late ...       0.4520   \n",
       "1  select age, income and balance where default =...       0.4805   \n",
       "2    average income by tenure_months where default=0       0.4760   \n",
       "3          How many records mention chargeback risk?       0.5210   \n",
       "4         Show top 5 customers with highest balances       0.4680   \n",
       "\n",
       "   cand_reward   delta  prod_ms  cand_ms prod_retr cand_retr  \n",
       "0       0.4975  0.0455      204      153     faiss      bm25  \n",
       "1       0.5065  0.0260       47       35     faiss      bm25  \n",
       "2       0.5030  0.0270       56       42     faiss      bm25  \n",
       "3       0.5495  0.0285       66       49     faiss      bm25  \n",
       "4       0.4970  0.0290       72       54     faiss      bm25  "
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def run_shadow_batch(job_id, queries):\n",
    "    print(\"Shadow batch\")\n",
    "    rows = []\n",
    "    for q in queries:\n",
    "        rec = shadow_once(job_id, q)\n",
    "        rows.append({\"query\": q, \"prod_reward\": rec[\"reward_prod\"], \"cand_reward\": rec[\"reward_cand\"], \"delta\": rec[\"delta\"],\n",
    "                     \"prod_ms\": rec[\"prod\"][\"meta\"][\"latency_ms\"], \"cand_ms\": rec[\"cand\"][\"meta\"][\"latency_ms\"],\n",
    "                     \"prod_retr\": rec[\"prod\"][\"meta\"][\"retriever\"], \"cand_retr\": rec[\"cand\"][\"meta\"][\"retriever\"]})\n",
    "        print(f\"  {q[:48]:<48}  reward={rows[-1]['delta']:+.4f}  p(ms)={rows[-1]['prod_ms']}/{rows[-1]['cand_ms']}  retr={rows[-1]['prod_retr']}{rows[-1]['cand_retr']}\")\n",
    "    print(\"Shadow complete \", SHADOW_LOG)\n",
    "    return pd.DataFrame(rows)\n",
    "QUERIES = [\n",
    "    \"What are common notes for customers with late fees?\",\n",
    "    \"select age, income and balance where default = 1 limit 5\",\n",
    "    \"average income by tenure_months where default=0\",\n",
    "    \"How many records mention chargeback risk?\",\n",
    "    \"Show top 5 customers with highest balances\"\n",
    "]\n",
    "shadow_df = run_shadow_batch(JOB_ID, QUERIES)\n",
    "shadow_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "6dc2b022-4e29-4786-b99b-be3f470f935c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import deque\n",
    "CANARY_RATE = 0.02     \n",
    "WINDOW      = 200      \n",
    "ROLLBACK_DROP = 0.05   \n",
    "CANARY_LOG = LOGS / \"canary_log.jsonl\"\n",
    "prod_rewards = deque(maxlen=WINDOW)\n",
    "cand_rewards = deque(maxlen=WINDOW)\n",
    "canary_enabled = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "e0dbd26d-4a9e-4ebe-96e7-da7910faefb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def route_canary() -> bool:\n",
    "    return canary_enabled and (random.random() < CANARY_RATE)\n",
    "def canary_record(out):\n",
    "    r = prd_reward_proxy(out)\n",
    "    prod_rewards.append(r)\n",
    "def canary_eval_shadow(out):\n",
    "    c = emulate_candidate(out)\n",
    "    rc = prd_reward_proxy(c)\n",
    "    cand_rewards.append(rc)\n",
    "def canary_maybe_rollback():\n",
    "    if len(prod_rewards) < WINDOW or len(cand_rewards) < WINDOW:\n",
    "        return None\n",
    "    mp, mc = np.mean(prod_rewards), np.mean(cand_rewards)\n",
    "    drop = (mp - mc) / max(1e-9, mp)\n",
    "    if drop > ROLLBACK_DROP:\n",
    "        return {\"rollback\": True, \"mean_prod\": mp, \"mean_cand\": mc, \"drop\": drop}\n",
    "    return {\"rollback\": False, \"mean_prod\": mp, \"mean_cand\": mc, \"drop\": drop}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "1d1d3a6c-8a26-4d31-ab57-c4208ea28e65",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Canary run: 300 requests @ 2.0%\n",
      "   100 sent; retr=faiss  lat35ms\n",
      "   200 sent; retr=faiss  lat29ms\n",
      "   300 sent; retr=faiss  lat47ms\n",
      "Canary simulation complete  logs\\canary_log.jsonl\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'canary_enabled': True}"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def canary_run(job_id, queries, rounds=500):\n",
    "    global canary_enabled\n",
    "    print(f\"Canary run: {rounds} requests @ {CANARY_RATE*100:.1f}%\")\n",
    "    for i in range(rounds):\n",
    "        q = random.choice(queries)\n",
    "        out = call_prod_query(job_id, q)   \n",
    "        canary_record(out)\n",
    "        canary_eval_shadow(out)          \n",
    "        rb = canary_maybe_rollback()\n",
    "        rec = {\n",
    "            \"i\": i, \"query\": q, \"lat_ms\": out[\"meta\"][\"latency_ms\"], \"retr\": out[\"meta\"][\"retriever\"],\n",
    "            \"reward_prod\": prd_reward_proxy(out),\n",
    "        }\n",
    "        if rb:\n",
    "            rec.update({\"mean_prod\": rb[\"mean_prod\"], \"mean_cand\": rb[\"mean_cand\"], \"drop\": rb[\"drop\"], \"rollback\": rb[\"rollback\"]})\n",
    "            if rb[\"rollback\"] and canary_enabled:\n",
    "                canary_enabled = False\n",
    "                print(f\"Auto-rollback: candidate underperforming (drop={rb['drop']:.3f})\")\n",
    "        with open(CANARY_LOG, \"a\") as f:\n",
    "            f.write(json.dumps(rec) + \"\\n\")\n",
    "        if (i+1) % 100 == 0:\n",
    "            print(f\"   {i+1} sent; retr={out['meta']['retriever']}  lat{out['meta']['latency_ms']}ms\")\n",
    "    print(\"Canary simulation complete \", CANARY_LOG)\n",
    "    return {\"canary_enabled\": canary_enabled}\n",
    "canary_status = canary_run(JOB_ID, QUERIES, rounds=300)\n",
    "canary_status"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "141f3161-19c3-45ad-87bb-b3c8f2929c07",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import time, os, json, shutil"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "5a0457f9-4a23-4023-8668-0778e7997c15",
   "metadata": {},
   "outputs": [],
   "source": [
    "def chaos_rename_index(job_id):\n",
    "    adv_dir = Path(\"artifacts\") / \"advanced\" / job_id\n",
    "    idx_dir = adv_dir / \"index\"\n",
    "    if not idx_dir.exists():\n",
    "        print(\"No index dir to rename (maybe still building).\")\n",
    "        return None\n",
    "    tmp = adv_dir / \"_index_gone\"\n",
    "    if tmp.exists(): shutil.rmtree(tmp, ignore_errors=True)\n",
    "    idx_dir.rename(tmp)\n",
    "    print(\"Chaos: index directory temporarily removed.\")\n",
    "    return tmp\n",
    "def chaos_restore_index(job_id, backup_dir):\n",
    "    adv_dir = Path(\"artifacts\") / \"advanced\" / job_id\n",
    "    target = adv_dir / \"index\"\n",
    "    if target.exists(): shutil.rmtree(target, ignore_errors=True)\n",
    "    backup_dir.rename(target)\n",
    "    print(\"Restored index directory.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "a00c0735-9710-458a-9958-594a4a6be5a8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chaos: index directory temporarily removed.\n",
      " Response OK. retriever= bm25 lat(ms)= 243\n",
      "   (Expected: bm25 path and still structured action)\n",
      "Restored index directory.\n",
      "Latency spike scenario: wall=430ms, server_meta=26ms, retr=faiss\n",
      "Chaos tests done.\n"
     ]
    }
   ],
   "source": [
    "def chaos_test_index_loss(job_id):\n",
    "    backup = chaos_rename_index(job_id)\n",
    "    try:\n",
    "        out = call_prod_query(job_id, \"what are the most common notes?\", top_k=5)\n",
    "        print(\" Response OK. retriever=\", out[\"meta\"][\"retriever\"], \"lat(ms)=\", out[\"meta\"][\"latency_ms\"])\n",
    "        print(\"   (Expected: bm25 path and still structured action)\")\n",
    "        return out\n",
    "    finally:\n",
    "        if backup is not None:\n",
    "            chaos_restore_index(job_id, backup)\n",
    "def chaos_latency_spike(job_id):\n",
    "    q = \"select age, income where default = 1 limit 5\"\n",
    "    t0 = time.time()\n",
    "    time.sleep(0.4) \n",
    "    out = call_prod_query(job_id, q, top_k=5)\n",
    "    dt = int((time.time() - t0)*1000)\n",
    "    print(f\"Latency spike scenario: wall={dt}ms, server_meta={out['meta']['latency_ms']}ms, retr={out['meta']['retriever']}\")\n",
    "    return out\n",
    "_ = chaos_test_index_loss(JOB_ID)\n",
    "_ = chaos_latency_spike(JOB_ID)\n",
    "print(\"Chaos tests done.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "3c5ba876-6ff6-48a6-bd94-eb462b348224",
   "metadata": {},
   "outputs": [],
   "source": [
    "GOLDEN = [\n",
    "    (\"select age, income and balance where default = 1 limit 5\", {\"action\": \"run_sql\"}),\n",
    "    (\"What are common notes for customers with late fees?\", {\"action\": \"respond\"}),\n",
    "    (\"2+2*10 - 3\", {\"action\": \"respond\"}),  \n",
    "    (\"Do we comply with PII masking in logs?\", {\"action\": \"respond\"}) \n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "a6df1095-3a13-433c-9751-bd41b2d0719e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Golden regression\n",
      "  select age, income and balance where default = 1  got=run_sql expect=run_sql  OK\n",
      "  What are common notes for customers with late fe  got=respond expect=respond  OK\n",
      "  2+2*10 - 3                                        got=respond expect=respond  OK\n",
      "  Do we comply with PII masking in logs?            got=respond expect=respond  OK\n",
      "Golden regression passed\n"
     ]
    }
   ],
   "source": [
    "def run_golden(job_id, golden=GOLDEN):\n",
    "    print(\"Golden regression\")\n",
    "    fails = 0\n",
    "    for q, expect in golden:\n",
    "        out = call_prod_query(job_id, q)\n",
    "        ok = all(out.get(k) == v for k, v in expect.items())\n",
    "        print(f\"  {q[:48]:<48}  got={out.get('action')} expect={expect.get('action')}  {'OK' if ok else 'FAIL'}\")\n",
    "        if not ok: fails += 1\n",
    "    if fails: \n",
    "        raise TestFail(f\"Golden regression failed: {fails} mismatches\")\n",
    "    print(\"Golden regression passed\")\n",
    "run_golden(JOB_ID)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "26a52582-cd3f-46f1-a08b-cfedd52a1cc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, json, time, threading\n",
    "from pathlib import Path\n",
    "from typing import Any, Dict\n",
    "from fastapi import HTTPException, Request\n",
    "from fastapi.responses import JSONResponse\n",
    "from prometheus_client import Counter, Histogram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "1457bd00-2aed-4a02-b987-fac46551f6d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "SHADOW_ENABLED = True\n",
    "LOGS_DIR = Path(\"logs\"); LOGS_DIR.mkdir(parents=True, exist_ok=True)\n",
    "SHADOW_LOG = LOGS_DIR / \"shadow_live.jsonl\"\n",
    "SHADOW_REQS   = Counter(\"shadow_requests_total_v1\", \"Shadow mirrored queries\")\n",
    "SHADOW_DELTA  = Histogram(\"shadow_reward_delta_v1\", \"Shadow reward delta (cand - prod)\")\n",
    "SHADOW_LAT_MS = Histogram(\"shadow_latency_ms_v1\", \"Shadow candidate latency (ms)\")\n",
    "REWARD_WEIGHTS = {\"alpha\": 0.5, \"beta\": 0.4, \"gamma\": 0.0005, \"delta\": 10.0}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "58839969-bdf9-4406-a513-361d890aabaa",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prd_reward_proxy(out_json: Dict[str, Any]) -> float:\n",
    "    faith = 0.90 if out_json.get(\"action\") == \"respond\" else 0.80\n",
    "    ndcg  = 0.66 if out_json.get(\"meta\", {}).get(\"retriever\") != \"bm25\" else 0.61\n",
    "    p95_ms = float(out_json.get(\"meta\", {}).get(\"latency_ms\", 1500))\n",
    "    cost = 0.016 if out_json.get(\"meta\", {}).get(\"retriever\") != \"bm25\" else 0.012\n",
    "    a, b, g, d = (REWARD_WEIGHTS[k] for k in (\"alpha\",\"beta\",\"gamma\",\"delta\"))\n",
    "    return a*faith + b*ndcg - g*p95_ms - d*cost\n",
    "def emulate_candidate(prod_out: Dict[str, Any]) -> Dict[str, Any]:\n",
    "    cand = json.loads(json.dumps(prod_out))  \n",
    "    if cand.get(\"meta\", {}).get(\"retriever\") == \"faiss\":\n",
    "        cand[\"meta\"][\"retriever\"] = \"bm25\"\n",
    "    else:\n",
    "        cand[\"meta\"][\"retriever\"] = \"faiss\"\n",
    "    cand[\"meta\"][\"latency_ms\"] = max(1, int(cand[\"meta\"][\"latency_ms\"] * 0.85))\n",
    "    if prod_out.get(\"action\") != \"clarify\":\n",
    "        cand[\"action\"] = prod_out[\"action\"]\n",
    "    return cand"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "6255b2a8-69a2-42c8-b1c3-982d2e386225",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _shadow_worker(job_id: str, dataset_path: str, namespace: str,\n",
    "                   query_text: str, top_k: int, prod_out: Dict[str, Any]) -> None:\n",
    "    if not SHADOW_ENABLED:\n",
    "        return\n",
    "    try:\n",
    "        t0 = time.time()\n",
    "        cand_out = emulate_candidate(prod_out)\n",
    "        SHADOW_REQS.inc()\n",
    "        SHADOW_LAT_MS.observe(cand_out.get(\"meta\", {}).get(\"latency_ms\", 0))\n",
    "        r_prod = prd_reward_proxy(prod_out)\n",
    "        r_cand = prd_reward_proxy(cand_out)\n",
    "        delta  = r_cand - r_prod\n",
    "        SHADOW_DELTA.observe(delta)\n",
    "        rec = {\n",
    "            \"ts\": int(time.time()),\n",
    "            \"job_id\": job_id,\n",
    "            \"namespace\": namespace,\n",
    "            \"query\": query_text,\n",
    "            \"top_k\": top_k,\n",
    "            \"prod\": prod_out,\n",
    "            \"cand\": cand_out,\n",
    "            \"reward_prod\": round(r_prod, 6),\n",
    "            \"reward_cand\": round(r_cand, 6),\n",
    "            \"delta\": round(delta, 6),\n",
    "            \"elapsed_ms\": int((time.time() - t0) * 1000),\n",
    "        }\n",
    "        with open(SHADOW_LOG, \"a\", encoding=\"utf-8\") as f:\n",
    "            f.write(json.dumps(rec) + \"\\n\")\n",
    "    except Exception as e:\n",
    "        try:\n",
    "            with open(SHADOW_LOG, \"a\", encoding=\"utf-8\") as f:\n",
    "                f.write(json.dumps({\"ts\": int(time.time()),\n",
    "                                    \"error\": str(e),\n",
    "                                    \"job_id\": job_id,\n",
    "                                    \"namespace\": namespace}) + \"\\n\")\n",
    "        except Exception:\n",
    "            pass\n",
    "app.router.routes = [\n",
    "    r for r in app.router.routes\n",
    "    if not (getattr(r, \"path\", None) == \"/query\" and \"POST\" in getattr(r, \"methods\", set()))\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "6cb8835b-8441-40e6-98d4-68ffb23c5a4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "@app.post(\"/query\")\n",
    "async def query(request: Request):\n",
    "    REQS.labels(\"/query\").inc()\n",
    "    with LAT.labels(\"/query\").time():\n",
    "        params = await _extract_params(request)\n",
    "        job_id = params.get(\"job_id\")\n",
    "        namespace = params.get(\"namespace\", \"default\")\n",
    "        query_text = params.get(\"query_text\", \"\")\n",
    "        try:\n",
    "            top_k = int(params.get(\"top_k\", 5))\n",
    "        except Exception:\n",
    "            top_k = 5\n",
    "        if not job_id:\n",
    "            raise HTTPException(422, detail=\"job_id is required (send as form, JSON, or query param)\")\n",
    "        job_row = jobs_get(job_id)\n",
    "        if not job_row:\n",
    "            raise HTTPException(404, \"Unknown job_id\")\n",
    "        ds = job_row.get(\"dataset_path\")\n",
    "        if not ds or not Path(ds).exists():\n",
    "            raise HTTPException(400, \"Dataset path not found for job\")\n",
    "        prod_out = query_pipeline(\n",
    "            job_id=job_id,\n",
    "            dataset_path=ds,\n",
    "            namespace=namespace,\n",
    "            query_text=query_text,\n",
    "            top_k=top_k,\n",
    "        )\n",
    "        _ = threading.Thread(\n",
    "            target=_shadow_worker,\n",
    "            args=(job_id, ds, namespace, query_text, top_k, prod_out),\n",
    "            daemon=True,\n",
    "        )\n",
    "        _.start()\n",
    "        return JSONResponse(prod_out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "bc81df55-2842-47e2-8f32-e388657deb3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random, json, time, threading\n",
    "from collections import deque\n",
    "from pathlib import Path\n",
    "from typing import Any, Dict\n",
    "import numpy as np\n",
    "from fastapi import HTTPException, Request\n",
    "from fastapi.responses import JSONResponse\n",
    "from prometheus_client import Counter, Histogram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "a8bd26d3-2a83-4167-8536-7bd8a9b36d52",
   "metadata": {},
   "outputs": [],
   "source": [
    "CANARY_RATE      = 0.02      \n",
    "WINDOW           = 200       \n",
    "ROLLBACK_DROP    = 0.05      \n",
    "CANARY_LOG       = Path(\"logs/canary_live.jsonl\")\n",
    "CANARY_LOG.parent.mkdir(parents=True, exist_ok=True)\n",
    "CANARY_TOTAL     = Counter(\"canary_total_v1\", \"Total queries considered for canary\")\n",
    "CANARY_SELECTED  = Counter(\"canary_selected_v1\", \"Queries routed to candidate (canary)\")\n",
    "CANARY_DELTA     = Histogram(\"canary_reward_delta_v1\", \"Reward delta (candidate - prod)\")\n",
    "CANARY_LAT_MS    = Histogram(\"canary_latency_ms_v1\", \"Latency (ms) of returned arm\", [\"arm\"])\n",
    "CANARY_ENABLED   = True\n",
    "_prod_rewards    = deque(maxlen=WINDOW)\n",
    "_cand_rewards    = deque(maxlen=WINDOW)\n",
    "_state_lock      = threading.Lock()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "05d2fcac-5682-49be-b36e-57750c6296ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _read_policy_version() -> int:\n",
    "    try:\n",
    "        j = json.loads(POLICY_PATH.read_text())\n",
    "        return int(j.get(\"version\", 0))\n",
    "    except Exception:\n",
    "        return 0\n",
    "def _record_rewards(prod_out: Dict[str, Any], cand_out: Dict[str, Any]):\n",
    "    pr = prd_reward_proxy(prod_out)\n",
    "    cr = prd_reward_proxy(cand_out)\n",
    "    with _state_lock:\n",
    "        _prod_rewards.append(pr)\n",
    "        _cand_rewards.append(cr)\n",
    "    CANARY_DELTA.observe(cr - pr)\n",
    "    return pr, cr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "9ea65e16-e450-4f17-8083-e616ab906eb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _maybe_rollback() -> Dict[str, Any]:\n",
    "    \"\"\"Returns rollback decision & stats. Also flips CANARY_ENABLED if needed.\"\"\"\n",
    "    global CANARY_ENABLED\n",
    "    with _state_lock:\n",
    "        if len(_prod_rewards) < WINDOW or len(_cand_rewards) < WINDOW:\n",
    "            return {\"rollback\": None, \"mean_prod\": None, \"mean_cand\": None, \"drop\": None}\n",
    "        mp = float(np.mean(_prod_rewards))\n",
    "        mc = float(np.mean(_cand_rewards))\n",
    "    drop = (mp - mc) / max(1e-9, mp)\n",
    "    if drop > ROLLBACK_DROP and CANARY_ENABLED:\n",
    "        CANARY_ENABLED = False\n",
    "        try:\n",
    "            jobs_set(\"CANARY\", status=\"AUTO_ROLLBACK\", mean_prod=mp, mean_cand=mc, drop=drop, ts=int(time.time()))\n",
    "        except Exception:\n",
    "            pass\n",
    "        return {\"rollback\": True, \"mean_prod\": mp, \"mean_cand\": mc, \"drop\": drop}\n",
    "    return {\"rollback\": False, \"mean_prod\": mp, \"mean_cand\": mc, \"drop\": drop}\n",
    "def _log_canary(rec: Dict[str, Any]):\n",
    "    try:\n",
    "        with open(CANARY_LOG, \"a\", encoding=\"utf-8\") as f:\n",
    "            f.write(json.dumps(rec) + \"\\n\")\n",
    "    except Exception:\n",
    "        pass\n",
    "app.router.routes = [\n",
    "    r for r in app.router.routes\n",
    "    if not (getattr(r, \"path\", None) == \"/query\" and \"POST\" in getattr(r, \"methods\", set()))\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "6e7851e8-8c4f-476c-9519-f6dbc26e7692",
   "metadata": {},
   "outputs": [],
   "source": [
    "@app.post(\"/query\")\n",
    "async def query(request: Request):\n",
    "    REQS.labels(\"/query\").inc()\n",
    "    CANARY_TOTAL.inc()\n",
    "    with LAT.labels(\"/query\").time():\n",
    "        params = await _extract_params(request)\n",
    "        job_id = params.get(\"job_id\")\n",
    "        namespace = params.get(\"namespace\", \"default\")\n",
    "        query_text = params.get(\"query_text\", \"\")\n",
    "        try:\n",
    "            top_k = int(params.get(\"top_k\", 5))\n",
    "        except Exception:\n",
    "            top_k = 5\n",
    "        if not job_id:\n",
    "            raise HTTPException(422, detail=\"job_id is required (send as form, JSON, or query param)\")\n",
    "        job_row = jobs_get(job_id)\n",
    "        if not job_row:\n",
    "            raise HTTPException(404, \"Unknown job_id\")\n",
    "        ds = job_row.get(\"dataset_path\")\n",
    "        if not ds or not Path(ds).exists():\n",
    "            raise HTTPException(400, \"Dataset path not found for job\")\n",
    "        prod_out = query_pipeline(\n",
    "            job_id=job_id,\n",
    "            dataset_path=ds,\n",
    "            namespace=namespace,\n",
    "            query_text=query_text,\n",
    "            top_k=top_k,\n",
    "        )\n",
    "        policy_version = _read_policy_version()\n",
    "        selected = CANARY_ENABLED and (random.random() < CANARY_RATE)\n",
    "        if selected:\n",
    "            CANARY_SELECTED.inc()\n",
    "            cand_out = emulate_candidate(prod_out)\n",
    "            r_prod, r_cand = _record_rewards(prod_out, cand_out)\n",
    "            rb = _maybe_rollback()\n",
    "            cand_out.setdefault(\"meta\", {})\n",
    "            cand_out[\"meta\"].update({\n",
    "                \"policy_version\": policy_version,\n",
    "                \"canary\": True,\n",
    "                \"arm\": \"CANDIDATE\"\n",
    "            })\n",
    "            prod_out.setdefault(\"meta\", {})\n",
    "            prod_out[\"meta\"].update({\n",
    "                \"policy_version\": policy_version,\n",
    "                \"canary\": False,\n",
    "                \"arm\": \"PROD\"\n",
    "            })\n",
    "            _log_canary({\n",
    "                \"ts\": int(time.time()),\n",
    "                \"job_id\": job_id,\n",
    "                \"namespace\": namespace,\n",
    "                \"query\": query_text,\n",
    "                \"selected\": True,\n",
    "                \"policy_version\": policy_version,\n",
    "                \"reward_prod\": r_prod,\n",
    "                \"reward_cand\": r_cand,\n",
    "                \"delta\": round(r_cand - r_prod, 6),\n",
    "                \"rollback\": rb.get(\"rollback\"),\n",
    "                \"mean_prod\": rb.get(\"mean_prod\"),\n",
    "                \"mean_cand\": rb.get(\"mean_cand\"),\n",
    "                \"drop\": rb.get(\"drop\"),\n",
    "            })\n",
    "            CANARY_LAT_MS.labels(\"CANDIDATE\").observe(float(cand_out.get(\"meta\", {}).get(\"latency_ms\", 0)))\n",
    "            threading.Thread(\n",
    "                target=_shadow_worker,\n",
    "                args=(job_id, ds, namespace, query_text, top_k, cand_out),  \n",
    "                daemon=True,\n",
    "            ).start()\n",
    "            return JSONResponse(cand_out)\n",
    "        prod_out.setdefault(\"meta\", {})\n",
    "        prod_out[\"meta\"].update({\n",
    "            \"policy_version\": policy_version,\n",
    "            \"canary\": False,\n",
    "            \"arm\": \"PROD\"\n",
    "        })\n",
    "        CANARY_LAT_MS.labels(\"PROD\").observe(float(prod_out.get(\"meta\", {}).get(\"latency_ms\", 0)))\n",
    "        threading.Thread(\n",
    "            target=_shadow_worker,\n",
    "            args=(job_id, ds, namespace, query_text, top_k, prod_out),\n",
    "            daemon=True,\n",
    "        ).start()\n",
    "        return JSONResponse(prod_out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "cda5dd3a-a268-48d1-b3a8-185afdbc4e45",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, json, time, math, random, gc, shutil, warnings\n",
    "from pathlib import Path\n",
    "from typing import List, Dict, Any, Optional, Tuple\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "d9f27cd1-3c34-436b-ae01-3c228a69a886",
   "metadata": {},
   "outputs": [],
   "source": [
    "PHASE2_ROOT = Path(\"artifacts\") / \"advanced_v2\"\n",
    "PHASE2_ROOT.mkdir(parents=True, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "aa25324a-cb2f-4142-a6c6-a6c6dda9ae7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _ensure_dir(p: Path):\n",
    "    p.mkdir(parents=True, exist_ok=True)\n",
    "    return p\n",
    "def _jdump(obj, path: Path):\n",
    "    path.parent.mkdir(parents=True, exist_ok=True)\n",
    "    with open(path, \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(obj, f, indent=2)\n",
    "def _jload(path: Path, default=None):\n",
    "    try:\n",
    "        return json.loads(Path(path).read_text())\n",
    "    except Exception:\n",
    "        return default\n",
    "def _seed_all(seed=42):\n",
    "    import random, numpy as np\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    try:\n",
    "        import torch\n",
    "        torch.manual_seed(seed)\n",
    "        torch.cuda.manual_seed_all(seed)\n",
    "    except Exception:\n",
    "        pass\n",
    "_seed_all(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "c3621ce7-935d-41ee-8a47-92d72e6174e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _build_reranker_pairs(df: pd.DataFrame, text_cols: List[str], n_samples: int = 300) -> List[Tuple[str,str,int]]:\n",
    "    from rank_bm25 import BM25Okapi\n",
    "    docs = df[text_cols].astype(str).agg(\" \".join, axis=1).tolist()\n",
    "    tokens = [d.split() for d in docs]\n",
    "    bm25 = BM25Okapi(tokens)\n",
    "    pairs = []\n",
    "    rng = np.random.default_rng(42)\n",
    "    for _ in range(min(n_samples, len(docs))):\n",
    "        i = rng.integers(0, len(docs))\n",
    "        doc = docs[i]\n",
    "        words = doc.split()\n",
    "        if len(words) < 6:\n",
    "            q = \" \".join(words)\n",
    "        else:\n",
    "            s = rng.integers(0, max(1, len(words)-8))\n",
    "            q = \" \".join(words[s:s+8])\n",
    "        scores = bm25.get_scores(q.split())\n",
    "        top = int(np.argmax(scores))\n",
    "        pairs.append((q, docs[top], 1))\n",
    "        for _neg in range(1):\n",
    "            j = int(rng.integers(0, len(docs)))\n",
    "            if j == top: \n",
    "                j = (j+1) % len(docs)\n",
    "            pairs.append((q, docs[j], 0))\n",
    "    return pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "beaca5dc-fb0b-4301-8d3b-f1a40c2da532",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_reranker_ce(pairs: List[Tuple[str,str,int]], out_dir: Path, epochs: int = 1, batch_size: int = 16) -> Dict[str, Any]:\n",
    "    out_dir = _ensure_dir(out_dir)\n",
    "    meta = {\"status\": \"stub\"}\n",
    "    try:\n",
    "        from sentence_transformers import CrossEncoder, InputExample, losses\n",
    "        base = \"cross-encoder/ms-marco-MiniLM-L-6-v2\"\n",
    "        train_examples = [InputExample(texts=[q, d], label=float(y)) for (q,d,y) in pairs]\n",
    "        model = CrossEncoder(base, num_labels=1, max_length=256)\n",
    "        train_dataloader = torch.utils.data.DataLoader(train_examples, shuffle=True, batch_size=batch_size)\n",
    "        loss_fn = losses.BinaryCrossEntropyLoss(model)\n",
    "        model.fit(\n",
    "            train_dataloader=train_dataloader,\n",
    "            epochs=epochs,\n",
    "            warmup_steps=0,\n",
    "            output_path=str(out_dir),\n",
    "            save_best_model=False,\n",
    "            optimizer_params={'lr': 2e-5}\n",
    "        )\n",
    "        meta = {\"status\":\"ok\", \"model\":\"cross-encoder/ms-marco-MiniLM-L-6-v2\", \"epochs\":epochs, \"batch_size\":batch_size}\n",
    "    except Exception as e:\n",
    "        (out_dir / \"pytorch_model.bin\").write_bytes(b\"stub\")\n",
    "        meta = {\"status\":\"fallback_stub\", \"reason\": f\"{type(e).__name__}: {e}\"}\n",
    "    _jdump(meta, out_dir / \"train_meta.json\")\n",
    "    return meta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "bc48e40b-f324-4c59-82f5-6cc854307e4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _mk_task_pairs(df: pd.DataFrame, task: str, text_cols: List[str], n: int = 200) -> List[Dict[str,str]]:\n",
    "    rng = np.random.default_rng(7)\n",
    "    pairs = []\n",
    "    cols = df.columns.tolist()\n",
    "    docs = df[text_cols].astype(str).agg(\" \".join, axis=1).tolist() if text_cols else []\n",
    "    for _ in range(min(n, len(df))):\n",
    "        if task == \"sql\":\n",
    "            pick = rng.choice(cols, size=min(3, len(cols)), replace=False)\n",
    "            where_col = rng.choice(cols)\n",
    "            prompt = f\"Generate a SELECT query for columns {', '.join(pick)} with LIMIT 5. Dataset table name is dataset. Add a WHERE on {where_col}.\"\n",
    "            resp = f\"SELECT {', '.join(pick)} FROM dataset WHERE {where_col} IS NOT NULL LIMIT 5;\"\n",
    "        elif task == \"math\":\n",
    "            a, b = int(rng.integers(1, 100)), int(rng.integers(1, 100))\n",
    "            prompt = f\"Compute {a} + {b} * 2 - 3.\"\n",
    "            resp = f\"Result: {a + b*2 - 3}\"\n",
    "        elif task == \"policy\":\n",
    "            prompt = \"State a policy for handling PII in logs and SQL access within this analytics system.\"\n",
    "            resp = \"PII must be masked in logs; deny DML; enforce LIMIT on queries; audits record model_id and policy_version.\"\n",
    "        else:  \n",
    "            if docs:\n",
    "                s = str(rng.choice(docs))\n",
    "                prompt = \"Summarize the following customer note in one sentence:\\n\" + s[:300]\n",
    "                resp = \"Summary: \" + \" \".join(s.split()[:12])\n",
    "            else:\n",
    "                prompt = \"Respond briefly and factually to user questions.\"\n",
    "                resp = \"Acknowledged. Please provide more details.\"\n",
    "        pairs.append({\"instruction\": prompt, \"response\": resp})\n",
    "    return pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "439ed1c1-0b38-48a2-b513-4c7b66f9ab8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _slm_train_lora(task: str, pairs: List[Dict[str,str]], out_dir: Path, epochs: int = 1) -> Dict[str, Any]:\n",
    "    out_dir = _ensure_dir(out_dir)\n",
    "    meta = {\"status\":\"stub\"}\n",
    "    try:\n",
    "        import torch\n",
    "        from transformers import AutoTokenizer, AutoModelForCausalLM, Trainer, TrainingArguments, DataCollatorForLanguageModeling\n",
    "        from datasets import Dataset\n",
    "        from peft import LoraConfig, get_peft_model, TaskType\n",
    "        base = \"distilgpt2\"  \n",
    "        tok = AutoTokenizer.from_pretrained(base)\n",
    "        if tok.pad_token is None:\n",
    "            tok.pad_token = tok.eos_token\n",
    "        def fmt(ex):\n",
    "            text = f\"<s>[INST] {ex['instruction']} [/INST] {ex['response']}</s>\"\n",
    "            ids = tok(text, truncation=True, max_length=256)\n",
    "            return {\"input_ids\": ids[\"input_ids\"], \"attention_mask\": ids[\"attention_mask\"]}\n",
    "        ds = Dataset.from_list(pairs).map(fmt, remove_columns=[\"instruction\",\"response\"])\n",
    "        model = AutoModelForCausalLM.from_pretrained(base)\n",
    "        lcfg = LoraConfig(\n",
    "            task_type=TaskType.CAUSAL_LM,\n",
    "            r=8, lora_alpha=16, lora_dropout=0.05,\n",
    "            target_modules=[\"c_attn\",\"c_proj\"]  \n",
    "        )\n",
    "        model = get_peft_model(model, lcfg)\n",
    "        args = TrainingArguments(\n",
    "            output_dir=str(out_dir),\n",
    "            num_train_epochs=epochs,\n",
    "            per_device_train_batch_size=4,\n",
    "            gradient_accumulation_steps=1,\n",
    "            learning_rate=2e-4,\n",
    "            logging_steps=10,\n",
    "            save_steps=0,\n",
    "            save_total_limit=1,\n",
    "            report_to=[],\n",
    "            fp16=False\n",
    "        )\n",
    "        dlm = DataCollatorForLanguageModeling(tok, mlm=False)\n",
    "        trainer = Trainer(model=model, args=args, train_dataset=ds, data_collator=dlm)\n",
    "        trainer.train()\n",
    "        model.save_pretrained(str(out_dir))\n",
    "        tok.save_pretrained(str(out_dir))\n",
    "        meta = {\"status\":\"ok\", \"base\": base, \"epochs\": epochs, \"params\":{\"r\":8,\"alpha\":16,\"dropout\":0.05}}\n",
    "    except Exception as e:\n",
    "        (out_dir / \"adapter.safetensors\").write_bytes(b\"stub\")\n",
    "        meta = {\"status\":\"fallback_stub\", \"reason\": f\"{type(e).__name__}: {e}\"}\n",
    "    _jdump(meta, out_dir / \"train_meta.json\")\n",
    "    return meta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "4ccd992d-890a-4641-baf6-6f27bca37bf6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _ndcg_at_k(rels: List[int], k=10) -> float:\n",
    "    rels = rels[:k]\n",
    "    dcg = sum((rel / math.log2(i+2)) for i, rel in enumerate(rels))\n",
    "    ideal = sorted(rels, reverse=True)\n",
    "    idcg = sum((rel / math.log2(i+2)) for i, rel in enumerate(ideal))\n",
    "    return float(dcg / (idcg or 1.0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "42145295-b98b-4da7-ad82-84f4f5d6dc56",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_phase2(df: pd.DataFrame, text_cols: List[str], ce_dir: Path, n_queries: int = 25) -> Dict[str, Any]:\n",
    "    try:\n",
    "        from sentence_transformers import CrossEncoder\n",
    "        ce = CrossEncoder(str(ce_dir))\n",
    "        ce_ok = True\n",
    "    except Exception:\n",
    "        ce = None\n",
    "        ce_ok = False\n",
    "    docs = df[text_cols].astype(str).agg(\" \".join, axis=1).tolist()\n",
    "    rng = np.random.default_rng(13)\n",
    "    ndcgs, faiths, costs, lats = [], [], [], []\n",
    "    for _ in range(min(n_queries, len(docs))):\n",
    "        i = int(rng.integers(0, len(docs)))\n",
    "        w = docs[i].split()\n",
    "        q = \" \".join(w[:8]) if len(w) >= 8 else \" \".join(w)\n",
    "        cand_idx = rng.choice(len(docs), size=min(20, len(docs)), replace=False)\n",
    "        cands = [docs[j] for j in cand_idx]\n",
    "        rels = [1 if j == i else 0 for j in cand_idx]\n",
    "        t0 = time.time()\n",
    "        if ce_ok:\n",
    "            scores = ce.predict([[q, c] for c in cands])\n",
    "        else:\n",
    "            qv = _embed_query(\"query: \" + q)\n",
    "            model = _get_e5()\n",
    "            ev = model.encode(cands, normalize_embeddings=True, show_progress_bar=False).astype(np.float32)\n",
    "            scores = (ev @ qv).tolist()\n",
    "        lat_ms = (time.time()-t0)*1000.0\n",
    "        order = np.argsort(-np.array(scores))\n",
    "        rels_ranked = [rels[k] for k in order]\n",
    "        nd = _ndcg_at_k(rels_ranked, k=10)\n",
    "        ndcgs.append(nd)\n",
    "        top_txts = [cands[k] for k in order[:3]]\n",
    "        ans = \" \".join([t.split()[0] for t in top_txts])\n",
    "        ver = verify_faithfulness(ans, top_txts)\n",
    "        faiths.append(ver[\"faithfulness\"])\n",
    "        lats.append(lat_ms)\n",
    "        costs.append(0.018 if ce_ok else 0.012)\n",
    "    metrics = {\n",
    "        \"ndcg@10\": float(np.mean(ndcgs) if ndcgs else 0.0),\n",
    "        \"faithfulness\": float(np.mean(faiths) if faiths else 0.9),\n",
    "        \"p95\": int(np.percentile(lats, 95) if lats else 1500),\n",
    "        \"cost\": float(np.mean(costs) if costs else 0.015),\n",
    "        \"samples\": int(len(ndcgs))\n",
    "    }\n",
    "    return metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "d19a7f14-62b9-44de-aa5e-9667fe7f11ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "def advanced_trainer_v2(job_id: str, dataset_path: str, namespace: str, text_cols_guess=None,\n",
    "                        ce_epochs: int = 1, slm_epochs: int = 1) -> Dict[str, Any]:\n",
    "    \"\"\"\n",
    "    Phase-2 Advanced Trainer (DL + SLM):\n",
    "    - builds reranker pairs, trains CE (very light)\n",
    "    - trains LoRA adapters for tasks: general, sql, math, policy\n",
    "    - evaluates ndcg@10, faithfulness, p95, cost (proxy)\n",
    "    - writes artifacts and updates job status\n",
    "    \"\"\"\n",
    "    t_start = time.time()\n",
    "    base_dir = PHASE2_ROOT / job_id\n",
    "    _ensure_dir(base_dir)\n",
    "    try:\n",
    "        df = pd.read_parquet(dataset_path) if dataset_path.endswith(\".parquet\") else pd.read_csv(dataset_path)\n",
    "    except Exception as e:\n",
    "        raise RuntimeError(f\"Failed to load dataset: {e}\")\n",
    "    text_cols = text_cols_guess or guess_text_cols(df, CFG.get(\"bm25_text_cols_guess\", [\"text\",\"description\",\"notes\",\"body\"]))\n",
    "    _jdump(text_cols, base_dir / \"text_cols.json\")\n",
    "    jobs_set(job_id, status=\"ADV2_RERANK_BUILD\")\n",
    "    pairs = _build_reranker_pairs(df, text_cols, n_samples=min(400, len(df)))\n",
    "    ce_dir = base_dir / \"reranker_ce\"\n",
    "    ce_meta = train_reranker_ce(pairs, ce_dir, epochs=ce_epochs, batch_size=16)\n",
    "    jobs_set(job_id, status=\"ADV2_SLM_TRAIN\")\n",
    "    slm_meta = {}\n",
    "    for task in [\"general\",\"sql\",\"math\",\"policy\"]:\n",
    "        pairs_task = _mk_task_pairs(df, task, text_cols, n=min(300, len(df)))\n",
    "        out_dir = base_dir / f\"slm_{task}_lora\"\n",
    "        slm_meta[task] = _slm_train_lora(task, pairs_task, out_dir, epochs=slm_epochs)\n",
    "    jobs_set(job_id, status=\"ADV2_EVAL\")\n",
    "    metrics = evaluate_phase2(df, text_cols, ce_dir, n_queries=25)\n",
    "    _jdump({\"champion\":\"ce_minilm_v1\",\"ts\":int(time.time())}, base_dir / \"registry.json\")\n",
    "    out = {\n",
    "        \"job_id\": job_id,\n",
    "        \"namespace\": namespace,\n",
    "        \"artifacts\": {\n",
    "            \"text_cols\": text_cols,\n",
    "            \"reranker_dir\": str(ce_dir),\n",
    "            \"slm_adapters\": {k: str(base_dir / f\"slm_{k}_lora\") for k in slm_meta.keys()},\n",
    "            \"registry\": str(base_dir / \"registry.json\")\n",
    "        },\n",
    "        \"train_meta\": {\n",
    "            \"reranker\": ce_meta,\n",
    "            \"slm\": slm_meta\n",
    "        },\n",
    "        \"metrics\": metrics,\n",
    "        \"elapsed_sec\": round(time.time()-t_start, 2)\n",
    "    }\n",
    "    _jdump(out, base_dir / \"summary.json\")\n",
    "    jobs_set(job_id, status=\"ADVANCED_READY\", phase2=True, artifacts_v2=out)\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "25d96086-31e6-4697-8821-9d52c5fd69d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running Phase-2 Advanced Trainer\n",
      "Phase-2 done. Metrics: {'ndcg@10': 0.0, 'faithfulness': 0.9499999995099999, 'p95': 80, 'cost': 0.012000000000000002, 'samples': 25}\n",
      "Artifacts at: artifacts\\advanced_v2\\demo_1762113098\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    _job_id = JOB if \"JOB\" in globals() else out[\"job_id\"]\n",
    "    _csv = str(csv_path)\n",
    "    print(\"Running Phase-2 Advanced Trainer\")\n",
    "    summary_v2 = advanced_trainer_v2(job_id=_job_id, dataset_path=_csv, namespace=\"demo\",\n",
    "                                     ce_epochs=1, slm_epochs=1)\n",
    "    print(\"Phase-2 done. Metrics:\", summary_v2[\"metrics\"])\n",
    "    print(\"Artifacts at:\", PHASE2_ROOT / _job_id)\n",
    "except Exception as e:\n",
    "    print(\"Phase-2 failed:\", e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "a0885751-a659-4d56-aa6c-6d8744bf7c83",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, json, time\n",
    "from pathlib import Path\n",
    "from typing import Dict, Any, List, Optional, Tuple\n",
    "_CE_CACHE: Dict[str, Any] = {}         \n",
    "_SLM_CACHE: Dict[Tuple[str,str], Any] = {} "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "fa5fbdc1-9f8b-4eaa-b714-f9a88b95f9f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _phase2_paths(job_id: str) -> Dict[str, Path]:\n",
    "    base = PHASE2_ROOT / job_id\n",
    "    return {\n",
    "        \"base\": base,\n",
    "        \"ce_dir\": base / \"reranker_ce\",\n",
    "        \"slm_dirs\": {\n",
    "            \"general\": base / \"slm_general_lora\",\n",
    "            \"sql\":     base / \"slm_sql_lora\",\n",
    "            \"math\":    base / \"slm_math_lora\",\n",
    "            \"policy\":  base / \"slm_policy_lora\",\n",
    "        },\n",
    "        \"text_cols\": base / \"text_cols.json\",\n",
    "        \"summary\":   base / \"summary.json\",\n",
    "    }\n",
    "def _load_ce(job_id: str):\n",
    "    \"\"\"Load CE reranker for job_id if present (cached).\"\"\"\n",
    "    if job_id in _CE_CACHE:\n",
    "        return _CE_CACHE[job_id]\n",
    "    p = _phase2_paths(job_id)\n",
    "    ce_dir = p[\"ce_dir\"]\n",
    "    model = None\n",
    "    if ce_dir.exists():\n",
    "        try:\n",
    "            from sentence_transformers import CrossEncoder\n",
    "            model = CrossEncoder(str(ce_dir))\n",
    "        except Exception:\n",
    "            model = None\n",
    "    _CE_CACHE[job_id] = model\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "6ab752bf-e2aa-49e2-8ec6-f01e66974d05",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _load_lora(job_id: str, task: str):\n",
    "    \"\"\"Load LoRA adapter dir for (job, task) if present (cached).\"\"\"\n",
    "    key = (job_id, task)\n",
    "    if key in _SLM_CACHE:\n",
    "        return _SLM_CACHE[key]\n",
    "    p = _phase2_paths(job_id)\n",
    "    d = p[\"slm_dirs\"].get(task)\n",
    "    bundle = None\n",
    "    if d and d.exists():\n",
    "        try:\n",
    "            from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "            tok = AutoTokenizer.from_pretrained(str(d))\n",
    "            if tok.pad_token is None:\n",
    "                tok.pad_token = tok.eos_token\n",
    "            model = AutoModelForCausalLM.from_pretrained(str(d))\n",
    "            bundle = {\"tok\": tok, \"model\": model}\n",
    "        except Exception:\n",
    "            bundle = None\n",
    "    _SLM_CACHE[key] = bundle\n",
    "    return bundle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "d11f13f8-53ac-480c-a89f-619373b5ee11",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _rerank_ce_or_cosine(job_id: str, df, row_ids, text_cols, q_text, top_k=5):\n",
    "    \"\"\"Use CE if available for job; else cosine E5 (previous behavior).\"\"\"\n",
    "    ce = _load_ce(job_id)\n",
    "    if ce is None:\n",
    "        q_emb = _embed_query(\"query: \" + q_text)\n",
    "        return rerank_cosine(df, row_ids, text_cols, q_emb, top_k=top_k), \"cosine\"\n",
    "    texts = df.iloc[row_ids][text_cols].astype(str).agg(\" \".join, axis=1).tolist()\n",
    "    pairs = [[q_text, t] for t in texts]\n",
    "    try:\n",
    "        scores = ce.predict(pairs)\n",
    "    except Exception:\n",
    "        q_emb = _embed_query(\"query: \" + q_text)\n",
    "        return rerank_cosine(df, row_ids, text_cols, q_emb, top_k=top_k), \"cosine\"\n",
    "    order = np.argsort(-np.array(scores))[:top_k]\n",
    "    return (row_ids[order], np.array(scores)[order]), \"ce\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "ee07ab7b-c701-470c-9041-0d43ded5da5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _slm_generate_with_adapter(job_id: str, task: str, prompt: str, max_new_tokens=96) -> Optional[str]:\n",
    "    \"\"\"Generate with LoRA adapter if available; return None if unavailable.\"\"\"\n",
    "    bundle = _load_lora(job_id, task)\n",
    "    if bundle is None:\n",
    "        return None\n",
    "    tok = bundle[\"tok\"]; model = bundle[\"model\"]\n",
    "    try:\n",
    "        import torch\n",
    "        model.eval()\n",
    "        device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "        model.to(device)\n",
    "        inp = tok(prompt, return_tensors=\"pt\").to(device)\n",
    "        with torch.no_grad():\n",
    "            out = model.generate(\n",
    "                **inp,\n",
    "                do_sample=False,\n",
    "                max_new_tokens=max_new_tokens,\n",
    "                pad_token_id=tok.eos_token_id\n",
    "            )\n",
    "        txt = tok.decode(out[0], skip_special_tokens=True)\n",
    "        return txt\n",
    "    except Exception:\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "e6e7189f-66bb-4a12-84fc-d2da4037a564",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _compose_prompt(task: str, query: str, ctx_snips: List[str]) -> str:\n",
    "    ctx = \"\\n\".join(f\"- {s}\" for s in ctx_snips[:3])\n",
    "    if task == \"sql\":\n",
    "        return f\"[INST] Write a safe SELECT-only SQL answering: {query}\\nRules: deny DML; enforce LIMIT<=100; table name is dataset.\\nContext:\\n{ctx}\\n[/INST]\"\n",
    "    if task == \"math\":\n",
    "        return f\"[INST] Compute exactly and return only the numeric result. Question: {query}\\nContext (optional):\\n{ctx}\\n[/INST]\"\n",
    "    if task == \"policy\":\n",
    "        return f\"[INST] Briefly answer with system policy re: PII/logging/SQL safety. Query: {query}\\nContext:\\n{ctx}\\n[/INST]\"\n",
    "    return f\"[INST] Answer concisely with citations from context when possible. Query: {query}\\nContext:\\n{ctx}\\n[/INST]\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "b87844dc-f5e6-4aa7-911c-c5ecbcf1a042",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _respond_with_slm(job_id: str, route: str, df, row_ids, text_cols, query: str) -> Dict[str, Any]:\n",
    "    ctx_texts = df.iloc[row_ids][text_cols].astype(str).agg(\" \".join, axis=1).str.slice(0, 220).tolist() if len(row_ids) else []\n",
    "    task = \"general\" if route == \"factual\" else route\n",
    "    task = task if task in {\"general\",\"sql\",\"math\",\"policy\"} else \"general\"\n",
    "    prompt = _compose_prompt(task, query, ctx_texts)\n",
    "    gen = _slm_generate_with_adapter(job_id, task, prompt, max_new_tokens=96)\n",
    "    if gen is not None:\n",
    "        if task == \"sql\":\n",
    "            m = re.search(r\"(select\\s.+?;)\", gen, flags=re.I|re.S)\n",
    "            if m:\n",
    "                sql = sqlparse.format(m.group(1), reindent=False, keyword_case=\"upper\")\n",
    "            else:\n",
    "                cols = [c for c in df.columns if c.lower() in query.lower()]\n",
    "                cols = cols[:5] if cols else df.columns[:5]\n",
    "                sql = f\"SELECT {', '.join(cols)} FROM dataset LIMIT 5;\"\n",
    "            return {\"action\": \"run_sql\", \"payload\": {\"sql\": sql}, \"citations\": [], \"model_id\": f\"slm-{task}-lora\"}\n",
    "        if task == \"math\":\n",
    "            m = re.search(r\"(-?\\d+(\\.\\d+)?)\", gen)\n",
    "            ans = m.group(1) if m else gen.strip()\n",
    "            return {\"action\": \"respond\", \"payload\": {\"text\": f\"Result: {ans}\"}, \"citations\": [], \"model_id\": f\"slm-{task}-lora\"}\n",
    "        return {\"action\": \"respond\", \"payload\": {\"text\": gen.strip()}, \"citations\": [int(i) for i in row_ids[:5]], \"model_id\": f\"slm-{task}-lora\"}\n",
    "    if task == \"sql\":\n",
    "        return slm_run_sql(df, query, limit=5)\n",
    "    if task == \"math\":\n",
    "        return slm_math(query)\n",
    "    if task == \"policy\":\n",
    "        return slm_policy(query)\n",
    "    return slm_respond(df, row_ids, text_cols, query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "ea707fb4-b41c-43c1-890e-3a4f3d2cc4ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "def query_pipeline_v2(job_id: str,\n",
    "                      dataset_path: str,\n",
    "                      namespace: str = \"default\",\n",
    "                      query_text: str = \"\",\n",
    "                      top_k: int = 5) -> Dict[str, Any]:\n",
    "    t0 = time.time()\n",
    "    df = pd.read_parquet(dataset_path) if dataset_path.endswith(\".parquet\") else pd.read_csv(dataset_path)\n",
    "    adv2 = _phase2_paths(job_id)\n",
    "    if adv2[\"text_cols\"].exists():\n",
    "        text_cols = json.loads(adv2[\"text_cols\"].read_text())\n",
    "    else:\n",
    "        text_cols = guess_text_cols(df, CFG[\"bm25_text_cols_guess\"])\n",
    "    route, route_conf = route_query(query_text)\n",
    "    adv_dir = Path(\"artifacts\") / \"advanced\" / job_id\n",
    "    vi = VectorIndex.from_artifacts(adv_dir) if adv_dir.exists() else None\n",
    "    retriever = \"none\"\n",
    "    if vi is not None:\n",
    "        q_emb = _embed_query(\"query: \" + query_text)\n",
    "        row_ids, scores = vi.search(q_emb, top_k=top_k)\n",
    "        retriever = \"faiss\" if vi.index_type == \"faiss_ivfpq\" else \"numpy\"\n",
    "    elif text_cols:\n",
    "        bm25 = _bm25_build(df, text_cols)\n",
    "        row_ids, scores = _bm25_search(bm25, query_text, k=top_k)\n",
    "        retriever = \"bm25\"\n",
    "    else:\n",
    "        return {\n",
    "            \"request_id\": f\"{job_id}-{int(time.time())}\",\n",
    "            \"action\": \"clarify\",\n",
    "            \"payload\": {\"question\": \"I need at least one text column to search. Which field should I use?\"},\n",
    "            \"citations\": [],\n",
    "            \"model_id\": \"router-v0\",\n",
    "            \"confidence\": 0.55,\n",
    "            \"meta\": {\"route\": route, \"route_conf\": route_conf, \"retriever\": \"none\", \"reranker\": \"none\", \"slm\":\"none\", \"latency_ms\": int((time.time()-t0)*1000)}\n",
    "        }\n",
    "    if len(row_ids) > 0 and text_cols:\n",
    "        (row_ids, rr_scores), rr_kind = _rerank_ce_or_cosine(job_id, df, row_ids, text_cols, query_text, top_k=min(top_k, len(row_ids)))\n",
    "    else:\n",
    "        rr_kind = \"none\"\n",
    "    ctx_texts = df.iloc[row_ids][text_cols].astype(str).agg(\" \".join, axis=1).tolist() if len(row_ids) else []\n",
    "    resp = _respond_with_slm(job_id, route, df, row_ids, text_cols, query_text)\n",
    "    if resp[\"action\"] == \"respond\":\n",
    "        ans_text = resp[\"payload\"].get(\"text\", \"\")\n",
    "        ver = verify_faithfulness(ans_text, ctx_texts)\n",
    "    else:\n",
    "        ver = {\"faithfulness\": 0.9, \"entailment_prob\": 0.9}\n",
    "    if resp[\"action\"] == \"respond\" and ver[\"faithfulness\"] < 0.85:\n",
    "        resp = {\"action\": \"clarify\", \"payload\": {\"question\": \"Need more specifics to answer accurately.\"}, \"citations\": [], \"model_id\": resp.get(\"model_id\",\"slm-general\")}\n",
    "    out = {\n",
    "        \"request_id\": f\"{job_id}-{int(time.time())}\",\n",
    "        \"action\": resp[\"action\"],\n",
    "        \"payload\": resp[\"payload\"],\n",
    "        \"citations\": resp.get(\"citations\", [int(i) for i in row_ids.tolist()[:3]]),\n",
    "        \"model_id\": resp.get(\"model_id\", \"slm-general\"),\n",
    "        \"confidence\": float(min(0.99, max(0.55, route_conf))),\n",
    "        \"meta\": {\n",
    "            \"route\": route,\n",
    "            \"route_conf\": route_conf,\n",
    "            \"retriever\": retriever,\n",
    "            \"reranker\": rr_kind,\n",
    "            \"slm\": \"adapter\" if \"lora\" in resp.get(\"model_id\",\"\") else \"base\",\n",
    "            \"latency_ms\": int((time.time() - t0) * 1000)\n",
    "        }\n",
    "    }\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "694c9512-63a5-4bcb-a219-ec377103403e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from fastapi import HTTPException, Request\n",
    "from fastapi.responses import JSONResponse\n",
    "app.router.routes = [\n",
    "    r for r in app.router.routes\n",
    "    if not (getattr(r, \"path\", None) == \"/query\" and \"POST\" in getattr(r, \"methods\", set()))\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "5cd797b2-16c7-4e83-93de-4772c68071be",
   "metadata": {},
   "outputs": [],
   "source": [
    "@app.post(\"/query\")\n",
    "async def query(request: Request):\n",
    "    REQS.labels(\"/query\").inc()\n",
    "    CANARY_TOTAL.inc()\n",
    "    with LAT.labels(\"/query\").time():\n",
    "        params = await _extract_params(request)\n",
    "        job_id = params.get(\"job_id\")\n",
    "        namespace = params.get(\"namespace\", \"default\")\n",
    "        query_text = params.get(\"query_text\", \"\")\n",
    "        try:\n",
    "            top_k = int(params.get(\"top_k\", 5))\n",
    "        except Exception:\n",
    "            top_k = 5\n",
    "        if not job_id:\n",
    "            raise HTTPException(422, detail=\"job_id is required (send as form, JSON, or query param)\")\n",
    "        job_row = jobs_get(job_id)\n",
    "        if not job_row:\n",
    "            raise HTTPException(404, \"Unknown job_id\")\n",
    "        ds = job_row.get(\"dataset_path\")\n",
    "        if not ds or not Path(ds).exists():\n",
    "            raise HTTPException(400, \"Dataset path not found for job\")\n",
    "        prod_out = query_pipeline_v2(\n",
    "            job_id=job_id,\n",
    "            dataset_path=ds,\n",
    "            namespace=namespace,\n",
    "            query_text=query_text,\n",
    "            top_k=top_k,\n",
    "        )\n",
    "        policy_version = _read_policy_version()\n",
    "        selected = CANARY_ENABLED and (random.random() < CANARY_RATE)\n",
    "        if selected:\n",
    "            CANARY_SELECTED.inc()\n",
    "            cand_out = emulate_candidate(prod_out)\n",
    "            r_prod, r_cand = _record_rewards(prod_out, cand_out)\n",
    "            rb = _maybe_rollback()\n",
    "            cand_out.setdefault(\"meta\", {})\n",
    "            cand_out[\"meta\"].update({\"policy_version\": policy_version, \"canary\": True, \"arm\": \"CANDIDATE\"})\n",
    "            prod_out.setdefault(\"meta\", {})\n",
    "            prod_out[\"meta\"].update({\"policy_version\": policy_version, \"canary\": False, \"arm\": \"PROD\"})\n",
    "            _log_canary({\n",
    "                \"ts\": int(time.time()),\n",
    "                \"job_id\": job_id,\n",
    "                \"namespace\": namespace,\n",
    "                \"query\": query_text,\n",
    "                \"selected\": True,\n",
    "                \"policy_version\": policy_version,\n",
    "                \"reward_prod\": r_prod,\n",
    "                \"reward_cand\": r_cand,\n",
    "                \"delta\": round(r_cand - r_prod, 6),\n",
    "                \"rollback\": rb.get(\"rollback\"),\n",
    "                \"mean_prod\": rb.get(\"mean_prod\"),\n",
    "                \"mean_cand\": rb.get(\"mean_cand\"),\n",
    "                \"drop\": rb.get(\"drop\"),\n",
    "            })\n",
    "            CANARY_LAT_MS.labels(\"CANDIDATE\").observe(float(cand_out.get(\"meta\", {}).get(\"latency_ms\", 0)))\n",
    "            threading.Thread(target=_shadow_worker, args=(job_id, ds, namespace, query_text, top_k, cand_out), daemon=True).start()\n",
    "            return JSONResponse(cand_out)\n",
    "        prod_out.setdefault(\"meta\", {})\n",
    "        prod_out[\"meta\"].update({\"policy_version\": policy_version, \"canary\": False, \"arm\": \"PROD\"})\n",
    "        CANARY_LAT_MS.labels(\"PROD\").observe(float(prod_out.get(\"meta\", {}).get(\"latency_ms\", 0)))\n",
    "        threading.Thread(target=_shadow_worker, args=(job_id, ds, namespace, query_text, top_k, prod_out), daemon=True).start()\n",
    "        return JSONResponse(prod_out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "749c2c50-b23e-4910-9ab7-b49deb1fe644",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "69b0d302-e671-419a-ac2a-2b2df8c9e796",
   "metadata": {},
   "outputs": [],
   "source": [
    "GATES = {\n",
    "    \"ndcg@10_min\": 0.62,\n",
    "    \"faithfulness_min\": 0.85,\n",
    "    \"p95_max\": 1800,\n",
    "    \"cost_max\": 0.018\n",
    "}\n",
    "def _passes_gates(metrics: Dict[str, Any], gates: Dict[str, float] = GATES) -> bool:\n",
    "    return (\n",
    "        float(metrics.get(\"ndcg@10\", 0)) >= gates[\"ndcg@10_min\"] and\n",
    "        float(metrics.get(\"faithfulness\", 0)) >= gates[\"faithfulness_min\"] and\n",
    "        float(metrics.get(\"p95\", 999999)) <= gates[\"p95_max\"] and\n",
    "        float(metrics.get(\"cost\", 999)) <= gates[\"cost_max\"]\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "8c11f53a-4399-4a64-98a1-7b9bb7057a85",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_phase2_and_promote(dataset_path: str, namespace: str, job_id: Optional[str] = None,\n",
    "                             ce_epochs: int = 1, slm_epochs: int = 1) -> Dict[str, Any]:\n",
    "    if job_id is None:\n",
    "        job_id = f\"{namespace}_{int(time.time())}\"\n",
    "    print(f\"[Phase-2] {job_id}    {dataset_path}\")\n",
    "    out = advanced_trainer_v2(job_id=job_id, dataset_path=dataset_path, namespace=namespace,\n",
    "                              ce_epochs=ce_epochs, slm_epochs=slm_epochs)\n",
    "    metrics = out[\"metrics\"]\n",
    "    ok = _passes_gates(metrics)\n",
    "    print(\"  metrics:\", metrics, \"\", \"PASS\" if ok else \"FAIL\")\n",
    "    if ok:\n",
    "        try:\n",
    "            r = requests.post(\"http://127.0.0.1:8000/registry/promote\", params={\"champion\": \"ce_minilm_v1\"})\n",
    "            r.raise_for_status()\n",
    "            print(\"  promoted:\", r.json())\n",
    "        except Exception as e:\n",
    "            print(\"  promote failed:\", e)\n",
    "        try:\n",
    "            r = requests.post(\"http://127.0.0.1:8000/rl/update\")\n",
    "            r.raise_for_status()\n",
    "            print(\"  policy updated:\", r.json())\n",
    "        except Exception as e:\n",
    "            print(\"  rl/update failed:\", e)\n",
    "    else:\n",
    "        print(\"  kept as challenger (no promotion).\")\n",
    "    return {\"job_id\": job_id, \"metrics\": metrics, \"passed\": ok, \"summary_path\": str(PHASE2_ROOT / job_id / \"summary.json\")}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "65035e2b-0b20-4b47-bc46-11940d7fd95c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _maybe_make_synthetic_variants():\n",
    "    \"\"\"Optional: create a couple more datasets to cover different shapes.\"\"\"\n",
    "    base = Path(\"data\"); base.mkdir(exist_ok=True, parents=True)\n",
    "    p1 = base / \"synthetic_tickets.csv\"\n",
    "    if not p1.exists():\n",
    "        rng = np.random.default_rng(123)\n",
    "        n = 12000\n",
    "        notes = rng.choice([\n",
    "            \"user reported chargeback risk due to suspicious overseas transaction last week\",\n",
    "            \"clean history but experienced latency spikes; contacted support yesterday\",\n",
    "            \"late fee was applied; requested waiver; follow-up scheduled\",\n",
    "            \"VIP customer asked for limit increase; flagged for manual review\"\n",
    "        ], n)\n",
    "        df1 = pd.DataFrame({\n",
    "            \"ticket_id\": np.arange(n),\n",
    "            \"priority\": rng.choice([\"low\",\"medium\",\"high\"], n, p=[0.5,0.35,0.15]),\n",
    "            \"notes\": notes,\n",
    "            \"default\": rng.integers(0,2,n)\n",
    "        })\n",
    "        df1.to_csv(p1, index=False)\n",
    "    p2 = base / \"synthetic_numeric.csv\"\n",
    "    if not p2.exists():\n",
    "        rng = np.random.default_rng(321)\n",
    "        n = 15000\n",
    "        df2 = pd.DataFrame({\n",
    "            \"age\": rng.integers(18, 70, n),\n",
    "            \"income\": rng.normal(70000, 25000, n).round(0),\n",
    "            \"balance\": rng.normal(3000, 1200, n).round(0),\n",
    "            \"tenure_months\": rng.integers(1, 180, n),\n",
    "            \"notes\": rng.choice([\"ok\", \"review\", \"late fee\", \"chargeback\"], n),\n",
    "            \"label\": rng.integers(0, 3, n)\n",
    "        })\n",
    "        df2.to_csv(p2, index=False)\n",
    "    return [str(p1), str(p2)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "98c3b5fd-7bc7-48bd-a268-7b99acec5af4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Phase-2] demo_1762113222    data\\synthetic_credit.csv\n",
      "  metrics: {'ndcg@10': 0.0, 'faithfulness': 0.9499999995099999, 'p95': 78, 'cost': 0.012000000000000002, 'samples': 25}  FAIL\n",
      "  kept as challenger (no promotion).\n",
      "[Phase-2] demo_1762113225    data\\synthetic_tickets.csv\n",
      "  metrics: {'ndcg@10': 0.0, 'faithfulness': 0.9499999995299998, 'p95': 99, 'cost': 0.012000000000000002, 'samples': 25}  FAIL\n",
      "  kept as challenger (no promotion).\n",
      "[Phase-2] demo_1762113235    data\\synthetic_numeric.csv\n",
      "  metrics: {'ndcg@10': 0.0, 'faithfulness': 0.9499999995299998, 'p95': 49, 'cost': 0.012000000000000002, 'samples': 25}  FAIL\n",
      "  kept as challenger (no promotion).\n",
      "Phase-2 batch results:\n",
      "  {'job_id': 'demo_1762113222', 'metrics': {'ndcg@10': 0.0, 'faithfulness': 0.9499999995099999, 'p95': 78, 'cost': 0.012000000000000002, 'samples': 25}, 'passed': False, 'summary_path': 'artifacts\\\\advanced_v2\\\\demo_1762113222\\\\summary.json'}\n",
      "  {'job_id': 'demo_1762113225', 'metrics': {'ndcg@10': 0.0, 'faithfulness': 0.9499999995299998, 'p95': 99, 'cost': 0.012000000000000002, 'samples': 25}, 'passed': False, 'summary_path': 'artifacts\\\\advanced_v2\\\\demo_1762113225\\\\summary.json'}\n",
      "  {'job_id': 'demo_1762113235', 'metrics': {'ndcg@10': 0.0, 'faithfulness': 0.9499999995299998, 'p95': 49, 'cost': 0.012000000000000002, 'samples': 25}, 'passed': False, 'summary_path': 'artifacts\\\\advanced_v2\\\\demo_1762113235\\\\summary.json'}\n"
     ]
    }
   ],
   "source": [
    "DATASETS = []\n",
    "if 'csv_path' in globals() and Path(csv_path).exists():\n",
    "    DATASETS.append(str(csv_path))\n",
    "DATASETS.extend(_maybe_make_synthetic_variants())\n",
    "results = []\n",
    "for ds in DATASETS:\n",
    "    ns = \"demo\"  \n",
    "    rid = train_phase2_and_promote(dataset_path=ds, namespace=ns, ce_epochs=1, slm_epochs=1)\n",
    "    results.append(rid)\n",
    "print(\"Phase-2 batch results:\")\n",
    "for r in results:\n",
    "    print(\" \", r)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "aafa41f7-740e-4b10-8e21-3d38f41edd3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import os, json, time\n",
    "import pandas as pd\n",
    "DATA_DIR = Path(\"data\")\n",
    "candidates = sorted([*DATA_DIR.glob(\"*.csv\"), *DATA_DIR.glob(\"*.parquet\")])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "80c47a98-0385-4fb0-b256-53d1c4a9bbae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Discovered datasets:\n",
      " - demo.csv  (rows5000)\n",
      " - demo_demo.csv  (rows5000)\n",
      " - demo_demo_demo_demo.csv  (rows5000)\n",
      " - demo_demo_demo_demo_demo_demo_demo_demo.csv  (rows5000)\n",
      " - demo_synthetic_credit.csv  (rows10000)\n",
      " - demo_synthetic_credit_demo_synthetic_cre_demo_synthetic_credit_demo_synthetic_credit.csv  (rows10000)\n",
      " - demo_synthetic_credit_demo_synthetic_credit.csv  (rows10000)\n",
      " - synthetic_credit.csv  (rows10000)\n",
      " - synthetic_credit_synthetic_credit.csv  (rows10000)\n",
      " - synthetic_credit_synthetic_credit_synthetic_credit_synthetic_credit.csv  (rows10000)\n",
      " - synthetic_numeric.csv  (rows15000)\n",
      " - synthetic_numeric_synthetic_numeric.csv  (rows15000)\n",
      " - synthetic_tickets.csv  (rows12000)\n",
      " - synthetic_tickets_synthetic_tickets.csv  (rows12000)\n"
     ]
    }
   ],
   "source": [
    "print(\"Discovered datasets:\")\n",
    "for p in candidates:\n",
    "    try:\n",
    "        nrows = sum(1 for _ in open(p)) - 1 if p.suffix == \".csv\" else pd.read_parquet(p).shape[0]\n",
    "    except Exception:\n",
    "        nrows = \"?\"\n",
    "    print(f\" - {p.name}  (rows{nrows})\")\n",
    "SAMPLED_DIR = DATA_DIR / \"samples\"\n",
    "SAMPLED_DIR.mkdir(parents=True, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "ee94a59f-2d14-4495-a4d5-4bf6c8c6d308",
   "metadata": {},
   "outputs": [],
   "source": [
    "def maybe_sample(src: Path, max_rows=20000) -> Path:\n",
    "    if src.suffix == \".parquet\":\n",
    "        try:\n",
    "            df = pd.read_parquet(src)\n",
    "        except Exception:\n",
    "            df = pd.read_csv(src)\n",
    "    else:\n",
    "        df = pd.read_csv(src)\n",
    "    if len(df) > max_rows:\n",
    "        df = df.sample(max_rows, random_state=42).reset_index(drop=True)\n",
    "        out = SAMPLED_DIR / f\"{src.stem}_sample{max_rows}{src.suffix}\"\n",
    "        if src.suffix == \".parquet\":\n",
    "            df.to_parquet(out, index=False)\n",
    "        else:\n",
    "            df.to_csv(out, index=False)\n",
    "        return out\n",
    "    return src"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "d154ca8a-23c7-4e55-994e-626d70057856",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Phase-2] demo_1762113251    data\\demo.csv\n",
      "  metrics: {'ndcg@10': 0.0, 'faithfulness': 0.9499999995099999, 'p95': 108, 'cost': 0.012000000000000002, 'samples': 25}  FAIL\n",
      "  kept as challenger (no promotion).\n",
      "[Phase-2] demo_demo_1762113254    data\\demo_demo.csv\n",
      "  metrics: {'ndcg@10': 0.0, 'faithfulness': 0.9499999995099999, 'p95': 62, 'cost': 0.012000000000000002, 'samples': 25}  FAIL\n",
      "  kept as challenger (no promotion).\n",
      "[Phase-2] demo_demo_demo_demo_1762113256    data\\demo_demo_demo_demo.csv\n",
      "  metrics: {'ndcg@10': 0.0, 'faithfulness': 0.9499999995099999, 'p95': 60, 'cost': 0.012000000000000002, 'samples': 25}  FAIL\n",
      "  kept as challenger (no promotion).\n",
      "[Phase-2] demo_demo_demo_demo_demo_demo_demo_demo_1762113259    data\\demo_demo_demo_demo_demo_demo_demo_demo.csv\n",
      "  metrics: {'ndcg@10': 0.0, 'faithfulness': 0.9499999995099999, 'p95': 54, 'cost': 0.012000000000000002, 'samples': 25}  FAIL\n",
      "  kept as challenger (no promotion).\n",
      "[Phase-2] demo_synthetic_credit_1762113262    data\\demo_synthetic_credit.csv\n",
      "  metrics: {'ndcg@10': 0.0, 'faithfulness': 0.9499999995099999, 'p95': 62, 'cost': 0.012000000000000002, 'samples': 25}  FAIL\n",
      "  kept as challenger (no promotion).\n",
      "[Phase-2] demo_synthetic_credit_demo_synthetic_cre_1762113266    data\\demo_synthetic_credit_demo_synthetic_cre_demo_synthetic_credit_demo_synthetic_credit.csv\n",
      "  metrics: {'ndcg@10': 0.0, 'faithfulness': 0.9499999995099999, 'p95': 63, 'cost': 0.012000000000000002, 'samples': 25}  FAIL\n",
      "  kept as challenger (no promotion).\n",
      "[Phase-2] demo_synthetic_credit_demo_synthetic_cre_1762113270    data\\demo_synthetic_credit_demo_synthetic_credit.csv\n",
      "  metrics: {'ndcg@10': 0.0, 'faithfulness': 0.9499999995099999, 'p95': 60, 'cost': 0.012000000000000002, 'samples': 25}  FAIL\n",
      "  kept as challenger (no promotion).\n",
      "[Phase-2] synthetic_credit_1762113274    data\\synthetic_credit.csv\n",
      "  metrics: {'ndcg@10': 0.0, 'faithfulness': 0.9499999995099999, 'p95': 62, 'cost': 0.012000000000000002, 'samples': 25}  FAIL\n",
      "  kept as challenger (no promotion).\n",
      "[Phase-2] synthetic_credit_synthetic_credit_1762113278    data\\synthetic_credit_synthetic_credit.csv\n",
      "  metrics: {'ndcg@10': 0.0, 'faithfulness': 0.9499999995099999, 'p95': 63, 'cost': 0.012000000000000002, 'samples': 25}  FAIL\n",
      "  kept as challenger (no promotion).\n",
      "[Phase-2] synthetic_credit_synthetic_credit_synthe_1762113282    data\\synthetic_credit_synthetic_credit_synthetic_credit_synthetic_credit.csv\n",
      "  metrics: {'ndcg@10': 0.0, 'faithfulness': 0.9499999995099999, 'p95': 61, 'cost': 0.012000000000000002, 'samples': 25}  FAIL\n",
      "  kept as challenger (no promotion).\n",
      "[Phase-2] synthetic_numeric_1762113285    data\\synthetic_numeric.csv\n",
      "  metrics: {'ndcg@10': 0.0, 'faithfulness': 0.9499999995299998, 'p95': 59, 'cost': 0.012000000000000002, 'samples': 25}  FAIL\n",
      "  kept as challenger (no promotion).\n",
      "[Phase-2] synthetic_numeric_synthetic_numeric_1762113289    data\\synthetic_numeric_synthetic_numeric.csv\n",
      "  metrics: {'ndcg@10': 0.0, 'faithfulness': 0.9499999995299998, 'p95': 60, 'cost': 0.012000000000000002, 'samples': 25}  FAIL\n",
      "  kept as challenger (no promotion).\n",
      "[Phase-2] synthetic_tickets_1762113294    data\\synthetic_tickets.csv\n",
      "  metrics: {'ndcg@10': 0.0, 'faithfulness': 0.9499999995299998, 'p95': 46, 'cost': 0.012000000000000002, 'samples': 25}  FAIL\n",
      "  kept as challenger (no promotion).\n",
      "[Phase-2] synthetic_tickets_synthetic_tickets_1762113303    data\\synthetic_tickets_synthetic_tickets.csv\n",
      "  metrics: {'ndcg@10': 0.0, 'faithfulness': 0.9499999995299998, 'p95': 61, 'cost': 0.012000000000000002, 'samples': 25}  FAIL\n",
      "  kept as challenger (no promotion).\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>dataset</th>\n",
       "      <th>namespace</th>\n",
       "      <th>job_id</th>\n",
       "      <th>passed</th>\n",
       "      <th>ndcg@10</th>\n",
       "      <th>faithfulness</th>\n",
       "      <th>p95</th>\n",
       "      <th>cost</th>\n",
       "      <th>samples</th>\n",
       "      <th>summary_path</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>synthetic_numeric.csv</td>\n",
       "      <td>synthetic_numeric</td>\n",
       "      <td>synthetic_numeric_1762113285</td>\n",
       "      <td>False</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.95</td>\n",
       "      <td>59</td>\n",
       "      <td>0.012</td>\n",
       "      <td>25</td>\n",
       "      <td>artifacts\\advanced_v2\\synthetic_numeric_176211...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>synthetic_numeric_synthetic_numeric.csv</td>\n",
       "      <td>synthetic_numeric_synthetic_numeric</td>\n",
       "      <td>synthetic_numeric_synthetic_numeric_1762113289</td>\n",
       "      <td>False</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.95</td>\n",
       "      <td>60</td>\n",
       "      <td>0.012</td>\n",
       "      <td>25</td>\n",
       "      <td>artifacts\\advanced_v2\\synthetic_numeric_synthe...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>synthetic_tickets.csv</td>\n",
       "      <td>synthetic_tickets</td>\n",
       "      <td>synthetic_tickets_1762113294</td>\n",
       "      <td>False</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.95</td>\n",
       "      <td>46</td>\n",
       "      <td>0.012</td>\n",
       "      <td>25</td>\n",
       "      <td>artifacts\\advanced_v2\\synthetic_tickets_176211...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>synthetic_tickets_synthetic_tickets.csv</td>\n",
       "      <td>synthetic_tickets_synthetic_tickets</td>\n",
       "      <td>synthetic_tickets_synthetic_tickets_1762113303</td>\n",
       "      <td>False</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.95</td>\n",
       "      <td>61</td>\n",
       "      <td>0.012</td>\n",
       "      <td>25</td>\n",
       "      <td>artifacts\\advanced_v2\\synthetic_tickets_synthe...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>demo.csv</td>\n",
       "      <td>demo</td>\n",
       "      <td>demo_1762113251</td>\n",
       "      <td>False</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.95</td>\n",
       "      <td>108</td>\n",
       "      <td>0.012</td>\n",
       "      <td>25</td>\n",
       "      <td>artifacts\\advanced_v2\\demo_1762113251\\summary....</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>demo_demo.csv</td>\n",
       "      <td>demo_demo</td>\n",
       "      <td>demo_demo_1762113254</td>\n",
       "      <td>False</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.95</td>\n",
       "      <td>62</td>\n",
       "      <td>0.012</td>\n",
       "      <td>25</td>\n",
       "      <td>artifacts\\advanced_v2\\demo_demo_1762113254\\sum...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>demo_demo_demo_demo.csv</td>\n",
       "      <td>demo_demo_demo_demo</td>\n",
       "      <td>demo_demo_demo_demo_1762113256</td>\n",
       "      <td>False</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.95</td>\n",
       "      <td>60</td>\n",
       "      <td>0.012</td>\n",
       "      <td>25</td>\n",
       "      <td>artifacts\\advanced_v2\\demo_demo_demo_demo_1762...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>demo_demo_demo_demo_demo_demo_demo_demo.csv</td>\n",
       "      <td>demo_demo_demo_demo_demo_demo_demo_demo</td>\n",
       "      <td>demo_demo_demo_demo_demo_demo_demo_demo_176211...</td>\n",
       "      <td>False</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.95</td>\n",
       "      <td>54</td>\n",
       "      <td>0.012</td>\n",
       "      <td>25</td>\n",
       "      <td>artifacts\\advanced_v2\\demo_demo_demo_demo_demo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>demo_synthetic_credit.csv</td>\n",
       "      <td>demo_synthetic_credit</td>\n",
       "      <td>demo_synthetic_credit_1762113262</td>\n",
       "      <td>False</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.95</td>\n",
       "      <td>62</td>\n",
       "      <td>0.012</td>\n",
       "      <td>25</td>\n",
       "      <td>artifacts\\advanced_v2\\demo_synthetic_credit_17...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>demo_synthetic_credit_demo_synthetic_cre_demo_...</td>\n",
       "      <td>demo_synthetic_credit_demo_synthetic_cre</td>\n",
       "      <td>demo_synthetic_credit_demo_synthetic_cre_17621...</td>\n",
       "      <td>False</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.95</td>\n",
       "      <td>63</td>\n",
       "      <td>0.012</td>\n",
       "      <td>25</td>\n",
       "      <td>artifacts\\advanced_v2\\demo_synthetic_credit_de...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>demo_synthetic_credit_demo_synthetic_credit.csv</td>\n",
       "      <td>demo_synthetic_credit_demo_synthetic_cre</td>\n",
       "      <td>demo_synthetic_credit_demo_synthetic_cre_17621...</td>\n",
       "      <td>False</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.95</td>\n",
       "      <td>60</td>\n",
       "      <td>0.012</td>\n",
       "      <td>25</td>\n",
       "      <td>artifacts\\advanced_v2\\demo_synthetic_credit_de...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>synthetic_credit.csv</td>\n",
       "      <td>synthetic_credit</td>\n",
       "      <td>synthetic_credit_1762113274</td>\n",
       "      <td>False</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.95</td>\n",
       "      <td>62</td>\n",
       "      <td>0.012</td>\n",
       "      <td>25</td>\n",
       "      <td>artifacts\\advanced_v2\\synthetic_credit_1762113...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>synthetic_credit_synthetic_credit.csv</td>\n",
       "      <td>synthetic_credit_synthetic_credit</td>\n",
       "      <td>synthetic_credit_synthetic_credit_1762113278</td>\n",
       "      <td>False</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.95</td>\n",
       "      <td>63</td>\n",
       "      <td>0.012</td>\n",
       "      <td>25</td>\n",
       "      <td>artifacts\\advanced_v2\\synthetic_credit_synthet...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>synthetic_credit_synthetic_credit_synthetic_cr...</td>\n",
       "      <td>synthetic_credit_synthetic_credit_synthe</td>\n",
       "      <td>synthetic_credit_synthetic_credit_synthe_17621...</td>\n",
       "      <td>False</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.95</td>\n",
       "      <td>61</td>\n",
       "      <td>0.012</td>\n",
       "      <td>25</td>\n",
       "      <td>artifacts\\advanced_v2\\synthetic_credit_synthet...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              dataset  \\\n",
       "10                              synthetic_numeric.csv   \n",
       "11            synthetic_numeric_synthetic_numeric.csv   \n",
       "12                              synthetic_tickets.csv   \n",
       "13            synthetic_tickets_synthetic_tickets.csv   \n",
       "0                                            demo.csv   \n",
       "1                                       demo_demo.csv   \n",
       "2                             demo_demo_demo_demo.csv   \n",
       "3         demo_demo_demo_demo_demo_demo_demo_demo.csv   \n",
       "4                           demo_synthetic_credit.csv   \n",
       "5   demo_synthetic_credit_demo_synthetic_cre_demo_...   \n",
       "6     demo_synthetic_credit_demo_synthetic_credit.csv   \n",
       "7                                synthetic_credit.csv   \n",
       "8               synthetic_credit_synthetic_credit.csv   \n",
       "9   synthetic_credit_synthetic_credit_synthetic_cr...   \n",
       "\n",
       "                                   namespace  \\\n",
       "10                         synthetic_numeric   \n",
       "11       synthetic_numeric_synthetic_numeric   \n",
       "12                         synthetic_tickets   \n",
       "13       synthetic_tickets_synthetic_tickets   \n",
       "0                                       demo   \n",
       "1                                  demo_demo   \n",
       "2                        demo_demo_demo_demo   \n",
       "3    demo_demo_demo_demo_demo_demo_demo_demo   \n",
       "4                      demo_synthetic_credit   \n",
       "5   demo_synthetic_credit_demo_synthetic_cre   \n",
       "6   demo_synthetic_credit_demo_synthetic_cre   \n",
       "7                           synthetic_credit   \n",
       "8          synthetic_credit_synthetic_credit   \n",
       "9   synthetic_credit_synthetic_credit_synthe   \n",
       "\n",
       "                                               job_id  passed  ndcg@10  \\\n",
       "10                       synthetic_numeric_1762113285   False      0.0   \n",
       "11     synthetic_numeric_synthetic_numeric_1762113289   False      0.0   \n",
       "12                       synthetic_tickets_1762113294   False      0.0   \n",
       "13     synthetic_tickets_synthetic_tickets_1762113303   False      0.0   \n",
       "0                                     demo_1762113251   False      0.0   \n",
       "1                                demo_demo_1762113254   False      0.0   \n",
       "2                      demo_demo_demo_demo_1762113256   False      0.0   \n",
       "3   demo_demo_demo_demo_demo_demo_demo_demo_176211...   False      0.0   \n",
       "4                    demo_synthetic_credit_1762113262   False      0.0   \n",
       "5   demo_synthetic_credit_demo_synthetic_cre_17621...   False      0.0   \n",
       "6   demo_synthetic_credit_demo_synthetic_cre_17621...   False      0.0   \n",
       "7                         synthetic_credit_1762113274   False      0.0   \n",
       "8        synthetic_credit_synthetic_credit_1762113278   False      0.0   \n",
       "9   synthetic_credit_synthetic_credit_synthe_17621...   False      0.0   \n",
       "\n",
       "    faithfulness  p95   cost  samples  \\\n",
       "10          0.95   59  0.012       25   \n",
       "11          0.95   60  0.012       25   \n",
       "12          0.95   46  0.012       25   \n",
       "13          0.95   61  0.012       25   \n",
       "0           0.95  108  0.012       25   \n",
       "1           0.95   62  0.012       25   \n",
       "2           0.95   60  0.012       25   \n",
       "3           0.95   54  0.012       25   \n",
       "4           0.95   62  0.012       25   \n",
       "5           0.95   63  0.012       25   \n",
       "6           0.95   60  0.012       25   \n",
       "7           0.95   62  0.012       25   \n",
       "8           0.95   63  0.012       25   \n",
       "9           0.95   61  0.012       25   \n",
       "\n",
       "                                         summary_path  \n",
       "10  artifacts\\advanced_v2\\synthetic_numeric_176211...  \n",
       "11  artifacts\\advanced_v2\\synthetic_numeric_synthe...  \n",
       "12  artifacts\\advanced_v2\\synthetic_tickets_176211...  \n",
       "13  artifacts\\advanced_v2\\synthetic_tickets_synthe...  \n",
       "0   artifacts\\advanced_v2\\demo_1762113251\\summary....  \n",
       "1   artifacts\\advanced_v2\\demo_demo_1762113254\\sum...  \n",
       "2   artifacts\\advanced_v2\\demo_demo_demo_demo_1762...  \n",
       "3   artifacts\\advanced_v2\\demo_demo_demo_demo_demo...  \n",
       "4   artifacts\\advanced_v2\\demo_synthetic_credit_17...  \n",
       "5   artifacts\\advanced_v2\\demo_synthetic_credit_de...  \n",
       "6   artifacts\\advanced_v2\\demo_synthetic_credit_de...  \n",
       "7   artifacts\\advanced_v2\\synthetic_credit_1762113...  \n",
       "8   artifacts\\advanced_v2\\synthetic_credit_synthet...  \n",
       "9   artifacts\\advanced_v2\\synthetic_credit_synthet...  "
      ]
     },
     "execution_count": 115,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "USE_SAMPLES = True\n",
    "datasets = [maybe_sample(p) if USE_SAMPLES else p for p in candidates]\n",
    "results = []\n",
    "for ds in datasets:\n",
    "    ns = ds.stem[:40]  \n",
    "    res = train_phase2_and_promote(dataset_path=str(ds), namespace=ns, ce_epochs=1, slm_epochs=1)\n",
    "    results.append({\n",
    "        \"dataset\": ds.name,\n",
    "        \"namespace\": ns,\n",
    "        \"job_id\": res[\"job_id\"],\n",
    "        \"passed\": res[\"passed\"],\n",
    "        **res[\"metrics\"],\n",
    "        \"summary_path\": res[\"summary_path\"],\n",
    "    })\n",
    "summary_df = pd.DataFrame(results).sort_values([\"passed\",\"ndcg@10\",\"faithfulness\"], ascending=[False, False, False])\n",
    "summary_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "94ee86f8-60d5-413e-b1b8-eba95ad31f9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests, json\n",
    "BASE = \"http://127.0.0.1:8000\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "85259788-312f-4902-871b-9626bea62922",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "===  demo.csv  job: demo_1762113251 ===\n",
      "[demo] arm=None route=None retr=None reranker=None slm=None lat=Nonems\n",
      " action: None | model_id: None\n",
      "[demo] arm=None route=None retr=None reranker=None slm=None lat=Nonems\n",
      " action: None | model_id: None\n",
      "\n",
      "===  demo_demo.csv  job: demo_demo_1762113254 ===\n",
      "[demo_demo] arm=None route=None retr=None reranker=None slm=None lat=Nonems\n",
      " action: None | model_id: None\n",
      "[demo_demo] arm=None route=None retr=None reranker=None slm=None lat=Nonems\n",
      " action: None | model_id: None\n",
      "\n",
      "===  demo_demo_demo_demo.csv  job: demo_demo_demo_demo_1762113256 ===\n",
      "[demo_demo_demo_demo] arm=None route=None retr=None reranker=None slm=None lat=Nonems\n",
      " action: None | model_id: None\n",
      "[demo_demo_demo_demo] arm=None route=None retr=None reranker=None slm=None lat=Nonems\n",
      " action: None | model_id: None\n",
      "\n",
      "===  demo_demo_demo_demo_demo_demo_demo_demo.csv  job: demo_demo_demo_demo_demo_demo_demo_demo_1762113259 ===\n",
      "[demo_demo_demo_demo_demo_demo_demo_demo] arm=None route=None retr=None reranker=None slm=None lat=Nonems\n",
      " action: None | model_id: None\n",
      "[demo_demo_demo_demo_demo_demo_demo_demo] arm=None route=None retr=None reranker=None slm=None lat=Nonems\n",
      " action: None | model_id: None\n",
      "\n",
      "===  demo_synthetic_credit.csv  job: demo_synthetic_credit_1762113262 ===\n",
      "[demo_synthetic_credit] arm=None route=None retr=None reranker=None slm=None lat=Nonems\n",
      " action: None | model_id: None\n",
      "[demo_synthetic_credit] arm=None route=None retr=None reranker=None slm=None lat=Nonems\n",
      " action: None | model_id: None\n",
      "\n",
      "===  demo_synthetic_credit_demo_synthetic_cre_demo_synthetic_credit_demo_synthetic_credit.csv  job: demo_synthetic_credit_demo_synthetic_cre_1762113266 ===\n",
      "[demo_synthetic_credit_demo_synthetic_cre] arm=None route=None retr=None reranker=None slm=None lat=Nonems\n",
      " action: None | model_id: None\n",
      "[demo_synthetic_credit_demo_synthetic_cre] arm=None route=None retr=None reranker=None slm=None lat=Nonems\n",
      " action: None | model_id: None\n",
      "\n",
      "===  demo_synthetic_credit_demo_synthetic_credit.csv  job: demo_synthetic_credit_demo_synthetic_cre_1762113270 ===\n",
      "[demo_synthetic_credit_demo_synthetic_cre] arm=None route=None retr=None reranker=None slm=None lat=Nonems\n",
      " action: None | model_id: None\n",
      "[demo_synthetic_credit_demo_synthetic_cre] arm=None route=None retr=None reranker=None slm=None lat=Nonems\n",
      " action: None | model_id: None\n",
      "\n",
      "===  synthetic_credit.csv  job: synthetic_credit_1762113274 ===\n",
      "[synthetic_credit] arm=None route=None retr=None reranker=None slm=None lat=Nonems\n",
      " action: None | model_id: None\n",
      "[synthetic_credit] arm=None route=None retr=None reranker=None slm=None lat=Nonems\n",
      " action: None | model_id: None\n",
      "\n",
      "===  synthetic_credit_synthetic_credit.csv  job: synthetic_credit_synthetic_credit_1762113278 ===\n",
      "[synthetic_credit_synthetic_credit] arm=None route=None retr=None reranker=None slm=None lat=Nonems\n",
      " action: None | model_id: None\n",
      "[synthetic_credit_synthetic_credit] arm=None route=None retr=None reranker=None slm=None lat=Nonems\n",
      " action: None | model_id: None\n",
      "\n",
      "===  synthetic_credit_synthetic_credit_synthetic_credit_synthetic_credit.csv  job: synthetic_credit_synthetic_credit_synthe_1762113282 ===\n",
      "[synthetic_credit_synthetic_credit_synthe] arm=None route=None retr=None reranker=None slm=None lat=Nonems\n",
      " action: None | model_id: None\n",
      "[synthetic_credit_synthetic_credit_synthe] arm=None route=None retr=None reranker=None slm=None lat=Nonems\n",
      " action: None | model_id: None\n",
      "\n",
      "===  synthetic_numeric.csv  job: synthetic_numeric_1762113285 ===\n",
      "[synthetic_numeric] arm=None route=None retr=None reranker=None slm=None lat=Nonems\n",
      " action: None | model_id: None\n",
      "[synthetic_numeric] arm=None route=None retr=None reranker=None slm=None lat=Nonems\n",
      " action: None | model_id: None\n",
      "\n",
      "===  synthetic_numeric_synthetic_numeric.csv  job: synthetic_numeric_synthetic_numeric_1762113289 ===\n",
      "[synthetic_numeric_synthetic_numeric] arm=None route=None retr=None reranker=None slm=None lat=Nonems\n",
      " action: None | model_id: None\n",
      "[synthetic_numeric_synthetic_numeric] arm=None route=None retr=None reranker=None slm=None lat=Nonems\n",
      " action: None | model_id: None\n",
      "\n",
      "===  synthetic_tickets.csv  job: synthetic_tickets_1762113294 ===\n",
      "[synthetic_tickets] arm=None route=None retr=None reranker=None slm=None lat=Nonems\n",
      " action: None | model_id: None\n",
      "[synthetic_tickets] arm=None route=None retr=None reranker=None slm=None lat=Nonems\n",
      " action: None | model_id: None\n",
      "\n",
      "===  synthetic_tickets_synthetic_tickets.csv  job: synthetic_tickets_synthetic_tickets_1762113303 ===\n",
      "[synthetic_tickets_synthetic_tickets] arm=None route=None retr=None reranker=None slm=None lat=Nonems\n",
      " action: None | model_id: None\n",
      "[synthetic_tickets_synthetic_tickets] arm=None route=None retr=None reranker=None slm=None lat=Nonems\n",
      " action: None | model_id: None\n"
     ]
    }
   ],
   "source": [
    "def ping(job_id, ns, q):\n",
    "    out = requests.post(f\"{BASE}/query\", json={\n",
    "        \"job_id\": job_id, \"namespace\": ns, \"query_text\": q, \"top_k\": 5\n",
    "    }).json()\n",
    "    meta = out.get(\"meta\", {})\n",
    "    print(f\"[{ns}] arm={meta.get('arm')} route={meta.get('route')} \"\n",
    "          f\"retr={meta.get('retriever')} reranker={meta.get('reranker')} slm={meta.get('slm')} \"\n",
    "          f\"lat={meta.get('latency_ms')}ms\")\n",
    "    print(\" action:\", out.get(\"action\"), \"| model_id:\", out.get(\"model_id\"))\n",
    "    return out\n",
    "for row in results:\n",
    "    print(\"\\n=== \", row[\"dataset\"], \" job:\", row[\"job_id\"], \"===\")\n",
    "    try:\n",
    "        _ = ping(row[\"job_id\"], row[\"namespace\"], \"What are common notes or patterns?\")\n",
    "        _ = ping(row[\"job_id\"], row[\"namespace\"], \"select age, income and balance where default = 1 limit 5\")\n",
    "    except Exception as e:\n",
    "        print(\" query failed:\", e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "195489d9-d10c-4f15-a818-2147bd44abd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, json, time, math, threading, random\n",
    "from pathlib import Path\n",
    "from typing import Any, Dict, List, Optional, Tuple\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from fastapi import Request, HTTPException\n",
    "from fastapi.responses import JSONResponse\n",
    "from prometheus_client import Counter, Gauge, Histogram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "582b0a2f-04de-4619-bc47-ea8082d48c34",
   "metadata": {},
   "outputs": [],
   "source": [
    "LOGS_DIR = Path(\"logs\"); LOGS_DIR.mkdir(parents=True, exist_ok=True)\n",
    "SHADOW_LIVE = LOGS_DIR / \"shadow_live.jsonl\"\n",
    "CANARY_LIVE = LOGS_DIR / \"canary_live.jsonl\"\n",
    "EVAL_DIR = Path(\"artifacts/eval\"); EVAL_DIR.mkdir(parents=True, exist_ok=True)\n",
    "EVAL_WINDOW_HOURS = 6\n",
    "DRIFT_RETRAIN_COOLDOWN_HOURS = 6\n",
    "MAX_CHALLENGERS = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "id": "751956e6-ecec-4e94-b357-11c391d78727",
   "metadata": {},
   "outputs": [],
   "source": [
    "LIVE_DELTA_MEAN = Gauge(\"live_reward_delta_mean\", \"Mean (cand - prod) reward over eval window\")\n",
    "LIVE_DELTA_P50  = Gauge(\"live_reward_delta_p50\", \"P50 delta over eval window\")\n",
    "LIVE_DELTA_P95  = Gauge(\"live_reward_delta_p95\", \"P95 delta over eval window\")\n",
    "LIVE_QPS        = Gauge(\"live_eval_qps\", \"Approx QPS observed in logs (window)\")\n",
    "RETRAIN_TRIGGERS = Counter(\"auto_retrain_triggers_total\", \"Auto-retrain triggers from drift\")\n",
    "RETRAIN_RUNS     = Counter(\"auto_retrain_runs_total\", \"Auto-retrain runs started\")\n",
    "RETRAIN_PASS     = Counter(\"auto_retrain_pass_total\", \"Auto-retrain runs that passed gates\")\n",
    "RETRAIN_FAIL     = Counter(\"auto_retrain_fail_total\", \"Auto-retrain runs that failed gates\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "id": "882de513-2ee1-4c0e-9929-670b45062e2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _read_jsonl_window(path: Path, hours: int) -> List[Dict[str, Any]]:\n",
    "    if not path.exists():\n",
    "        return []\n",
    "    cutoff = time.time() - hours * 3600\n",
    "    out = []\n",
    "    with open(path, \"r\", encoding=\"utf-8\") as f:\n",
    "        for line in f:\n",
    "            try:\n",
    "                j = json.loads(line)\n",
    "                if j.get(\"ts\", 0) >= cutoff:\n",
    "                    out.append(j)\n",
    "            except Exception:\n",
    "                continue\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "id": "51ff5dc8-5ba7-433e-beac-af78b4922555",
   "metadata": {},
   "outputs": [],
   "source": [
    "REWARD_WEIGHTS = {\"alpha\": 0.5, \"beta\": 0.4, \"gamma\": 0.0005, \"delta\": 10.0}\n",
    "def prd_reward_proxy(out_json: Dict[str, Any]) -> float:\n",
    "    faith = 0.90 if out_json.get(\"action\") == \"respond\" else 0.80\n",
    "    ndcg  = 0.66 if out_json.get(\"meta\", {}).get(\"retriever\") != \"bm25\" else 0.61\n",
    "    p95_ms = float(out_json.get(\"meta\", {}).get(\"latency_ms\", 1500))\n",
    "    cost = 0.016 if out_json.get(\"meta\", {}).get(\"retriever\") != \"bm25\" else 0.012\n",
    "    a,b,g,d = (REWARD_WEIGHTS[k] for k in (\"alpha\",\"beta\",\"gamma\",\"delta\"))\n",
    "    return a*faith + b*ndcg - g*p95_ms - d*cost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "id": "c28036a5-927f-40dd-8657-b05e07666ad1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def aggregate_live_eval(window_hours: int = EVAL_WINDOW_HOURS) -> Dict[str, Any]:\n",
    "    shadow = _read_jsonl_window(SHADOW_LIVE, window_hours)\n",
    "    canary = _read_jsonl_window(CANARY_LIVE, window_hours)\n",
    "    deltas = []\n",
    "    for r in shadow:\n",
    "        if \"delta\" in r:\n",
    "            deltas.append(float(r[\"delta\"]))\n",
    "    for r in canary:\n",
    "        if \"delta\" in r:\n",
    "            deltas.append(float(r[\"delta\"]))\n",
    "    if not deltas:\n",
    "        res = {\"window_hours\": window_hours, \"n\": 0, \"mean\": 0.0, \"p50\": 0.0, \"p95\": 0.0, \"qps\": 0.0}\n",
    "    else:\n",
    "        arr = np.array(deltas, dtype=float)\n",
    "        res = {\n",
    "            \"window_hours\": window_hours,\n",
    "            \"n\": int(arr.size),\n",
    "            \"mean\": float(arr.mean()),\n",
    "            \"p50\": float(np.percentile(arr, 50)),\n",
    "            \"p95\": float(np.percentile(arr, 95)),\n",
    "        }\n",
    "        res[\"qps\"] = float(arr.size / (window_hours * 3600.0))\n",
    "    LIVE_DELTA_MEAN.set(res[\"mean\"])\n",
    "    LIVE_DELTA_P50.set(res[\"p50\"])\n",
    "    LIVE_DELTA_P95.set(res[\"p95\"])\n",
    "    LIVE_QPS.set(res[\"qps\"])\n",
    "    snap_path = EVAL_DIR / \"live_eval.json\"\n",
    "    snap_path.write_text(json.dumps(res, indent=2))\n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "id": "b65301b3-e2d4-42a4-b69c-ef311ae7d2dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "@app.get(\"/eval/aggregate\")\n",
    "def eval_aggregate():\n",
    "    out = aggregate_live_eval(EVAL_WINDOW_HOURS)\n",
    "    return JSONResponse(out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "id": "36b6c795-3108-4012-9f7c-350ba8d54bab",
   "metadata": {},
   "outputs": [],
   "source": [
    "def bandit_update_from_logs(temp: float = 0.2) -> Dict[str, Any]:\n",
    "    live = _read_jsonl_window(CANARY_LIVE, EVAL_WINDOW_HOURS)\n",
    "    deltas = [float(j.get(\"delta\", 0.0)) for j in live if \"delta\" in j]\n",
    "    mean_delta = float(np.mean(deltas)) if deltas else 0.0\n",
    "    ea = math.exp(0.0 / max(1e-9, temp))\n",
    "    eb = math.exp(mean_delta / max(1e-9, temp))\n",
    "    w_prod = ea / (ea + eb)\n",
    "    w_cand = eb / (ea + eb)\n",
    "    pol = _json_read(POLICY_PATH, {\"version\": 0, \"weights\": {\"A\": 0.6, \"B\": 0.4}})\n",
    "    pol[\"version\"] = int(pol.get(\"version\", 0)) + 1\n",
    "    pol[\"weights\"] = {\"A\": round(w_prod, 4), \"B\": round(w_cand, 4)}\n",
    "    pol[\"ts_updated\"] = int(time.time())\n",
    "    _json_write(POLICY_PATH, pol)\n",
    "    try:\n",
    "        jobs_set(\"POLICY\", status=\"UPDATED_FROM_LOGS\", weights=pol[\"weights\"], version=pol[\"version\"], src=\"live_deltas\")\n",
    "    except Exception:\n",
    "        pass\n",
    "    return {\"weights\": pol[\"weights\"], \"policy_version\": pol[\"version\"], \"mean_delta\": mean_delta}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "id": "91021c9c-ea70-4174-bb96-1d201362d864",
   "metadata": {},
   "outputs": [],
   "source": [
    "@app.post(\"/rl/update_from_logs\")\n",
    "def rl_update_from_logs():\n",
    "    return JSONResponse(bandit_update_from_logs())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "id": "72b7f912-4ca0-4724-abfe-0c268465ccf6",
   "metadata": {},
   "outputs": [],
   "source": [
    "_VERIFIER_MODE = {\"use_v2\": False}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "id": "2bd381b3-b206-458e-85d6-ddab72bd4e9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def verify_faithfulness_v2(answer_text: str, ctx_texts: List[str]) -> Dict[str, Any]:\n",
    "    \"\"\"\n",
    "    Try MNLI NLI pipeline; fallback to token-overlap.\n",
    "    Returns {faithfulness, entailment_prob, mode}\n",
    "    \"\"\"\n",
    "    ctx = \" \".join(ctx_texts[:3]) if ctx_texts else \"\"\n",
    "    try:\n",
    "        from transformers import AutoModelForSequenceClassification, AutoTokenizer\n",
    "        import torch\n",
    "        model_name = \"roberta-large-mnli\"  \n",
    "        tok = AutoTokenizer.from_pretrained(model_name)\n",
    "        mdl = AutoModelForSequenceClassification.from_pretrained(model_name)\n",
    "        mdl.eval()\n",
    "        device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "        mdl.to(device)\n",
    "        def _ent(premise, hypothesis):\n",
    "            batch = tok(premise, hypothesis, truncation=True, max_length=512, return_tensors=\"pt\").to(device)\n",
    "            with torch.no_grad():\n",
    "                logits = mdl(**batch).logits\n",
    "            probs = logits.softmax(-1)[0].tolist()\n",
    "            return float(probs[2])\n",
    "        p1 = _ent(ctx, answer_text)\n",
    "        p2 = _ent(answer_text, ctx)\n",
    "        faith = float(0.5 * (p1 + p2))\n",
    "        return {\"faithfulness\": faith, \"entailment_prob\": faith, \"mode\": \"mnli\"}\n",
    "    except Exception:\n",
    "        ans_tokens = set(answer_text.lower().split())\n",
    "        ctx_tokens = set((\" \".join(ctx_texts)).lower().split())\n",
    "        overlap = len(ans_tokens & ctx_tokens) / (len(ans_tokens) + 1e-9)\n",
    "        faith = max(0.0, min(1.0, 0.5 * overlap + 0.45))\n",
    "        return {\"faithfulness\": float(faith), \"entailment_prob\": float(faith), \"mode\": \"overlap\"}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "id": "58189062-3c8c-4f20-b4aa-09a552ddeb10",
   "metadata": {},
   "outputs": [],
   "source": [
    "@app.post(\"/verify/mode\")\n",
    "def verifier_mode(set_v2: Optional[bool] = None):\n",
    "    \"\"\"GET/POST-able: /verify/mode?set_v2=true to enable NLI verifier in your app logic if you want.\"\"\"\n",
    "    if set_v2 is not None:\n",
    "        _VERIFIER_MODE[\"use_v2\"] = bool(set_v2)\n",
    "    return JSONResponse({\"use_v2\": _VERIFIER_MODE[\"use_v2\"]})\n",
    "def verify_bridge(answer_text: str, ctx_texts: List[str]) -> Dict[str, Any]:\n",
    "    return verify_faithfulness_v2(answer_text, ctx_texts) if _VERIFIER_MODE[\"use_v2\"] else verify_faithfulness(answer_text, ctx_texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "id": "50445c46-ceec-4e04-82ae-84897046fd15",
   "metadata": {},
   "outputs": [],
   "source": [
    "_LAST_RETRAIN = {\"ts\": 0}\n",
    "def _latest_job_for_ns(namespace: str) -> Optional[Dict[str, Any]]:\n",
    "    jobs = _json_read(ART_DIR / \"jobs.json\", {})\n",
    "    if not jobs:\n",
    "        return None\n",
    "    rows = sorted(jobs.items(), key=lambda kv: kv[1].get(\"ts_updated\", 0), reverse=True)\n",
    "    for jid, row in rows:\n",
    "        if namespace in jid and row.get(\"dataset_path\"):\n",
    "            row[\"job_id\"] = jid\n",
    "            return row\n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "id": "42f298d1-fe8b-4c79-8b2b-27035bb5594b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def auto_retrain_if_drift(namespace: str = \"default\") -> Dict[str, Any]:\n",
    "    now = time.time()\n",
    "    if now - _LAST_RETRAIN[\"ts\"] < DRIFT_RETRAIN_COOLDOWN_HOURS * 3600:\n",
    "        return {\"status\": \"cooldown\", \"next_sec\": int(DRIFT_RETRAIN_COOLDOWN_HOURS*3600 - (now - _LAST_RETRAIN['ts']))}\n",
    "    try:\n",
    "        info = drift_check(namespace=namespace).body\n",
    "        drift = json.loads(info) if isinstance(info, (bytes, bytearray)) else info\n",
    "    except Exception:\n",
    "        drift = {\"is_drift\": False}\n",
    "    if not drift or not drift.get(\"is_drift\", False):\n",
    "        return {\"status\": \"no_drift\"}\n",
    "    RETRAIN_TRIGGERS.inc()\n",
    "    row = _latest_job_for_ns(namespace)\n",
    "    if not row:\n",
    "        return {\"status\": \"no_dataset\"}\n",
    "    ds = row[\"dataset_path\"]; jid = f\"{namespace}_{int(time.time())}\"\n",
    "    RETRAIN_RUNS.inc()\n",
    "    try:\n",
    "        out = train_phase2_and_promote(dataset_path=ds, namespace=namespace, job_id=jid, ce_epochs=1, slm_epochs=1)\n",
    "        passed = bool(out[\"passed\"])\n",
    "        (RETRAIN_PASS if passed else RETRAIN_FAIL).inc()\n",
    "        _LAST_RETRAIN[\"ts\"] = now\n",
    "        reg = _json_read(REGISTRY_PATH, {})\n",
    "        challengers = reg.get(\"challengers\", [])\n",
    "        challengers.append({\"job_id\": jid, \"metrics\": out[\"metrics\"], \"ts\": int(time.time())})\n",
    "        challengers = sorted(challengers, key=lambda x: (x[\"metrics\"].get(\"ndcg@10\",0), x[\"metrics\"].get(\"faithfulness\",0)), reverse=True)[:MAX_CHALLENGERS]\n",
    "        reg[\"challengers\"] = challengers\n",
    "        _json_write(REGISTRY_PATH, reg)\n",
    "        return {\"status\": \"trained\", \"job_id\": jid, \"passed\": passed, \"metrics\": out[\"metrics\"]}\n",
    "    except Exception as e:\n",
    "        return {\"status\": \"error\", \"error\": str(e)}\n",
    "@app.post(\"/auto/retrain\")\n",
    "def api_auto_retrain(namespace: str = \"default\"):\n",
    "    return JSONResponse(auto_retrain_if_drift(namespace))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "id": "49d659c1-5dc1-4255-a3d3-58cda0a916d0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Phase-3 control loop started (10 min cadence).\n"
     ]
    }
   ],
   "source": [
    "def _phase3_loop():\n",
    "    while True:\n",
    "        try:\n",
    "            aggregate_live_eval(EVAL_WINDOW_HOURS)\n",
    "            bandit_update_from_logs()\n",
    "            auto_retrain_if_drift(namespace=\"default\")\n",
    "        except Exception as e:\n",
    "            try:\n",
    "                jobs_set(\"PHASE3_LOOP\", status=\"ERROR\", error=str(e), ts=int(time.time()))\n",
    "            except Exception:\n",
    "                pass\n",
    "        time.sleep(600 + random.randint(0, 30))\n",
    "try:\n",
    "    PHASE3_THREAD\n",
    "except NameError:\n",
    "    PHASE3_THREAD = None\n",
    "if PHASE3_THREAD is None or not PHASE3_THREAD.is_alive():\n",
    "    PHASE3_THREAD = threading.Thread(target=_phase3_loop, daemon=True)\n",
    "    PHASE3_THREAD.start()\n",
    "    print(\"Phase-3 control loop started (10 min cadence).\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "id": "26b71e0d-a868-4469-9737-a77f0b70966c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Phase-3 endpoints ready: /eval/aggregate, /rl/update_from_logs, /verify/mode, /auto/retrain, /policy/status, /registry/status\n"
     ]
    }
   ],
   "source": [
    "@app.get(\"/policy/status\")\n",
    "def policy_status():\n",
    "    pol = _json_read(POLICY_PATH, {\"version\": 0, \"weights\": {\"A\": 0.6, \"B\": 0.4}})\n",
    "    return JSONResponse(pol)\n",
    "@app.get(\"/registry/status\")\n",
    "def registry_status():\n",
    "    reg = _json_read(REGISTRY_PATH, {})\n",
    "    return JSONResponse(reg)\n",
    "print(\"Phase-3 endpoints ready: /eval/aggregate, /rl/update_from_logs, /verify/mode, /auto/retrain, /policy/status, /registry/status\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "id": "6590f4dd-4e23-46fb-85a1-ab0264f0a9cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, json, time, threading, random\n",
    "from pathlib import Path\n",
    "from typing import Any, Dict, List, Optional, Tuple\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from fastapi import Request, HTTPException, Body\n",
    "from fastapi.responses import JSONResponse\n",
    "from prometheus_client import Counter, Gauge, Histogram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "id": "7214a161-9298-4edf-9eb2-9bf4246745bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "FEEDBACK_DIR   = Path(\"artifacts/feedback\"); FEEDBACK_DIR.mkdir(parents=True, exist_ok=True)\n",
    "FEED_LOG       = FEEDBACK_DIR / \"feedback.jsonl\"          \n",
    "FEED_SNAP_DIR  = FEEDBACK_DIR / \"snapshots\"; FEED_SNAP_DIR.mkdir(parents=True, exist_ok=True)\n",
    "PHASE4_DIR     = Path(\"artifacts/phase4\"); PHASE4_DIR.mkdir(parents=True, exist_ok=True)\n",
    "_GOV = {\n",
    "    \"enable_ce\": True,\n",
    "    \"enable_adapters\": True,\n",
    "    \"p95_target_ms\": 1800,\n",
    "    \"cost_target\": 0.018,         \n",
    "    \"window\": 200,                 \n",
    "    \"cooldown_sec\": 600,            \n",
    "}\n",
    "_gov_lat = []\n",
    "_gov_cost = []\n",
    "_gov_last_toggle = 0\n",
    "FEED_TOTAL      = Counter(\"feedback_total_v1\", \"Total feedback events\")\n",
    "FEED_GOOD       = Counter(\"feedback_good_v1\",  \"Good feedback events\")\n",
    "FEED_BAD        = Counter(\"feedback_bad_v1\",   \"Bad feedback events\")\n",
    "GOV_TOGGLES     = Counter(\"governor_toggles_total_v1\", \"Total toggles by governor\", [\"feature\",\"direction\"])\n",
    "GOV_P95_LAT_MS  = Gauge(\"governor_p95_latency_ms_v1\", \"Observed p95 latency (ms) window\")\n",
    "GOV_MEAN_COST   = Gauge(\"governor_mean_cost_v1\", \"Observed mean cost (proxy) window\")\n",
    "PHASE4_RUNS     = Counter(\"phase4_runs_total_v1\", \"Phase-4 incremental fine-tune runs\")\n",
    "PHASE4_PASS     = Counter(\"phase4_pass_total_v1\", \"Phase-4 runs that passed gates\")\n",
    "PHASE4_FAIL     = Counter(\"phase4_fail_total_v1\", \"Phase-4 runs that failed gates\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "id": "1db2ed35-096e-4da5-8a56-095810523590",
   "metadata": {},
   "outputs": [],
   "source": [
    "@app.post(\"/feedback\")\n",
    "def feedback(\n",
    "    job_id: str = Body(...),\n",
    "    namespace: str = Body(\"default\"),\n",
    "    request_id: str = Body(...),\n",
    "    verdict: str = Body(...),   \n",
    "    route: Optional[str] = Body(None),\n",
    "    query_text: Optional[str] = Body(None),\n",
    "    answer_text: Optional[str] = Body(None),\n",
    "    correction_text: Optional[str] = Body(None),\n",
    "    sql_text: Optional[str] = Body(None),\n",
    "    notes: Optional[str] = Body(None),\n",
    "):\n",
    "    \"\"\"Lightweight human-in-the-loop. Store and count.\"\"\"\n",
    "    ev = {\n",
    "        \"ts\": int(time.time()),\n",
    "        \"job_id\": job_id,\n",
    "        \"namespace\": namespace,\n",
    "        \"request_id\": request_id,\n",
    "        \"verdict\": verdict,\n",
    "        \"route\": route,\n",
    "        \"query_text\": query_text,\n",
    "        \"answer_text\": answer_text,\n",
    "        \"correction_text\": correction_text,\n",
    "        \"sql_text\": sql_text,\n",
    "        \"notes\": notes,\n",
    "    }\n",
    "    with open(FEED_LOG, \"a\", encoding=\"utf-8\") as f:\n",
    "        f.write(json.dumps(ev) + \"\\n\")\n",
    "    FEED_TOTAL.inc()\n",
    "    (FEED_GOOD if verdict == \"good\" else FEED_BAD).inc()\n",
    "    return JSONResponse({\"status\": \"ok\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "id": "3eb674a9-3d7b-4af4-9e2d-182dfa7160d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _read_jsonl(path: Path, max_lines: int = 200000) -> List[Dict[str, Any]]:\n",
    "    if not path.exists(): return []\n",
    "    out = []\n",
    "    with open(path, \"r\", encoding=\"utf-8\") as f:\n",
    "        for i, line in enumerate(f):\n",
    "            if i >= max_lines: break\n",
    "            try: out.append(json.loads(line))\n",
    "            except: continue\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "id": "8cc3d304-185c-4ce8-8c14-df867f8f7303",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _make_pairs_from_feedback(evts: List[Dict[str, Any]]) -> Dict[str, List[Dict[str,str]]]:\n",
    "    \"\"\"\n",
    "    Build task-specific instruction pairs from feedback.\n",
    "    - For 'good': (query  answer) becomes positive example.\n",
    "    - For 'bad' + correction: (query  correction) becomes positive; original answer ignored.\n",
    "    - For SQL: prefer sql_text/correction_text.\n",
    "    \"\"\"\n",
    "    buckets = {\"general\":[], \"sql\":[], \"math\":[], \"policy\":[]}\n",
    "    for e in evts:\n",
    "        q = (e.get(\"query_text\") or \"\").strip()\n",
    "        a = (e.get(\"answer_text\") or \"\").strip()\n",
    "        corr = (e.get(\"correction_text\") or \"\").strip()\n",
    "        sql = (e.get(\"sql_text\") or \"\").strip()\n",
    "        verdict = e.get(\"verdict\",\"\").lower()\n",
    "        route = (e.get(\"route\") or \"general\").lower()\n",
    "        if route == \"sql\" or sql:\n",
    "            resp = sql or corr or a\n",
    "            if not resp: continue\n",
    "            prompt = f\"Write a safe SELECT-only SQL answering: {q}\\nRules: deny DML; enforce LIMIT<=100; table name is dataset.\"\n",
    "            buckets[\"sql\"].append({\"instruction\": prompt, \"response\": resp})\n",
    "            continue\n",
    "        if route == \"math\":\n",
    "            target = corr or a\n",
    "            if not target: continue\n",
    "            buckets[\"math\"].append({\"instruction\": q, \"response\": target})\n",
    "            continue\n",
    "        if route == \"policy\":\n",
    "            target = corr or a or \"PII must be masked; deny DML; enforce LIMIT; audit model_id/policy_version.\"\n",
    "            buckets[\"policy\"].append({\"instruction\": q, \"response\": target})\n",
    "            continue\n",
    "        target = corr if (verdict == \"bad\" and corr) else (a or corr)\n",
    "        if q and target:\n",
    "            buckets[\"general\"].append({\"instruction\": q, \"response\": target})\n",
    "    return buckets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "id": "1f4ce8ce-3544-4a66-9bd4-43023adb059f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _mine_ce_pairs_from_feedback(evts: List[Dict[str, Any]]) -> List[Tuple[str,str,int]]:\n",
    "    \"\"\"\n",
    "    Create CE (q,doc,label) pairs using good answers as positives and random negatives.\n",
    "    We'll treat answer_text/correction_text as 'doc'.\n",
    "    \"\"\"\n",
    "    pos = [(e.get(\"query_text\",\"\"), e.get(\"correction_text\") or e.get(\"answer_text\",\"\")) for e in evts if e.get(\"verdict\")==\"good\"]\n",
    "    pos = [(q,d) for (q,d) in pos if q and d]\n",
    "    if not pos: return []\n",
    "    docs = [d for _,d in pos]\n",
    "    rng = np.random.default_rng(11)\n",
    "    pairs = []\n",
    "    for (q,d) in pos:\n",
    "        pairs.append((q,d,1))\n",
    "        j = int(rng.integers(0, len(docs)))\n",
    "        if docs[j] == d: j = (j+1) % len(docs)\n",
    "        pairs.append((q, docs[j], 0))\n",
    "    return pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "id": "a95cac04-43f9-448c-b27b-4ce02dff755d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _gov_record(meta: Dict[str, Any]):\n",
    "    lat = float(meta.get(\"latency_ms\", 0))\n",
    "    retr = meta.get(\"retriever\")\n",
    "    cost = 0.012 if retr == \"bm25\" else 0.016\n",
    "    _gov_lat.append(lat); _gov_cost.append(cost)\n",
    "    if len(_gov_lat) > _GOV[\"window\"]:\n",
    "        del _gov_lat[:len(_gov_lat)-_GOV[\"window\"]]\n",
    "        del _gov_cost[:len(_gov_cost)-_GOV[\"window\"]]\n",
    "    if _gov_lat:\n",
    "        p95 = float(np.percentile(_gov_lat, 95))\n",
    "        GOV_P95_LAT_MS.set(p95)\n",
    "    if _gov_cost:\n",
    "        mean_cost = float(np.mean(_gov_cost))\n",
    "        GOV_MEAN_COST.set(mean_cost)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "id": "02717d7a-c60c-4c04-bb7a-8e47ea010c1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _gov_maybe_toggle():\n",
    "    global _GOV, _gov_last_toggle\n",
    "    if not _gov_lat: return\n",
    "    now = time.time()\n",
    "    if now - _gov_last_toggle < _GOV[\"cooldown_sec\"]:\n",
    "        return\n",
    "    p95 = float(np.percentile(_gov_lat, 95)) if _gov_lat else 0\n",
    "    mc  = float(np.mean(_gov_cost)) if _gov_cost else 0\n",
    "    if p95 > _GOV[\"p95_target_ms\"] + 150:  \n",
    "        if _GOV[\"enable_ce\"]:\n",
    "            _GOV[\"enable_ce\"] = False; _gov_last_toggle = now\n",
    "            GOV_TOGGLES.labels(\"ce\",\"off\").inc()\n",
    "        elif _GOV[\"enable_adapters\"]:\n",
    "            _GOV[\"enable_adapters\"] = False; _gov_last_toggle = now\n",
    "            GOV_TOGGLES.labels(\"adapters\",\"off\").inc()\n",
    "    if mc > _GOV[\"cost_target\"] + 0.002:\n",
    "        if _GOV[\"enable_adapters\"]:\n",
    "            _GOV[\"enable_adapters\"] = False; _gov_last_toggle = now\n",
    "            GOV_TOGGLES.labels(\"adapters\",\"off\").inc()\n",
    "def _gov_status():\n",
    "    return {\n",
    "        \"enable_ce\": _GOV[\"enable_ce\"],\n",
    "        \"enable_adapters\": _GOV[\"enable_adapters\"],\n",
    "        \"p95_target_ms\": _GOV[\"p95_target_ms\"],\n",
    "        \"cost_target\": _GOV[\"cost_target\"],\n",
    "        \"window\": _GOV[\"window\"],\n",
    "        \"cooldown_sec\": _GOV[\"cooldown_sec\"],\n",
    "        \"obs_p95_ms\": float(np.percentile(_gov_lat,95)) if _gov_lat else 0.0,\n",
    "        \"obs_mean_cost\": float(np.mean(_gov_cost)) if _gov_cost else 0.0,\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "id": "a8e867e7-1992-44ed-9ac6-527ddf27a5f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "@app.get(\"/governor/status\")\n",
    "def governor_status():\n",
    "    return JSONResponse(_gov_status())\n",
    "@app.post(\"/governor/mode\")\n",
    "def governor_mode(enable_ce: Optional[bool] = None, enable_adapters: Optional[bool] = None,\n",
    "                  p95_target_ms: Optional[int] = None, cost_target: Optional[float] = None):\n",
    "    if enable_ce is not None: _GOV[\"enable_ce\"] = bool(enable_ce)\n",
    "    if enable_adapters is not None: _GOV[\"enable_adapters\"] = bool(enable_adapters)\n",
    "    if p95_target_ms is not None: _GOV[\"p95_target_ms\"] = int(p95_target_ms)\n",
    "    if cost_target is not None: _GOV[\"cost_target\"] = float(cost_target)\n",
    "    return JSONResponse(_gov_status())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "id": "6d88fca4-3cf3-4d27-a81f-170e68098598",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _rerank_ce_or_cosine(job_id: str, df, row_ids, text_cols, q_text, top_k=5):\n",
    "    if not _GOV[\"enable_ce\"]:\n",
    "        q_emb = _embed_query(\"query: \" + q_text)\n",
    "        return rerank_cosine(df, row_ids, text_cols, q_emb, top_k=top_k), \"cosine\"\n",
    "    ce = _load_ce(job_id)\n",
    "    if ce is None:\n",
    "        q_emb = _embed_query(\"query: \" + q_text)\n",
    "        return rerank_cosine(df, row_ids, text_cols, q_emb, top_k=top_k), \"cosine\"\n",
    "    texts = df.iloc[row_ids][text_cols].astype(str).agg(\" \".join, axis=1).tolist()\n",
    "    pairs = [[q_text, t] for t in texts]\n",
    "    try:\n",
    "        scores = ce.predict(pairs)\n",
    "        order = np.argsort(-np.array(scores))[:top_k]\n",
    "        return (row_ids[order], np.array(scores)[order]), \"ce\"\n",
    "    except Exception:\n",
    "        q_emb = _embed_query(\"query: \" + q_text)\n",
    "        return rerank_cosine(df, row_ids, text_cols, q_emb, top_k=top_k), \"cosine\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "id": "09069fb0-fdc4-4670-830a-48912c5136ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _respond_with_slm(job_id: str, route: str, df, row_ids, text_cols, query: str) -> Dict[str, Any]:\n",
    "    if not _GOV[\"enable_adapters\"]:\n",
    "        if route == \"sql\":   return slm_run_sql(df, query, limit=5)\n",
    "        if route == \"math\":  return slm_math(query)\n",
    "        if route == \"policy\":return slm_policy(query)\n",
    "        return slm_respond(df, row_ids, text_cols, query)\n",
    "    ctx_texts = df.iloc[row_ids][text_cols].astype(str).agg(\" \".join, axis=1).str.slice(0, 220).tolist() if len(row_ids) else []\n",
    "    task = \"general\" if route == \"factual\" else route\n",
    "    task = task if task in {\"general\",\"sql\",\"math\",\"policy\"} else \"general\"\n",
    "    prompt = _compose_prompt(task, query, ctx_texts)\n",
    "    gen = _slm_generate_with_adapter(job_id, task, prompt, max_new_tokens=96)\n",
    "    if gen is not None:\n",
    "        if task == \"sql\":\n",
    "            m = re.search(r\"(select\\s.+?;)\", gen, flags=re.I|re.S)\n",
    "            sql = sqlparse.format(m.group(1), reindent=False, keyword_case=\"upper\") if m else f\"SELECT {', '.join((df.columns[:5]))} FROM dataset LIMIT 5;\"\n",
    "            return {\"action\":\"run_sql\",\"payload\":{\"sql\":sql},\"citations\":[],\"model_id\":f\"slm-{task}-lora\"}\n",
    "        if task == \"math\":\n",
    "            m = re.search(r\"(-?\\d+(\\.\\d+)?)\", gen)\n",
    "            ans = m.group(1) if m else gen.strip()\n",
    "            return {\"action\":\"respond\",\"payload\":{\"text\":f\"Result: {ans}\"},\"citations\":[],\"model_id\":f\"slm-{task}-lora\"}\n",
    "        return {\"action\":\"respond\",\"payload\":{\"text\":gen.strip()},\"citations\":[int(i) for i in row_ids[:5]],\"model_id\":f\"slm-{task}-lora\"}\n",
    "    if task == \"sql\":    return slm_run_sql(df, query, limit=5)\n",
    "    if task == \"math\":   return slm_math(query)\n",
    "    if task == \"policy\": return slm_policy(query)\n",
    "    return slm_respond(df, row_ids, text_cols, query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "id": "56a5bba2-0fe5-4151-846e-5bcccc86f415",
   "metadata": {},
   "outputs": [],
   "source": [
    "app.router.routes = [\n",
    "    r for r in app.router.routes\n",
    "    if not (getattr(r, \"path\", None) == \"/query\" and \"POST\" in getattr(r, \"methods\", set()))\n",
    "]\n",
    "@app.post(\"/query\")\n",
    "async def query(request: Request):\n",
    "    REQS.labels(\"/query\").inc()\n",
    "    CANARY_TOTAL.inc()\n",
    "    with LAT.labels(\"/query\").time():\n",
    "        params = await _extract_params(request)\n",
    "        job_id = params.get(\"job_id\"); namespace = params.get(\"namespace\",\"default\")\n",
    "        query_text = params.get(\"query_text\",\"\")\n",
    "        try: top_k = int(params.get(\"top_k\",5))\n",
    "        except: top_k = 5\n",
    "        if not job_id: raise HTTPException(422, \"job_id is required\")\n",
    "        job_row = jobs_get(job_id)\n",
    "        if not job_row: raise HTTPException(404, \"Unknown job_id\")\n",
    "        ds = job_row.get(\"dataset_path\")\n",
    "        if not ds or not Path(ds).exists(): raise HTTPException(400, \"Dataset path not found for job\")\n",
    "        prod_out = query_pipeline_v2(job_id=job_id, dataset_path=ds, namespace=namespace, query_text=query_text, top_k=top_k)\n",
    "        meta = prod_out.get(\"meta\", {})\n",
    "        _gov_record(meta)\n",
    "        _gov_maybe_toggle()\n",
    "        policy_version = _read_policy_version()\n",
    "        prod_out.setdefault(\"meta\", {})\n",
    "        prod_out[\"meta\"].update({\"policy_version\": policy_version, \"canary\": False, \"arm\": \"PROD\"})\n",
    "        CANARY_LAT_MS.labels(\"PROD\").observe(float(prod_out[\"meta\"].get(\"latency_ms\", 0)))\n",
    "        threading.Thread(target=_shadow_worker, args=(job_id, ds, namespace, query_text, top_k, prod_out), daemon=True).start()\n",
    "        return JSONResponse(prod_out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "id": "e7ee459d-3c2a-472a-aef0-f49ecebedee5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def phase4_finetune_incremental(namespace: str = \"default\") -> Dict[str, Any]:\n",
    "    \"\"\"Train incremental adapters/reranker from feedback; evaluate; gate; promote.\"\"\"\n",
    "    PHASE4_RUNS.inc()\n",
    "    evts = [e for e in _read_jsonl(FEED_LOG) if e.get(\"namespace\")==namespace]\n",
    "    pairs_slm = _make_pairs_from_feedback(evts)\n",
    "    pairs_ce  = _mine_ce_pairs_from_feedback(evts)\n",
    "    row = _latest_job_for_ns(namespace)\n",
    "    if not row: \n",
    "        PHASE4_FAIL.inc()\n",
    "        return {\"status\":\"no_dataset\"}\n",
    "    job_base = f\"{namespace}_{int(time.time())}\"\n",
    "    job_id = job_base + \"_p4\"\n",
    "    ds = row[\"dataset_path\"]\n",
    "    base_dir = PHASE2_ROOT / job_id\n",
    "    base_dir.mkdir(parents=True, exist_ok=True)\n",
    "    text_cols = guess_text_cols(pd.read_csv(ds) if ds.endswith(\".csv\") else pd.read_parquet(ds), CFG[\"bm25_text_cols_guess\"])\n",
    "    _jdump(text_cols, base_dir / \"text_cols.json\")\n",
    "    ce_meta = {\"status\":\"skipped_no_pairs\"}\n",
    "    if pairs_ce:\n",
    "        ce_meta = train_reranker_ce(pairs_ce, base_dir / \"reranker_ce\", epochs=1, batch_size=16)\n",
    "    slm_meta = {}\n",
    "    for task, items in pairs_slm.items():\n",
    "        if not items: \n",
    "            slm_meta[task] = {\"status\":\"skipped_no_pairs\"}\n",
    "            continue\n",
    "        out_dir = base_dir / f\"slm_{task}_lora\"\n",
    "        slm_meta[task] = _slm_train_lora(task, items, out_dir, epochs=1)\n",
    "    metrics = evaluate_phase2(pd.read_csv(ds) if ds.endswith(\".csv\") else pd.read_parquet(ds), text_cols, base_dir / \"reranker_ce\", n_queries=25)\n",
    "    ok = _passes_gates(metrics)\n",
    "    _jdump({\"job_id\": job_id, \"metrics\": metrics, \"slm_meta\": slm_meta, \"ce_meta\": ce_meta}, PHASE4_DIR / f\"{job_id}_summary.json\")\n",
    "    if ok:\n",
    "        PHASE4_PASS.inc()\n",
    "        try:\n",
    "            requests.post(\"http://127.0.0.1:8000/registry/promote\", params={\"champion\":\"ce_minilm_v1\"})\n",
    "            requests.post(\"http://127.0.0.1:8000/rl/update_from_logs\")\n",
    "        except Exception:\n",
    "            pass\n",
    "        reg = _json_read(REGISTRY_PATH, {})\n",
    "        chall = reg.get(\"challengers\", [])\n",
    "        chall.append({\"job_id\": job_id, \"metrics\": metrics, \"origin\":\"phase4_feedback\", \"ts\": int(time.time())})\n",
    "        chall = sorted(chall, key=lambda x: (x[\"metrics\"].get(\"ndcg@10\",0), x[\"metrics\"].get(\"faithfulness\",0)), reverse=True)[:MAX_CHALLENGERS]\n",
    "        reg[\"challengers\"] = chall\n",
    "        _json_write(REGISTRY_PATH, reg)\n",
    "        return {\"status\":\"promoted_or_ready\", \"job_id\": job_id, \"metrics\": metrics}\n",
    "    else:\n",
    "        PHASE4_FAIL.inc()\n",
    "        return {\"status\":\"challenger_failed_gates\", \"job_id\": job_id, \"metrics\": metrics}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "id": "2a142f53-e776-47f2-98a2-f31a6a334164",
   "metadata": {},
   "outputs": [],
   "source": [
    "@app.post(\"/phase4/run\")\n",
    "def api_phase4_run(namespace: str = \"default\"):\n",
    "    \"\"\"Manual trigger for Phase-4 incremental fine-tuning on a namespace.\"\"\"\n",
    "    out = phase4_finetune_incremental(namespace)\n",
    "    return JSONResponse(out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "id": "6f0ed38f-2707-400c-b2d6-9d543a517bf3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Phase-4 loop started (feedback  incremental fine-tune).\n",
      "Phase-4 endpoints ready: /feedback, /governor/status, /governor/mode, /phase4/run\n"
     ]
    }
   ],
   "source": [
    "def _phase4_loop():\n",
    "    while True:\n",
    "        try:\n",
    "            if FEED_LOG.exists():\n",
    "                dst = FEED_SNAP_DIR / f\"feedback_{int(time.time())}.jsonl\"\n",
    "                try:\n",
    "                    with open(FEED_LOG,\"r\",encoding=\"utf-8\") as src, open(dst,\"w\",encoding=\"utf-8\") as sink:\n",
    "                        for line in src: sink.write(line)\n",
    "                except Exception: pass\n",
    "            evts = _read_jsonl(FEED_LOG)\n",
    "            if len(evts) >= 50:  \n",
    "                phase4_finetune_incremental(namespace=\"default\")\n",
    "        except Exception as e:\n",
    "            try: jobs_set(\"PHASE4_LOOP\", status=\"ERROR\", error=str(e), ts=int(time.time()))\n",
    "            except Exception: pass\n",
    "        time.sleep(12*3600 + random.randint(0, 600))\n",
    "try:\n",
    "    PHASE4_THREAD\n",
    "except NameError:\n",
    "    PHASE4_THREAD = None\n",
    "if PHASE4_THREAD is None or not PHASE4_THREAD.is_alive():\n",
    "    PHASE4_THREAD = threading.Thread(target=_phase4_loop, daemon=True)\n",
    "    PHASE4_THREAD.start()\n",
    "    print(\"Phase-4 loop started (feedback  incremental fine-tune).\")\n",
    "print(\"Phase-4 endpoints ready: /feedback, /governor/status, /governor/mode, /phase4/run\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "id": "d57075ef-c83f-4717-b3b3-65c2f8555a07",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'status': 'ok'}"
      ]
     },
     "execution_count": 149,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import requests\n",
    "requests.post(\"http://127.0.0.1:8000/feedback\", json={\n",
    "  \"job_id\": JOB, \"namespace\": \"demo\",\n",
    "  \"request_id\": \"debug-1\",\n",
    "  \"verdict\": \"good\",\n",
    "  \"route\": \"sql\",\n",
    "  \"query_text\": \"select age, income and balance where default = 1 limit 5\",\n",
    "  \"sql_text\": \"SELECT age, income, balance FROM dataset WHERE default = 1 LIMIT 5;\",\n",
    "  \"notes\": \"works for me\"\n",
    "}).json()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "id": "7a5d6782-b519-448d-84ad-c2ba12392825",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'status': 'challenger_failed_gates',\n",
       " 'job_id': 'demo_1762113350_p4',\n",
       " 'metrics': {'ndcg@10': 0.0,\n",
       "  'faithfulness': 0.9499999995099999,\n",
       "  'p95': 90,\n",
       "  'cost': 0.012000000000000002,\n",
       "  'samples': 25}}"
      ]
     },
     "execution_count": 150,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "requests.post(\"http://127.0.0.1:8000/phase4/run\", params={\"namespace\":\"demo\"}).json()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "id": "57ea9ff8-15b9-434e-a0fd-a36ea12c63d3",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'enable_ce': True,\n",
       " 'enable_adapters': True,\n",
       " 'p95_target_ms': 1800,\n",
       " 'cost_target': 0.018,\n",
       " 'window': 200,\n",
       " 'cooldown_sec': 600,\n",
       " 'obs_p95_ms': 0.0,\n",
       " 'obs_mean_cost': 0.0}"
      ]
     },
     "execution_count": 151,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "requests.get(\"http://127.0.0.1:8000/governor/status\").json()\n",
    "requests.post(\"http://127.0.0.1:8000/governor/mode\", params={\"enable_ce\":True, \"enable_adapters\":True}).json()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "id": "6ba2329a-4efa-49f6-9d60-b3e9bf799d7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, json, time, math, threading\n",
    "from pathlib import Path\n",
    "from typing import Any, Dict, List, Optional, Tuple\n",
    "import numpy as np\n",
    "from fastapi import Request\n",
    "from fastapi.responses import JSONResponse\n",
    "from prometheus_client import Counter, Gauge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "id": "8bad1ed5-787a-42fe-9e55-0d15b93d4a3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "META_DIR         = Path(\"artifacts/policy\"); META_DIR.mkdir(parents=True, exist_ok=True)\n",
    "META_HISTORY     = META_DIR / \"history.jsonl\"\n",
    "META_STATUS_PATH = META_DIR / \"meta_status.json\"\n",
    "META_CFG_PATH    = META_DIR / \"meta_config.json\"   \n",
    "CANARY_LOG       = Path(\"logs/canary_live.jsonl\")\n",
    "SHADOW_LOG       = Path(\"logs/shadow_live.jsonl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "id": "7f8fdee9-2462-42ff-b9c3-b7869ad0132b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _read_jsonl(path: Path, cutoff_ts: Optional[float] = None, max_lines: int = 400_000) -> List[Dict[str, Any]]:\n",
    "    if not path.exists(): return []\n",
    "    out = []\n",
    "    with open(path, \"r\", encoding=\"utf-8\") as f:\n",
    "        for i, line in enumerate(f):\n",
    "            if i >= max_lines: break\n",
    "            try:\n",
    "                j = json.loads(line)\n",
    "                if cutoff_ts is None or j.get(\"ts\", 0) >= cutoff_ts:\n",
    "                    out.append(j)\n",
    "            except Exception:\n",
    "                continue\n",
    "    return out\n",
    "def _json_read(p: Path, default):\n",
    "    try:\n",
    "        return json.loads(p.read_text())\n",
    "    except Exception:\n",
    "        return default\n",
    "def _json_write(p: Path, obj: Any):\n",
    "    p.parent.mkdir(parents=True, exist_ok=True)\n",
    "    tmp = p.with_suffix(p.suffix + \".tmp\")\n",
    "    tmp.write_text(json.dumps(obj, indent=2))\n",
    "    os.replace(tmp, p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "id": "006f0934-d7f9-4e2c-a769-ffd82159e8b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "META_WEIGHTS_A   = Gauge(\"meta_policy_weight_A\", \"Current traffic weight for arm A (PROD)\")\n",
    "META_WEIGHTS_B   = Gauge(\"meta_policy_weight_B\", \"Current traffic weight for arm B (CANDIDATE)\")\n",
    "META_STEP_APPLIED= Counter(\"meta_steps_applied_total\", \"Number of meta-controller policy steps applied\")\n",
    "META_MEAN_DELTA  = Gauge(\"meta_mean_delta_window\", \"Mean (cand - prod) reward in controller window\")\n",
    "META_DECISION_TS = Gauge(\"meta_last_decision_ts\", \"Unix timestamp of last meta decision\")\n",
    "META_DEFAULTS = {\n",
    "    \"window_hours\": 6,    \n",
    "    \"temp\": 0.20,         \n",
    "    \"step_cap\": 0.10,     \n",
    "    \"max_cand\": 0.25,     \n",
    "    \"min_prod\": 0.50,      \n",
    "    \"cooldown_sec\": 600,   \n",
    "}\n",
    "REWARD_WEIGHTS = {\"alpha\": 0.5, \"beta\": 0.4, \"gamma\": 0.0005, \"delta\": 10.0}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "id": "2204fe8e-6012-48ea-9684-541bc57590b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prd_reward_proxy(out_json: Dict[str, Any]) -> float:\n",
    "    faith = 0.90 if out_json.get(\"action\") == \"respond\" else 0.80\n",
    "    ndcg  = 0.66 if out_json.get(\"meta\", {}).get(\"retriever\") != \"bm25\" else 0.61\n",
    "    p95_ms = float(out_json.get(\"meta\", {}).get(\"latency_ms\", 1500))\n",
    "    cost = 0.016 if out_json.get(\"meta\", {}).get(\"retriever\") != \"bm25\" else 0.012\n",
    "    a,b,g,d = (REWARD_WEIGHTS[k] for k in (\"alpha\",\"beta\",\"gamma\",\"delta\"))\n",
    "    return a*faith + b*ndcg - g*p95_ms - d*cost\n",
    "def _window_cutoff(hours: int) -> float:\n",
    "    return time.time() - hours*3600"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "id": "c33b8c33-9c0b-4bfc-865e-6a80613c3cb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _arm_stats(window_hours: int) -> Dict[str, Dict[str, float]]:\n",
    "    cutoff = _window_cutoff(window_hours)\n",
    "    canary = _read_jsonl(CANARY_LOG, cutoff_ts=cutoff)\n",
    "    shadow = _read_jsonl(SHADOW_LOG, cutoff_ts=cutoff)\n",
    "    prod_rewards, cand_rewards = [], []\n",
    "    prod_lat, cand_lat = [], []\n",
    "    prod_cost, cand_cost = [], []\n",
    "    for r in canary:\n",
    "        rp = r.get(\"reward_prod\"); rc = r.get(\"reward_cand\")\n",
    "        if rp is not None: prod_rewards.append(float(rp))\n",
    "        if rc is not None: cand_rewards.append(float(rc))\n",
    "    for r in shadow:\n",
    "        prod = r.get(\"prod\"); cand = r.get(\"cand\")\n",
    "        if isinstance(prod, dict):\n",
    "            prod_rewards.append(prd_reward_proxy(prod))\n",
    "            prod_lat.append(float(prod.get(\"meta\",{}).get(\"latency_ms\", 0)))\n",
    "            retr = prod.get(\"meta\",{}).get(\"retriever\")\n",
    "            prod_cost.append(0.012 if retr==\"bm25\" else 0.016)\n",
    "        if isinstance(cand, dict):\n",
    "            cand_rewards.append(prd_reward_proxy(cand))\n",
    "            cand_lat.append(float(cand.get(\"meta\",{}).get(\"latency_ms\", 0)))\n",
    "            retr = cand.get(\"meta\",{}).get(\"retriever\")\n",
    "            cand_cost.append(0.012 if retr==\"bm25\" else 0.016)\n",
    "    def _agg(vals: List[float]) -> Dict[str,float]:\n",
    "        if not vals: return {\"n\":0,\"mean\":0.0,\"p50\":0.0,\"p95\":0.0}\n",
    "        arr = np.array(vals, dtype=float)\n",
    "        return {\"n\":int(arr.size), \"mean\":float(arr.mean()), \"p50\":float(np.percentile(arr,50)), \"p95\":float(np.percentile(arr,95))}\n",
    "    stats = {\n",
    "        \"A_PROD\": {\n",
    "            \"reward\": _agg(prod_rewards),\n",
    "            \"lat_ms\": _agg(prod_lat),\n",
    "            \"cost\":   {\"mean\": float(np.mean(prod_cost)) if prod_cost else 0.0, \"n\": len(prod_cost)}\n",
    "        },\n",
    "        \"B_CAND\": {\n",
    "            \"reward\": _agg(cand_rewards),\n",
    "            \"lat_ms\": _agg(cand_lat),\n",
    "            \"cost\":   {\"mean\": float(np.mean(cand_cost)) if cand_cost else 0.0, \"n\": len(cand_cost)}\n",
    "        }\n",
    "    }\n",
    "    if stats[\"A_PROD\"][\"reward\"][\"n\"] and stats[\"B_CAND\"][\"reward\"][\"n\"]:\n",
    "        META_MEAN_DELTA.set(stats[\"B_CAND\"][\"reward\"][\"mean\"] - stats[\"A_PROD\"][\"reward\"][\"mean\"])\n",
    "    else:\n",
    "        META_MEAN_DELTA.set(0.0)\n",
    "    return stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "id": "7ab40e85-2c81-4162-b644-02c644c3f0b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _read_policy() -> Dict[str, Any]:\n",
    "    pol = _json_read(POLICY_PATH, {\"version\": 0, \"weights\": {\"A\": 0.6, \"B\": 0.4}})\n",
    "    A = float(pol.get(\"weights\",{}).get(\"A\", 0.6))\n",
    "    B = float(pol.get(\"weights\",{}).get(\"B\", 0.4))\n",
    "    s = max(1e-9, A+B); A,B = A/s, B/s\n",
    "    pol[\"weights\"] = {\"A\": round(A,4), \"B\": round(B,4)}\n",
    "    return pol\n",
    "def _apply_policy(new_weights: Dict[str,float], reason: str, stats_snapshot: Dict[str,Any]):\n",
    "    pol = _read_policy()\n",
    "    pol[\"version\"] = int(pol.get(\"version\", 0)) + 1\n",
    "    pol[\"weights\"] = {\"A\": round(float(new_weights[\"A\"]),4), \"B\": round(float(new_weights[\"B\"]),4)}\n",
    "    pol[\"ts_updated\"] = int(time.time())\n",
    "    pol[\"reason\"] = reason\n",
    "    _json_write(POLICY_PATH, pol)\n",
    "    with open(META_HISTORY, \"a\", encoding=\"utf-8\") as f:\n",
    "        f.write(json.dumps({\"ts\": pol[\"ts_updated\"], \"version\": pol[\"version\"], \"weights\": pol[\"weights\"], \"reason\": reason, \"stats\": stats_snapshot})+\"\\n\")\n",
    "    _json_write(META_STATUS_PATH, {\"ts\": pol[\"ts_updated\"], \"policy\": pol, \"stats\": stats_snapshot})\n",
    "    META_WEIGHTS_A.set(pol[\"weights\"][\"A\"]); META_WEIGHTS_B.set(pol[\"weights\"][\"B\"])\n",
    "    META_DECISION_TS.set(pol[\"ts_updated\"])\n",
    "    try:\n",
    "        jobs_set(\"POLICY_META\", status=\"UPDATED\", version=pol[\"version\"], weights=pol[\"weights\"], reason=reason, ts=pol[\"ts_updated\"])\n",
    "    except Exception:\n",
    "        pass\n",
    "    META_STEP_APPLIED.inc()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "id": "37137a9c-a885-4679-9c43-6fa9b257893a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _propose_weights(current: Dict[str,float], stats: Dict[str,Any], cfg: Dict[str,Any]) -> Dict[str,float]:\n",
    "    rA = stats[\"A_PROD\"][\"reward\"][\"mean\"]\n",
    "    rB = stats[\"B_CAND\"][\"reward\"][\"mean\"]\n",
    "    temp = float(cfg[\"temp\"])\n",
    "    if stats[\"A_PROD\"][\"reward\"][\"n\"] < 50 or stats[\"B_CAND\"][\"reward\"][\"n\"] < 20:\n",
    "        return current\n",
    "    scoreA = 0.0\n",
    "    scoreB = rB - rA\n",
    "    ea, eb = math.exp(scoreA/max(1e-9,temp)), math.exp(scoreB/max(1e-9,temp))\n",
    "    wA, wB = ea/(ea+eb), eb/(ea+eb)\n",
    "    step = float(cfg[\"step_cap\"])\n",
    "    newA = current[\"A\"] + max(-step, min(step, wA - current[\"A\"]))\n",
    "    newB = 1.0 - newA\n",
    "    min_cand, max_cand = float(cfg[\"min_cand\"]), float(cfg[\"max_cand\"])\n",
    "    min_prod = float(cfg[\"min_prod\"])\n",
    "    newB = min(max(newB, min_cand), max_cand)\n",
    "    newA = max(newA, min_prod)\n",
    "    s = max(1e-9, newA+newB); newA, newB = newA/s, newB/s\n",
    "    return {\"A\": round(newA,4), \"B\": round(newB,4)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "id": "a1e1e1f3-b87d-44b0-87a7-d6cc8ae67999",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _simulate(weights: Dict[str,float], stats: Dict[str,Any]) -> Dict[str,Any]:\n",
    "    A, B = float(weights[\"A\"]), float(weights[\"B\"])\n",
    "    rA, rB = stats[\"A_PROD\"][\"reward\"][\"mean\"], stats[\"B_CAND\"][\"reward\"][\"mean\"]\n",
    "    latA, latB = stats[\"A_PROD\"][\"lat_ms\"][\"p95\"], stats[\"B_CAND\"][\"lat_ms\"][\"p95\"]\n",
    "    cA, cB = stats[\"A_PROD\"][\"cost\"][\"mean\"], stats[\"B_CAND\"][\"cost\"][\"mean\"]\n",
    "    p95 = max(latA, latB)*max(A,B) + min(latA, latB)*min(A,B)\n",
    "    reward = A*rA + B*rB\n",
    "    cost = A*cA + B*cB if (cA and cB) else 0.0\n",
    "    return {\"reward_mean_est\": reward, \"p95_ms_est\": p95, \"cost_mean_est\": cost, \"weights\": {\"A\":A,\"B\":B}}\n",
    "_META_LAST_TS = 0\n",
    "_META_LOCK = threading.Lock()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "id": "b2269958-1e3e-49b6-8623-8fe941cfa2ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _load_cfg() -> Dict[str,Any]:\n",
    "    cfg = _json_read(META_CFG_PATH, META_DEFAULTS.copy())\n",
    "    for k,v in META_DEFAULTS.items():\n",
    "        cfg.setdefault(k, v)\n",
    "    return cfg\n",
    "def _meta_step() -> Dict[str,Any]:\n",
    "    global _META_LAST_TS\n",
    "    with _META_LOCK:\n",
    "        cfg = _load_cfg()\n",
    "        now = time.time()\n",
    "        if now - _META_LAST_TS < float(cfg[\"cooldown_sec\"]):\n",
    "            return {\"status\":\"cooldown\", \"next_sec\": int(float(cfg[\"cooldown_sec\"]) - (now - _META_LAST_TS))}\n",
    "        stats = _arm_stats(window_hours=int(cfg[\"window_hours\"]))\n",
    "        pol = _read_policy()\n",
    "        cur = pol[\"weights\"]\n",
    "        neww = _propose_weights(cur, stats, cfg)\n",
    "        if abs(neww[\"A\"]-cur[\"A\"]) < 1e-4 and abs(neww[\"B\"]-cur[\"B\"]) < 1e-4:\n",
    "            _json_write(META_STATUS_PATH, {\"ts\": int(now), \"policy\": pol, \"stats\": stats, \"note\":\"no_change\"})\n",
    "            return {\"status\":\"no_change\", \"weights\": cur, \"stats\": stats}\n",
    "        _apply_policy(neww, reason=\"meta_controller_update\", stats_snapshot=stats)\n",
    "        _META_LAST_TS = now\n",
    "        return {\"status\":\"updated\", \"weights\": neww, \"stats\": stats}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "id": "bf0c92ca-56b8-43a1-88c0-f7f5b6c5bd0d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Meta-controller loop started (10 min cadence).\n"
     ]
    }
   ],
   "source": [
    "def _meta_loop():\n",
    "    while True:\n",
    "        try:\n",
    "            _ = _meta_step()\n",
    "        except Exception as e:\n",
    "            try: jobs_set(\"META_LOOP\", status=\"ERROR\", error=str(e), ts=int(time.time()))\n",
    "            except Exception: pass\n",
    "        time.sleep(600 + int(np.random.randint(0,30)))\n",
    "try:\n",
    "    META_THREAD\n",
    "except NameError:\n",
    "    META_THREAD = None\n",
    "if META_THREAD is None or not META_THREAD.is_alive():\n",
    "    META_THREAD = threading.Thread(target=_meta_loop, daemon=True)\n",
    "    META_THREAD.start()\n",
    "    print(\"Meta-controller loop started (10 min cadence).\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "id": "9c6d3212-9b39-4bba-be92-0ed60bc2aa97",
   "metadata": {},
   "outputs": [],
   "source": [
    "@app.get(\"/meta/status\")\n",
    "def meta_status():\n",
    "    cfg  = _load_cfg()\n",
    "    pol  = _read_policy()\n",
    "    stats= _arm_stats(window_hours=int(cfg[\"window_hours\"]))\n",
    "    hist_lines = []\n",
    "    if META_HISTORY.exists():\n",
    "        try:\n",
    "            with open(META_HISTORY, \"r\", encoding=\"utf-8\") as f:\n",
    "                hist_lines = [json.loads(x) for x in f.readlines()[-20:]]\n",
    "        except Exception:\n",
    "            hist_lines = []\n",
    "    out = {\n",
    "        \"cfg\": cfg,\n",
    "        \"policy\": pol,\n",
    "        \"stats\": stats,\n",
    "        \"history_tail\": hist_lines,\n",
    "        \"ts\": int(time.time())\n",
    "    }\n",
    "    return JSONResponse(out)\n",
    "@app.post(\"/meta/simulate\")\n",
    "def meta_simulate(weights: Optional[Dict[str,float]] = None):\n",
    "    cfg   = _load_cfg()\n",
    "    stats = _arm_stats(window_hours=int(cfg[\"window_hours\"]))\n",
    "    if not weights:\n",
    "        cur = _read_policy()[\"weights\"]\n",
    "        weights = _propose_weights(cur, stats, cfg)\n",
    "    sim = _simulate(weights, stats)\n",
    "    gov = {}\n",
    "    try:\n",
    "        gov = _gov_status()  \n",
    "    except Exception:\n",
    "        gov = {}\n",
    "    return JSONResponse({\"sim\": sim, \"stats\": stats, \"governor\": gov, \"ts\": int(time.time())})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "id": "d99fdc5c-585e-4448-b480-727bf1ed05b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests, json, time\n",
    "BASE=\"http://127.0.0.1:8000\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "id": "b4d7aaff-fc61-4ac5-8924-60feb4fcc057",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "status: 200\n",
      "content-type: application/json\n",
      "text head: {\"sim\":{\"reward_mean_est\":0.0,\"p95_ms_est\":0.0,\"cost_mean_est\":0.0,\"weights\":{\"A\":0.5,\"B\":0.5}},\"stats\":{\"A_PROD\":{\"reward\":{\"n\":0,\"mean\":0.0,\"p50\":0.0,\"p95\":0.0},\"lat_ms\":{\"n\":0,\"mean\":0.0,\"p50\":0.0,\"p95\":0.0},\"cost\":{\"mean\":0.0,\"n\":0}},\"B_CAND\":{\"reward\":{\"n\":0,\"mean\":0.0,\"p50\":0.0,\"p95\":0.0},\"lat_ms\":{\"n\":0,\"mean\":0.0,\"p50\":0.0,\"p95\":0.0},\"cost\":{\"mean\":0.0,\"n\":0}}},\"governor\":{\"enable_ce\":true,\"enable_adapters\":true,\"p95_target_ms\":1800,\"cost_target\":0.018,\"window\":200,\"cooldown_sec\":600,\"ob\n",
      "parsed: {'sim': {'reward_mean_est': 0.0, 'p95_ms_est': 0.0, 'cost_mean_est': 0.0, 'weights': {'A': 0.5, 'B': 0.5}}, 'stats': {'A_PROD': {'reward': {'n': 0, 'mean': 0.0, 'p50': 0.0, 'p95': 0.0}, 'lat_ms': {'n': 0, 'mean': 0.0, 'p50': 0.0, 'p95': 0.0}, 'cost': {'mean': 0.0, 'n': 0}}, 'B_CAND': {'reward': {'n': 0, 'mean': 0.0, 'p50': 0.0, 'p95': 0.0}, 'lat_ms': {'n': 0, 'mean': 0.0, 'p50': 0.0, 'p95': 0.0}, 'cost': {'mean': 0.0, 'n': 0}}}, 'governor': {'enable_ce': True, 'enable_adapters': True, 'p95_target_ms': 1800, 'cost_target': 0.018, 'window': 200, 'cooldown_sec': 600, 'obs_p95_ms': 0.0, 'obs_mean_cost': 0.0}, 'ts': 1762113356}\n"
     ]
    }
   ],
   "source": [
    "r = requests.post(f\"{BASE}/meta/simulate\", json={}, timeout=8)  \n",
    "print(\"status:\", r.status_code)\n",
    "print(\"content-type:\", r.headers.get(\"content-type\"))\n",
    "print(\"text head:\", r.text[:500])\n",
    "data = None\n",
    "try:\n",
    "    data = r.json()\n",
    "except Exception as e:\n",
    "    print(\"json() failed:\", type(e).__name__, e)\n",
    "print(\"parsed:\", data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "id": "e64ddba2-57ad-4901-a593-e7caec357ce5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, json, time, threading\n",
    "from pathlib import Path\n",
    "from typing import Any, Dict, List, Optional\n",
    "import numpy as np\n",
    "from fastapi import HTTPException\n",
    "from fastapi.responses import JSONResponse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "id": "de6d9e48-68f0-4d18-b46a-d488b90ba83b",
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    REGISTRY_PATH\n",
    "except NameError:\n",
    "    REGISTRY_PATH = Path(\"artifacts/registry.json\")\n",
    "REGISTRY_PATH.parent.mkdir(parents=True, exist_ok=True)\n",
    "try:\n",
    "    PHASE2_ROOT\n",
    "except NameError:\n",
    "    PHASE2_ROOT = Path(\"artifacts/advanced_v2\")\n",
    "    PHASE2_ROOT.mkdir(parents=True, exist_ok=True)\n",
    "try:\n",
    "    MAX_CHALLENGERS\n",
    "except NameError:\n",
    "    MAX_CHALLENGERS = 3\n",
    "CANARY_LOG = Path(\"logs/canary_live.jsonl\")\n",
    "SHADOW_LOG = Path(\"logs/shadow_live.jsonl\")\n",
    "try:\n",
    "    GATES\n",
    "except NameError:\n",
    "    GATES = {\n",
    "        \"ndcg@10_min\": 0.62,\n",
    "        \"faithfulness_min\": 0.85,\n",
    "        \"p95_max\": 1800,\n",
    "        \"cost_max\": 0.018\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "id": "2b90a24f-8356-4a4d-8cbf-c4f2ce9f531f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _json_read(path: Path, default):\n",
    "    try:\n",
    "        return json.loads(path.read_text())\n",
    "    except Exception:\n",
    "        return default\n",
    "def _json_write(path: Path, obj: Any):\n",
    "    path.parent.mkdir(parents=True, exist_ok=True)\n",
    "    tmp = path.with_suffix(path.suffix + \".tmp\")\n",
    "    tmp.write_text(json.dumps(obj, indent=2))\n",
    "    os.replace(tmp, path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "id": "8e7f80c0-fe99-4eb6-88b7-cf7ac8899add",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _read_jsonl_window(path: Path, hours: int = 6, max_lines: int = 400_000) -> List[Dict[str, Any]]:\n",
    "    if not path.exists():\n",
    "        return []\n",
    "    cutoff = time.time() - hours * 3600\n",
    "    out = []\n",
    "    with open(path, \"r\", encoding=\"utf-8\") as f:\n",
    "        for i, line in enumerate(f):\n",
    "            if i >= max_lines:\n",
    "                break\n",
    "            try:\n",
    "                j = json.loads(line)\n",
    "                if j.get(\"ts\", 0) >= cutoff:\n",
    "                    out.append(j)\n",
    "            except Exception:\n",
    "                continue\n",
    "    return out\n",
    "def _passes_gates(metrics: Dict[str, Any], gates: Dict[str, float] = GATES) -> bool:\n",
    "    return (\n",
    "        float(metrics.get(\"ndcg@10\", 0)) >= gates[\"ndcg@10_min\"] and\n",
    "        float(metrics.get(\"faithfulness\", 0)) >= gates[\"faithfulness_min\"] and\n",
    "        float(metrics.get(\"p95\", 9e9)) <= gates[\"p95_max\"] and\n",
    "        float(metrics.get(\"cost\", 9e9)) <= gates[\"cost_max\"]\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "id": "08449057-85d8-48b6-998b-381e592019b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _job_summary(job_id: str) -> Optional[Dict[str, Any]]:\n",
    "    \"\"\"Read Phase-2/4 summary for job_id if present.\"\"\"\n",
    "    p = PHASE2_ROOT / job_id / \"summary.json\"\n",
    "    if p.exists():\n",
    "        try:\n",
    "            return json.loads(p.read_text())\n",
    "        except Exception:\n",
    "            return None\n",
    "    p4 = PHASE2_ROOT / job_id\n",
    "    if p4.exists() and (p4 / \"reranker_ce\").exists():\n",
    "        return {\"job_id\": job_id, \"artifacts\": {\"reranker_dir\": str(p4 / \"reranker_ce\")}}\n",
    "    return None\n",
    "def _rank_key(item: Dict[str, Any]):\n",
    "    \"\"\"Sort challengers by ndcg@10 desc, then faithfulness desc, then p95 asc, cost asc.\"\"\"\n",
    "    m = item.get(\"metrics\", {})\n",
    "    return (-float(m.get(\"ndcg@10\", 0)), -float(m.get(\"faithfulness\", 0)),\n",
    "            float(m.get(\"p95\", 1e9)), float(m.get(\"cost\", 1e9)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "id": "7cd7cbb9-b294-4955-ab61-e810e984885b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _load_registry() -> Dict[str, Any]:\n",
    "    reg = _json_read(REGISTRY_PATH, {})\n",
    "    reg.setdefault(\"champion\", None)\n",
    "    reg.setdefault(\"challengers\", [])\n",
    "    reg.setdefault(\"history\", [])  \n",
    "    return reg\n",
    "def _save_registry(reg: Dict[str, Any]):\n",
    "    reg[\"challengers\"] = sorted(reg.get(\"challengers\", []), key=_rank_key)[:MAX_CHALLENGERS]\n",
    "    _json_write(REGISTRY_PATH, reg)\n",
    "def _live_delta_mean(hours: int = 6) -> float:\n",
    "    \"\"\"Compute mean (candidate - prod) reward delta over window.\"\"\"\n",
    "    canary = _read_jsonl_window(CANARY_LOG, hours=hours)\n",
    "    deltas = [float(j[\"delta\"]) for j in canary if \"delta\" in j]\n",
    "    if not deltas:\n",
    "        shadow = _read_jsonl_window(Path(\"logs/shadow_live.jsonl\"), hours=hours)\n",
    "        deltas = [float(j[\"delta\"]) for j in shadow if \"delta\" in j]\n",
    "    return float(np.mean(deltas)) if deltas else 0.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "id": "3e145b94-1a7e-450c-b11b-bed8a1f8d991",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _add_or_update_challenger(job_id: str, metrics: Dict[str, Any], origin: str = \"phase2\"):\n",
    "    reg = _load_registry()\n",
    "    found = False\n",
    "    for c in reg[\"challengers\"]:\n",
    "        if c.get(\"job_id\") == job_id:\n",
    "            c[\"metrics\"] = metrics\n",
    "            c[\"origin\"] = c.get(\"origin\", origin)\n",
    "            c[\"ts\"] = int(time.time())\n",
    "            found = True\n",
    "            break\n",
    "    if not found:\n",
    "        reg[\"challengers\"].append({\"job_id\": job_id, \"metrics\": metrics, \"origin\": origin, \"ts\": int(time.time())})\n",
    "    _save_registry(reg)\n",
    "def _evict_challenger(job_id: str):\n",
    "    reg = _load_registry()\n",
    "    reg[\"challengers\"] = [c for c in reg[\"challengers\"] if c.get(\"job_id\") != job_id]\n",
    "    _save_registry(reg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "id": "eb3a00fb-12fc-49ff-a980-a6230ca1d4a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _promote(job_id: str, metrics: Optional[Dict[str, Any]] = None, reason: str = \"manual\"):\n",
    "    \"\"\"Promote job_id to champion; move previous champion to history; keep challengers tidy.\"\"\"\n",
    "    reg = _load_registry()\n",
    "    if metrics is None:\n",
    "        m = None\n",
    "        for c in reg[\"challengers\"]:\n",
    "            if c.get(\"job_id\") == job_id:\n",
    "                m = c.get(\"metrics\")\n",
    "                break\n",
    "        if m is None:\n",
    "            s = _job_summary(job_id)\n",
    "            m = (s or {}).get(\"metrics\", {})\n",
    "        metrics = m or {}\n",
    "    champ = {\"job_id\": job_id, \"metrics\": metrics, \"ts\": int(time.time()), \"reason\": reason}\n",
    "    if reg.get(\"champion\"):\n",
    "        reg[\"history\"] = (reg.get(\"history\") or []) + [reg[\"champion\"]]\n",
    "        reg[\"history\"] = sorted(reg[\"history\"], key=lambda h: h.get(\"ts\", 0), reverse=True)[:10]\n",
    "    reg[\"champion\"] = champ\n",
    "    reg[\"challengers\"] = [c for c in reg[\"challengers\"] if c.get(\"job_id\") != job_id]\n",
    "    _save_registry(reg)\n",
    "    try:\n",
    "        import requests\n",
    "        requests.post(\"http://127.0.0.1:8000/rl/update_from_logs\", timeout=2)\n",
    "    except Exception:\n",
    "        pass\n",
    "    return champ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "id": "aebdd798-5617-483e-8ec9-a85bcb762603",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _autopromote_if_worthy(min_delta: float = 0.0) -> Optional[Dict[str, Any]]:\n",
    "    \"\"\"\n",
    "    Auto-promote the best challenger if:\n",
    "      - it passes gates AND\n",
    "      - live mean delta over window >= min_delta AND\n",
    "      - it strictly improves over the current champion on ndcg@10 or equals ndcg but better faithfulness/p95/cost.\n",
    "    \"\"\"\n",
    "    reg = _load_registry()\n",
    "    chals = sorted(reg.get(\"challengers\", []), key=_rank_key)\n",
    "    if not chals:\n",
    "        return None\n",
    "    best = chals[0]\n",
    "    metrics = best.get(\"metrics\", {})\n",
    "    if not _passes_gates(metrics, GATES):\n",
    "        return None\n",
    "    delta = _live_delta_mean(hours=6)\n",
    "    if delta < min_delta:\n",
    "        return None\n",
    "    champ = reg.get(\"champion\")\n",
    "    if champ and champ.get(\"metrics\"):\n",
    "        cm = champ[\"metrics\"]\n",
    "        def cmp_tuple(m):\n",
    "            return (float(m.get(\"ndcg@10\", 0)), float(m.get(\"faithfulness\", 0)),\n",
    "                    -float(m.get(\"p95\", 1e9)), -float(m.get(\"cost\", 1e9)))\n",
    "        if cmp_tuple(metrics) <= cmp_tuple(cm):\n",
    "            return None\n",
    "    return _promote(best[\"job_id\"], metrics=metrics, reason=\"autopromote\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "id": "11a32d31-c0df-41ab-b9bf-00080abbc6e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "@app.get(\"/registry/challengers\")\n",
    "def registry_challengers():\n",
    "    reg = _load_registry()\n",
    "    reg[\"challengers\"] = sorted(reg.get(\"challengers\", []), key=_rank_key)[:MAX_CHALLENGERS]\n",
    "    return JSONResponse({\n",
    "        \"champion\": reg.get(\"champion\"),\n",
    "        \"challengers\": reg.get(\"challengers\"),\n",
    "        \"history\": reg.get(\"history\", []),\n",
    "        \"ts\": int(time.time())\n",
    "    })\n",
    "@app.post(\"/registry/promote\")\n",
    "def registry_promote_api(job_id: str):\n",
    "    art_dir = PHASE2_ROOT / job_id\n",
    "    if not art_dir.exists():\n",
    "        reg = _load_registry()\n",
    "        if not any(c.get(\"job_id\") == job_id for c in reg.get(\"challengers\", [])):\n",
    "            raise HTTPException(404, f\"Unknown job_id artifacts for {job_id}\")\n",
    "    champ = _promote(job_id, metrics=None, reason=\"manual\")\n",
    "    return JSONResponse({\"champion\": champ})\n",
    "@app.post(\"/registry/drop\")\n",
    "def registry_drop(job_id: str):\n",
    "    _evict_challenger(job_id)\n",
    "    return JSONResponse({\"dropped\": job_id})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "id": "84c45615-8cf1-4422-8171-7c071a84c0cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _registry_maintainer_loop():\n",
    "    while True:\n",
    "        try:\n",
    "            reg = _load_registry()\n",
    "            pruned = []\n",
    "            keep = []\n",
    "            for c in reg.get(\"challengers\", []):\n",
    "                jid = c.get(\"job_id\")\n",
    "                if not jid:\n",
    "                    continue\n",
    "                if not (PHASE2_ROOT / jid).exists():\n",
    "                    pruned.append(jid)\n",
    "                else:\n",
    "                    keep.append(c)\n",
    "            if pruned:\n",
    "                reg[\"challengers\"] = keep\n",
    "            reg[\"challengers\"] = sorted(reg.get(\"challengers\", []), key=_rank_key)[:MAX_CHALLENGERS]\n",
    "            promoted = _autopromote_if_worthy(min_delta=0.0)  \n",
    "            _save_registry(reg)\n",
    "            try:\n",
    "                jobs_set(\"REGISTRY_LOOP\", status=\"OK\", pruned=pruned, promoted=bool(promoted), ts=int(time.time()))\n",
    "            except Exception:\n",
    "                pass\n",
    "        except Exception as e:\n",
    "            try:\n",
    "                jobs_set(\"REGISTRY_LOOP\", status=\"ERROR\", error=str(e), ts=int(time.time()))\n",
    "            except Exception:\n",
    "                pass\n",
    "        time.sleep(600 + int(np.random.randint(0, 30)))  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "id": "77784df2-4adb-49f0-a716-3b66aa0a9c48",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Registry maintainer loop started (10 min cadence).\n",
      "Registry endpoints ready: /registry/challengers, /registry/promote, /registry/drop\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    REGISTRY_THREAD\n",
    "except NameError:\n",
    "    REGISTRY_THREAD = None\n",
    "if REGISTRY_THREAD is None or not REGISTRY_THREAD.is_alive():\n",
    "    REGISTRY_THREAD = threading.Thread(target=_registry_maintainer_loop, daemon=True)\n",
    "    REGISTRY_THREAD.start()\n",
    "    print(\"Registry maintainer loop started (10 min cadence).\")\n",
    "print(\"Registry endpoints ready: /registry/challengers, /registry/promote, /registry/drop\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "id": "7e84d00b-6034-4ff5-86f5-112c06d413cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"champion\": \"xgb_v0\",\n",
      "  \"challengers\": [],\n",
      "  \"history\": [],\n",
      "  \"ts\": 1762113362\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "import requests, json\n",
    "s = requests.get(\"http://127.0.0.1:8000/registry/challengers\").json()\n",
    "print(json.dumps(s, indent=2)[:800])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "id": "a7ccae97-e894-4138-afba-bf7733af400c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'dropped': '<JOB_ID>'}"
      ]
     },
     "execution_count": 179,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "requests.post(\"http://127.0.0.1:8000/registry/promote\", params={\"job_id\": \"<JOB_ID>\"}).json()\n",
    "requests.post(\"http://127.0.0.1:8000/registry/drop\", params={\"job_id\": \"<JOB_ID>\"}).json()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "id": "71db0daf-8271-42ac-a01d-0caa9fb2d8b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, json, time, math, random, threading\n",
    "from pathlib import Path\n",
    "from typing import Any, Dict, List, Optional, Tuple\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import concurrent.futures as cf\n",
    "import requests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "id": "8f2fbfd6-f7ae-495f-81ec-0de6d8714842",
   "metadata": {},
   "outputs": [],
   "source": [
    "BASE_URL = \"http://127.0.0.1:8000\"\n",
    "EVAL_ROOT = Path(\"artifacts/eval\"); EVAL_ROOT.mkdir(parents=True, exist_ok=True)\n",
    "DEFAULT_TOP_K = 5\n",
    "REQUEST_TIMEOUT = 10  \n",
    "RETRY = 2            \n",
    "RANDOM_SEED = 42\n",
    "random.seed(RANDOM_SEED); np.random.seed(RANDOM_SEED)\n",
    "GATES = {\n",
    "    \"ndcg@10_min\": 0.62,\n",
    "    \"faithfulness_min\": 0.85,\n",
    "    \"p95_max\": 1800,     \n",
    "    \"cost_max\": 0.018\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "id": "1f756d39-c8c5-4590-8212-ea0e709ed543",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Phase5 SAFE: patched _save_index  numpy_only (no FAISS).\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 182,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def _safe_patch_index_builder() -> bool:\n",
    "    \"\"\"Patch _save_index  numpy-only to avoid FAISS IVFPQ training if uploads happen.\"\"\"\n",
    "    try:\n",
    "        import numpy as _np, json as _json, os as _os\n",
    "        def _save_index_light(emb, out_dir, index_name=\"numpy_only\"):\n",
    "            _os.makedirs(out_dir, exist_ok=True)\n",
    "            npy_path = _os.path.join(out_dir, \"embeddings.npy\")\n",
    "            _np.save(npy_path, emb.astype(_np.float16))\n",
    "            meta = {\"index_type\":\"numpy\",\"path\":npy_path,\"dim\":int(emb.shape[1]),\"n\":int(emb.shape[0])}\n",
    "            _json.dump(meta, open(_os.path.join(out_dir, \"index_meta.json\"), \"w\"), indent=2)\n",
    "            return meta\n",
    "        if '_save_index' in globals():\n",
    "            globals()['_save_index'] = _save_index_light\n",
    "            print(\"Phase5 SAFE: patched _save_index  numpy_only (no FAISS).\")\n",
    "            return True\n",
    "    except Exception as e:\n",
    "        print(\"Phase5 SAFE: patch skipped:\", e)\n",
    "    return False\n",
    "_safe_patch_index_builder()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "id": "0b65a5d2-df45-457a-a8fc-b8721fc99a18",
   "metadata": {},
   "outputs": [],
   "source": [
    "SUITES: List[Tuple[str, List[str]]] = [\n",
    "    (\"factual\", [\n",
    "        \"What are common notes or patterns?\",\n",
    "        \"How many records mention chargeback risk?\",\n",
    "        \"Summarize key trends in notes.\"\n",
    "    ]),\n",
    "    (\"sql\", [\n",
    "        \"select age, income and balance where default = 1 limit 5\",\n",
    "        \"average income by tenure_months where default=0 limit 5\",\n",
    "        \"select * from dataset limit 5\"\n",
    "    ]),\n",
    "    (\"math\", [\n",
    "        \"2+2*10 - 3\",\n",
    "        \"compute 1000/25 + 17 - 3\"\n",
    "    ]),\n",
    "    (\"policy\", [\n",
    "        \"Do we comply with PII masking in logs?\",\n",
    "        \"What SQL safety rules apply?\"\n",
    "    ]),\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "id": "015b126c-c01d-4c5a-ab53-257d70ab8453",
   "metadata": {},
   "outputs": [],
   "source": [
    "def passes_gates(metrics: Dict[str, Any], gates: Dict[str, float]=GATES) -> bool:\n",
    "    return (\n",
    "        float(metrics.get(\"ndcg@10\", 0)) >= gates[\"ndcg@10_min\"] and\n",
    "        float(metrics.get(\"faithfulness\", 0)) >= gates[\"faithfulness_min\"] and\n",
    "        float(metrics.get(\"p95\", 9e9)) <= gates[\"p95_max\"] and\n",
    "        float(metrics.get(\"cost\", 9e9)) <= gates[\"cost_max\"]\n",
    "    )\n",
    "def maybe_promote(job_id: str, reason: str=\"phase5_eval\") -> Dict[str, Any]:\n",
    "    try:\n",
    "        r = requests.post(f\"{BASE_URL}/registry/promote\", params={\"job_id\": job_id}, timeout=REQUEST_TIMEOUT)\n",
    "        if r.ok:\n",
    "            _ = requests.post(f\"{BASE_URL}/rl/update_from_logs\", timeout=REQUEST_TIMEOUT)\n",
    "            return {\"status\":\"promoted\", \"job_id\": job_id, \"resp\": r.json()}\n",
    "        return {\"status\":\"no_promote\", \"job_id\": job_id, \"code\": r.status_code, \"text\": r.text[:200]}\n",
    "    except Exception as e:\n",
    "        return {\"status\":\"error\", \"job_id\": job_id, \"err\": str(e)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "id": "8e8437d4-082e-47d3-bf75-f39d8f6dad2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _latest_jobs(max_jobs=5) -> List[Tuple[str, str]]:\n",
    "    \"\"\"Return [(job_id, namespace), ...] by recency, distinct namespaces.\"\"\"\n",
    "    jobs_path = Path(\"artifacts/jobs.json\")\n",
    "    if not jobs_path.exists():\n",
    "        return []\n",
    "    try:\n",
    "        d = json.loads(jobs_path.read_text())\n",
    "    except Exception:\n",
    "        return []\n",
    "    rows = sorted(d.items(), key=lambda kv: kv[1].get(\"ts_updated\", 0), reverse=True)\n",
    "    out, seen_ns = [], set()\n",
    "    for jid, row in rows:\n",
    "        ns = row.get(\"namespace\", \"default\")\n",
    "        ds = row.get(\"dataset_path\")\n",
    "        if not ds or not Path(ds).exists():\n",
    "            continue\n",
    "        if ns in seen_ns:\n",
    "            continue\n",
    "        seen_ns.add(ns)\n",
    "        out.append((jid, ns))\n",
    "        if len(out) >= max_jobs:\n",
    "            break\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "id": "890ff1e9-d54a-48be-8a79-b8b8f1ca8a1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def call_query(job_id: str, namespace: str, q: str, top_k: int=DEFAULT_TOP_K) -> Dict[str, Any]:\n",
    "    body = {\"job_id\": job_id, \"namespace\": namespace, \"query_text\": q, \"top_k\": top_k}\n",
    "    last_err = None\n",
    "    for _ in range(RETRY+1):\n",
    "        try:\n",
    "            r = requests.post(f\"{BASE_URL}/query\", json=body, timeout=REQUEST_TIMEOUT)\n",
    "            r.raise_for_status()\n",
    "            return r.json()\n",
    "        except Exception as e:\n",
    "            last_err = e\n",
    "            time.sleep(0.2)\n",
    "    return {\"error\": f\"{last_err}\", \"job_id\": job_id, \"namespace\": namespace, \"query_text\": q}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "id": "9f035eca-db13-41ba-bb2b-052654594b4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _iter_eval_jobs(datasets: Optional[List[Path]], reuse_existing_job: bool) -> List[Tuple[str,str]]:\n",
    "    \"\"\"\n",
    "    If reuse_existing_job=True, use existing jobs; else tries to upload datasets (discouraged in Phase5).\n",
    "    \"\"\"\n",
    "    if reuse_existing_job:\n",
    "        jobs = _latest_jobs(max_jobs=5)\n",
    "        if not jobs and 'JOB' in globals():\n",
    "            jobs = [(globals()['JOB'], 'demo')]\n",
    "        if not jobs:\n",
    "            raise RuntimeError(\"No existing jobs found. Upload once earlier, then rerun.\")\n",
    "        return jobs\n",
    "    jobs: List[Tuple[str,str]] = []\n",
    "    if not datasets:\n",
    "        raise RuntimeError(\"No datasets provided and reuse_existing_job=False.\")\n",
    "    for p in datasets:\n",
    "        with open(p, \"rb\") as f:\n",
    "            resp = requests.post(f\"{BASE_URL}/upload\",\n",
    "                                 data={\"namespace\": p.stem[:40]},\n",
    "                                 files={\"file\": (p.name, f, \"text/csv\" if p.suffix.lower()==\".csv\" else \"application/octet-stream\")},\n",
    "                                 timeout=60)\n",
    "        resp.raise_for_status()\n",
    "        j = resp.json()\n",
    "        jobs.append((j[\"job_id\"], p.stem[:40]))\n",
    "    return jobs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "id": "0a2849a6-79a9-4fc0-95e8-424ff18daf77",
   "metadata": {},
   "outputs": [],
   "source": [
    "def batch_eval(\n",
    "    jobs: List[Tuple[str,str]],\n",
    "    suites: List[Tuple[str, List[str]]] = SUITES,\n",
    "    repeats: int = 1,\n",
    "    concurrency: int = 2,\n",
    "    top_k: int = DEFAULT_TOP_K\n",
    ") -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Returns a DataFrame with columns:\n",
    "      job_id, namespace, suite, query, request_id, action, model_id, route, retriever, reranker, slm, latency_ms, confidence, reward, error\n",
    "    \"\"\"\n",
    "    rows: List[Dict[str, Any]] = []\n",
    "    tasks: List[Tuple[str,str,str]] = []\n",
    "    for (suite, qs) in suites:\n",
    "        for _ in range(max(1, repeats)):\n",
    "            for q in qs:\n",
    "                for (job_id, ns) in jobs:\n",
    "                    tasks.append((job_id, ns, q))\n",
    "    random.shuffle(tasks)\n",
    "    def _one(job_id, ns, q):\n",
    "        out = call_query(job_id, ns, q, top_k=top_k)\n",
    "        if \"error\" in out:\n",
    "            return {\n",
    "                \"job_id\": job_id, \"namespace\": ns, \"suite\": _suite_of(q), \"query\": q,\n",
    "                \"error\": out[\"error\"]\n",
    "            }\n",
    "        meta = out.get(\"meta\", {})\n",
    "        faith = 0.90 if out.get(\"action\") == \"respond\" else 0.80\n",
    "        ndcg  = 0.66 if meta.get(\"retriever\") != \"bm25\" else 0.61\n",
    "        lat   = float(meta.get(\"latency_ms\", 1500))\n",
    "        cost  = 0.016 if meta.get(\"retriever\") != \"bm25\" else 0.012\n",
    "        a,b,g,d = 0.5, 0.4, 0.0005, 10.0\n",
    "        reward = a*faith + b*ndcg - g*lat - d*cost\n",
    "        return {\n",
    "            \"job_id\": job_id,\n",
    "            \"namespace\": ns,\n",
    "            \"suite\": _suite_of(q),\n",
    "            \"query\": q,\n",
    "            \"request_id\": out.get(\"request_id\"),\n",
    "            \"action\": out.get(\"action\"),\n",
    "            \"model_id\": out.get(\"model_id\"),\n",
    "            \"route\": meta.get(\"route\"),\n",
    "            \"retriever\": meta.get(\"retriever\"),\n",
    "            \"reranker\": meta.get(\"reranker\"),\n",
    "            \"slm\": meta.get(\"slm\"),\n",
    "            \"latency_ms\": meta.get(\"latency_ms\"),\n",
    "            \"confidence\": out.get(\"confidence\"),\n",
    "            \"reward\": float(reward),\n",
    "            \"error\": None\n",
    "        }\n",
    "    def _suite_of(q: str) -> str:\n",
    "        for s, qs in suites:\n",
    "            if q in qs: return s\n",
    "        return \"misc\"\n",
    "    max_workers = max(1, int(concurrency))\n",
    "    with cf.ThreadPoolExecutor(max_workers=max_workers) as ex:\n",
    "        futs = [ex.submit(_one, jid, ns, q) for (jid, ns, q) in tasks]\n",
    "        for fut in cf.as_completed(futs):\n",
    "            try:\n",
    "                rows.append(fut.result())\n",
    "            except Exception as e:\n",
    "                rows.append({\"error\": str(e)})\n",
    "    df = pd.DataFrame(rows)\n",
    "    cols = [\"job_id\",\"namespace\",\"suite\",\"query\",\"request_id\",\"action\",\"model_id\",\n",
    "            \"route\",\"retriever\",\"reranker\",\"slm\",\"latency_ms\",\"confidence\",\"reward\",\"error\"]\n",
    "    return df[[c for c in cols if c in df.columns]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "id": "1dc97e74-bb34-46cd-b181-7349fb1a58b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def summarize_eval(df: pd.DataFrame) -> Dict[str, Any]:\n",
    "    if df.empty:\n",
    "        return {\"overall\":{}, \"by_job\":{}, \"by_suite\":{}}\n",
    "    ok = df[df[\"error\"].isna()].copy()\n",
    "    def _agg(g):\n",
    "        return {\n",
    "            \"n\": int(g.shape[0]),\n",
    "            \"p95_ms\": float(np.percentile(g[\"latency_ms\"].dropna(), 95)) if \"latency_ms\" in g and g[\"latency_ms\"].notna().any() else None,\n",
    "            \"reward_mean\": float(g[\"reward\"].mean()) if \"reward\" in g else None,\n",
    "            \"respond_rate\": float((g[\"action\"]==\"respond\").mean()) if \"action\" in g else None,\n",
    "            \"sql_rate\": float((g[\"action\"]==\"run_sql\").mean()) if \"action\" in g else None,\n",
    "        }\n",
    "    overall = _agg(ok)\n",
    "    by_job = ok.groupby(\"job_id\").apply(_agg).to_dict()\n",
    "    by_suite = ok.groupby(\"suite\").apply(_agg).to_dict()\n",
    "    return {\"overall\": overall, \"by_job\": by_job, \"by_suite\": by_suite}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "id": "288b8acf-f038-429e-bc9b-62190eb7d1f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_phase5(\n",
    "    datasets: Optional[List[str]] = None,\n",
    "    repeats: int = 1,\n",
    "    concurrency: int = 2,\n",
    "    top_k: int = DEFAULT_TOP_K,\n",
    "    reuse_existing_job: bool = True,\n",
    "    auto_promote_best: bool = False\n",
    ") -> Dict[str, Any]:\n",
    "    \"\"\"\n",
    "    Full Phase-5 with optional uploads (discouraged). Safe by default (reuse jobs).\n",
    "    \"\"\"\n",
    "    if datasets:\n",
    "        datasets = [str(Path(p)) for p in datasets]\n",
    "    jobs = _iter_eval_jobs(\n",
    "        datasets=[Path(p) for p in (datasets or [])],\n",
    "        reuse_existing_job=reuse_existing_job\n",
    "    )\n",
    "    df = batch_eval(jobs=jobs, suites=SUITES, repeats=repeats, concurrency=concurrency, top_k=top_k)\n",
    "    summary = summarize_eval(df)\n",
    "    stamp = int(time.time())\n",
    "    out_dir = EVAL_ROOT / f\"phase5_{stamp}\"\n",
    "    out_dir.mkdir(parents=True, exist_ok=True)\n",
    "    df.to_csv(out_dir / \"detail.csv\", index=False)\n",
    "    (out_dir / \"summary.json\").write_text(json.dumps(summary, indent=2))\n",
    "    promotion = {}\n",
    "    if auto_promote_best and not df.empty:\n",
    "        means = df[df[\"error\"].isna()].groupby(\"job_id\")[\"reward\"].mean().sort_values(ascending=False)\n",
    "        if len(means):\n",
    "            best_job = str(means.index[0])\n",
    "            promotion = maybe_promote(best_job, reason=\"phase5_eval\")\n",
    "    return {\"out_dir\": str(out_dir), \"jobs\": jobs, \"summary\": summary, \"promotion\": promotion}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "id": "eac9d16e-9db9-4058-97ff-282073f9a575",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_phase5_existing_only(\n",
    "    repeats: int = 1,\n",
    "    concurrency: int = 2,\n",
    "    top_k: int = DEFAULT_TOP_K,\n",
    "    auto_promote_best: bool = False\n",
    ") -> Dict[str, Any]:\n",
    "    jobs = _latest_jobs(max_jobs=5)\n",
    "    if not jobs and 'JOB' in globals():\n",
    "        jobs = [(globals()['JOB'], 'demo')]\n",
    "    if not jobs:\n",
    "        raise RuntimeError(\"No existing jobs found. Upload once earlier, then rerun.\")\n",
    "    df = batch_eval(jobs=jobs, suites=SUITES, repeats=repeats, concurrency=concurrency, top_k=top_k)\n",
    "    summary = summarize_eval(df)\n",
    "    stamp = int(time.time())\n",
    "    out_dir = EVAL_ROOT / f\"phase5_safe_{stamp}\"\n",
    "    out_dir.mkdir(parents=True, exist_ok=True)\n",
    "    df.to_csv(out_dir / \"detail.csv\", index=False)\n",
    "    (out_dir / \"summary.json\").write_text(json.dumps(summary, indent=2))\n",
    "    promotion = {}\n",
    "    if auto_promote_best and not df.empty:\n",
    "        means = df[df[\"error\"].isna()].groupby(\"job_id\")[\"reward\"].mean().sort_values(ascending=False)\n",
    "        if len(means):\n",
    "            best_job = str(means.index[0])\n",
    "            promotion = maybe_promote(best_job, reason=\"phase5_safe_eval\")\n",
    "    return {\"out_dir\": str(out_dir), \"jobs\": jobs, \"summary\": summary, \"promotion\": promotion}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "id": "df35c5e3-0617-484f-a44d-3352210b851d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Artifacts dir: artifacts\\eval\\phase5_safe_1762113400\n",
      "{\n",
      "  \"n\": 10,\n",
      "  \"p95_ms\": 206.24999999999983,\n",
      "  \"reward_mean\": 0.45565,\n",
      "  \"respond_rate\": 0.0,\n",
      "  \"sql_rate\": 0.3\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "phase5 = run_phase5_existing_only(repeats=1, concurrency=2, top_k=5, auto_promote_best=False)\n",
    "print(\"Artifacts dir:\", phase5[\"out_dir\"])\n",
    "print(json.dumps(phase5[\"summary\"][\"overall\"], indent=2))\n",
    "phase5 = run_phase5(datasets=None, repeats=1, concurrency=2, top_k=5, reuse_existing_job=True, auto_promote_best=False)\n",
    "phase5 = run_phase5(datasets=[\"data/synthetic_credit.csv\"], repeats=1, concurrency=2, top_k=5, reuse_existing_job=False, auto_promote_best=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "id": "55e48abf-943c-4ff3-b11f-ab73e03f7830",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, json, time, math, random, threading\n",
    "from pathlib import Path\n",
    "from typing import Any, Dict, List, Optional, Tuple\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import requests\n",
    "import concurrent.futures as cf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "id": "e6729a8b-8420-4578-bbaf-131936ba381b",
   "metadata": {},
   "outputs": [],
   "source": [
    "BASE = \"http://127.0.0.1:8000\"\n",
    "RANDOM_SEED = 42\n",
    "DEFAULT_QPS = 10\n",
    "DEFAULT_DURATION_S = 60\n",
    "DEFAULT_TOP_K = 5\n",
    "REQUEST_TIMEOUT = 8\n",
    "random.seed(RANDOM_SEED); np.random.seed(RANDOM_SEED)\n",
    "SUITES: List[Tuple[str, List[str]]] = [\n",
    "    (\"factual\", [\n",
    "        \"What are common notes or patterns?\",\n",
    "        \"How many records mention chargeback risk?\",\n",
    "        \"Summarize key trends in notes.\"\n",
    "    ]),\n",
    "    (\"sql\", [\n",
    "        \"select age, income and balance where default = 1 limit 5\",\n",
    "        \"average income by tenure_months where default=0 limit 5\",\n",
    "        \"select * from dataset limit 5\"\n",
    "    ]),\n",
    "    (\"math\", [\n",
    "        \"2+2*10 - 3\",\n",
    "        \"compute 1000/25 + 17 - 3\"\n",
    "    ]),\n",
    "    (\"policy\", [\n",
    "        \"Do we comply with PII masking in logs?\",\n",
    "        \"What SQL safety rules apply?\"\n",
    "    ]),\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "id": "956642a6-3519-469c-9955-b3bd8338b40c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from prometheus_client import CollectorRegistry, Counter, Histogram, Gauge, start_http_server"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "id": "c05648a4-89a6-4283-81e5-384c2418f437",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Notebook Prometheus exporter at http://127.0.0.1:8001/metrics\n"
     ]
    }
   ],
   "source": [
    "_registry = CollectorRegistry()\n",
    "LT_REQS    = Counter(\"lt_requests_total\", \"Load-test requests\", [\"scenario\",\"status\"], registry=_registry)\n",
    "LT_ERRORS  = Counter(\"lt_errors_total\", \"Load-test errors\", [\"scenario\",\"type\"], registry=_registry)\n",
    "LT_LAT_MS  = Histogram(\"lt_latency_ms\", \"Latency (ms)\", [\"scenario\"], registry=_registry,\n",
    "                       buckets=[50,100,200,300,500,800,1200,1800,2500,5000,10000])\n",
    "LT_QPS     = Gauge(\"lt_qps\", \"Approx QPS (per scenario)\", [\"scenario\"], registry=_registry)\n",
    "LT_ACTIONS = Counter(\"lt_actions_total\", \"Action distribution\", [\"scenario\",\"action\"], registry=_registry)\n",
    "LT_RETR    = Counter(\"lt_retriever_total\", \"Retriever distribution\", [\"scenario\",\"retriever\"], registry=_registry)\n",
    "try:\n",
    "    start_http_server(8001, registry=_registry)\n",
    "    print(\"Notebook Prometheus exporter at http://127.0.0.1:8001/metrics\")\n",
    "except OSError:\n",
    "    print(\"Prometheus exporter already running on :8001\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "id": "09f09101-cfed-48ed-b5bf-1993dc5eda74",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _latest_job_for_ns(ns: Optional[str]=None) -> Optional[Tuple[str,str]]:\n",
    "    \"\"\"Find the newest job (optionally for a namespace). Returns (job_id, namespace).\"\"\"\n",
    "    jobs_path = Path(\"artifacts/jobs.json\")\n",
    "    if not jobs_path.exists():\n",
    "        return None\n",
    "    try:\n",
    "        d = json.loads(jobs_path.read_text())\n",
    "    except Exception:\n",
    "        return None\n",
    "    rows = sorted(d.items(), key=lambda kv: kv[1].get(\"ts_updated\", 0), reverse=True)\n",
    "    for jid, row in rows:\n",
    "        if ns and ns not in jid:\n",
    "            continue\n",
    "        ds = row.get(\"dataset_path\")\n",
    "        if ds and Path(ds).exists():\n",
    "            return (jid, row.get(\"namespace\",\"default\"))\n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "id": "b8d46211-27d7-4c62-9a24-0190f661a7e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _pick_queries(suites=SUITES) -> List[Tuple[str,str]]:\n",
    "    \"\"\"Flatten suites -> list of (suite_name, query) pairs.\"\"\"\n",
    "    out = []\n",
    "    for (suite, qs) in suites:\n",
    "        for q in qs:\n",
    "            out.append((suite, q))\n",
    "    return out\n",
    "def _call_query(job_id: str, namespace: str, query_text: str, top_k: int = DEFAULT_TOP_K) -> Dict[str, Any]:\n",
    "    body = {\"job_id\": job_id, \"namespace\": namespace, \"query_text\": query_text, \"top_k\": top_k}\n",
    "    r = requests.post(f\"{BASE}/query\", json=body, timeout=REQUEST_TIMEOUT)\n",
    "    r.raise_for_status()\n",
    "    return r.json()\n",
    "def _percentiles(a: List[float], ps=(50,95,99)) -> Dict[str, float]:\n",
    "    if not a: return {f\"p{p}\": None for p in ps}\n",
    "    arr = np.array(a, dtype=float)\n",
    "    return {f\"p{p}\": float(np.percentile(arr, p)) for p in ps}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "id": "20decc72-99fb-431e-a7a4-b4d67f698934",
   "metadata": {},
   "outputs": [],
   "source": [
    "def chaos_disable_ce_adapters(disable_ce: bool=True, disable_adapters: bool=True) -> Dict[str, Any]:\n",
    "    \"\"\"Use governor toggles (already in your server) to force cheaper path.\"\"\"\n",
    "    params = {}\n",
    "    if disable_ce is not None: params[\"enable_ce\"] = (not disable_ce)\n",
    "    if disable_adapters is not None: params[\"enable_adapters\"] = (not disable_adapters)\n",
    "    r = requests.post(f\"{BASE}/governor/mode\", params=params, timeout=5)\n",
    "    r.raise_for_status()\n",
    "    return r.json()\n",
    "def chaos_enable_ce_adapters() -> Dict[str, Any]:\n",
    "    return chaos_disable_ce_adapters(disable_ce=False, disable_adapters=False)\n",
    "def chaos_set_verifier(v2: bool=True) -> Dict[str, Any]:\n",
    "    r = requests.post(f\"{BASE}/verify/mode\", params={\"set_v2\": str(bool(v2)).lower()}, timeout=5)\n",
    "    r.raise_for_status()\n",
    "    return r.json()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "id": "53206f3e-ce56-430d-9118-e12aa2da40d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def chaos_remove_index_dir(job_id: str) -> Optional[Path]:\n",
    "    \"\"\"Temporarily remove FAISS/numpy index to force BM25 fallback.\"\"\"\n",
    "    adv_dir = Path(\"artifacts\") / \"advanced\" / job_id\n",
    "    idx_dir = adv_dir / \"index\"\n",
    "    if not idx_dir.exists(): \n",
    "        print(\"No index dir to remove (maybe already missing).\")\n",
    "        return None\n",
    "    tmp = adv_dir / \"_index_gone_phase6\"\n",
    "    if tmp.exists(): \n",
    "        import shutil; shutil.rmtree(tmp, ignore_errors=True)\n",
    "    idx_dir.rename(tmp)\n",
    "    print(\"Chaos: index directory removed.\")\n",
    "    return tmp\n",
    "def chaos_restore_index_dir(job_id: str, backup_dir: Optional[Path]):\n",
    "    if backup_dir is None: \n",
    "        return\n",
    "    adv_dir = Path(\"artifacts\") / \"advanced\" / job_id\n",
    "    target = adv_dir / \"index\"\n",
    "    if target.exists():\n",
    "        import shutil; shutil.rmtree(target, ignore_errors=True)\n",
    "    backup_dir.rename(target)\n",
    "    print(\"Chaos: index directory restored.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "id": "ea3e46f3-f59a-4d09-b1b7-c4304567e594",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_test(job_id: str, namespace: str, qps: int = DEFAULT_QPS, duration_s: int = DEFAULT_DURATION_S,\n",
    "              suites=SUITES, top_k: int = DEFAULT_TOP_K, scenario: str = \"baseline\") -> Dict[str, Any]:\n",
    "    \"\"\"\n",
    "    Fire queries at ~QPS for duration_s. Records Prom metrics locally & returns summary dict.\n",
    "    \"\"\"\n",
    "    pairs = _pick_queries(suites)\n",
    "    latencies = []\n",
    "    errors = 0\n",
    "    start = time.time()\n",
    "    deadline = start + duration_s\n",
    "    sent = 0\n",
    "    while time.time() < deadline:\n",
    "        tick_start = time.time()\n",
    "        with cf.ThreadPoolExecutor(max_workers=qps) as ex:\n",
    "            futs = []\n",
    "            for _ in range(qps):\n",
    "                (suite, q) = random.choice(pairs)\n",
    "                futs.append(ex.submit(_one_request, job_id, namespace, q, top_k, scenario, suite))\n",
    "            for fut in cf.as_completed(futs):\n",
    "                ok, ms = fut.result()\n",
    "                sent += 1\n",
    "                if ok:\n",
    "                    latencies.append(ms)\n",
    "                else:\n",
    "                    errors += 1\n",
    "        elapsed = time.time() - tick_start\n",
    "        sleep_left = max(0, 1.0 - elapsed)\n",
    "        time.sleep(sleep_left)\n",
    "    p = _percentiles(latencies, ps=(50,95,99))\n",
    "    summary = {\n",
    "        \"scenario\": scenario,\n",
    "        \"sent\": sent,\n",
    "        \"ok\": len(latencies),\n",
    "        \"errors\": errors,\n",
    "        \"p50_ms\": p[\"p50\"],\n",
    "        \"p95_ms\": p[\"p95\"],\n",
    "        \"p99_ms\": p[\"p99\"],\n",
    "        \"error_rate\": (errors / max(1, sent)),\n",
    "        \"qps_observed\": sent / max(0.001, (time.time() - start)),\n",
    "    }\n",
    "    LT_QPS.labels(scenario).set(summary[\"qps_observed\"])\n",
    "    return summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "id": "0bb3b15b-aa9d-41b3-a597-b360c08f83c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _one_request(job_id: str, namespace: str, q: str, top_k: int, scenario: str, suite: str):\n",
    "    t0 = time.time()\n",
    "    try:\n",
    "        out = _call_query(job_id, namespace, q, top_k=top_k)\n",
    "        ms = int((time.time() - t0) * 1000)\n",
    "        LT_REQS.labels(scenario,\"ok\").inc()\n",
    "        LT_LAT_MS.labels(scenario).observe(ms)\n",
    "        meta = out.get(\"meta\", {})\n",
    "        LT_ACTIONS.labels(scenario, out.get(\"action\",\"?\")).inc()\n",
    "        LT_RETR.labels(scenario, meta.get(\"retriever\",\"?\")).inc()\n",
    "        return True, ms\n",
    "    except Exception as e:\n",
    "        LT_REQS.labels(scenario,\"error\").inc()\n",
    "        LT_ERRORS.labels(scenario, type(e).__name__).inc()\n",
    "        return False, None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "id": "6008c1b5-2591-4b25-8256-b56b6032dbbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "GOLDEN_EXTRA = [\n",
    "    (\"sql\",    \"select default, avg(income) from dataset group by default limit 5\", \"run_sql\"),\n",
    "    (\"sql\",    \"select age, tenure_months from dataset where balance > 1000 limit 5\", \"run_sql\"),\n",
    "    (\"math\",   \"((12+8)/5) * 3 - 4\", \"respond\"),\n",
    "    (\"policy\", \"Outline log redaction and SQL guard rules\", \"respond\"),\n",
    "    (\"factual\",\"List frequent phrases about late fee or chargeback\", \"respond\"),\n",
    "]\n",
    "def run_golden(job_id: str, namespace: str, tests=GOLDEN_EXTRA) -> pd.DataFrame:\n",
    "    rows = []\n",
    "    for route, q, expect_action in tests:\n",
    "        try:\n",
    "            out = _call_query(job_id, namespace, q, top_k=DEFAULT_TOP_K)\n",
    "            got = out.get(\"action\")\n",
    "            ok = (got == expect_action)\n",
    "            rows.append({\"route\": route, \"query\": q, \"expect\": expect_action, \"got\": got, \"ok\": ok})\n",
    "        except Exception as e:\n",
    "            rows.append({\"route\": route, \"query\": q, \"expect\": expect_action, \"got\": str(e), \"ok\": False})\n",
    "    df = pd.DataFrame(rows)\n",
    "    print(\"Golden extra  pass rate:\", float(df[\"ok\"].mean() if len(df) else 0))\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "id": "e6b75b22-a08f-447e-ad78-1b2474c00baf",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using job: synthetic_credit_1762113406 namespace: default\n",
      "Warm-up\n",
      "Baseline load\n",
      "Baseline summary: {'scenario': 'baseline', 'sent': 300, 'ok': 300, 'errors': 0, 'p50_ms': 562.0, 'p95_ms': 1003.7500000000002, 'p99_ms': 1087.2999999999997, 'error_rate': 0.0, 'qps_observed': 9.663736482693581}\n",
      "Chaos A: governor OFF (CE/adapters)\n",
      "{'enable_ce': False, 'enable_adapters': False, 'p95_target_ms': 1800, 'cost_target': 0.018, 'window': 200, 'cooldown_sec': 600, 'obs_p95_ms': 119.0, 'obs_mean_cost': 0.016000000000000004}\n",
      "Cheap-path summary: {'scenario': 'no_ce_no_adapters', 'sent': 300, 'ok': 300, 'errors': 0, 'p50_ms': 545.0, 'p95_ms': 982.0, 'p99_ms': 1063.4499999999996, 'error_rate': 0.0, 'qps_observed': 9.794651130309322}\n"
     ]
    }
   ],
   "source": [
    "pair = _latest_job_for_ns() or (os.environ.get(\"JOB\") or \"\", \"demo\")\n",
    "if not pair[0]:\n",
    "    raise RuntimeError(\"No job found. Upload once (you already did earlier) and rerun this cell.\")\n",
    "JOB_ID, NS = pair\n",
    "print(\"Using job:\", JOB_ID, \"namespace:\", NS)\n",
    "print(\"Warm-up\")\n",
    "_ = load_test(JOB_ID, NS, qps=3, duration_s=5, scenario=\"warmup\")\n",
    "print(\"Baseline load\")\n",
    "baseline = load_test(JOB_ID, NS, qps=10, duration_s=30, scenario=\"baseline\")\n",
    "print(\"Baseline summary:\", baseline)\n",
    "print(\"Chaos A: governor OFF (CE/adapters)\")\n",
    "print(chaos_disable_ce_adapters(disable_ce=True, disable_adapters=True))\n",
    "cheap = load_test(JOB_ID, NS, qps=10, duration_s=30, scenario=\"no_ce_no_adapters\")\n",
    "print(\"Cheap-path summary:\", cheap)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "id": "3c70e92b-99cd-46bd-9aa0-b8045f693677",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chaos B: remove index dir (BM25 fallback)\n",
      "Chaos: index directory removed.\n",
      "BM25 fallback summary: {'scenario': 'bm25_fallback', 'sent': 140, 'ok': 140, 'errors': 0, 'p50_ms': 804.5, 'p95_ms': 1500.05, 'p99_ms': 1737.6699999999978, 'error_rate': 0.0, 'qps_observed': 6.63199813789845}\n",
      "Chaos: index directory restored.\n",
      "Chaos C: enable NLI verifier\n",
      "{'use_v2': True}\n",
      "Heavy-verifier summary: {'scenario': 'verifier_heavy', 'sent': 160, 'ok': 160, 'errors': 0, 'p50_ms': 459.5, 'p95_ms': 816.0, 'p99_ms': 844.3299999999999, 'error_rate': 0.0, 'qps_observed': 7.953089181568947}\n",
      "{'use_v2': False}\n",
      "Restore governor ON (CE/adapters)\n",
      "{'enable_ce': True, 'enable_adapters': True, 'p95_target_ms': 1800, 'cost_target': 0.018, 'window': 200, 'cooldown_sec': 600, 'obs_p95_ms': 146.0, 'obs_mean_cost': 0.015200000000000005}\n",
      "Golden extra  pass rate: 0.4\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>route</th>\n",
       "      <th>query</th>\n",
       "      <th>expect</th>\n",
       "      <th>got</th>\n",
       "      <th>ok</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>sql</td>\n",
       "      <td>select default, avg(income) from dataset group...</td>\n",
       "      <td>run_sql</td>\n",
       "      <td>run_sql</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>sql</td>\n",
       "      <td>select age, tenure_months from dataset where b...</td>\n",
       "      <td>run_sql</td>\n",
       "      <td>run_sql</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>math</td>\n",
       "      <td>((12+8)/5) * 3 - 4</td>\n",
       "      <td>respond</td>\n",
       "      <td>clarify</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>policy</td>\n",
       "      <td>Outline log redaction and SQL guard rules</td>\n",
       "      <td>respond</td>\n",
       "      <td>clarify</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>factual</td>\n",
       "      <td>List frequent phrases about late fee or charge...</td>\n",
       "      <td>respond</td>\n",
       "      <td>clarify</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     route                                              query   expect  \\\n",
       "0      sql  select default, avg(income) from dataset group...  run_sql   \n",
       "1      sql  select age, tenure_months from dataset where b...  run_sql   \n",
       "2     math                                 ((12+8)/5) * 3 - 4  respond   \n",
       "3   policy          Outline log redaction and SQL guard rules  respond   \n",
       "4  factual  List frequent phrases about late fee or charge...  respond   \n",
       "\n",
       "       got     ok  \n",
       "0  run_sql   True  \n",
       "1  run_sql   True  \n",
       "2  clarify  False  \n",
       "3  clarify  False  \n",
       "4  clarify  False  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "print(\"Chaos B: remove index dir (BM25 fallback)\")\n",
    "bak = chaos_remove_index_dir(JOB_ID)\n",
    "try:\n",
    "    bm25 = load_test(JOB_ID, NS, qps=10, duration_s=20, scenario=\"bm25_fallback\")\n",
    "    print(\"BM25 fallback summary:\", bm25)\n",
    "finally:\n",
    "    chaos_restore_index_dir(JOB_ID, bak)\n",
    "print(\"Chaos C: enable NLI verifier\")\n",
    "print(chaos_set_verifier(True))\n",
    "heavy = load_test(JOB_ID, NS, qps=8, duration_s=20, scenario=\"verifier_heavy\")\n",
    "print(\"Heavy-verifier summary:\", heavy)\n",
    "print(chaos_set_verifier(False))\n",
    "print(\"Restore governor ON (CE/adapters)\")\n",
    "print(chaos_enable_ce_adapters())\n",
    "golden_df = run_golden(JOB_ID, NS)\n",
    "display(golden_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "id": "7ad3822c-9c38-4bbd-be67-1204f19f8dc1",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Roll-up: {'p95_max': 1500.05, 'error_rate_max': 0.0, 'qps_avg': 8.510868733117576}\n",
      "NOTE: Notebook Prometheus metrics are live on :8001; app metrics remain on :8000/metrics\n"
     ]
    }
   ],
   "source": [
    "def _rollup(*runs):\n",
    "    ok = [r for r in runs if r]\n",
    "    if not ok: return {}\n",
    "    return {\n",
    "        \"p95_max\": float(np.max([r[\"p95_ms\"] for r in ok if r[\"p95_ms\"] is not None] or [0])),\n",
    "        \"error_rate_max\": float(np.max([r[\"error_rate\"] for r in ok] or [0])),\n",
    "        \"qps_avg\": float(np.mean([r[\"qps_observed\"] for r in ok] or [0]))\n",
    "    }\n",
    "rollup = _rollup(baseline, cheap, (bm25 if 'bm25' in locals() else None), heavy)\n",
    "print(\"Roll-up:\", rollup)\n",
    "print(\"NOTE: Notebook Prometheus metrics are live on :8001; app metrics remain on :8000/metrics\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "id": "ee496af9-3dd1-48da-8885-4fbb71028f8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import textwrap, os, json, time\n",
    "OPS = Path(\"ops\"); OPS.mkdir(parents=True, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "id": "945d8323-0bcc-4768-96b1-0cb5baaefb8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "files = {\n",
    "  \"Dockerfile\": r\"\"\"\n",
    "  FROM python:3.11-slim\n",
    "  WORKDIR /app\n",
    "  ENV PYTHONDONTWRITEBYTECODE=1 PYTHONUNBUFFERED=1\n",
    "  RUN apt-get update && apt-get install -y --no-install-recommends build-essential && rm -rf /var/lib/apt/lists/*\n",
    "  COPY ops/requirements.txt /tmp/requirements.txt\n",
    "  RUN pip install --no-cache-dir -r /tmp/requirements.txt\n",
    "  COPY . /app\n",
    "  EXPOSE 8000\n",
    "  CMD [\"python\",\"-u\",\"-c\",\"import uvicorn,app; uvicorn.run(app.app, host='0.0.0.0', port=8000)\"]\n",
    "  \"\"\",\n",
    "  \"requirements.txt\": r\"\"\"\n",
    "  fastapi==0.115.0\n",
    "  uvicorn==0.30.6\n",
    "  prometheus-client==0.20.0\n",
    "  numpy\n",
    "  pandas\n",
    "  scikit-learn\n",
    "  xgboost\n",
    "  lightgbm\n",
    "  sentence-transformers\n",
    "  transformers==4.44.2\n",
    "  accelerate\n",
    "  bitsandbytes\n",
    "  huggingface-hub\n",
    "  rank-bm25\n",
    "  datasketch\n",
    "  statsmodels\n",
    "  regex\n",
    "  sqlparse\n",
    "  lark-parser\n",
    "  sympy\n",
    "  gymnasium==0.29.1\n",
    "  stable-baselines3==2.3.2\n",
    "  python-dotenv\n",
    "  pyyaml\n",
    "  ujson\n",
    "  requests\n",
    "  \"\"\",\n",
    "  \"prometheus.yml\": r\"\"\"\n",
    "  global:\n",
    "    scrape_interval: 5s\n",
    "  scrape_configs:\n",
    "    - job_name: \"self-optimizing-ml-api\"\n",
    "      static_configs:\n",
    "        - targets: [\"host.docker.internal:8000\",\"127.0.0.1:8000\"]\n",
    "  \"\"\",\n",
    "  \"alerts.yml\": r\"\"\"\n",
    "  groups:\n",
    "    - name: latency-and-errors\n",
    "      rules:\n",
    "        - alert: HighLatencyP95\n",
    "          expr: histogram_quantile(0.95, sum(rate(api_latency_ms_bucket[5m])) by (le, route)) > 1.8\n",
    "          for: 3m\n",
    "          labels: { severity: \"page\" }\n",
    "          annotations: { summary: \"p95 > 1.8s on {{ $labels.route }}\" }\n",
    "        - alert: ErrorSpikes\n",
    "          expr: increase(canary_total_v1[5m]) > 0 and (increase(lt_errors_total[5m]) > 5)\n",
    "          for: 2m\n",
    "          labels: { severity: \"warn\" }\n",
    "          annotations: { summary: \"canary active & errors rising\" }\n",
    "  \"\"\",\n",
    "  \".env.example\": r\"\"\"\n",
    "  # copy to .env and customize when deploying\n",
    "  HF_HOME=/app/.cache/huggingface\n",
    "  TRANSFORMERS_CACHE=/app/.cache/huggingface\n",
    "  \"\"\",\n",
    "  \"README-OPS.md\": r\"\"\"\n",
    "  # Ops quickstart (tomorrow)\n",
    "  - You already generated these files in `ops/`.\n",
    "  - From a terminal with Docker: `cp ops/.env.example .env && docker compose -f ops/docker-compose.yml up --build`\n",
    "  - Prometheus will try `127.0.0.1:8000/metrics`.\n",
    "  \"\"\",\n",
    "  \"docker-compose.yml\": r\"\"\"\n",
    "  services:\n",
    "    api:\n",
    "      build:\n",
    "        context: .\n",
    "        dockerfile: ops/Dockerfile\n",
    "      env_file:\n",
    "        - .env\n",
    "      ports: [\"8000:8000\"]\n",
    "      volumes:\n",
    "        - ./:/app\n",
    "    prometheus:\n",
    "      image: prom/prometheus:v2.55.0\n",
    "      command:\n",
    "        - \"--config.file=/etc/prometheus/prometheus.yml\"\n",
    "      volumes:\n",
    "        - ./ops/prometheus.yml:/etc/prometheus/prometheus.yml:ro\n",
    "      ports: [\"9090:9090\"]\n",
    "  \"\"\"\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "id": "d988b415-7f88-4d06-a7a3-3ca2eb4e73fd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "wrote ops\\Dockerfile\n",
      "wrote ops\\requirements.txt\n",
      "wrote ops\\prometheus.yml\n",
      "wrote ops\\alerts.yml\n",
      "wrote ops\\.env.example\n",
      "wrote ops\\README-OPS.md\n",
      "wrote ops\\docker-compose.yml\n",
      "\n",
      "ops bundle ready in ./ops\n"
     ]
    }
   ],
   "source": [
    "for name, content in files.items():\n",
    "    p = OPS / name\n",
    "    p.write_text(textwrap.dedent(content).strip()+\"\\n\", encoding=\"utf-8\")\n",
    "    print(\"wrote\", p)\n",
    "print(\"\\nops bundle ready in ./ops\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "id": "2cd7726e-5e7a-4bed-b7a5-b08f02322915",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests, time, json, re\n",
    "BASE = \"http://127.0.0.1:8000\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "id": "d03bc9e9-6b30-4743-bd2f-a6685079e265",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ping_health():\n",
    "    r = requests.get(f\"{BASE}/healthz\", timeout=5)\n",
    "    r.raise_for_status()\n",
    "    return r.json()\n",
    "def grab_metrics():\n",
    "    r = requests.get(f\"{BASE}/metrics\", timeout=5)\n",
    "    r.raise_for_status()\n",
    "    return r.text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "id": "e272ff37-691a-4b5c-bb7a-a3c924d08565",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "health: {\n",
      "  \"status\": \"OK\",\n",
      "  \"cwd\": \"C:\\\\Users\\\\aniru\",\n",
      "  \"artifacts_dir\": \"C:\\\\Users\\\\aniru\\\\artifacts\",\n",
      "  \"jobs_count\": 27\n",
      "}\n",
      "sample metrics (top 15):\n",
      "  api_latency_ms_bucket               13560.0\n",
      "  api_latency_ms_count                1314.0\n",
      "  api_latency_ms_created              5286339294.178213\n",
      "  api_latency_ms_sum                  136.06619549999937\n",
      "  api_requests_created                5286339294.178213\n",
      "  api_requests_total                  1314.0\n",
      "  canary_latency_ms_v1_bucket         950.0\n",
      "  canary_latency_ms_v1_count          950.0\n",
      "  canary_latency_ms_v1_created        1762113399.3061578\n",
      "  canary_latency_ms_v1_sum            97165.0\n",
      "  canary_reward_delta_v1_bucket       0.0\n",
      "  canary_reward_delta_v1_count        0.0\n",
      "  canary_reward_delta_v1_created      1762113209.6169426\n",
      "  canary_reward_delta_v1_sum          0.0\n",
      "  canary_selected_v1_created          1762113209.6169426\n"
     ]
    }
   ],
   "source": [
    "def parse_prom(text, keep_prefixes=(\"api_\", \"shadow_\", \"canary_\", \"live_\", \"lt_\", \"governor_\", \"meta_\")):\n",
    "    out = {}\n",
    "    for line in text.splitlines():\n",
    "        if not line or line.startswith(\"#\"): \n",
    "            continue\n",
    "        name = line.split(\"{\",1)[0].split(\" \")[0]\n",
    "        if any(name.startswith(k) for k in keep_prefixes):\n",
    "            try:\n",
    "                val = float(line.rsplit(\" \",1)[-1])\n",
    "            except:\n",
    "                continue\n",
    "            out.setdefault(name, 0.0)\n",
    "            out[name] += val\n",
    "    return out\n",
    "print(\"health:\", json.dumps(ping_health(), indent=2))\n",
    "m = parse_prom(grab_metrics())\n",
    "print(\"sample metrics (top 15):\")\n",
    "for k in sorted(m)[:15]:\n",
    "    print(f\"  {k:35s} {m[k]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "id": "fe8ff2db-87a8-4941-9d68-5e357f37a3eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import concurrent.futures as cf\n",
    "import numpy as np, random, time, requests, json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "id": "aa04494b-97ff-4181-994e-6ea750dc085c",
   "metadata": {},
   "outputs": [],
   "source": [
    "BASE = \"http://127.0.0.1:8000\"\n",
    "SMOKE_QUERIES = [\n",
    "    \"What are common notes for customers with late fees?\",\n",
    "    \"select age, income and balance where default = 1 limit 5\",\n",
    "    \"2+2*10 - 3\",\n",
    "    \"Do we comply with PII masking in logs?\"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "id": "ec42e54c-3fb3-40cc-ac04-2df5164d6217",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "using job: synthetic_credit_1762113406 namespace: default\n",
      "smoke summary: {'sent': 12, 'ok': 12, 'errors': 0, 'p50_ms': np.float64(0.0), 'p95_ms': np.float64(0.0)}\n"
     ]
    }
   ],
   "source": [
    "def call_query(job_id, namespace, query_text, top_k=5, timeout=8):\n",
    "    body = {\"job_id\": job_id, \"namespace\": namespace, \"query_text\": query_text, \"top_k\": top_k}\n",
    "    r = requests.post(f\"{BASE}/query\", json=body, timeout=timeout)\n",
    "    r.raise_for_status()\n",
    "    return r.json()\n",
    "jobs = (Path(\"artifacts\")/\"jobs.json\")\n",
    "if not jobs.exists():\n",
    "    raise RuntimeError(\"No jobs.json found. Run earlier cells that uploaded a dataset.\")\n",
    "J = json.loads(jobs.read_text())\n",
    "job_id, row = sorted(J.items(), key=lambda kv: kv[1].get(\"ts_updated\",0), reverse=True)[0]\n",
    "ns = row.get(\"namespace\",\"default\")\n",
    "print(\"using job:\", job_id, \"namespace:\", ns)\n",
    "lat_ms = []\n",
    "ok = 0\n",
    "errors = 0\n",
    "with cf.ThreadPoolExecutor(max_workers=4) as ex:\n",
    "    futs = []\n",
    "    for i in range(12):\n",
    "        q = random.choice(SMOKE_QUERIES)\n",
    "        futs.append(ex.submit(lambda: call_query(job_id, ns, q)))\n",
    "    for fut in cf.as_completed(futs):\n",
    "        t0 = time.time()\n",
    "        try:\n",
    "            out = fut.result()\n",
    "            ms = int((time.time()-t0)*1000)\n",
    "            lat_ms.append(ms)\n",
    "            ok += 1\n",
    "        except Exception as e:\n",
    "            errors += 1\n",
    "summary = {\n",
    "    \"sent\": ok+errors,\n",
    "    \"ok\": ok,\n",
    "    \"errors\": errors,\n",
    "    \"p50_ms\": (np.percentile(lat_ms,50) if lat_ms else None),\n",
    "    \"p95_ms\": (np.percentile(lat_ms,95) if lat_ms else None),\n",
    "}\n",
    "print(\"smoke summary:\", summary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "id": "0987416c-e21a-480d-81fa-44aeb579d38f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd, requests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "id": "86cbe8f6-246d-4e60-aa4b-dafe7c5e9fae",
   "metadata": {},
   "outputs": [],
   "source": [
    "BASE = \"http://127.0.0.1:8000\"\n",
    "GOLDEN = [\n",
    "    (\"sql\",    \"select age, income and balance where default = 1 limit 5\", \"run_sql\"),\n",
    "    (\"factual\",\"What are common notes for customers with late fees?\", \"respond\"),\n",
    "    (\"math\",   \"2+2*10 - 3\", \"respond\"),\n",
    "    (\"policy\", \"Do we comply with PII masking in logs?\", \"respond\"),\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "id": "23dfd384-d9eb-4494-9777-b421805bc591",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "golden pass-rate: 0.25\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>route</th>\n",
       "      <th>query</th>\n",
       "      <th>expect</th>\n",
       "      <th>got</th>\n",
       "      <th>ok</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>sql</td>\n",
       "      <td>select age, income and balance where default =...</td>\n",
       "      <td>run_sql</td>\n",
       "      <td>run_sql</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>factual</td>\n",
       "      <td>What are common notes for customers with late ...</td>\n",
       "      <td>respond</td>\n",
       "      <td>clarify</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>math</td>\n",
       "      <td>2+2*10 - 3</td>\n",
       "      <td>respond</td>\n",
       "      <td>clarify</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>policy</td>\n",
       "      <td>Do we comply with PII masking in logs?</td>\n",
       "      <td>respond</td>\n",
       "      <td>clarify</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     route                                              query   expect  \\\n",
       "0      sql  select age, income and balance where default =...  run_sql   \n",
       "1  factual  What are common notes for customers with late ...  respond   \n",
       "2     math                                         2+2*10 - 3  respond   \n",
       "3   policy             Do we comply with PII masking in logs?  respond   \n",
       "\n",
       "       got     ok  \n",
       "0  run_sql   True  \n",
       "1  clarify  False  \n",
       "2  clarify  False  \n",
       "3  clarify  False  "
      ]
     },
     "execution_count": 218,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def latest_job():\n",
    "    J = json.loads(Path(\"artifacts/jobs.json\").read_text())\n",
    "    jid, row = sorted(J.items(), key=lambda kv: kv[1].get(\"ts_updated\",0), reverse=True)[0]\n",
    "    return jid, row.get(\"namespace\",\"default\")\n",
    "jid, ns = latest_job()\n",
    "rows = []\n",
    "for route, q, expect in GOLDEN:\n",
    "    out = requests.post(f\"{BASE}/query\", json={\"job_id\":jid,\"namespace\":ns,\"query_text\":q,\"top_k\":5}, timeout=8).json()\n",
    "    got = out.get(\"action\")\n",
    "    rows.append({\"route\":route, \"query\":q, \"expect\":expect, \"got\":got, \"ok\": got==expect})\n",
    "df = pd.DataFrame(rows)\n",
    "print(\"golden pass-rate:\", float(df[\"ok\"].mean()))\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "id": "da45a4a0-75e1-454d-ba59-1a1afda07b1a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "governor status  {'enable_ce': True, 'enable_adapters': True, 'p95_target_ms': 1800, 'cost_target': 0.018, 'window': 200, 'cooldown_sec': 600, 'obs_p95_ms': 128.0, 'obs_mean_cost': 0.015620000000000005}\n",
      "policy status    {'version': 614, 'weights': {'A': 0.5, 'B': 0.5}, 'ts_updated': 1762113326}\n",
      "meta status      {'cfg': {'window_hours': 6, 'temp': 0.2, 'step_cap': 0.1, 'max_cand': 0.25, 'min_prod': 0.5, 'cooldown_sec': 600}, 'policy': {'version': 614, 'weights': {'A': 0.5, 'B': 0.5}, 'ts_updated': 1762113326}}\n",
      "restored governor (CE/adapters on)  {'enable_ce': True, 'enable_adapters': True, 'p95_target_ms': 1800, 'cost_target': 0.018, 'window': 200, 'cooldown_sec': 600, 'obs_p95_ms': 128.0, 'obs_mean_cost': 0.015620000000000005}\n"
     ]
    }
   ],
   "source": [
    "import requests, json\n",
    "BASE = \"http://127.0.0.1:8000\"\n",
    "print(\"governor status \", requests.get(f\"{BASE}/governor/status\", timeout=5).json())\n",
    "print(\"policy status   \", requests.get(f\"{BASE}/policy/status\", timeout=5).json())\n",
    "print(\"meta status     \", {k:v for k,v in requests.get(f\"{BASE}/meta/status\", timeout=5).json().items() if k in (\"cfg\",\"policy\")})\n",
    "_ = requests.post(f\"{BASE}/governor/mode\", params={\"enable_ce\":True,\"enable_adapters\":True}, timeout=5).json()\n",
    "print(\"restored governor (CE/adapters on) \", _)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "id": "d7c285fc-cbde-41df-a598-955291830d53",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time, requests, re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "id": "3a80bef3-cbd4-4457-9a9a-3117c9c99de1",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tailing counters for ~10s\n",
      "api_requests_total{route=\"/upload\"} 3.0\n",
      "api_requests_total{route=\"/advanced\"} 20.0\n",
      "api_requests_total{route=\"/query\"} 1307.0\n",
      "shadow_requests_total_v1_total 966.0\n",
      "shadow_requests_total_v1_created 1.7621132081933153e+09\n",
      "canary_total_v1_total 994.0\n",
      "---\n",
      "api_requests_total{route=\"/upload\"} 3.0\n",
      "api_requests_total{route=\"/advanced\"} 20.0\n",
      "api_requests_total{route=\"/query\"} 1307.0\n",
      "shadow_requests_total_v1_total 966.0\n",
      "shadow_requests_total_v1_created 1.7621132081933153e+09\n",
      "canary_total_v1_total 994.0\n",
      "---\n",
      "api_requests_total{route=\"/upload\"} 3.0\n",
      "api_requests_total{route=\"/advanced\"} 20.0\n",
      "api_requests_total{route=\"/query\"} 1307.0\n",
      "shadow_requests_total_v1_total 966.0\n",
      "shadow_requests_total_v1_created 1.7621132081933153e+09\n",
      "canary_total_v1_total 994.0\n",
      "---\n",
      "api_requests_total{route=\"/upload\"} 3.0\n",
      "api_requests_total{route=\"/advanced\"} 20.0\n",
      "api_requests_total{route=\"/query\"} 1307.0\n",
      "shadow_requests_total_v1_total 966.0\n",
      "shadow_requests_total_v1_created 1.7621132081933153e+09\n",
      "canary_total_v1_total 994.0\n",
      "---\n",
      "api_requests_total{route=\"/upload\"} 3.0\n",
      "api_requests_total{route=\"/advanced\"} 20.0\n",
      "api_requests_total{route=\"/query\"} 1307.0\n",
      "shadow_requests_total_v1_total 966.0\n",
      "shadow_requests_total_v1_created 1.7621132081933153e+09\n",
      "canary_total_v1_total 994.0\n",
      "---\n"
     ]
    }
   ],
   "source": [
    "def scrape(name_prefixes=(\"api_requests_total\",\"canary_total_v1\",\"shadow_requests_total_v1\")):\n",
    "    txt = requests.get(\"http://127.0.0.1:8000/metrics\", timeout=5).text\n",
    "    want = [p for p in name_prefixes]\n",
    "    lines = [ln for ln in txt.splitlines() if ln and not ln.startswith(\"#\") and any(ln.startswith(w) for w in want)]\n",
    "    return lines\n",
    "print(\"tailing counters for ~10s\")\n",
    "t0 = time.time()\n",
    "while time.time() - t0 < 10:\n",
    "    ls = scrape()\n",
    "    print(\"\\n\".join(ls[:6]))\n",
    "    time.sleep(2)\n",
    "    print(\"---\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "id": "23d6ae36-04c1-47e1-b716-48731ab7339b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Health: 200\n",
      "{\n",
      "  \"status\": \"OK\",\n",
      "  \"cwd\": \"C:\\\\Users\\\\aniru\",\n",
      "  \"artifacts_dir\": \"C:\\\\Users\\\\aniru\\\\artifacts\",\n",
      "  \"jobs_count\": 27\n",
      "}\n",
      "\n",
      "--- /metrics (first 60 lines) ---\n",
      "# HELP python_gc_objects_collected_total Objects collected during gc\n",
      "# TYPE python_gc_objects_collected_total counter\n",
      "python_gc_objects_collected_total{generation=\"0\"} 60379.0\n",
      "python_gc_objects_collected_total{generation=\"1\"} 16745.0\n",
      "python_gc_objects_collected_total{generation=\"2\"} 39556.0\n",
      "# HELP python_gc_objects_uncollectable_total Uncollectable objects found during GC\n",
      "# TYPE python_gc_objects_uncollectable_total counter\n",
      "python_gc_objects_uncollectable_total{generation=\"0\"} 0.0\n",
      "python_gc_objects_uncollectable_total{generation=\"1\"} 0.0\n",
      "python_gc_objects_uncollectable_total{generation=\"2\"} 0.0\n",
      "# HELP python_gc_collections_total Number of times this generation was collected\n",
      "# TYPE python_gc_collections_total counter\n",
      "python_gc_collections_total{generation=\"0\"} 6704.0\n",
      "python_gc_collections_total{generation=\"1\"} 609.0\n",
      "python_gc_collections_total{generation=\"2\"} 21.0\n",
      "# HELP python_info Python platform information\n",
      "# TYPE python_info gauge\n",
      "python_info{implementation=\"CPython\",major=\"3\",minor=\"12\",patchlevel=\"7\",version=\"3.12.7\"} 1.0\n",
      "# HELP api_requests_total Requests\n",
      "# TYPE api_requests_total counter\n",
      "api_requests_total{route=\"/upload\"} 3.0\n",
      "api_requests_total{route=\"/advanced\"} 20.0\n",
      "api_requests_total{route=\"/query\"} 1307.0\n",
      "# HELP api_requests_created Requests\n",
      "# TYPE api_requests_created gauge\n",
      "api_requests_created{route=\"/upload\"} 1.7621130914414222e+09\n",
      "api_requests_created{route=\"/advanced\"} 1.7621130983207455e+09\n",
      "api_requests_created{route=\"/query\"} 1.762113104416045e+09\n",
      "# HELP api_latency_ms Latency (ms)\n",
      "# TYPE api_latency_ms histogram\n",
      "api_latency_ms_bucket{le=\"0.005\",route=\"/upload\"} 0.0\n",
      "api_latency_ms_bucket{le=\"0.01\",route=\"/upload\"} 0.0\n",
      "api_latency_ms_bucket{le=\"0.025\",route=\"/upload\"} 0.0\n",
      "api_latency_ms_bucket{le=\"0.05\",route=\"/upload\"} 0.0\n",
      "api_latency_ms_bucket{le=\"0.075\",route=\"/upload\"} 0.0\n",
      "api_latency_ms_bucket{le=\"0.1\",route=\"/upload\"} 0.0\n",
      "api_latency_ms_bucket{le=\"0.25\",route=\"/upload\"} 0.0\n",
      "api_latency_ms_bucket{le=\"0.5\",route=\"/upload\"} 0.0\n",
      "api_latency_ms_bucket{le=\"0.75\",route=\"/upload\"} 0.0\n",
      "api_latency_ms_bucket{le=\"1.0\",route=\"/upload\"} 0.0\n",
      "api_latency_ms_bucket{le=\"2.5\",route=\"/upload\"} 0.0\n",
      "api_latency_ms_bucket{le=\"5.0\",route=\"/upload\"} 0.0\n",
      "api_latency_ms_bucket{le=\"7.5\",route=\"/upload\"} 3.0\n",
      "api_latency_ms_bucket{le=\"10.0\",route=\"/upload\"} 3.0\n",
      "api_latency_ms_bucket{le=\"+Inf\",route=\"/upload\"} 3.0\n",
      "api_latency_ms_count{route=\"/upload\"} 3.0\n",
      "api_latency_ms_sum{route=\"/upload\"} 18.844511999999895\n",
      "api_latency_ms_bucket{le=\"0.005\",route=\"/advanced\"} 14.0\n",
      "api_latency_ms_bucket{le=\"0.01\",route=\"/advanced\"} 18.0\n",
      "api_latency_ms_bucket{le=\"0.025\",route=\"/advanced\"} 20.0\n",
      "api_latency_ms_bucket{le=\"0.05\",route=\"/advanced\"} 20.0\n",
      "api_latency_ms_bucket{le=\"0.075\",route=\"/advanced\"} 20.0\n",
      "api_latency_ms_bucket{le=\"0.1\",route=\"/advanced\"} 20.0\n",
      "api_latency_ms_bucket{le=\"0.25\",route=\"/advanced\"} 20.0\n",
      "api_latency_ms_bucket{le=\"0.5\",route=\"/advanced\"} 20.0\n",
      "api_latency_ms_bucket{le=\"0.75\",route=\"/advanced\"} 20.0\n",
      "api_latency_ms_bucket{le=\"1.0\",route=\"/advanced\"} 20.0\n",
      "api_latency_ms_bucket{le=\"2.5\",route=\"/advanced\"} 20.0\n",
      "api_latency_ms_bucket{le=\"5.0\",route=\"/advanced\"} 20.0\n",
      "api_latency_ms_bucket{le=\"7.5\",route=\"/advanced\"} 20.0\n",
      "... (truncated) ...\n"
     ]
    }
   ],
   "source": [
    "import requests, json, textwrap\n",
    "BASE = \"http://127.0.0.1:8000\"\n",
    "h = requests.get(f\"{BASE}/healthz\", timeout=5)\n",
    "print(\"Health:\", h.status_code)\n",
    "print(json.dumps(h.json(), indent=2))\n",
    "m = requests.get(f\"{BASE}/metrics\", timeout=5).text.splitlines()\n",
    "print(\"\\n--- /metrics (first 60 lines) ---\")\n",
    "print(\"\\n\".join(m[:60]))\n",
    "print(\"... (truncated) ...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "id": "6ea618c8-8e6f-4fd8-99b5-a88554423f90",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time, numpy as np, requests, json\n",
    "BASE = \"http://127.0.0.1:8000\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "id": "f16fc65c-6a56-4156-b319-6c72b687afab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using job: synthetic_credit_1762113406 namespace: default\n",
      "- What are common notes or patterns?                action=clarify  retr=numpy  318ms\n",
      "- select age, income and balance where default = 1  action=run_sql  retr=numpy  192ms\n",
      "- 2+2*10 - 3                                        action=clarify  retr=numpy  146ms\n",
      "- Do we comply with PII masking in logs?            action=clarify  retr=numpy  165ms\n",
      "\n",
      "Latency summary (client-side): p50=178ms  p95=299ms  p99=314ms\n"
     ]
    }
   ],
   "source": [
    "def _latest_job_for_ns(ns=None):\n",
    "    import json\n",
    "    from pathlib import Path\n",
    "    p = Path(\"artifacts/jobs.json\")\n",
    "    if not p.exists():\n",
    "        return None\n",
    "    d = json.loads(p.read_text())\n",
    "    rows = sorted(d.items(), key=lambda kv: kv[1].get(\"ts_updated\", 0), reverse=True)\n",
    "    for jid, row in rows:\n",
    "        if ns and ns not in jid:\n",
    "            continue\n",
    "        ds = row.get(\"dataset_path\")\n",
    "        if ds:\n",
    "            return jid, row.get(\"namespace\",\"default\")\n",
    "    return None\n",
    "pair = _latest_job_for_ns() or (globals().get(\"JOB\"), \"demo\")\n",
    "if not pair[0]:\n",
    "    raise RuntimeError(\"No job found. Upload once earlier, then rerun this cell.\")\n",
    "JOB_ID, NS = pair\n",
    "print(\"Using job:\", JOB_ID, \"namespace:\", NS)\n",
    "QUERIES = [\n",
    "    \"What are common notes or patterns?\",\n",
    "    \"select age, income and balance where default = 1 limit 5\",\n",
    "    \"2+2*10 - 3\",\n",
    "    \"Do we comply with PII masking in logs?\"\n",
    "]\n",
    "lats = []\n",
    "outs = []\n",
    "for q in QUERIES:\n",
    "    t0 = time.time()\n",
    "    r = requests.post(f\"{BASE}/query\", json={\"job_id\": JOB_ID, \"namespace\": NS, \"query_text\": q, \"top_k\": 5}, timeout=8)\n",
    "    dt_ms = int((time.time()-t0)*1000)\n",
    "    lats.append(dt_ms)\n",
    "    outs.append(r.json())\n",
    "    meta = outs[-1].get(\"meta\", {})\n",
    "    print(f\"- {q[:48]:<48}  action={outs[-1].get('action')}  retr={meta.get('retriever')}  {dt_ms}ms\")\n",
    "if lats:\n",
    "    arr = np.array(lats)\n",
    "    print(\"\\nLatency summary (client-side):\",\n",
    "          f\"p50={np.percentile(arr,50):.0f}ms  p95={np.percentile(arr,95):.0f}ms  p99={np.percentile(arr,99):.0f}ms\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 225,
   "id": "b74553e2-55f5-430f-b1a9-d29d8f9c0a43",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Golden pass rate: 0.25\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>route</th>\n",
       "      <th>query</th>\n",
       "      <th>expect</th>\n",
       "      <th>got</th>\n",
       "      <th>ok</th>\n",
       "      <th>lat_ms</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>sql</td>\n",
       "      <td>select age, income and balance where default =...</td>\n",
       "      <td>run_sql</td>\n",
       "      <td>run_sql</td>\n",
       "      <td>True</td>\n",
       "      <td>109</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>factual</td>\n",
       "      <td>What are common notes for customers with late ...</td>\n",
       "      <td>respond</td>\n",
       "      <td>clarify</td>\n",
       "      <td>False</td>\n",
       "      <td>105</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>math</td>\n",
       "      <td>((12+8)/5) * 3 - 4</td>\n",
       "      <td>respond</td>\n",
       "      <td>clarify</td>\n",
       "      <td>False</td>\n",
       "      <td>107</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>policy</td>\n",
       "      <td>Outline log redaction and SQL guard rules</td>\n",
       "      <td>respond</td>\n",
       "      <td>clarify</td>\n",
       "      <td>False</td>\n",
       "      <td>108</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     route                                              query   expect  \\\n",
       "0      sql  select age, income and balance where default =...  run_sql   \n",
       "1  factual  What are common notes for customers with late ...  respond   \n",
       "2     math                                 ((12+8)/5) * 3 - 4  respond   \n",
       "3   policy          Outline log redaction and SQL guard rules  respond   \n",
       "\n",
       "       got     ok  lat_ms  \n",
       "0  run_sql   True     109  \n",
       "1  clarify  False     105  \n",
       "2  clarify  False     107  \n",
       "3  clarify  False     108  "
      ]
     },
     "execution_count": 225,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd, requests\n",
    "BASE = \"http://127.0.0.1:8000\"\n",
    "tests = [\n",
    "    (\"sql\",    \"select age, income and balance where default = 1 limit 5\", \"run_sql\"),\n",
    "    (\"factual\",\"What are common notes for customers with late fees?\",       \"respond\"),\n",
    "    (\"math\",   \"((12+8)/5) * 3 - 4\",                                        \"respond\"),\n",
    "    (\"policy\", \"Outline log redaction and SQL guard rules\",                 \"respond\"),\n",
    "]\n",
    "rows = []\n",
    "for route, q, expect in tests:\n",
    "    out = requests.post(f\"{BASE}/query\", json={\"job_id\": JOB_ID, \"namespace\": NS, \"query_text\": q, \"top_k\": 5}, timeout=8).json()\n",
    "    got = out.get(\"action\")\n",
    "    ok = (got == expect)\n",
    "    rows.append({\"route\": route, \"query\": q, \"expect\": expect, \"got\": got, \"ok\": ok, \"lat_ms\": out.get(\"meta\",{}).get(\"latency_ms\")})\n",
    "df_golden = pd.DataFrame(rows)\n",
    "print(\"Golden pass rate:\", float(df_golden[\"ok\"].mean()))\n",
    "df_golden"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "id": "645327fc-5264-4767-b538-6678e86eae50",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time, numpy as np, pandas as pd, requests, random\n",
    "BASE = \"http://127.0.0.1:8000\"\n",
    "random.seed(42)\n",
    "SUITES = [\n",
    "    (\"factual\", [\n",
    "        \"What are common notes or patterns?\",\n",
    "        \"How many records mention chargeback risk?\",\n",
    "        \"Summarize key trends in notes.\"\n",
    "    ]),\n",
    "    (\"sql\", [\n",
    "        \"select age, income and balance where default = 1 limit 5\",\n",
    "        \"average income by tenure_months where default=0 limit 5\",\n",
    "        \"select * from dataset limit 5\"\n",
    "    ]),\n",
    "    (\"math\", [\n",
    "        \"2+2*10 - 3\",\n",
    "        \"compute 1000/25 + 17 - 3\"\n",
    "    ]),\n",
    "    (\"policy\", [\n",
    "        \"Do we comply with PII masking in logs?\",\n",
    "        \"What SQL safety rules apply?\"\n",
    "    ]),\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "id": "658d1a5c-5b8f-4c13-b248-dfd04b5dfb8d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overall p95(ms): 202.84999999999997\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>n</th>\n",
       "      <th>p95_ms</th>\n",
       "      <th>reward_mean</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>suite</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>factual</th>\n",
       "      <td>3</td>\n",
       "      <td>206.90</td>\n",
       "      <td>0.415000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>math</th>\n",
       "      <td>2</td>\n",
       "      <td>107.45</td>\n",
       "      <td>0.447250</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>policy</th>\n",
       "      <td>2</td>\n",
       "      <td>94.80</td>\n",
       "      <td>0.449500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>sql</th>\n",
       "      <td>3</td>\n",
       "      <td>181.30</td>\n",
       "      <td>0.422333</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         n  p95_ms  reward_mean\n",
       "suite                          \n",
       "factual  3  206.90     0.415000\n",
       "math     2  107.45     0.447250\n",
       "policy   2   94.80     0.449500\n",
       "sql      3  181.30     0.422333"
      ]
     },
     "execution_count": 227,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def _pick_pairs():\n",
    "    pairs=[]\n",
    "    for suite, qs in SUITES:\n",
    "        for q in qs:\n",
    "            pairs.append((suite,q))\n",
    "    return pairs\n",
    "pairs = _pick_pairs()\n",
    "rows=[]\n",
    "for suite, q in pairs:\n",
    "    t0 = time.time()\n",
    "    out = requests.post(f\"{BASE}/query\", json={\"job_id\": JOB_ID, \"namespace\": NS, \"query_text\": q, \"top_k\": 5}, timeout=8).json()\n",
    "    dt = int((time.time()-t0)*1000)\n",
    "    meta = out.get(\"meta\", {})\n",
    "    faith = 0.90 if out.get(\"action\")==\"respond\" else 0.80\n",
    "    ndcg  = 0.66 if meta.get(\"retriever\")!=\"bm25\" else 0.61\n",
    "    cost  = 0.016 if meta.get(\"retriever\")!=\"bm25\" else 0.012\n",
    "    reward = 0.5*faith + 0.4*ndcg - 0.0005*dt - 10.0*cost\n",
    "    rows.append({\n",
    "        \"suite\": suite, \"query\": q, \"action\": out.get(\"action\"),\n",
    "        \"retriever\": meta.get(\"retriever\"), \"reranker\": meta.get(\"reranker\"), \"slm\": meta.get(\"slm\"),\n",
    "        \"lat_ms\": meta.get(\"latency_ms\"), \"reward\": reward\n",
    "    })\n",
    "eval_df = pd.DataFrame(rows)\n",
    "summary = eval_df.groupby(\"suite\").agg(n=(\"suite\",\"size\"),\n",
    "                                       p95_ms=(\"lat_ms\", lambda s: float(np.percentile(s.dropna(),95)) if s.notna().any() else None),\n",
    "                                       reward_mean=(\"reward\",\"mean\"))\n",
    "print(\"Overall p95(ms):\", float(np.percentile(eval_df[\"lat_ms\"].dropna(),95)) if eval_df[\"lat_ms\"].notna().any() else None)\n",
    "summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 228,
   "id": "2214172b-f3d2-4ff2-87b6-43b895774bc1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Governor status (before): {'enable_ce': True, 'enable_adapters': True, 'p95_target_ms': 1800, 'cost_target': 0.018, 'window': 200, 'cooldown_sec': 600, 'obs_p95_ms': 134.34999999999988, 'obs_mean_cost': 0.015980000000000005}\n",
      "Enable heavy verifier: {'use_v2': True}\n",
      "Probe lat (ms): 95\n",
      "Disable heavy verifier: {'use_v2': False}\n",
      "Governor: CE+adapters ON: {'enable_ce': True, 'enable_adapters': True, 'p95_target_ms': 1800, 'cost_target': 0.018, 'window': 200, 'cooldown_sec': 600, 'obs_p95_ms': 128.2999999999999, 'obs_mean_cost': 0.016000000000000004}\n"
     ]
    }
   ],
   "source": [
    "import requests, time\n",
    "BASE = \"http://127.0.0.1:8000\"\n",
    "print(\"Governor status (before):\", requests.get(f\"{BASE}/governor/status\", timeout=5).json())\n",
    "print(\"Enable heavy verifier:\", requests.post(f\"{BASE}/verify/mode\", params={\"set_v2\": \"true\"}, timeout=5).json())\n",
    "probe = requests.post(f\"{BASE}/query\", json={\"job_id\": JOB_ID, \"namespace\": NS, \"query_text\": \"Summarize key trends in notes.\", \"top_k\": 5}, timeout=8).json()\n",
    "print(\"Probe lat (ms):\", probe.get(\"meta\",{}).get(\"latency_ms\"))\n",
    "print(\"Disable heavy verifier:\", requests.post(f\"{BASE}/verify/mode\", params={\"set_v2\": \"false\"}, timeout=5).json())\n",
    "print(\"Governor: CE+adapters ON:\", requests.post(f\"{BASE}/governor/mode\", params={\"enable_ce\": True, \"enable_adapters\": True}, timeout=5).json())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 229,
   "id": "d1dc8005-8f25-4fe9-8e0a-ea4d42fc8900",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import os, json, time, hashlib, sys, platform, subprocess, textwrap, requests\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 230,
   "id": "d6d383a6-2e7e-4134-87bb-fc11bc719382",
   "metadata": {},
   "outputs": [],
   "source": [
    "BASE = \"http://127.0.0.1:8000\"\n",
    "STAMP = str(int(time.time()))\n",
    "REL_ROOT = Path(f\"artifacts/release_{STAMP}\")\n",
    "CFG_DIR = REL_ROOT / \"config\"; CFG_DIR.mkdir(parents=True, exist_ok=True)\n",
    "VAL_DIR = REL_ROOT / \"validation\"; VAL_DIR.mkdir(parents=True, exist_ok=True)\n",
    "DOCS_DIR = Path(\"ops\"); DOCS_DIR.mkdir(parents=True, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 231,
   "id": "0a0ab52d-f7e9-44a2-8dbb-fb9568175f92",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _safe_get(url, **kw):\n",
    "    try:\n",
    "        r = requests.get(url, timeout=kw.get(\"timeout\", 8))\n",
    "        r.raise_for_status()\n",
    "        return r.json() if (\"json\" in url or url.endswith((\"/status\",\"/mode\"))) else r.text\n",
    "    except Exception as e:\n",
    "        return {\"error\": str(e)}\n",
    "def _latest_job():\n",
    "    p = Path(\"artifacts/jobs.json\")\n",
    "    if not p.exists(): return None, None\n",
    "    d = json.loads(p.read_text())\n",
    "    jid, row = sorted(d.items(), key=lambda kv: kv[1].get(\"ts_updated\",0), reverse=True)[0]\n",
    "    return jid, row.get(\"namespace\",\"default\")\n",
    "def _hash_file(p: Path):\n",
    "    h = hashlib.sha256()\n",
    "    with open(p, \"rb\") as f:\n",
    "        for chunk in iter(lambda: f.read(1<<20), b\"\"): h.update(chunk)\n",
    "    return h.hexdigest()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 232,
   "id": "7163c2a2-d69c-41a4-a0e4-70186e5209cf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1162"
      ]
     },
     "execution_count": 232,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "api_md = f\"\"\"# API Quick Reference (v0.2)\n",
    "Base URL: `{BASE}`\n",
    "## Health & Metrics\n",
    "- `GET /healthz`  status, cwd, artifacts path\n",
    "- `GET /metrics`  Prometheus metrics\n",
    "## Data & Jobs\n",
    "- `POST /upload` (multipart: file, namespace, target?)  `{{job_id, status, profile_uri, rows_used}}`\n",
    "- `GET /advanced?job_id=...`  advanced status/artifacts\n",
    "## Query\n",
    "- `POST /query` JSON: `{{job_id, namespace, query_text, top_k}}`\n",
    "  - Response: `{{request_id, action, payload, citations, model_id, confidence, meta}}`\n",
    "  - `action`  `respond | run_sql | clarify`\n",
    "## Feedback (HITL)\n",
    "- `POST /feedback` JSON: `{{job_id, namespace, request_id, verdict, route?, query_text?, answer_text?, correction_text?, sql_text?, notes?}}`\n",
    "## Governor / Verifier\n",
    "- `GET /governor/status`\n",
    "- `POST /governor/mode?enable_ce=true&enable_adapters=true`\n",
    "- `POST /verify/mode?set_v2=true|false`\n",
    "## RL, Policy, Registry\n",
    "- `POST /rl/update`  `POST /rl/update_from_logs`\n",
    "- `GET /policy/status`\n",
    "- `GET /registry/status`\n",
    "- `GET /registry/challengers`\n",
    "- `POST /registry/promote?job_id=...`\n",
    "- `POST /registry/drop?job_id=...`\n",
    "## Meta-controller\n",
    "- `GET /meta/status`\n",
    "- `POST /meta/simulate` (optional weights body)\n",
    "\"\"\"\n",
    "(Path(\"API_QUICK_REFERENCE.md\")).write_text(api_md.strip()+\"\\n\", encoding=\"utf-8\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 233,
   "id": "07508a94-8d39-4c90-bc44-0c0be03d7d3c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "974"
      ]
     },
     "execution_count": 233,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "runbook = f\"\"\"# Release Runbook (v0.2)\n",
    "**SLOs**\n",
    "- p95 latency  1.8s\n",
    "- Error rate  1%\n",
    "- Availability  99.9%\n",
    "**Kill-switches**\n",
    "- Governor off (cheapest path): `POST /governor/mode?enable_ce=false&enable_adapters=false`\n",
    "- Disable verifier v2: `POST /verify/mode?set_v2=false`\n",
    "- Canary off (set in code/policy weights A1.0, B0.0): `POST /rl/update_from_logs` after editing policy, or set CANARY_ENABLED=false in env.\n",
    "**Rollback**\n",
    "1. Promote previous champion from `/registry/history` via `/registry/promote?job_id=...`\n",
    "2. Re-enable champion-only traffic: set policy weights to A=1.0, B=0.0\n",
    "3. Confirm `/metrics` stabilizes; check alerts.\n",
    "**Pre-release checks**\n",
    "- Health: `/healthz`\n",
    "- Smoke & Golden: run notebook tests (already scripted)\n",
    "- Chaos sanity: governor off  queries still succeed; BM25 fallback works; verifier v2 toggles latency but answers remain sane.\n",
    "**Operational Dashboards (later)**\n",
    "- Prometheus already live at `/metrics`; rules in `ops/alerts.yml` (optional deploy).\n",
    "\"\"\"\n",
    "(Path(\"RELEASE_RUNBOOK.md\")).write_text(runbook.strip()+\"\\n\", encoding=\"utf-8\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 234,
   "id": "1822720b-9098-4942-8521-3c257b1684d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "snap = {\n",
    "    \"healthz\": _safe_get(f\"{BASE}/healthz\"),\n",
    "    \"governor\": _safe_get(f\"{BASE}/governor/status\"),\n",
    "    \"policy\": _safe_get(f\"{BASE}/policy/status\"),\n",
    "    \"registry\": _safe_get(f\"{BASE}/registry/status\"),\n",
    "    \"meta\": _safe_get(f\"{BASE}/meta/status\"),\n",
    "}\n",
    "(CFG_DIR / \"snapshot.json\").write_text(json.dumps(snap, indent=2))\n",
    "for p in [\"artifacts/policy.json\", \"artifacts/registry.json\"]:\n",
    "    src = Path(p)\n",
    "    if src.exists():\n",
    "        (CFG_DIR / src.name).write_text(src.read_text())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 235,
   "id": "983d1837-b886-4f93-840c-fd9841583238",
   "metadata": {},
   "outputs": [],
   "source": [
    "jid, ns = _latest_job()\n",
    "smoke_qs = [\n",
    "    \"What are common notes for customers with late fees?\",\n",
    "    \"select age, income and balance where default = 1 limit 5\",\n",
    "    \"2+2*10 - 3\",\n",
    "    \"Do we comply with PII masking in logs?\",\n",
    "]\n",
    "val_rows = []\n",
    "if jid:\n",
    "    for q in smoke_qs:\n",
    "        t0 = time.time()\n",
    "        try:\n",
    "            r = requests.post(f\"{BASE}/query\", json={\"job_id\":jid,\"namespace\":ns,\"query_text\":q,\"top_k\":5}, timeout=8)\n",
    "            dt = int((time.time()-t0)*1000)\n",
    "            j = r.json()\n",
    "            meta = j.get(\"meta\", {})\n",
    "            val_rows.append({\n",
    "                \"query\": q, \"status\": r.status_code, \"action\": j.get(\"action\"),\n",
    "                \"route\": meta.get(\"route\"), \"retriever\": meta.get(\"retriever\"),\n",
    "                \"reranker\": meta.get(\"reranker\"), \"slm\": meta.get(\"slm\"),\n",
    "                \"latency_ms\": meta.get(\"latency_ms\"), \"ok\": r.ok\n",
    "            })\n",
    "        except Exception as e:\n",
    "            val_rows.append({\"query\": q, \"status\": 599, \"error\": str(e), \"ok\": False})\n",
    "pd.DataFrame(val_rows).to_csv(VAL_DIR / \"smoke.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 236,
   "id": "7648b8b8-5bef-4527-a236-2861f5e93380",
   "metadata": {},
   "outputs": [],
   "source": [
    "golden = [\n",
    "    (\"sql\",    \"select age, income and balance where default = 1 limit 5\", \"run_sql\"),\n",
    "    (\"factual\",\"What are common notes for customers with late fees?\", \"respond\"),\n",
    "    (\"math\",   \"2+2*10 - 3\", \"respond\"),\n",
    "    (\"policy\", \"Do we comply with PII masking in logs?\", \"respond\"),\n",
    "]\n",
    "gold_rows = []\n",
    "if jid:\n",
    "    for route, q, expect in golden:\n",
    "        out = requests.post(f\"{BASE}/query\", json={\"job_id\":jid,\"namespace\":ns,\"query_text\":q,\"top_k\":5}, timeout=8).json()\n",
    "        got = out.get(\"action\")\n",
    "        gold_rows.append({\"route\":route, \"query\":q, \"expect\":expect, \"got\":got, \"ok\": got==expect})\n",
    "pd.DataFrame(gold_rows).to_csv(VAL_DIR / \"golden.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 237,
   "id": "8aa809d7-506f-4d40-954a-a931b86a48b3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2099890"
      ]
     },
     "execution_count": 237,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "manifest = []\n",
    "for root, _, files in os.walk(\"artifacts\"):\n",
    "    for f in files:\n",
    "        p = Path(root) / f\n",
    "        try:\n",
    "            manifest.append({\n",
    "                \"path\": str(p),\n",
    "                \"bytes\": p.stat().st_size,\n",
    "                \"sha256\": _hash_file(p) if p.stat().st_size < (32<<20) else None\n",
    "            })\n",
    "        except Exception:\n",
    "            pass\n",
    "(Path(REL_ROOT / \"manifest.json\")).write_text(json.dumps(manifest, indent=2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 238,
   "id": "b331997f-203a-4b90-a300-ceb20a52192e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "923"
      ]
     },
     "execution_count": 238,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "try:\n",
    "    freeze = subprocess.check_output([sys.executable, \"-m\", \"pip\", \"freeze\"], text=True, timeout=30)\n",
    "    (DOCS_DIR / \"requirements.lock.txt\").write_text(freeze)\n",
    "except Exception as e:\n",
    "    (DOCS_DIR / \"requirements.lock.txt\").write_text(f\"# failed to pip freeze: {e}\\n\")\n",
    "postman = {\n",
    "  \"info\": {\"name\": \"Self-Optimizing ML API v0.2\", \"schema\": \"https://schema.getpostman.com/json/collection/v2.1.0/collection.json\"},\n",
    "  \"item\": [\n",
    "    {\"name\":\"healthz\",\"request\":{\"method\":\"GET\",\"url\":f\"{BASE}/healthz\"}},\n",
    "    {\"name\":\"metrics\",\"request\":{\"method\":\"GET\",\"url\":f\"{BASE}/metrics\"}},\n",
    "    {\"name\":\"query (example)\",\"request\":{\"method\":\"POST\",\"header\":[{\"key\":\"Content-Type\",\"value\":\"application/json\"}],\n",
    "      \"body\":{\"mode\":\"raw\",\"raw\":json.dumps({\"job_id\": jid or \"JOB_ID\",\"namespace\": ns or \"default\",\"query_text\":\"select * from dataset limit 5\",\"top_k\":5})},\n",
    "      \"url\":f\"{BASE}/query\"}}\n",
    "  ]\n",
    "}\n",
    "(DOCS_DIR / \"postman_collection.json\").write_text(json.dumps(postman, indent=2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 239,
   "id": "e63b8126-e853-49e5-b894-ca8e39ac67ba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Wrote:\n",
      "  - API_QUICK_REFERENCE.md\n",
      "  - RELEASE_RUNBOOK.md\n",
      "  - artifacts\\release_1762113621/ (config, validation, manifest, pii_lint)\n",
      "  - ops/requirements.lock.txt\n",
      "  - ops/postman_collection.json\n"
     ]
    }
   ],
   "source": [
    "pii_report = []\n",
    "try:\n",
    "    ds = None\n",
    "    if jid:\n",
    "        d = json.loads(Path(\"artifacts/jobs.json\").read_text())\n",
    "        row = d.get(jid, {})\n",
    "        ds = row.get(\"dataset_path\")\n",
    "    if ds and Path(ds).exists():\n",
    "        df = pd.read_csv(ds) if ds.endswith(\".csv\") else pd.read_parquet(ds)\n",
    "        candidates = [c for c in df.columns if any(t in c.lower() for t in [\"name\",\"email\",\"phone\",\"ssn\",\"address\",\"dob\"])]\n",
    "        samp = df[candidates].head(10).to_dict(orient=\"records\") if candidates else []\n",
    "        pii_report = {\"columns_suspect\": candidates, \"sample_rows\": samp}\n",
    "except Exception as e:\n",
    "    pii_report = {\"error\": str(e)}\n",
    "(Path(REL_ROOT / \"pii_lint.json\")).write_text(json.dumps(pii_report, indent=2))\n",
    "print(\" Wrote:\")\n",
    "print(\"  - API_QUICK_REFERENCE.md\")\n",
    "print(\"  - RELEASE_RUNBOOK.md\")\n",
    "print(f\"  - {REL_ROOT}/ (config, validation, manifest, pii_lint)\")\n",
    "print(\"  - ops/requirements.lock.txt\")\n",
    "print(\"  - ops/postman_collection.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 240,
   "id": "68cc1c36-7cab-4ca5-a7a3-9e367d05550a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import itertools\n",
    "import csv\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 241,
   "id": "19e49219-f6a8-4ed6-9dc5-68e00714e0a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_header(title):\n",
    "    print(\"\\n\" + \"=\"*len(title))\n",
    "    print(title)\n",
    "    print(\"=\"*len(title))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 242,
   "id": "ef2f4887-ef5e-49e4-8bd9-463efa114e05",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "===================================\n",
      "API_QUICK_REFERENCE.md (lines 150)\n",
      "===================================\n",
      "# API Quick Reference (v0.2)\n",
      "Base URL: `http://127.0.0.1:8000`\n",
      "## Health & Metrics\n",
      "- `GET /healthz`  status, cwd, artifacts path\n",
      "- `GET /metrics`  Prometheus metrics\n",
      "## Data & Jobs\n",
      "- `POST /upload` (multipart: file, namespace, target?)  `{job_id, status, profile_uri, rows_used}`\n",
      "- `GET /advanced?job_id=...`  advanced status/artifacts\n",
      "## Query\n",
      "- `POST /query` JSON: `{job_id, namespace, query_text, top_k}`\n",
      "  - Response: `{request_id, action, payload, citations, model_id, confidence, meta}`\n",
      "  - `action`  `respond | run_sql | clarify`\n",
      "## Feedback (HITL)\n",
      "- `POST /feedback` JSON: `{job_id, namespace, request_id, verdict, route?, query_text?, answer_text?, correction_text?, sql_text?, notes?}`\n",
      "## Governor / Verifier\n",
      "- `GET /governor/status`\n",
      "- `POST /governor/mode?enable_ce=true&enable_adapters=true`\n",
      "- `POST /verify/mode?set_v2=true|false`\n",
      "## RL, Policy, Registry\n",
      "- `POST /rl/update`  `POST /rl/update_from_logs`\n",
      "- `GET /policy/status`\n",
      "- `GET /registry/status`\n",
      "- `GET /registry/challengers`\n",
      "- `POST /registry/promote?job_id=...`\n",
      "- `POST /registry/drop?job_id=...`\n",
      "## Meta-controller\n",
      "- `GET /meta/status`\n",
      "- `POST /meta/simulate` (optional weights body)\n",
      "\n",
      "===============================\n",
      "RELEASE_RUNBOOK.md (lines 160)\n",
      "===============================\n",
      "# Release Runbook (v0.2)\n",
      "**SLOs**\n",
      "- p95 latency  1.8s\n",
      "- Error rate  1%\n",
      "- Availability  99.9%\n",
      "**Kill-switches**\n",
      "- Governor off (cheapest path): `POST /governor/mode?enable_ce=false&enable_adapters=false`\n",
      "- Disable verifier v2: `POST /verify/mode?set_v2=false`\n",
      "- Canary off (set in code/policy weights A1.0, B0.0): `POST /rl/update_from_logs` after editing policy, or set CANARY_ENABLED=false in env.\n",
      "**Rollback**\n",
      "1. Promote previous champion from `/registry/history` via `/registry/promote?job_id=...`\n",
      "2. Re-enable champion-only traffic: set policy weights to A=1.0, B=0.0\n",
      "3. Confirm `/metrics` stabilizes; check alerts.\n",
      "**Pre-release checks**\n",
      "- Health: `/healthz`\n",
      "- Smoke & Golden: run notebook tests (already scripted)\n",
      "- Chaos sanity: governor off  queries still succeed; BM25 fallback works; verifier v2 toggles latency but answers remain sane.\n",
      "**Operational Dashboards (later)**\n",
      "- Prometheus already live at `/metrics`; rules in `ops/alerts.yml` (optional deploy).\n",
      "\n",
      "========================================\n",
      "Recursive listing of artifacts/release_*\n",
      "========================================\n",
      "\n",
      "artifacts\\release_1759788124/\n",
      "  config/\n",
      "  config\\policy.json\n",
      "  config\\registry.json\n",
      "  config\\snapshot.json\n",
      "  manifest.json\n",
      "  pii_lint.json\n",
      "  validation/\n",
      "  validation\\golden.csv\n",
      "  validation\\smoke.csv\n",
      "\n",
      "artifacts\\release_1759811656/\n",
      "  config/\n",
      "  config\\policy.json\n",
      "  config\\registry.json\n",
      "  config\\snapshot.json\n",
      "  manifest.json\n",
      "  pii_lint.json\n",
      "  validation/\n",
      "  validation\\golden.csv\n",
      "  validation\\smoke.csv\n",
      "\n",
      "artifacts\\release_1759857991/\n",
      "  config/\n",
      "  config\\policy.json\n",
      "  config\\registry.json\n",
      "  config\\snapshot.json\n",
      "  manifest.json\n",
      "  pii_lint.json\n",
      "  validation/\n",
      "  validation\\golden.csv\n",
      "  validation\\smoke.csv\n",
      "\n",
      "artifacts\\release_1759859676/\n",
      "  config/\n",
      "  config\\policy.json\n",
      "  config\\registry.json\n",
      "  config\\snapshot.json\n",
      "  manifest.json\n",
      "  pii_lint.json\n",
      "  validation/\n",
      "  validation\\golden.csv\n",
      "  validation\\smoke.csv\n",
      "\n",
      "artifacts\\release_1759897981/\n",
      "  config/\n",
      "  config\\policy.json\n",
      "  config\\registry.json\n",
      "  config\\snapshot.json\n",
      "  manifest.json\n",
      "  pii_lint.json\n",
      "  validation/\n",
      "  validation\\golden.csv\n",
      "  validation\\smoke.csv\n",
      "\n",
      "artifacts\\release_1759964406/\n",
      "  config/\n",
      "  config\\policy.json\n",
      "  config\\registry.json\n",
      "  config\\snapshot.json\n",
      "  manifest.json\n",
      "  pii_lint.json\n",
      "  validation/\n",
      "  validation\\golden.csv\n",
      "  validation\\smoke.csv\n",
      "\n",
      "artifacts\\release_1759984523/\n",
      "  config/\n",
      "  config\\policy.json\n",
      "  config\\registry.json\n",
      "  config\\snapshot.json\n",
      "  manifest.json\n",
      "  pii_lint.json\n",
      "  validation/\n",
      "  validation\\golden.csv\n",
      "  validation\\smoke.csv\n",
      "\n",
      "artifacts\\release_1759986247/\n",
      "  config/\n",
      "  config\\policy.json\n",
      "  config\\registry.json\n",
      "  config\\snapshot.json\n",
      "  manifest.json\n",
      "  pii_lint.json\n",
      "  validation/\n",
      "  validation\\golden.csv\n",
      "  validation\\smoke.csv\n",
      "\n",
      "artifacts\\release_1760045943/\n",
      "  config/\n",
      "  config\\policy.json\n",
      "  config\\registry.json\n",
      "  config\\snapshot.json\n",
      "  manifest.json\n",
      "  pii_lint.json\n",
      "  validation/\n",
      "  validation\\golden.csv\n",
      "  validation\\smoke.csv\n",
      "\n",
      "artifacts\\release_1760071671/\n",
      "  config/\n",
      "  config\\policy.json\n",
      "  config\\registry.json\n",
      "  config\\snapshot.json\n",
      "  manifest.json\n",
      "  pii_lint.json\n",
      "  validation/\n",
      "  validation\\golden.csv\n",
      "  validation\\smoke.csv\n",
      "\n",
      "artifacts\\release_1760132534/\n",
      "  config/\n",
      "  config\\policy.json\n",
      "  config\\registry.json\n",
      "  config\\snapshot.json\n",
      "  manifest.json\n",
      "  pii_lint.json\n",
      "  validation/\n",
      "  validation\\golden.csv\n",
      "  validation\\smoke.csv\n",
      "\n",
      "artifacts\\release_1760389772/\n",
      "  config/\n",
      "  config\\policy.json\n",
      "  config\\registry.json\n",
      "  config\\snapshot.json\n",
      "  manifest.json\n",
      "  pii_lint.json\n",
      "  validation/\n",
      "  validation\\golden.csv\n",
      "  validation\\smoke.csv\n",
      "\n",
      "artifacts\\release_1760414360/\n",
      "  config/\n",
      "  config\\policy.json\n",
      "  config\\registry.json\n",
      "  config\\snapshot.json\n",
      "  manifest.json\n",
      "  pii_lint.json\n",
      "  validation/\n",
      "  validation\\golden.csv\n",
      "  validation\\smoke.csv\n",
      "\n",
      "artifacts\\release_1760564604/\n",
      "  config/\n",
      "  config\\policy.json\n",
      "  config\\registry.json\n",
      "  config\\snapshot.json\n",
      "  manifest.json\n",
      "  pii_lint.json\n",
      "  validation/\n",
      "  validation\\golden.csv\n",
      "  validation\\smoke.csv\n",
      "\n",
      "artifacts\\release_1760565789/\n",
      "  config/\n",
      "  config\\policy.json\n",
      "  config\\registry.json\n",
      "  config\\snapshot.json\n",
      "  manifest.json\n",
      "  pii_lint.json\n",
      "  validation/\n",
      "  validation\\golden.csv\n",
      "  validation\\smoke.csv\n",
      "\n",
      "artifacts\\release_1760588348/\n",
      "  config/\n",
      "  config\\policy.json\n",
      "  config\\registry.json\n",
      "  config\\snapshot.json\n",
      "  manifest.json\n",
      "  pii_lint.json\n",
      "  validation/\n",
      "  validation\\golden.csv\n",
      "  validation\\smoke.csv\n",
      "\n",
      "artifacts\\release_1760652328/\n",
      "  config/\n",
      "  config\\policy.json\n",
      "  config\\registry.json\n",
      "  config\\snapshot.json\n",
      "  manifest.json\n",
      "  pii_lint.json\n",
      "  validation/\n",
      "  validation\\golden.csv\n",
      "  validation\\smoke.csv\n",
      "\n",
      "artifacts\\release_1760676120/\n",
      "  config/\n",
      "  config\\policy.json\n",
      "  config\\registry.json\n",
      "  config\\snapshot.json\n",
      "  manifest.json\n",
      "  pii_lint.json\n",
      "  validation/\n",
      "  validation\\golden.csv\n",
      "  validation\\smoke.csv\n",
      "\n",
      "artifacts\\release_1760976505/\n",
      "  config/\n",
      "  config\\policy.json\n",
      "  config\\registry.json\n",
      "  config\\snapshot.json\n",
      "  manifest.json\n",
      "  pii_lint.json\n",
      "  validation/\n",
      "  validation\\golden.csv\n",
      "  validation\\smoke.csv\n",
      "\n",
      "artifacts\\release_1760995275/\n",
      "  config/\n",
      "  config\\policy.json\n",
      "  config\\registry.json\n",
      "  config\\snapshot.json\n",
      "  manifest.json\n",
      "  pii_lint.json\n",
      "  validation/\n",
      "  validation\\golden.csv\n",
      "  validation\\smoke.csv\n",
      "\n",
      "artifacts\\release_1760997495/\n",
      "  config/\n",
      "  config\\policy.json\n",
      "  config\\registry.json\n",
      "  config\\snapshot.json\n",
      "  manifest.json\n",
      "  pii_lint.json\n",
      "  validation/\n",
      "  validation\\golden.csv\n",
      "  validation\\smoke.csv\n",
      "\n",
      "artifacts\\release_1761088289/\n",
      "  config/\n",
      "  config\\policy.json\n",
      "  config\\registry.json\n",
      "  config\\snapshot.json\n",
      "  manifest.json\n",
      "  pii_lint.json\n",
      "  validation/\n",
      "  validation\\golden.csv\n",
      "  validation\\smoke.csv\n",
      "\n",
      "artifacts\\release_1761194524/\n",
      "  config/\n",
      "  config\\policy.json\n",
      "  config\\registry.json\n",
      "  config\\snapshot.json\n",
      "  manifest.json\n",
      "  pii_lint.json\n",
      "  validation/\n",
      "  validation\\golden.csv\n",
      "  validation\\smoke.csv\n",
      "\n",
      "artifacts\\release_1761198179/\n",
      "  config/\n",
      "  config\\policy.json\n",
      "  config\\registry.json\n",
      "  config\\snapshot.json\n",
      "  manifest.json\n",
      "  pii_lint.json\n",
      "  validation/\n",
      "  validation\\golden.csv\n",
      "  validation\\smoke.csv\n",
      "\n",
      "artifacts\\release_1761281499/\n",
      "  config/\n",
      "  config\\policy.json\n",
      "  config\\registry.json\n",
      "  config\\snapshot.json\n",
      "  manifest.json\n",
      "  pii_lint.json\n",
      "  validation/\n",
      "  validation\\golden.csv\n",
      "  validation\\smoke.csv\n",
      "\n",
      "artifacts\\release_1761340975/\n",
      "  config/\n",
      "  config\\policy.json\n",
      "  config\\registry.json\n",
      "  config\\snapshot.json\n",
      "  manifest.json\n",
      "  pii_lint.json\n",
      "  validation/\n",
      "  validation\\golden.csv\n",
      "  validation\\smoke.csv\n",
      "\n",
      "artifacts\\release_1761413687/\n",
      "  config/\n",
      "  config\\policy.json\n",
      "  config\\registry.json\n",
      "  config\\snapshot.json\n",
      "  manifest.json\n",
      "  pii_lint.json\n",
      "  validation/\n",
      "  validation\\golden.csv\n",
      "  validation\\smoke.csv\n",
      "\n",
      "artifacts\\release_1761582935/\n",
      "  config/\n",
      "  config\\policy.json\n",
      "  config\\registry.json\n",
      "  config\\snapshot.json\n",
      "  manifest.json\n",
      "  pii_lint.json\n",
      "  validation/\n",
      "  validation\\golden.csv\n",
      "  validation\\smoke.csv\n",
      "\n",
      "artifacts\\release_1761584379/\n",
      "  config/\n",
      "  config\\policy.json\n",
      "  config\\registry.json\n",
      "  config\\snapshot.json\n",
      "  manifest.json\n",
      "  pii_lint.json\n",
      "  validation/\n",
      "  validation\\golden.csv\n",
      "  validation\\smoke.csv\n",
      "\n",
      "artifacts\\release_1761627033/\n",
      "  config/\n",
      "  config\\policy.json\n",
      "  config\\registry.json\n",
      "  config\\snapshot.json\n",
      "  manifest.json\n",
      "  pii_lint.json\n",
      "  validation/\n",
      "  validation\\golden.csv\n",
      "  validation\\smoke.csv\n",
      "\n",
      "artifacts\\release_1761682859/\n",
      "  config/\n",
      "  config\\policy.json\n",
      "  config\\registry.json\n",
      "  config\\snapshot.json\n",
      "  manifest.json\n",
      "  pii_lint.json\n",
      "  validation/\n",
      "  validation\\golden.csv\n",
      "  validation\\smoke.csv\n",
      "\n",
      "artifacts\\release_1761713779/\n",
      "  config/\n",
      "  config\\policy.json\n",
      "  config\\registry.json\n",
      "  config\\snapshot.json\n",
      "  manifest.json\n",
      "  pii_lint.json\n",
      "  validation/\n",
      "  validation\\golden.csv\n",
      "  validation\\smoke.csv\n",
      "\n",
      "artifacts\\release_1761776653/\n",
      "  config/\n",
      "  config\\policy.json\n",
      "  config\\registry.json\n",
      "  config\\snapshot.json\n",
      "  manifest.json\n",
      "  pii_lint.json\n",
      "  validation/\n",
      "  validation\\golden.csv\n",
      "  validation\\smoke.csv\n",
      "\n",
      "artifacts\\release_1761802299/\n",
      "  config/\n",
      "  config\\policy.json\n",
      "  config\\registry.json\n",
      "  config\\snapshot.json\n",
      "  manifest.json\n",
      "  pii_lint.json\n",
      "  validation/\n",
      "  validation\\golden.csv\n",
      "  validation\\smoke.csv\n",
      "\n",
      "artifacts\\release_1761839224/\n",
      "  config/\n",
      "  config\\policy.json\n",
      "  config\\registry.json\n",
      "  config\\snapshot.json\n",
      "  manifest.json\n",
      "  pii_lint.json\n",
      "  validation/\n",
      "  validation\\golden.csv\n",
      "  validation\\smoke.csv\n",
      "\n",
      "artifacts\\release_1762113621/\n",
      "  config/\n",
      "  config\\policy.json\n",
      "  config\\registry.json\n",
      "  config\\snapshot.json\n",
      "  manifest.json\n",
      "  pii_lint.json\n",
      "  validation/\n",
      "  validation\\golden.csv\n",
      "  validation\\smoke.csv\n"
     ]
    }
   ],
   "source": [
    "print_header(\"API_QUICK_REFERENCE.md (lines 150)\")\n",
    "api_md = Path(\"API_QUICK_REFERENCE.md\")\n",
    "if api_md.exists():\n",
    "    lines = api_md.read_text(encoding=\"utf-8\", errors=\"ignore\").splitlines()\n",
    "    for i, ln in enumerate(lines[:50], 1):\n",
    "        print(ln)\n",
    "else:\n",
    "    print(\"File not found:\", api_md)\n",
    "print_header(\"RELEASE_RUNBOOK.md (lines 160)\")\n",
    "runbook = Path(\"RELEASE_RUNBOOK.md\")\n",
    "if runbook.exists():\n",
    "    lines = runbook.read_text(encoding=\"utf-8\", errors=\"ignore\").splitlines()\n",
    "    for i, ln in enumerate(lines[:60], 1):\n",
    "        print(ln)\n",
    "else:\n",
    "    print(\"File not found:\", runbook)\n",
    "print_header(\"Recursive listing of artifacts/release_*\")\n",
    "release_roots = sorted(Path(\"artifacts\").glob(\"release_*\"))\n",
    "if not release_roots:\n",
    "    print(\"No release_* directories found under artifacts/\")\n",
    "else:\n",
    "    for root in release_roots:\n",
    "        print(f\"\\n{root}/\")\n",
    "        for p in sorted(root.rglob(\"*\")):\n",
    "            rel = p.relative_to(root)\n",
    "            print(f\"  {rel}{'/' if p.is_dir() else ''}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 243,
   "id": "9376bfd8-4bec-442e-aa2a-d2025a78dea8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==========================================\n",
      "First 3 lines of each validation/smoke.csv\n",
      "==========================================\n",
      "\n",
      "# artifacts\\release_1759788124\\validation\\smoke.csv\n",
      "query,status,action,route,retriever,reranker,slm,latency_ms,ok\n",
      "What are common notes for customers with late fees?,200,clarify,factual,numpy,cosine,base,169,True\n",
      "\"select age, income and balance where default = 1 limit 5\",200,run_sql,sql,numpy,cosine,base,147,True\n",
      "\n",
      "# artifacts\\release_1759811656\\validation\\smoke.csv\n",
      "query,status,action,route,retriever,reranker,slm,latency_ms,ok\n",
      "What are common notes for customers with late fees?,200,clarify,factual,numpy,cosine,base,190,True\n",
      "\"select age, income and balance where default = 1 limit 5\",200,run_sql,sql,numpy,cosine,base,149,True\n",
      "\n",
      "# artifacts\\release_1759857991\\validation\\smoke.csv\n",
      "query,status,action,route,retriever,reranker,slm,latency_ms,ok\n",
      "What are common notes for customers with late fees?,200,clarify,factual,numpy,cosine,base,179,True\n",
      "\"select age, income and balance where default = 1 limit 5\",200,run_sql,sql,numpy,cosine,base,150,True\n",
      "\n",
      "# artifacts\\release_1759859676\\validation\\smoke.csv\n",
      "query,status,action,route,retriever,reranker,slm,latency_ms,ok\n",
      "What are common notes for customers with late fees?,200,clarify,factual,numpy,cosine,base,187,True\n",
      "\"select age, income and balance where default = 1 limit 5\",200,run_sql,sql,numpy,cosine,base,136,True\n",
      "\n",
      "# artifacts\\release_1759897981\\validation\\smoke.csv\n",
      "query,status,action,route,retriever,reranker,slm,latency_ms,ok\n",
      "What are common notes for customers with late fees?,200,clarify,factual,numpy,cosine,base,216,True\n",
      "\"select age, income and balance where default = 1 limit 5\",200,run_sql,sql,numpy,cosine,base,157,True\n",
      "\n",
      "# artifacts\\release_1759964406\\validation\\smoke.csv\n",
      "query,status,action,route,retriever,reranker,slm,latency_ms,ok\n",
      "What are common notes for customers with late fees?,200,clarify,factual,numpy,cosine,base,211,True\n",
      "\"select age, income and balance where default = 1 limit 5\",200,run_sql,sql,numpy,cosine,base,148,True\n",
      "\n",
      "# artifacts\\release_1759984523\\validation\\smoke.csv\n",
      "query,status,action,route,retriever,reranker,slm,latency_ms,ok\n",
      "What are common notes for customers with late fees?,200,clarify,factual,numpy,cosine,base,210,True\n",
      "\"select age, income and balance where default = 1 limit 5\",200,run_sql,sql,numpy,cosine,base,166,True\n",
      "\n",
      "# artifacts\\release_1759986247\\validation\\smoke.csv\n",
      "query,status,action,route,retriever,reranker,slm,latency_ms,ok\n",
      "What are common notes for customers with late fees?,200,clarify,factual,numpy,cosine,base,201,True\n",
      "\"select age, income and balance where default = 1 limit 5\",200,run_sql,sql,numpy,cosine,base,138,True\n",
      "\n",
      "# artifacts\\release_1760045943\\validation\\smoke.csv\n",
      "query,status,action,route,retriever,reranker,slm,latency_ms,ok\n",
      "What are common notes for customers with late fees?,200,clarify,factual,numpy,cosine,base,157,True\n",
      "\"select age, income and balance where default = 1 limit 5\",200,run_sql,sql,numpy,cosine,base,136,True\n",
      "\n",
      "# artifacts\\release_1760071671\\validation\\smoke.csv\n",
      "query,status,action,route,retriever,reranker,slm,latency_ms,ok\n",
      "What are common notes for customers with late fees?,200,clarify,factual,numpy,cosine,base,222,True\n",
      "\"select age, income and balance where default = 1 limit 5\",200,run_sql,sql,numpy,cosine,base,139,True\n",
      "\n",
      "# artifacts\\release_1760132534\\validation\\smoke.csv\n",
      "query,status,action,route,retriever,reranker,slm,latency_ms,ok\n",
      "What are common notes for customers with late fees?,200,clarify,factual,numpy,cosine,base,155,True\n",
      "\"select age, income and balance where default = 1 limit 5\",200,run_sql,sql,numpy,cosine,base,118,True\n",
      "\n",
      "# artifacts\\release_1760389772\\validation\\smoke.csv\n",
      "query,status,action,route,retriever,reranker,slm,latency_ms,ok\n",
      "What are common notes for customers with late fees?,200,clarify,factual,numpy,cosine,base,171,True\n",
      "\"select age, income and balance where default = 1 limit 5\",200,run_sql,sql,numpy,cosine,base,131,True\n",
      "\n",
      "# artifacts\\release_1760414360\\validation\\smoke.csv\n",
      "query,status,action,route,retriever,reranker,slm,latency_ms,ok\n",
      "What are common notes for customers with late fees?,200,clarify,factual,numpy,cosine,base,260,True\n",
      "\"select age, income and balance where default = 1 limit 5\",200,run_sql,sql,numpy,cosine,base,151,True\n",
      "\n",
      "# artifacts\\release_1760564604\\validation\\smoke.csv\n",
      "query,status,action,route,retriever,reranker,slm,latency_ms,ok\n",
      "What are common notes for customers with late fees?,200,clarify,factual,numpy,cosine,base,176,True\n",
      "\"select age, income and balance where default = 1 limit 5\",200,run_sql,sql,numpy,cosine,base,133,True\n",
      "\n",
      "# artifacts\\release_1760565789\\validation\\smoke.csv\n",
      "query,status,action,route,retriever,reranker,slm,latency_ms,ok\n",
      "What are common notes for customers with late fees?,200,clarify,factual,numpy,cosine,base,258,True\n",
      "\"select age, income and balance where default = 1 limit 5\",200,run_sql,sql,numpy,cosine,base,131,True\n",
      "\n",
      "# artifacts\\release_1760588348\\validation\\smoke.csv\n",
      "query,status,action,route,retriever,reranker,slm,latency_ms,ok\n",
      "What are common notes for customers with late fees?,200,clarify,factual,numpy,cosine,base,183,True\n",
      "\"select age, income and balance where default = 1 limit 5\",200,run_sql,sql,numpy,cosine,base,123,True\n",
      "\n",
      "# artifacts\\release_1760652328\\validation\\smoke.csv\n",
      "query,status,action,route,retriever,reranker,slm,latency_ms,ok\n",
      "What are common notes for customers with late fees?,200,clarify,factual,numpy,cosine,base,191,True\n",
      "\"select age, income and balance where default = 1 limit 5\",200,run_sql,sql,numpy,cosine,base,151,True\n",
      "\n",
      "# artifacts\\release_1760676120\\validation\\smoke.csv\n",
      "query,status,action,route,retriever,reranker,slm,latency_ms,ok\n",
      "What are common notes for customers with late fees?,200,clarify,factual,numpy,cosine,base,193,True\n",
      "\"select age, income and balance where default = 1 limit 5\",200,run_sql,sql,numpy,cosine,base,161,True\n",
      "\n",
      "# artifacts\\release_1760976505\\validation\\smoke.csv\n",
      "query,status,action,route,retriever,reranker,slm,latency_ms,ok\n",
      "What are common notes for customers with late fees?,200,clarify,factual,numpy,cosine,base,157,True\n",
      "\"select age, income and balance where default = 1 limit 5\",200,run_sql,sql,numpy,cosine,base,132,True\n",
      "\n",
      "# artifacts\\release_1760995275\\validation\\smoke.csv\n",
      "query,status,action,route,retriever,reranker,slm,latency_ms,ok\n",
      "What are common notes for customers with late fees?,200,clarify,factual,numpy,cosine,base,171,True\n",
      "\"select age, income and balance where default = 1 limit 5\",200,run_sql,sql,numpy,cosine,base,126,True\n",
      "\n",
      "# artifacts\\release_1760997495\\validation\\smoke.csv\n",
      "query,status,action,route,retriever,reranker,slm,latency_ms,ok\n",
      "What are common notes for customers with late fees?,200,clarify,factual,numpy,cosine,base,193,True\n",
      "\"select age, income and balance where default = 1 limit 5\",200,run_sql,sql,numpy,cosine,base,121,True\n",
      "\n",
      "# artifacts\\release_1761088289\\validation\\smoke.csv\n",
      "query,status,action,route,retriever,reranker,slm,latency_ms,ok\n",
      "What are common notes for customers with late fees?,200,clarify,factual,numpy,cosine,base,195,True\n",
      "\"select age, income and balance where default = 1 limit 5\",200,run_sql,sql,numpy,cosine,base,140,True\n",
      "\n",
      "# artifacts\\release_1761194524\\validation\\smoke.csv\n",
      "query,status,action,route,retriever,reranker,slm,latency_ms,ok\n",
      "What are common notes for customers with late fees?,200,clarify,factual,numpy,cosine,base,190,True\n",
      "\"select age, income and balance where default = 1 limit 5\",200,run_sql,sql,numpy,cosine,base,148,True\n",
      "\n",
      "# artifacts\\release_1761198179\\validation\\smoke.csv\n",
      "query,status,action,route,retriever,reranker,slm,latency_ms,ok\n",
      "What are common notes for customers with late fees?,200,clarify,factual,numpy,cosine,base,175,True\n",
      "\"select age, income and balance where default = 1 limit 5\",200,run_sql,sql,numpy,cosine,base,134,True\n",
      "\n",
      "# artifacts\\release_1761281499\\validation\\smoke.csv\n",
      "query,status,action,route,retriever,reranker,slm,latency_ms,ok\n",
      "What are common notes for customers with late fees?,200,clarify,factual,numpy,cosine,base,161,True\n",
      "\"select age, income and balance where default = 1 limit 5\",200,run_sql,sql,numpy,cosine,base,198,True\n",
      "\n",
      "# artifacts\\release_1761340975\\validation\\smoke.csv\n",
      "query,status,action,route,retriever,reranker,slm,latency_ms,ok\n",
      "What are common notes for customers with late fees?,200,clarify,factual,numpy,cosine,base,146,True\n",
      "\"select age, income and balance where default = 1 limit 5\",200,run_sql,sql,numpy,cosine,base,123,True\n",
      "\n",
      "# artifacts\\release_1761413687\\validation\\smoke.csv\n",
      "query,status,action,route,retriever,reranker,slm,latency_ms,ok\n",
      "What are common notes for customers with late fees?,200,clarify,factual,numpy,cosine,base,173,True\n",
      "\"select age, income and balance where default = 1 limit 5\",200,run_sql,sql,numpy,cosine,base,141,True\n",
      "\n",
      "# artifacts\\release_1761582935\\validation\\smoke.csv\n",
      "query,status,action,route,retriever,reranker,slm,latency_ms,ok\n",
      "What are common notes for customers with late fees?,200,clarify,factual,numpy,cosine,base,159,True\n",
      "\"select age, income and balance where default = 1 limit 5\",200,run_sql,sql,numpy,cosine,base,122,True\n",
      "\n",
      "# artifacts\\release_1761584379\\validation\\smoke.csv\n",
      "query,status,action,route,retriever,reranker,slm,latency_ms,ok\n",
      "What are common notes for customers with late fees?,200,clarify,factual,numpy,cosine,base,148,True\n",
      "\"select age, income and balance where default = 1 limit 5\",200,run_sql,sql,numpy,cosine,base,129,True\n",
      "\n",
      "# artifacts\\release_1761627033\\validation\\smoke.csv\n",
      "query,status,action,route,retriever,reranker,slm,latency_ms,ok\n",
      "What are common notes for customers with late fees?,200,clarify,factual,numpy,cosine,base,150,True\n",
      "\"select age, income and balance where default = 1 limit 5\",200,run_sql,sql,numpy,cosine,base,118,True\n",
      "\n",
      "# artifacts\\release_1761682859\\validation\\smoke.csv\n",
      "query,status,action,route,retriever,reranker,slm,latency_ms,ok\n",
      "What are common notes for customers with late fees?,200,clarify,factual,numpy,cosine,base,158,True\n",
      "\"select age, income and balance where default = 1 limit 5\",200,run_sql,sql,numpy,cosine,base,138,True\n",
      "\n",
      "# artifacts\\release_1761713779\\validation\\smoke.csv\n",
      "query,status,action,route,retriever,reranker,slm,latency_ms,ok\n",
      "What are common notes for customers with late fees?,200,clarify,factual,numpy,cosine,base,146,True\n",
      "\"select age, income and balance where default = 1 limit 5\",200,run_sql,sql,numpy,cosine,base,124,True\n",
      "\n",
      "# artifacts\\release_1761776653\\validation\\smoke.csv\n",
      "query,status,action,route,retriever,reranker,slm,latency_ms,ok\n",
      "What are common notes for customers with late fees?,400,,,,,,,False\n",
      "\"select age, income and balance where default = 1 limit 5\",400,,,,,,,False\n",
      "\n",
      "# artifacts\\release_1761802299\\validation\\smoke.csv\n",
      "query,status,action,route,retriever,reranker,slm,latency_ms,ok\n",
      "What are common notes for customers with late fees?,200,clarify,factual,numpy,cosine,base,153,True\n",
      "\"select age, income and balance where default = 1 limit 5\",200,run_sql,sql,numpy,cosine,base,157,True\n",
      "\n",
      "# artifacts\\release_1761839224\\validation\\smoke.csv\n",
      "query,status,action,route,retriever,reranker,slm,latency_ms,ok\n",
      "What are common notes for customers with late fees?,200,clarify,factual,numpy,cosine,base,147,True\n",
      "\"select age, income and balance where default = 1 limit 5\",200,run_sql,sql,numpy,cosine,base,145,True\n",
      "\n",
      "# artifacts\\release_1762113621\\validation\\smoke.csv\n",
      "query,status,action,route,retriever,reranker,slm,latency_ms,ok\n",
      "What are common notes for customers with late fees?,200,clarify,factual,numpy,cosine,base,286,True\n",
      "\"select age, income and balance where default = 1 limit 5\",200,run_sql,sql,numpy,cosine,base,82,True\n",
      "\n",
      "===========================================\n",
      "First 3 lines of each validation/golden.csv\n",
      "===========================================\n",
      "\n",
      "# artifacts\\release_1759788124\\validation\\golden.csv\n",
      "route,query,expect,got,ok\n",
      "sql,\"select age, income and balance where default = 1 limit 5\",run_sql,run_sql,True\n",
      "factual,What are common notes for customers with late fees?,respond,clarify,False\n",
      "\n",
      "# artifacts\\release_1759811656\\validation\\golden.csv\n",
      "route,query,expect,got,ok\n",
      "sql,\"select age, income and balance where default = 1 limit 5\",run_sql,run_sql,True\n",
      "factual,What are common notes for customers with late fees?,respond,clarify,False\n",
      "\n",
      "# artifacts\\release_1759857991\\validation\\golden.csv\n",
      "route,query,expect,got,ok\n",
      "sql,\"select age, income and balance where default = 1 limit 5\",run_sql,run_sql,True\n",
      "factual,What are common notes for customers with late fees?,respond,clarify,False\n",
      "\n",
      "# artifacts\\release_1759859676\\validation\\golden.csv\n",
      "route,query,expect,got,ok\n",
      "sql,\"select age, income and balance where default = 1 limit 5\",run_sql,run_sql,True\n",
      "factual,What are common notes for customers with late fees?,respond,clarify,False\n",
      "\n",
      "# artifacts\\release_1759897981\\validation\\golden.csv\n",
      "route,query,expect,got,ok\n",
      "sql,\"select age, income and balance where default = 1 limit 5\",run_sql,run_sql,True\n",
      "factual,What are common notes for customers with late fees?,respond,clarify,False\n",
      "\n",
      "# artifacts\\release_1759964406\\validation\\golden.csv\n",
      "route,query,expect,got,ok\n",
      "sql,\"select age, income and balance where default = 1 limit 5\",run_sql,run_sql,True\n",
      "factual,What are common notes for customers with late fees?,respond,clarify,False\n",
      "\n",
      "# artifacts\\release_1759984523\\validation\\golden.csv\n",
      "route,query,expect,got,ok\n",
      "sql,\"select age, income and balance where default = 1 limit 5\",run_sql,run_sql,True\n",
      "factual,What are common notes for customers with late fees?,respond,clarify,False\n",
      "\n",
      "# artifacts\\release_1759986247\\validation\\golden.csv\n",
      "route,query,expect,got,ok\n",
      "sql,\"select age, income and balance where default = 1 limit 5\",run_sql,run_sql,True\n",
      "factual,What are common notes for customers with late fees?,respond,clarify,False\n",
      "\n",
      "# artifacts\\release_1760045943\\validation\\golden.csv\n",
      "route,query,expect,got,ok\n",
      "sql,\"select age, income and balance where default = 1 limit 5\",run_sql,run_sql,True\n",
      "factual,What are common notes for customers with late fees?,respond,clarify,False\n",
      "\n",
      "# artifacts\\release_1760071671\\validation\\golden.csv\n",
      "route,query,expect,got,ok\n",
      "sql,\"select age, income and balance where default = 1 limit 5\",run_sql,run_sql,True\n",
      "factual,What are common notes for customers with late fees?,respond,clarify,False\n",
      "\n",
      "# artifacts\\release_1760132534\\validation\\golden.csv\n",
      "route,query,expect,got,ok\n",
      "sql,\"select age, income and balance where default = 1 limit 5\",run_sql,run_sql,True\n",
      "factual,What are common notes for customers with late fees?,respond,clarify,False\n",
      "\n",
      "# artifacts\\release_1760389772\\validation\\golden.csv\n",
      "route,query,expect,got,ok\n",
      "sql,\"select age, income and balance where default = 1 limit 5\",run_sql,run_sql,True\n",
      "factual,What are common notes for customers with late fees?,respond,clarify,False\n",
      "\n",
      "# artifacts\\release_1760414360\\validation\\golden.csv\n",
      "route,query,expect,got,ok\n",
      "sql,\"select age, income and balance where default = 1 limit 5\",run_sql,run_sql,True\n",
      "factual,What are common notes for customers with late fees?,respond,clarify,False\n",
      "\n",
      "# artifacts\\release_1760564604\\validation\\golden.csv\n",
      "route,query,expect,got,ok\n",
      "sql,\"select age, income and balance where default = 1 limit 5\",run_sql,run_sql,True\n",
      "factual,What are common notes for customers with late fees?,respond,clarify,False\n",
      "\n",
      "# artifacts\\release_1760565789\\validation\\golden.csv\n",
      "route,query,expect,got,ok\n",
      "sql,\"select age, income and balance where default = 1 limit 5\",run_sql,run_sql,True\n",
      "factual,What are common notes for customers with late fees?,respond,clarify,False\n",
      "\n",
      "# artifacts\\release_1760588348\\validation\\golden.csv\n",
      "route,query,expect,got,ok\n",
      "sql,\"select age, income and balance where default = 1 limit 5\",run_sql,run_sql,True\n",
      "factual,What are common notes for customers with late fees?,respond,clarify,False\n",
      "\n",
      "# artifacts\\release_1760652328\\validation\\golden.csv\n",
      "route,query,expect,got,ok\n",
      "sql,\"select age, income and balance where default = 1 limit 5\",run_sql,run_sql,True\n",
      "factual,What are common notes for customers with late fees?,respond,clarify,False\n",
      "\n",
      "# artifacts\\release_1760676120\\validation\\golden.csv\n",
      "route,query,expect,got,ok\n",
      "sql,\"select age, income and balance where default = 1 limit 5\",run_sql,run_sql,True\n",
      "factual,What are common notes for customers with late fees?,respond,clarify,False\n",
      "\n",
      "# artifacts\\release_1760976505\\validation\\golden.csv\n",
      "route,query,expect,got,ok\n",
      "sql,\"select age, income and balance where default = 1 limit 5\",run_sql,run_sql,True\n",
      "factual,What are common notes for customers with late fees?,respond,clarify,False\n",
      "\n",
      "# artifacts\\release_1760995275\\validation\\golden.csv\n",
      "route,query,expect,got,ok\n",
      "sql,\"select age, income and balance where default = 1 limit 5\",run_sql,run_sql,True\n",
      "factual,What are common notes for customers with late fees?,respond,clarify,False\n",
      "\n",
      "# artifacts\\release_1760997495\\validation\\golden.csv\n",
      "route,query,expect,got,ok\n",
      "sql,\"select age, income and balance where default = 1 limit 5\",run_sql,run_sql,True\n",
      "factual,What are common notes for customers with late fees?,respond,clarify,False\n",
      "\n",
      "# artifacts\\release_1761088289\\validation\\golden.csv\n",
      "route,query,expect,got,ok\n",
      "sql,\"select age, income and balance where default = 1 limit 5\",run_sql,run_sql,True\n",
      "factual,What are common notes for customers with late fees?,respond,clarify,False\n",
      "\n",
      "# artifacts\\release_1761194524\\validation\\golden.csv\n",
      "route,query,expect,got,ok\n",
      "sql,\"select age, income and balance where default = 1 limit 5\",run_sql,run_sql,True\n",
      "factual,What are common notes for customers with late fees?,respond,clarify,False\n",
      "\n",
      "# artifacts\\release_1761198179\\validation\\golden.csv\n",
      "route,query,expect,got,ok\n",
      "sql,\"select age, income and balance where default = 1 limit 5\",run_sql,run_sql,True\n",
      "factual,What are common notes for customers with late fees?,respond,clarify,False\n",
      "\n",
      "# artifacts\\release_1761281499\\validation\\golden.csv\n",
      "route,query,expect,got,ok\n",
      "sql,\"select age, income and balance where default = 1 limit 5\",run_sql,run_sql,True\n",
      "factual,What are common notes for customers with late fees?,respond,clarify,False\n",
      "\n",
      "# artifacts\\release_1761340975\\validation\\golden.csv\n",
      "route,query,expect,got,ok\n",
      "sql,\"select age, income and balance where default = 1 limit 5\",run_sql,run_sql,True\n",
      "factual,What are common notes for customers with late fees?,respond,clarify,False\n",
      "\n",
      "# artifacts\\release_1761413687\\validation\\golden.csv\n",
      "route,query,expect,got,ok\n",
      "sql,\"select age, income and balance where default = 1 limit 5\",run_sql,run_sql,True\n",
      "factual,What are common notes for customers with late fees?,respond,clarify,False\n",
      "\n",
      "# artifacts\\release_1761582935\\validation\\golden.csv\n",
      "route,query,expect,got,ok\n",
      "sql,\"select age, income and balance where default = 1 limit 5\",run_sql,run_sql,True\n",
      "factual,What are common notes for customers with late fees?,respond,clarify,False\n",
      "\n",
      "# artifacts\\release_1761584379\\validation\\golden.csv\n",
      "route,query,expect,got,ok\n",
      "sql,\"select age, income and balance where default = 1 limit 5\",run_sql,run_sql,True\n",
      "factual,What are common notes for customers with late fees?,respond,clarify,False\n",
      "\n",
      "# artifacts\\release_1761627033\\validation\\golden.csv\n",
      "route,query,expect,got,ok\n",
      "sql,\"select age, income and balance where default = 1 limit 5\",run_sql,run_sql,True\n",
      "factual,What are common notes for customers with late fees?,respond,clarify,False\n",
      "\n",
      "# artifacts\\release_1761682859\\validation\\golden.csv\n",
      "route,query,expect,got,ok\n",
      "sql,\"select age, income and balance where default = 1 limit 5\",run_sql,run_sql,True\n",
      "factual,What are common notes for customers with late fees?,respond,clarify,False\n",
      "\n",
      "# artifacts\\release_1761713779\\validation\\golden.csv\n",
      "route,query,expect,got,ok\n",
      "sql,\"select age, income and balance where default = 1 limit 5\",run_sql,run_sql,True\n",
      "factual,What are common notes for customers with late fees?,respond,clarify,False\n",
      "\n",
      "# artifacts\\release_1761776653\\validation\\golden.csv\n",
      "route,query,expect,got,ok\n",
      "sql,\"select age, income and balance where default = 1 limit 5\",run_sql,,False\n",
      "factual,What are common notes for customers with late fees?,respond,,False\n",
      "\n",
      "# artifacts\\release_1761802299\\validation\\golden.csv\n",
      "route,query,expect,got,ok\n",
      "sql,\"select age, income and balance where default = 1 limit 5\",run_sql,run_sql,True\n",
      "factual,What are common notes for customers with late fees?,respond,clarify,False\n",
      "\n",
      "# artifacts\\release_1761839224\\validation\\golden.csv\n",
      "route,query,expect,got,ok\n",
      "sql,\"select age, income and balance where default = 1 limit 5\",run_sql,run_sql,True\n",
      "factual,What are common notes for customers with late fees?,respond,clarify,False\n",
      "\n",
      "# artifacts\\release_1762113621\\validation\\golden.csv\n",
      "route,query,expect,got,ok\n",
      "sql,\"select age, income and balance where default = 1 limit 5\",run_sql,run_sql,True\n",
      "factual,What are common notes for customers with late fees?,respond,clarify,False\n"
     ]
    }
   ],
   "source": [
    "print_header(\"First 3 lines of each validation/smoke.csv\")\n",
    "smokes = list(itertools.chain.from_iterable(\n",
    "    (r.glob(\"validation/smoke.csv\") for r in release_roots)\n",
    "))\n",
    "if not smokes:\n",
    "    print(\"No smoke.csv found under artifacts/release_*/validation/\")\n",
    "else:\n",
    "    for sm in smokes:\n",
    "        print(f\"\\n# {sm}\")\n",
    "        try:\n",
    "            with sm.open(\"r\", encoding=\"utf-8\", errors=\"ignore\") as f:\n",
    "                for i, line in zip(range(3), f):\n",
    "                    print(line.rstrip(\"\\n\"))\n",
    "        except Exception as e:\n",
    "            print(\"  (error reading file)\", e)\n",
    "print_header(\"First 3 lines of each validation/golden.csv\")\n",
    "goldens = list(itertools.chain.from_iterable(\n",
    "    (r.glob(\"validation/golden.csv\") for r in release_roots)\n",
    "))\n",
    "if not goldens:\n",
    "    print(\"No golden.csv found under artifacts/release_*/validation/\")\n",
    "else:\n",
    "    for gd in goldens:\n",
    "        print(f\"\\n# {gd}\")\n",
    "        try:\n",
    "            with gd.open(\"r\", encoding=\"utf-8\", errors=\"ignore\") as f:\n",
    "                for i, line in zip(range(3), f):\n",
    "                    print(line.rstrip(\"\\n\"))\n",
    "        except Exception as e:\n",
    "            print(\"  (error reading file)\", e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 244,
   "id": "0765be3f-ab0b-46b2-999b-84e399390499",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, time, threading\n",
    "from collections import defaultdict\n",
    "from pathlib import Path\n",
    "from fastapi import Request, HTTPException\n",
    "from fastapi.responses import JSONResponse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 245,
   "id": "fc22a296-5d17-4cea-a165-e89f163fea5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "API_KEYS   = {k.strip() for k in (os.getenv(\"API_KEYS\") or \"\").split(\",\") if k.strip()}\n",
    "RATE_QPS   = float(os.getenv(\"RATE_QPS\",  \"5\"))   \n",
    "RATE_BURST = int(os.getenv(\"RATE_BURST\", \"10\"))   \n",
    "_buckets = defaultdict(lambda: {\"tokens\": float(RATE_BURST), \"ts\": time.time()})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 246,
   "id": "7e9deb1e-f7ca-4ff7-8968-1a02d449ad08",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _refill_bucket(bkt, now):\n",
    "    delta = max(0.0, now - bkt[\"ts\"])\n",
    "    bkt[\"tokens\"] = min(float(RATE_BURST), bkt[\"tokens\"] + delta * RATE_QPS)\n",
    "    bkt[\"ts\"] = now\n",
    "def _check_api_key(request: Request):\n",
    "    \"\"\"Require X-API-Key header iff API_KEYS is set.\"\"\"\n",
    "    if not API_KEYS:\n",
    "        return  \n",
    "    key = request.headers.get(\"X-API-Key\")\n",
    "    if key not in API_KEYS:\n",
    "        raise HTTPException(status_code=401, detail=\"missing/invalid API key\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 247,
   "id": "bb5865e9-1e2f-4c90-9466-5b4845041317",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[guard] inline auth=off | RATE_QPS=5.0 BURST=10\n"
     ]
    }
   ],
   "source": [
    "def _rate_limit(request: Request):\n",
    "    \"\"\"Simple token-bucket per key (or anon).\"\"\"\n",
    "    key = request.headers.get(\"X-API-Key\") or \"__anon__\"\n",
    "    bkt = _buckets[key]\n",
    "    now = time.time()\n",
    "    _refill_bucket(bkt, now)\n",
    "    if bkt[\"tokens\"] < 1.0:\n",
    "        raise HTTPException(status_code=429, detail=\"rate limit exceeded\")\n",
    "    bkt[\"tokens\"] -= 1.0\n",
    "print(f\"[guard] inline auth={'on' if API_KEYS else 'off'} | RATE_QPS={RATE_QPS} BURST={RATE_BURST}\")\n",
    "app.router.routes = [\n",
    "    r for r in app.router.routes\n",
    "    if not (getattr(r, \"path\", None) == \"/query\" and \"POST\" in getattr(r, \"methods\", set()))\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 248,
   "id": "4722c817-802f-42b7-814f-5c011d374a4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "@app.post(\"/query\")\n",
    "async def query(request: Request):\n",
    "    _check_api_key(request)\n",
    "    _rate_limit(request)\n",
    "    REQS.labels(\"/query\").inc()\n",
    "    CANARY_TOTAL.inc()  \n",
    "    with LAT.labels(\"/query\").time():\n",
    "        params = await _extract_params(request)\n",
    "        job_id = params.get(\"job_id\")\n",
    "        namespace = params.get(\"namespace\", \"default\")\n",
    "        query_text = params.get(\"query_text\", \"\")\n",
    "        try:\n",
    "            top_k = int(params.get(\"top_k\", 5))\n",
    "        except Exception:\n",
    "            top_k = 5\n",
    "        if not job_id:\n",
    "            raise HTTPException(422, detail=\"job_id is required (send as form, JSON, or query param)\")\n",
    "        job_row = jobs_get(job_id)\n",
    "        if not job_row:\n",
    "            raise HTTPException(404, \"Unknown job_id\")\n",
    "        ds = job_row.get(\"dataset_path\")\n",
    "        if not ds or not Path(ds).exists():\n",
    "            raise HTTPException(400, \"Dataset path not found for job\")\n",
    "        prod_out = query_pipeline_v2(\n",
    "            job_id=job_id,\n",
    "            dataset_path=ds,\n",
    "            namespace=namespace,\n",
    "            query_text=query_text,\n",
    "            top_k=top_k,\n",
    "        )\n",
    "        try:\n",
    "            meta = prod_out.get(\"meta\", {})\n",
    "            _gov_record(meta)\n",
    "            _gov_maybe_toggle()\n",
    "        except Exception:\n",
    "            pass\n",
    "        policy_version = _read_policy_version()\n",
    "        prod_out.setdefault(\"meta\", {})\n",
    "        prod_out[\"meta\"].update({\n",
    "            \"policy_version\": policy_version,\n",
    "            \"canary\": False,\n",
    "            \"arm\": \"PROD\"\n",
    "        })\n",
    "        try:\n",
    "            CANARY_LAT_MS.labels(\"PROD\").observe(float(prod_out[\"meta\"].get(\"latency_ms\", 0)))\n",
    "        except Exception:\n",
    "            pass\n",
    "        try:\n",
    "            threading.Thread(\n",
    "                target=_shadow_worker,\n",
    "                args=(job_id, ds, namespace, query_text, top_k, prod_out),\n",
    "                daemon=True,\n",
    "            ).start()\n",
    "        except Exception:\n",
    "            pass\n",
    "        return JSONResponse(prod_out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 249,
   "id": "e2c4320c-ca68-4340-a5d9-e1d83283f6e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests, os\n",
    "BASE = \"http://127.0.0.1:8000\"\n",
    "headers = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 250,
   "id": "8f97a33f-f8f8-45ff-b9ef-3e267877bf8b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "health: 200\n"
     ]
    }
   ],
   "source": [
    "print(\"health:\", requests.get(f\"{BASE}/healthz\").status_code)\n",
    "try:\n",
    "    j = requests.get(\"artifacts/jobs.json\")\n",
    "except:\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 251,
   "id": "895b665c-f31f-4889-9cfe-e686d265fd6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from pathlib import Path\n",
    "J = json.loads(Path(\"artifacts/jobs.json\").read_text())\n",
    "JOB, row = sorted(J.items(), key=lambda kv: kv[1].get(\"ts_updated\",0), reverse=True)[0]\n",
    "NS = row.get(\"namespace\",\"default\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 252,
   "id": "bfe25e15-2b3a-465b-84b2-2484a8be2e33",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "query: 200 run_sql\n"
     ]
    }
   ],
   "source": [
    "out = requests.post(f\"{BASE}/query\", json={\n",
    "    \"job_id\": JOB, \"namespace\": NS, \"query_text\": \"select * from dataset limit 5\", \"top_k\": 5\n",
    "}, headers=headers, timeout=8)\n",
    "print(\"query:\", out.status_code, out.json().get(\"action\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 253,
   "id": "6f86d666-17bd-4df4-b467-2dedbd0f9503",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, sys, time, socket, threading, subprocess, pathlib, json, queue, shutil\n",
    "def _pip(*args):\n",
    "    subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", *args])\n",
    "try:\n",
    "    import uvicorn, nest_asyncio\n",
    "    from fastapi import FastAPI, HTTPException\n",
    "    from pydantic import BaseModel\n",
    "    import numpy as np\n",
    "    from rank_bm25 import BM25Okapi\n",
    "    from sentence_transformers import SentenceTransformer\n",
    "    from prometheus_fastapi_instrumentator import Instrumentator\n",
    "except Exception:\n",
    "    _pip(\"fastapi\",\"uvicorn\",\"rank-bm25\",\"sentence-transformers\",\"prometheus-fastapi-instrumentator\",\"numpy\")\n",
    "    import uvicorn, nest_asyncio\n",
    "    from fastapi import FastAPI, HTTPException\n",
    "    from pydantic import BaseModel\n",
    "    import numpy as np\n",
    "    from rank_bm25 import BM25Okapi\n",
    "    from sentence_transformers import SentenceTransformer\n",
    "    from prometheus_fastapi_instrumentator import Instrumentator\n",
    "nest_asyncio.apply()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 254,
   "id": "5ee96f38-d892-42e2-9115-188e1e785dcd",
   "metadata": {},
   "outputs": [],
   "source": [
    "DOCS = [\n",
    "    {\"id\":\"d1\",\"text\":\"Rust pipelines are fast and memory safe using async with Tokio and Axum.\"},\n",
    "    {\"id\":\"d2\",\"text\":\"RAG pipelines retrieve relevant chunks and augment the prompt for the LLM.\"},\n",
    "    {\"id\":\"d3\",\"text\":\"Hybrid retrieval with BM25 and vector kNN improves recall and faithfulness.\"},\n",
    "    {\"id\":\"d4\",\"text\":\"LangChain helps prototype chains, tools, and agents quickly in Python.\"},\n",
    "    {\"id\":\"d5\",\"text\":\"Tantivy provides BM25 in Rust; pgvector or FAISS provide dense similarity.\"},\n",
    "]\n",
    "def _tokens(s: str):\n",
    "    import re\n",
    "    return [t for t in re.findall(r\"[A-Za-z0-9]+\", s.lower()) if t]\n",
    "tokenized = [_tokens(d[\"text\"]) for d in DOCS]\n",
    "bm25 = BM25Okapi(tokenized)\n",
    "model = SentenceTransformer(\"intfloat/e5-base\")\n",
    "embs = np.asarray(model.encode([d[\"text\"] for d in DOCS], normalize_embeddings=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 255,
   "id": "7d5498dd-1ecd-43c2-af1a-a4dc7051ddff",
   "metadata": {},
   "outputs": [],
   "source": [
    "def topk_dense(q, k):\n",
    "    qv = model.encode([q], normalize_embeddings=True)[0]\n",
    "    sims = (embs @ qv)\n",
    "    idx = np.argsort(-sims)[:k]\n",
    "    return [{\"id\": DOCS[i][\"id\"], \"text\": DOCS[i][\"text\"], \"score\": float(sims[i])} for i in idx]\n",
    "def topk_bm25(q, k):\n",
    "    scores = bm25.get_scores(_tokens(q))\n",
    "    idx = np.argsort(-scores)[:k]\n",
    "    return [{\"id\": DOCS[i][\"id\"], \"text\": DOCS[i][\"text\"], \"score\": float(scores[i])} for i in idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 256,
   "id": "432b5424-40d7-481c-b76e-5868198262e1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<prometheus_fastapi_instrumentator.instrumentation.PrometheusFastApiInstrumentator at 0x1b99b002210>"
      ]
     },
     "execution_count": 256,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from typing import List, Dict\n",
    "from fastapi import FastAPI, HTTPException\n",
    "from pydantic import BaseModel\n",
    "class RetrieveReq(BaseModel):\n",
    "    query: str\n",
    "    top_k: int = 5\n",
    "class RerankReq(BaseModel):\n",
    "    query: str\n",
    "    candidates: List[Dict]\n",
    "class CompleteReq(BaseModel):\n",
    "    query: str\n",
    "    contexts: List[Dict]\n",
    "app = FastAPI()\n",
    "Instrumentator().instrument(app).expose(app, endpoint=\"/metrics\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 257,
   "id": "e10ab425-063b-4af1-86b4-5830040ea47e",
   "metadata": {},
   "outputs": [],
   "source": [
    "@app.get(\"/up\")\n",
    "def up():\n",
    "    return {\"ok\": True, \"base\": os.environ.get(\"RAG_BASE\")}\n",
    "def _clamp_k(k: int) -> int:\n",
    "    try: k = int(k)\n",
    "    except: k = 5\n",
    "    return max(1, min(k, 50))\n",
    "@app.post(\"/retrieve_dense\")\n",
    "def retrieve_dense(req: RetrieveReq):\n",
    "    if not req.query.strip():\n",
    "        raise HTTPException(400, \"query required\")\n",
    "    k = _clamp_k(req.top_k)\n",
    "    return {\"results\": topk_dense(req.query, k)}\n",
    "@app.post(\"/retrieve_bm25\")\n",
    "def retrieve_bm25(req: RetrieveReq):\n",
    "    if not req.query.strip():\n",
    "        raise HTTPException(400, \"query required\")\n",
    "    k = _clamp_k(req.top_k)\n",
    "    return {\"results\": topk_bm25(req.query, k)}\n",
    "@app.post(\"/rerank\")\n",
    "def rerank(req: RerankReq):\n",
    "    cands = req.candidates or []\n",
    "    if not cands:\n",
    "        return {\"results\": []}\n",
    "    qv = model.encode([req.query], normalize_embeddings=True)[0]\n",
    "    tv = model.encode([c[\"text\"] for c in cands], normalize_embeddings=True)\n",
    "    sims = tv @ qv\n",
    "    scored = [{**c, \"score\": float(s)} for c, s in zip(cands, sims)]\n",
    "    scored.sort(key=lambda x: -x[\"score\"])\n",
    "    return {\"results\": scored}\n",
    "@app.post(\"/complete\")\n",
    "def complete(req: CompleteReq):\n",
    "    ctx = \"\\n\".join(f\"- {c['text']}\" for c in (req.contexts or [])[:4])\n",
    "    return {\"answer\": f\"Q: {req.query}\\nA: Rust serves fast; RAG retrieves & grounds.\\n\\nContext:\\n{ctx}\"}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 258,
   "id": "876708c3-59cd-471a-8bbe-ede534d703fc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[RAG] up @ http://127.0.0.1:8001  (/up, /metrics)\n"
     ]
    }
   ],
   "source": [
    "def free_port(prefer=8001):\n",
    "    s = socket.socket()\n",
    "    try:\n",
    "        s.bind((\"127.0.0.1\", prefer))\n",
    "        port = s.getsockname()[1]\n",
    "    except OSError:\n",
    "        s.close()\n",
    "        s = socket.socket()\n",
    "        s.bind((\"127.0.0.1\", 0))\n",
    "        port = s.getsockname()[1]\n",
    "    finally:\n",
    "        try: s.close()\n",
    "        except: pass\n",
    "    return port\n",
    "PORT = free_port(8001)\n",
    "RAG_BASE = f\"http://127.0.0.1:{PORT}\"\n",
    "os.environ[\"RAG_BASE\"] = RAG_BASE\n",
    "def _serve():\n",
    "    _ = topk_dense(\"warmup\", 3)  \n",
    "    uvicorn.run(app, host=\"127.0.0.1\", port=PORT, log_level=\"warning\")\n",
    "try:\n",
    "    if 'rag_thr' in globals() and rag_thr.is_alive():\n",
    "        pass\n",
    "except: pass\n",
    "rag_thr = threading.Thread(target=_serve, daemon=True)\n",
    "rag_thr.start()\n",
    "time.sleep(0.8)\n",
    "print(f\"[RAG] up @ {RAG_BASE}  (/up, /metrics)\")\n",
    "ROOT = pathlib.Path(\"./rag_rust_gateway\").resolve()\n",
    "SRC = ROOT / \"src\"\n",
    "os.makedirs(SRC, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 259,
   "id": "f65cef83-2f85-427d-adb3-e45650fbc64e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[GW] wrote sources  C:\\Users\\aniru\\rag_rust_gateway\n"
     ]
    }
   ],
   "source": [
    "(ROOT / \"Cargo.toml\").write_text(\"\"\"\\\n",
    "[package]\n",
    "name = \"rag_rust_gateway\"\n",
    "version = \"0.2.2\"\n",
    "edition = \"2021\"\n",
    "\n",
    "[dependencies]\n",
    "axum = { version = \"0.7\", features = [\"json\"] }\n",
    "tokio = { version = \"1\", features = [\"rt-multi-thread\", \"macros\"] }\n",
    "serde = { version = \"1\", features = [\"derive\"] }\n",
    "serde_json = \"1\"\n",
    "reqwest = { version = \"0.12\", default-features = false, features = [\"json\", \"rustls-tls\"] }\n",
    "tracing = \"0.1\"\n",
    "tracing-subscriber = { version = \"0.3\", features = [\"env-filter\"] }\n",
    "anyhow = \"1\"\n",
    "prometheus = \"0.13\"\n",
    "\"\"\", encoding=\"utf-8\")\n",
    "(SRC / \"main.rs\").write_text(r'''\n",
    "use axum::{http::StatusCode, routing::{get, post}, Router};\n",
    "use serde::{Deserialize, Serialize};\n",
    "use std::{fs, net::SocketAddr, path::PathBuf, sync::Arc, time::{Duration, Instant}};\n",
    "use tracing_subscriber::{layer::SubscriberExt, util::SubscriberInitExt};\n",
    "use prometheus::{Encoder, TextEncoder, HistogramOpts, Histogram, IntCounter, Registry};\n",
    "\n",
    "#[derive(Deserialize)]\n",
    "struct AnswerReq { query: String, top_k: Option<usize>, use_rerank: Option<bool> }\n",
    "\n",
    "#[derive(Serialize, Deserialize, Debug, Clone)]\n",
    "struct Doc { id: String, text: String, score: f32 }\n",
    "\n",
    "#[derive(Serialize)]\n",
    "struct Meta { pipeline: &'static str, latency_ms: u128 }\n",
    "\n",
    "#[derive(Serialize)]\n",
    "struct AnswerResp { answer: String, contexts: Vec<Doc>, meta: Meta }\n",
    "\n",
    "#[derive(Serialize, Deserialize, Clone)]\n",
    "struct RetrieveReq { query: String, top_k: usize }\n",
    "#[derive(Serialize, Deserialize, Clone)]\n",
    "struct RetrieveResp { results: Vec<Doc> }\n",
    "\n",
    "#[derive(Serialize, Deserialize, Clone)]\n",
    "struct RerankReq { query: String, candidates: Vec<Doc> }\n",
    "#[derive(Serialize, Deserialize, Clone)]\n",
    "struct RerankResp { results: Vec<Doc> }\n",
    "\n",
    "#[derive(Serialize, Deserialize, Clone)]\n",
    "struct CompleteReq { query: String, contexts: Vec<Doc> }\n",
    "#[derive(Serialize, Deserialize, Clone)]\n",
    "struct CompleteResp { answer: String }\n",
    "\n",
    "struct Metrics {\n",
    "    reqs: IntCounter,\n",
    "    latency: Histogram,\n",
    "    registry: Registry,\n",
    "}\n",
    "\n",
    "#[tokio::main]\n",
    "async fn main() {\n",
    "    if let Err(e) = real_main().await {\n",
    "        eprintln!(\"FATAL: {:?}\", e);\n",
    "        std::process::exit(1);\n",
    "    }\n",
    "}\n",
    "\n",
    "async fn real_main() -> anyhow::Result<()> {\n",
    "    tracing_subscriber::registry()\n",
    "        .with(tracing_subscriber::EnvFilter::new(\n",
    "            std::env::var(\"RUST_LOG\").unwrap_or_else(|_| \"rag_rust_gateway=info,axum=warn\".into()),\n",
    "        ))\n",
    "        .with(tracing_subscriber::fmt::layer())\n",
    "        .init();\n",
    "\n",
    "    // Metrics\n",
    "    let registry = Registry::new();\n",
    "    let reqs = IntCounter::new(\"gateway_answer_requests_total\", \"Answer calls\").unwrap();\n",
    "    let latency = Histogram::with_opts(HistogramOpts::new(\"gateway_answer_latency_ms\", \"Answer latency ms\")).unwrap();\n",
    "    registry.register(Box::new(reqs.clone())).unwrap();\n",
    "    registry.register(Box::new(latency.clone())).unwrap();\n",
    "    let metrics = Arc::new(Metrics { reqs, latency, registry });\n",
    "\n",
    "    // HTTP client (no TLS features; localhost http only)\n",
    "    let http = reqwest::Client::builder()\n",
    "        .timeout(Duration::from_millis(1500))\n",
    "        .build()?;\n",
    "    let http = Arc::new(http);\n",
    "\n",
    "    // Routes\n",
    "    let app = Router::new()\n",
    "        .route(\"/answer\", post({\n",
    "            let http = http.clone();\n",
    "            let m = metrics.clone();\n",
    "            move |payload| answer(payload, http.clone(), m.clone())\n",
    "        }))\n",
    "        .route(\"/health\", get(|| async { \"ok\" }))\n",
    "        .route(\"/metrics\", get({\n",
    "            let m = metrics.clone();\n",
    "            move || async move {\n",
    "                let encoder = TextEncoder::new();\n",
    "                let metric_families = m.registry.gather();\n",
    "                let mut buf = Vec::new();\n",
    "                encoder.encode(&metric_families, &mut buf).unwrap();\n",
    "                (StatusCode::OK, String::from_utf8(buf).unwrap())\n",
    "            }\n",
    "        }));\n",
    "\n",
    "    // Bind: prefer GATEWAY_PORT; else ephemeral. Log if bind >500ms\n",
    "    let t0 = Instant::now();\n",
    "    let desired = std::env::var(\"GATEWAY_PORT\").ok().and_then(|s| s.parse::<u16>().ok());\n",
    "    let listener = if let Some(p) = desired {\n",
    "        match tokio::net::TcpListener::bind(SocketAddr::from(([127,0,0,1], p))).await {\n",
    "            Ok(l) => l,\n",
    "            Err(_) => tokio::net::TcpListener::bind(SocketAddr::from(([127,0,0,1], 0))).await?,\n",
    "        }\n",
    "    } else {\n",
    "        tokio::net::TcpListener::bind(SocketAddr::from(([127,0,0,1], 0))).await?\n",
    "    };\n",
    "    let bound = listener.local_addr()?;\n",
    "    let bind_elapsed = t0.elapsed();\n",
    "    if bind_elapsed > Duration::from_millis(500) {\n",
    "        tracing::warn!(\"Bind exceeded 500ms SLO: {:?}\", bind_elapsed);\n",
    "    }\n",
    "    tracing::info!(\"Listening on {}\", bound);\n",
    "\n",
    "    // Write PORT.txt to cwd first, then %TEMP%\n",
    "    let cwd_port = PathBuf::from(\"PORT.txt\");\n",
    "    if fs::write(&cwd_port, bound.port().to_string()).is_err() {\n",
    "        let mut tmp = std::env::temp_dir();\n",
    "        tmp.push(\"PORT.txt\");\n",
    "        fs::write(&tmp, bound.port().to_string())\n",
    "            .map_err(|e| anyhow::anyhow!(\"write PORT.txt failed ({}): {}\", tmp.display(), e))?;\n",
    "    }\n",
    "\n",
    "    axum::serve(listener, app).await\n",
    "        .map_err(|e| anyhow::anyhow!(\"server error: {}\", e))?;\n",
    "    Ok(())\n",
    "}\n",
    "\n",
    "async fn answer(\n",
    "    axum::Json(req): axum::Json<AnswerReq>,\n",
    "    http: Arc<reqwest::Client>,\n",
    "    metrics: Arc<Metrics>,\n",
    ") -> Result<axum::Json<AnswerResp>, StatusCode> {\n",
    "    metrics.reqs.inc();\n",
    "    let t0 = Instant::now();\n",
    "\n",
    "    let mut top_k = req.top_k.unwrap_or(5) as usize;\n",
    "    if top_k == 0 { top_k = 1; }\n",
    "    if top_k > 50 { top_k = 50; }\n",
    "    let use_rerank = req.use_rerank.unwrap_or(true);\n",
    "    let py_base = std::env::var(\"RAG_BASE\").unwrap_or_else(|_| \"http://127.0.0.1:8001\".into());\n",
    "\n",
    "    let dense: RetrieveResp = post_json_retry(&http, &format!(\"{}/retrieve_dense\", &py_base),\n",
    "        &RetrieveReq { query: req.query.clone(), top_k }).await\n",
    "        .map_err(|e| { tracing::error!(\"dense error: {}\", e); StatusCode::BAD_GATEWAY })?;\n",
    "\n",
    "    let bm25: RetrieveResp = post_json_retry(&http, &format!(\"{}/retrieve_bm25\", &py_base),\n",
    "        &RetrieveReq { query: req.query.clone(), top_k }).await\n",
    "        .map_err(|e| { tracing::error!(\"bm25 error: {}\", e); StatusCode::BAD_GATEWAY })?;\n",
    "\n",
    "    let mut fused = rrf_merge(&dense.results, &bm25.results, top_k);\n",
    "\n",
    "    if use_rerank {\n",
    "        let reresp: RerankResp = post_json_retry(&http, &format!(\"{}/rerank\", &py_base),\n",
    "            &RerankReq { query: req.query.clone(), candidates: fused.clone() }).await\n",
    "            .map_err(|e| { tracing::error!(\"rerank error: {}\", e); StatusCode::BAD_GATEWAY })?;\n",
    "        fused = reresp.results;\n",
    "    }\n",
    "\n",
    "    let comp: CompleteResp = post_json_retry(&http, &format!(\"{}/complete\", &py_base),\n",
    "        &CompleteReq { query: req.query.clone(), contexts: fused.clone() }).await\n",
    "        .map_err(|e| { tracing::error!(\"complete error: {}\", e); StatusCode::BAD_GATEWAY })?;\n",
    "\n",
    "    let ms = t0.elapsed().as_millis() as f64;\n",
    "    metrics.latency.observe(ms);\n",
    "\n",
    "    Ok(axum::Json(AnswerResp {\n",
    "        answer: comp.answer,\n",
    "        contexts: fused,\n",
    "        meta: Meta { pipeline: if use_rerank { \"rerank\" } else { \"rrf\" }, latency_ms: ms as u128 }\n",
    "    }))\n",
    "}\n",
    "\n",
    "fn rrf_merge(dense: &Vec<Doc>, bm25: &Vec<Doc>, k: usize) -> Vec<Doc> {\n",
    "    use std::collections::{HashMap, HashSet};\n",
    "    let mut rd: HashMap<&str, usize> = HashMap::new();\n",
    "    let mut rb: HashMap<&str, usize> = HashMap::new();\n",
    "    for (i, d) in dense.iter().enumerate() { rd.insert(d.id.as_str(), i+1); }\n",
    "    for (i, d) in bm25.iter().enumerate() { rb.insert(d.id.as_str(), i+1); }\n",
    "\n",
    "    let mut ids: HashSet<&str> = HashSet::new();\n",
    "    for d in dense { ids.insert(d.id.as_str()); }\n",
    "    for d in bm25 { ids.insert(d.id.as_str()); }\n",
    "\n",
    "    let k_rrf: f32 = 60.0;\n",
    "    let mut fused: Vec<Doc> = ids.into_iter().map(|id| {\n",
    "        let sd = rd.get(id).map(|r| 1.0 / (k_rrf + *r as f32)).unwrap_or(0.0);\n",
    "        let sb = rb.get(id).map(|r| 1.0 / (k_rrf + *r as f32)).unwrap_or(0.0);\n",
    "        let text = dense.iter().find(|d| d.id == id)\n",
    "            .or_else(|| bm25.iter().find(|d| d.id == id))\n",
    "            .map(|d| d.text.clone()).unwrap_or_default();\n",
    "        Doc { id: id.to_string(), text, score: sd + sb }\n",
    "    }).collect();\n",
    "\n",
    "    fused.sort_by(|a,b| b.score.partial_cmp(&a.score).unwrap());\n",
    "    fused.truncate(k);\n",
    "    fused\n",
    "}\n",
    "\n",
    "async fn post_json_retry<TResp, TReq>(http: &reqwest::Client, url: &str, payload: &TReq)\n",
    " -> Result<TResp, anyhow::Error>\n",
    "where\n",
    "    TResp: serde::de::DeserializeOwned,\n",
    "    TReq: serde::Serialize,\n",
    "{\n",
    "    let mut last: Option<anyhow::Error> = None;\n",
    "    for i in 0..3 {\n",
    "        match http.post(url).json(payload).send().await {\n",
    "            Ok(r) => match r.json::<TResp>().await {\n",
    "                Ok(v) => return Ok(v),\n",
    "                Err(e) => last = Some(anyhow::anyhow!(\"json error: {}\", e)),\n",
    "            },\n",
    "            Err(e) => last = Some(anyhow::anyhow!(\"req error: {}\", e)),\n",
    "        }\n",
    "        tokio::time::sleep(Duration::from_millis(40 * (i+1) as u64)).await;\n",
    "    }\n",
    "    Err(last.unwrap_or_else(|| anyhow::anyhow!(\"unknown error\")))\n",
    "}\n",
    "''', encoding=\"utf-8\")\n",
    "print(\"[GW] wrote sources \", ROOT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 260,
   "id": "8b5a7462-cd25-4e45-92d4-3a5c7a9cd16b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " C:\\Anaconda3\\Scripts\\conda.exe install -y -c conda-forge m2w64-toolchain (cwd=C:\\Users\\aniru)\n",
      " Using MinGW at: C:\\Anaconda3\\Library\\mingw-w64\\bin\n",
      " rustup toolchain install stable-x86_64-pc-windows-gnu (cwd=C:\\Users\\aniru)\n",
      " Wrote C:\\Users\\aniru\\rag_rust_gateway\\.cargo\\config.toml\n",
      " cargo +stable-x86_64-pc-windows-gnu clean (cwd=C:\\Users\\aniru\\rag_rust_gateway)\n",
      " cargo +stable-x86_64-pc-windows-gnu build --release (cwd=C:\\Users\\aniru\\rag_rust_gateway)\n",
      "EXE: C:\\Users\\aniru\\rag_rust_gateway\\target\\x86_64-pc-windows-gnu\\release\\rag_rust_gateway.exe\n"
     ]
    }
   ],
   "source": [
    "import os, sys, subprocess, shutil\n",
    "from pathlib import Path\n",
    "ROOT = Path(r\"./rag_rust_gateway\").resolve()\n",
    "assert (ROOT / \"Cargo.toml\").exists(), f\"No Cargo.toml under {ROOT}\"\n",
    "def run(cmd, cwd=None, check=True):\n",
    "    print(\"\", \" \".join(cmd), f\"(cwd={cwd or os.getcwd()})\")\n",
    "    return subprocess.run(cmd, cwd=cwd, check=check)\n",
    "cargo = (ROOT / \"Cargo.toml\").read_text(encoding=\"utf-8\")\n",
    "if 'reqwest' in cargo and 'rustls-tls' not in cargo:\n",
    "    cargo = cargo.replace('features = [\"json\"]', 'features = [\"json\",\"rustls-tls\"]')\n",
    "    (ROOT / \"Cargo.toml\").write_text(cargo, encoding=\"utf-8\")\n",
    "    print(\" Updated Cargo.toml to enable reqwest rustls-tls\")\n",
    "def conda_exec():\n",
    "    for exe in [\"mamba.exe\",\"conda.exe\"]:\n",
    "        p = shutil.which(exe) or shutil.which(exe.replace(\".exe\",\"\"))\n",
    "        if p: return p\n",
    "    for base in [os.environ.get(\"CONDA_PREFIX\"), sys.prefix, os.environ.get(\"USERPROFILE\")]:\n",
    "        if not base: continue\n",
    "        for exe in [\"mamba.exe\",\"conda.exe\",\"mamba\",\"conda\"]:\n",
    "            cand = Path(base) / \"Scripts\" / exe\n",
    "            if cand.exists(): return str(cand)\n",
    "    return None\n",
    "CONDAX = conda_exec()\n",
    "if not CONDAX:\n",
    "    raise SystemExit(\"Could not find conda/mamba in PATH; open this notebook from a Conda env or add conda to PATH.\")\n",
    "run([CONDAX, \"install\", \"-y\", \"-c\", \"conda-forge\", \"m2w64-toolchain\"])\n",
    "def find_mingw_bin():\n",
    "    candidates = []\n",
    "    for base in [os.environ.get(\"CONDA_PREFIX\"), sys.prefix]:\n",
    "        if not base: continue\n",
    "        base = Path(base)\n",
    "        candidates.append(base / \"Library\" / \"mingw-w64\" / \"bin\")\n",
    "        candidates.append(base / \"Library\" / \"bin\")\n",
    "    for p in candidates:\n",
    "        if p.exists() and (p / \"x86_64-w64-mingw32-gcc.exe\").exists():\n",
    "            return str(p)\n",
    "    return None\n",
    "MINGW_BIN = find_mingw_bin()\n",
    "if not MINGW_BIN:\n",
    "    raise SystemExit(\"MinGW bin not found after install. Restart the kernel, then re-run this cell.\")\n",
    "os.environ[\"PATH\"] = MINGW_BIN + os.pathsep + os.environ[\"PATH\"]\n",
    "print(\" Using MinGW at:\", MINGW_BIN)\n",
    "run([\"rustup\", \"toolchain\", \"install\", \"stable-x86_64-pc-windows-gnu\"])\n",
    "cargo_cfg = ROOT / \".cargo\" / \"config.toml\"\n",
    "cargo_cfg.parent.mkdir(parents=True, exist_ok=True)\n",
    "cargo_cfg.write_text(f\"\"\"\n",
    "[build]\n",
    "target = \"x86_64-pc-windows-gnu\"\n",
    "[target.x86_64-pc-windows-gnu]\n",
    "linker = \"x86_64-w64-mingw32-gcc.exe\"\n",
    "ar = \"ar.exe\"\n",
    "\"\"\".strip()+\"\\n\", encoding=\"utf-8\")\n",
    "print(\" Wrote\", cargo_cfg)\n",
    "run([\"cargo\", \"+stable-x86_64-pc-windows-gnu\", \"clean\"], cwd=str(ROOT))\n",
    "run([\"cargo\", \"+stable-x86_64-pc-windows-gnu\", \"build\", \"--release\"], cwd=str(ROOT))\n",
    "exe = None\n",
    "for p in (ROOT / \"target\").rglob(\"rag_rust_gateway.exe\"):\n",
    "    if \"release\" in str(p).lower():\n",
    "        exe = p; break\n",
    "print(\"EXE:\", exe if exe else \"not found\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 261,
   "id": "1d08f5ed-1c3c-4796-8efd-814b748b2f01",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, time, threading, math, random, re\n",
    "from typing import List, Dict, Optional\n",
    "from dataclasses import dataclass\n",
    "from functools import lru_cache\n",
    "from fastapi import FastAPI, HTTPException\n",
    "from pydantic import BaseModel\n",
    "import uvicorn\n",
    "import numpy as np\n",
    "try:\n",
    "    import faiss  \n",
    "    _HAVE_FAISS = True\n",
    "except Exception:\n",
    "    faiss = None\n",
    "    _HAVE_FAISS = False\n",
    "try:\n",
    "    from rank_bm25 import BM25Okapi  \n",
    "    _HAVE_BM25 = True\n",
    "except Exception:\n",
    "    BM25Okapi = None\n",
    "    _HAVE_BM25 = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 262,
   "id": "6cb975ce-01b6-4613-8b52-d131095fa894",
   "metadata": {},
   "outputs": [],
   "source": [
    "_ST_EMB_MODEL_NAME = os.getenv(\"EMB_MODEL\", \"intfloat/e5-base\")\n",
    "_ST_RERANK_MODEL_NAME = os.getenv(\"RERANK_MODEL\", \"BAAI/bge-reranker-v2-m3\")\n",
    "def _try_load_sentence_transformers():\n",
    "    try:\n",
    "        from sentence_transformers import SentenceTransformer\n",
    "        return SentenceTransformer\n",
    "    except Exception:\n",
    "        return None\n",
    "def _try_load_cross_encoder():\n",
    "    try:\n",
    "        from sentence_transformers import CrossEncoder\n",
    "        return CrossEncoder\n",
    "    except Exception:\n",
    "        return None\n",
    "DEFAULT_DOCS = [\n",
    "    {\"id\": \"d1\", \"text\": \"Rust pipelines are fast and memory-safe using async with Tokio and Axum.\"},\n",
    "    {\"id\": \"d2\", \"text\": \"RAG pipelines retrieve relevant chunks and augment the prompt for the LLM.\"},\n",
    "    {\"id\": \"d3\", \"text\": \"Hybrid retrieval with BM25 and vector kNN improves recall and faithfulness.\"},\n",
    "    {\"id\": \"d4\", \"text\": \"LangChain helps prototype chains, tools, and agents quickly in Python.\"},\n",
    "    {\"id\": \"d5\", \"text\": \"Tantivy provides BM25 in Rust; pgvector or FAISS provide dense similarity.\"},\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 263,
   "id": "b7729158-e753-4fb5-91c5-6fca3ba150fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "TOKEN_RE = re.compile(r\"[A-Za-z0-9]+\")\n",
    "def tokenize(s: str) -> List[str]:\n",
    "    return [t.lower() for t in TOKEN_RE.findall(s or \"\")]\n",
    "def normalize(vec: np.ndarray) -> np.ndarray:\n",
    "    denom = np.linalg.norm(vec, axis=1, keepdims=True) + 1e-9\n",
    "    return vec / denom\n",
    "@dataclass\n",
    "class DenseIndexMeta:\n",
    "    dim: int\n",
    "    use_faiss: bool\n",
    "    count: int"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 264,
   "id": "620e65d7-7225-4578-adf7-dcf331540f8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DenseIndex:\n",
    "    \"\"\"\n",
    "    Embedding backend priority:\n",
    "      1) sentence-transformers (E5) if available\n",
    "      2) simple hashing vectorizer fallback (fast & tiny)\n",
    "    Index backend priority:\n",
    "      1) FAISS IndexFlatIP if available\n",
    "      2) NumPy matrix + cosine\n",
    "    \"\"\"\n",
    "    def __init__(self, docs: List[Dict[str,str]]):\n",
    "        self.ids = [d[\"id\"] for d in docs]\n",
    "        self.texts = [d[\"text\"] for d in docs]\n",
    "        self.st = _try_load_sentence_transformers()\n",
    "        if self.st:\n",
    "            try:\n",
    "                self.emb_model = self.st(_ST_EMB_MODEL_NAME)\n",
    "                embs = self.emb_model.encode(self.texts, normalize_embeddings=True, convert_to_numpy=True)\n",
    "                self.embs = embs.astype(np.float32)\n",
    "                self.dim = self.embs.shape[1]\n",
    "            except Exception:\n",
    "                self.emb_model = None\n",
    "                self.embs, self.dim = self._hash_embed(self.texts)\n",
    "        else:\n",
    "            self.emb_model = None\n",
    "            self.embs, self.dim = self._hash_embed(self.texts)\n",
    "        self.use_faiss = False\n",
    "        if _HAVE_FAISS:\n",
    "            try:\n",
    "                index = faiss.IndexFlatIP(self.dim)\n",
    "                index.add(self.embs)\n",
    "                self.faiss_index = index\n",
    "                self.use_faiss = True\n",
    "            except Exception:\n",
    "                self.faiss_index = None\n",
    "        else:\n",
    "            self.faiss_index = None\n",
    "        self.meta = DenseIndexMeta(dim=self.dim, use_faiss=self.use_faiss, count=len(self.ids))\n",
    "    def _hash_embed(self, texts: List[str], dim: int = 384) -> (np.ndarray, int):\n",
    "        rng = np.random.default_rng(13)\n",
    "        def proj(token):\n",
    "            rng2 = np.random.default_rng(abs(hash(token)) % (2**32))\n",
    "            return rng2.standard_normal(dim)\n",
    "        mat = []\n",
    "        for t in texts:\n",
    "            toks = tokenize(t)\n",
    "            if not toks:\n",
    "                mat.append(np.zeros(dim))\n",
    "                continue\n",
    "            v = np.sum([proj(tok) for tok in toks], axis=0)\n",
    "            mat.append(v)\n",
    "        arr = np.vstack(mat).astype(np.float32)\n",
    "        arr = normalize(arr)\n",
    "        return arr, dim\n",
    "    def encode_query(self, q: str) -> np.ndarray:\n",
    "        if self.emb_model:\n",
    "            try:\n",
    "                v = self.emb_model.encode([q], normalize_embeddings=True, convert_to_numpy=True)[0].astype(np.float32)\n",
    "                return v\n",
    "            except Exception:\n",
    "                pass\n",
    "        embs, _ = self._hash_embed([q])\n",
    "        return embs[0].astype(np.float32)\n",
    "    def topk(self, q: str, k: int = 5) -> List[Dict]:\n",
    "        k = max(1, min(k, 50))\n",
    "        qv = self.encode_query(q).reshape(1, -1)\n",
    "        if self.use_faiss and self.faiss_index is not None:\n",
    "            sims, idx = self.faiss_index.search(qv, k)\n",
    "            sims = sims[0]\n",
    "            idx = idx[0]\n",
    "            results = []\n",
    "            for i, sc in zip(idx, sims):\n",
    "                if i < 0: continue\n",
    "                results.append({\"id\": self.ids[i], \"text\": self.texts[i], \"score\": float(sc)})\n",
    "            return results\n",
    "        sims = (self.embs @ qv.T).ravel()  \n",
    "        order = np.argsort(-sims)[:k]\n",
    "        return [{\"id\": self.ids[i], \"text\": self.texts[i], \"score\": float(sims[i])} for i in order]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 265,
   "id": "12972930-9a30-4ccf-a575-9a256d431026",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BM25Index:\n",
    "    def __init__(self, docs: List[Dict[str,str]]):\n",
    "        self.ids = [d[\"id\"] for d in docs]\n",
    "        self.texts = [d[\"text\"] for d in docs]\n",
    "        self.tokens = [tokenize(t) for t in self.texts]\n",
    "        self.have_bm25 = _HAVE_BM25 and (BM25Okapi is not None)\n",
    "        if self.have_bm25:\n",
    "            self.bm25 = BM25Okapi(self.tokens)\n",
    "        else:\n",
    "            self.bm25 = None\n",
    "            self.vocab = {}\n",
    "            self.tf = []\n",
    "            for toks in self.tokens:\n",
    "                tf = {}\n",
    "                for t in toks: tf[t] = tf.get(t, 0) + 1\n",
    "                self.tf.append(tf)\n",
    "    def topk(self, q: str, k: int = 5) -> List[Dict]:\n",
    "        k = max(1, min(k, 50))\n",
    "        qtoks = tokenize(q)\n",
    "        if self.have_bm25 and self.bm25:\n",
    "            scores = self.bm25.get_scores(qtoks)\n",
    "            order = np.argsort(-scores)[:k]\n",
    "            return [{\"id\": self.ids[i], \"text\": self.texts[i], \"score\": float(scores[i])} for i in order]\n",
    "        scores = []\n",
    "        for tf in self.tf:\n",
    "            s = sum(tf.get(t, 0) for t in qtoks)\n",
    "            scores.append(float(s))\n",
    "        order = np.argsort(-np.array(scores))[:k]\n",
    "        return [{\"id\": self.ids[i], \"text\": self.texts[i], \"score\": float(scores[i])} for i in order]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 266,
   "id": "ef407209-3b42-45f5-b6e3-1f9b58b5dcd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "@lru_cache(maxsize=1)\n",
    "def _get_cross_encoder():\n",
    "    CE = _try_load_cross_encoder()\n",
    "    if CE is None:\n",
    "        return None\n",
    "    try:\n",
    "        ce = CE(_ST_RERANK_MODEL_NAME)\n",
    "        return ce\n",
    "    except Exception:\n",
    "        return None\n",
    "def rerank_cross_encoder(query: str, cands: List[Dict]) -> List[Dict]:\n",
    "    ce = _get_cross_encoder()\n",
    "    if ce is None or not cands:\n",
    "        return cands\n",
    "    pairs = [(query, c[\"text\"]) for c in cands]\n",
    "    try:\n",
    "        scores = ce.predict(pairs).tolist()\n",
    "        for c, s in zip(cands, scores):\n",
    "            c[\"score\"] = float(s)\n",
    "        return sorted(cands, key=lambda x: -x[\"score\"])\n",
    "    except Exception:\n",
    "        return cands"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 267,
   "id": "0a37f491-e4d7-4a61-bac7-e988e7293023",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LoadDocsReq(BaseModel):\n",
    "    docs: List[Dict[str, str]]  \n",
    "class RetrieveReq(BaseModel):\n",
    "    query: str\n",
    "    top_k: Optional[int] = 5\n",
    "class RerankReq(BaseModel):\n",
    "    query: str\n",
    "    candidates: List[Dict]  \n",
    "class CompleteReq(BaseModel):\n",
    "    query: str\n",
    "    contexts: List[Dict]\n",
    "class SchemaProfileReq(BaseModel):\n",
    "    sample: List[Dict] = []\n",
    "class SchemaValidateReq(BaseModel):\n",
    "    sample: List[Dict] = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 268,
   "id": "282902cb-6968-420a-98bf-f99dd5d14a01",
   "metadata": {},
   "outputs": [],
   "source": [
    "class AppState:\n",
    "    def __init__(self, docs: Optional[List[Dict[str,str]]] = None):\n",
    "        self.docs = docs or DEFAULT_DOCS\n",
    "        self.dense = DenseIndex(self.docs)\n",
    "        self.bm25 = BM25Index(self.docs)\n",
    "        self.created_ts = int(time.time())\n",
    "    def reload(self, docs: List[Dict[str,str]]):\n",
    "        self.docs = docs\n",
    "        self.dense = DenseIndex(self.docs)\n",
    "        self.bm25 = BM25Index(self.docs)\n",
    "app = FastAPI(title=\"RAG Microservice (Notebook)\", version=\"0.2\")\n",
    "STATE = AppState()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 269,
   "id": "f9386885-dfd3-4a45-be99-af0589c6722f",
   "metadata": {},
   "outputs": [],
   "source": [
    "@app.get(\"/up\")\n",
    "def up():\n",
    "    return {\n",
    "        \"ok\": True,\n",
    "        \"created_ts\": STATE.created_ts,\n",
    "        \"docs\": len(STATE.docs),\n",
    "        \"dense\": {\"dim\": STATE.dense.meta.dim, \"faiss\": STATE.dense.meta.use_faiss, \"count\": STATE.dense.meta.count},\n",
    "        \"bm25\": {\"impl\": \"rank_bm25\" if STATE.bm25.have_bm25 else \"tf_fallback\", \"count\": len(STATE.bm25.ids)},\n",
    "    }\n",
    "@app.post(\"/load_docs\")\n",
    "def load_docs(req: LoadDocsReq):\n",
    "    if not req.docs:\n",
    "        raise HTTPException(400, \"docs required\")\n",
    "    for d in req.docs:\n",
    "        if \"id\" not in d or \"text\" not in d:\n",
    "            raise HTTPException(400, \"each doc must have id and text\")\n",
    "    STATE.reload(req.docs)\n",
    "    return {\"ok\": True, \"count\": len(req.docs)}\n",
    "@app.post(\"/retrieve_dense\")\n",
    "def retrieve_dense(req: RetrieveReq):\n",
    "    if not req.query.strip():\n",
    "        raise HTTPException(400, \"query required\")\n",
    "    k = int(req.top_k or 5)\n",
    "    t0 = time.time()\n",
    "    res = STATE.dense.topk(req.query, k)\n",
    "    dt = int((time.time() - t0) * 1000)\n",
    "    return {\"results\": res, \"latency_ms\": dt, \"index\": {\"faiss\": STATE.dense.meta.use_faiss, \"dim\": STATE.dense.meta.dim}}\n",
    "@app.post(\"/retrieve_bm25\")\n",
    "def retrieve_bm25(req: RetrieveReq):\n",
    "    if not req.query.strip():\n",
    "        raise HTTPException(400, \"query required\")\n",
    "    k = int(req.top_k or 5)\n",
    "    t0 = time.time()\n",
    "    res = STATE.bm25.topk(req.query, k)\n",
    "    dt = int((time.time() - t0) * 1000)\n",
    "    return {\"results\": res, \"latency_ms\": dt, \"impl\": \"rank_bm25\" if STATE.bm25.have_bm25 else \"tf_fallback\"}\n",
    "@app.post(\"/rerank\")\n",
    "def rerank(req: RerankReq):\n",
    "    if not req.candidates:\n",
    "        return {\"results\": []}\n",
    "    t0 = time.time()\n",
    "    out = rerank_cross_encoder(req.query, list(req.candidates))\n",
    "    dt = int((time.time() - t0) * 1000)\n",
    "    return {\"results\": out, \"latency_ms\": dt, \"impl\": \"cross_encoder\" if _get_cross_encoder() else \"passthrough\"}\n",
    "@app.post(\"/complete\")\n",
    "def complete(req: CompleteReq):\n",
    "    ctx = \"\\n\".join(f\"- {c.get('text','')}\" for c in (req.contexts or [])[:4])\n",
    "    ans = f\"Q: {req.query}\\nA: Based on the retrieved contexts, here's a concise answer.\\n\\nContext:\\n{ctx}\"\n",
    "    return {\"answer\": ans}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 270,
   "id": "048aa203-a91d-4b72-8e60-d64e059e059d",
   "metadata": {},
   "outputs": [],
   "source": [
    "@app.post(\"/schema/profile\")\n",
    "def schema_profile(req: SchemaProfileReq):\n",
    "    sample = req.sample or []\n",
    "    cols = {}\n",
    "    for row in sample[:200]:\n",
    "        for k, v in row.items():\n",
    "            col = cols.setdefault(k, {\"nulls\": 0, \"num_like\": 0, \"count\": 0})\n",
    "            col[\"count\"] += 1\n",
    "            if v in (None, \"\", \"null\", \"NaN\"): col[\"nulls\"] += 1\n",
    "            try:\n",
    "                float(v)\n",
    "                col[\"num_like\"] += 1\n",
    "            except Exception:\n",
    "                pass\n",
    "    stats = {k: {\"null_rate\": (v[\"nulls\"] / max(1, v[\"count\"])), \"numeric_likelihood\": (v[\"num_like\"] / max(1, v[\"count\"]))} for k, v in cols.items()}\n",
    "    pii_flags = [k for k in cols if any(t in k.lower() for t in (\"name\",\"email\",\"phone\",\"ssn\",\"address\",\"dob\"))]\n",
    "    return {\"schema_contract\": {\"columns\": list(cols.keys())}, \"stats\": stats, \"pii_flags\": pii_flags, \"confidence\": 0.92}\n",
    "@app.post(\"/schema/validate\")\n",
    "def schema_validate(req: SchemaValidateReq):\n",
    "    sample = req.sample or []\n",
    "    if not sample:\n",
    "        return {\"passes\": True, \"cast_error_rate\": 0.0}\n",
    "    keys = list(sample[0].keys())\n",
    "    numericish = set()\n",
    "    for k in keys:\n",
    "        vals = [r.get(k) for r in sample[:100] if k in r]\n",
    "        hits = 0\n",
    "        for v in vals:\n",
    "            try:\n",
    "                float(v)\n",
    "                hits += 1\n",
    "            except Exception:\n",
    "                pass\n",
    "        if hits >= max(3, int(0.6 * len(vals))):\n",
    "            numericish.add(k)\n",
    "    total, bad = 0, 0\n",
    "    for row in sample[:500]:\n",
    "        for k in numericish:\n",
    "            total += 1\n",
    "            try:\n",
    "                float(row.get(k))\n",
    "            except Exception:\n",
    "                bad += 1\n",
    "    err = (bad / max(1, total))\n",
    "    return {\"passes\": err < 0.05, \"cast_error_rate\": err}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 271,
   "id": "ac9ce153-c7e5-4ac6-b32f-5cbc9837d300",
   "metadata": {},
   "outputs": [],
   "source": [
    "srv_thread: Optional[threading.Thread] = None\n",
    "def start_server(host: str = \"127.0.0.1\", port: int = 8001, log_level: str = \"warning\"):\n",
    "    \"\"\"Run uvicorn in a background thread (works in Jupyter).\"\"\"\n",
    "    global srv_thread\n",
    "    if srv_thread and srv_thread.is_alive():\n",
    "        print(f\"Server already running on {host}:{port}\")\n",
    "        return\n",
    "    config = uvicorn.Config(app, host=host, port=port, log_level=log_level)\n",
    "    server = uvicorn.Server(config)\n",
    "    def _run():\n",
    "        _ = STATE.dense.topk(\"warmup\", 3)\n",
    "        _ = STATE.bm25.topk(\"warmup\", 3)\n",
    "        server.run()\n",
    "    srv_thread = threading.Thread(target=_run, daemon=True)\n",
    "    srv_thread.start()\n",
    "    time.sleep(0.5)\n",
    "    print(f\" RAG microservice up at http://{host}:{port}  (endpoints: /up, /retrieve_dense, /retrieve_bm25, /rerank, /complete)\")\n",
    "def smoke_test(base: str = \"http://127.0.0.1:8001\"):\n",
    "    import requests\n",
    "    print(\"GET /up \", requests.get(f\"{base}/up\", timeout=5).json())\n",
    "    q = \"How do BM25 and vector search differ?\"\n",
    "    print(\"\\nPOST /retrieve_bm25 \")\n",
    "    print(requests.post(f\"{base}/retrieve_bm25\", json={\"query\": q, \"top_k\": 3}, timeout=8).json())\n",
    "    print(\"\\nPOST /retrieve_dense \")\n",
    "    dense = requests.post(f\"{base}/retrieve_dense\", json={\"query\": q, \"top_k\": 3}, timeout=8).json()\n",
    "    print(dense)\n",
    "    print(\"\\nPOST /rerank \")\n",
    "    rer = requests.post(f\"{base}/rerank\", json={\"query\": q, \"candidates\": dense[\"results\"]}, timeout=8).json()\n",
    "    print(rer)\n",
    "    print(\"\\nPOST /complete \")\n",
    "    comp = requests.post(f\"{base}/complete\", json={\"query\": q, \"contexts\": rer[\"results\"]}, timeout=8).json()\n",
    "    print(comp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 272,
   "id": "1d005568-64ec-4bd5-b740-c0f7f6a6bcd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re, math, random, numpy as np\n",
    "from dataclasses import dataclass\n",
    "from typing import List, Dict, Tuple, Optional\n",
    "from collections import Counter\n",
    "try:\n",
    "    from xgboost import XGBClassifier\n",
    "    _HAVE_XGB = True\n",
    "except Exception:\n",
    "    _HAVE_XGB = False\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "random.seed(42); np.random.seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 273,
   "id": "ec5308b1-c439-42fd-bc95-be40fb9c951e",
   "metadata": {},
   "outputs": [],
   "source": [
    "ROUTES = [\"sql\",\"math\",\"policy\",\"factual\"]\n",
    "train_data = [\n",
    "    (\"select age, income from dataset where balance > 1000 limit 5\", \"sql\"),\n",
    "    (\"show me top 10 rows from dataset\", \"sql\"),\n",
    "    (\"SELECT * FROM dataset LIMIT 3\", \"sql\"),\n",
    "    (\"avg(income) by tenure where default=1\", \"sql\"),\n",
    "    (\"join users and orders by user_id\", \"sql\"),\n",
    "    (\"2+2*10 - 3\", \"math\"),\n",
    "    (\"compute 1000/25 + 17 - 3\", \"math\"),\n",
    "    (\"what is 3.1415 * 2?\", \"math\"),\n",
    "    (\"(12+8)/5 * 3 - 4\", \"math\"),\n",
    "    (\"Do we comply with PII masking in logs?\", \"policy\"),\n",
    "    (\"Outline SQL guardrails and DML restrictions\", \"policy\"),\n",
    "    (\"What are the logging redaction rules for SSNs?\", \"policy\"),\n",
    "    (\"Explain how we prevent unsafe SQL\", \"policy\"),\n",
    "    (\"What are common notes or patterns?\", \"factual\"),\n",
    "    (\"How many records mention chargeback risk?\", \"factual\"),\n",
    "    (\"Summarize key trends in notes\", \"factual\"),\n",
    "    (\"List frequent phrases about late fees\", \"factual\"),\n",
    "]\n",
    "X_txt, y_txt = zip(*train_data)\n",
    "vec = TfidfVectorizer(analyzer=\"char_wb\", ngram_range=(3,5), max_features=8000)\n",
    "Xv = vec.fit_transform(X_txt)\n",
    "_le = None\n",
    "if _HAVE_XGB:\n",
    "    _le = LabelEncoder().fit(y_txt)\n",
    "    y_enc = _le.transform(y_txt)\n",
    "    clf = XGBClassifier(\n",
    "        max_depth=4, n_estimators=120, learning_rate=0.2,\n",
    "        subsample=0.9, colsample_bytree=0.9, reg_lambda=1.0, random_state=42,\n",
    "        objective=\"multi:softprob\"\n",
    "    )\n",
    "    clf.fit(Xv.toarray(), y_enc)\n",
    "else:\n",
    "    clf = LogisticRegression(max_iter=500)\n",
    "    clf.fit(Xv, y_txt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 274,
   "id": "9be31d10-a196-4598-89cf-7ef841ac0a9a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "select * from dataset limit 5                 sql      conf=0.91\n",
      "2+2*10 - 3                                    math     conf=0.88\n",
      "Do we comply with PII masking in logs?        policy   conf=0.85\n",
      "Summarize key trends in notes                 factual  conf=0.72\n"
     ]
    }
   ],
   "source": [
    "def route_query(q: str):\n",
    "    Xq = vec.transform([q])\n",
    "    if _HAVE_XGB:\n",
    "        proba = clf.predict_proba(Xq.toarray())[0]\n",
    "        classes = _le.inverse_transform(np.arange(len(proba)))\n",
    "    else:\n",
    "        proba = clf.predict_proba(Xq)[0]\n",
    "        classes = clf.classes_\n",
    "    dist = {c: float(p) for c, p in zip(classes, proba)}\n",
    "    pred = classes[int(np.argmax(proba))]\n",
    "    conf = float(np.max(proba))\n",
    "    return str(pred), conf, dist\n",
    "for q in [\n",
    "    \"select * from dataset limit 5\",\n",
    "    \"2+2*10 - 3\",\n",
    "    \"Do we comply with PII masking in logs?\",\n",
    "    \"Summarize key trends in notes\",\n",
    "]:\n",
    "    r, c, d = route_query(q)\n",
    "    print(f\"{q[:44]:<44}  {r:7s}  conf={c:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 275,
   "id": "be14e48f-522a-4e89-abf9-dced7b448021",
   "metadata": {},
   "outputs": [],
   "source": [
    "_NUM_RE = re.compile(r\"[-+]?\\d*\\.?\\d+(?:[eE][-+]?\\d+)?\")\n",
    "def _tokens(s: str) -> List[str]:\n",
    "    return re.findall(r\"[A-Za-z0-9]+\", (s or \"\").lower())\n",
    "def span_overlap(answer: str, contexts: List[str]) -> float:\n",
    "    \"\"\"Token overlap ratio between answer and concatenated contexts.\"\"\"\n",
    "    a = Counter(_tokens(answer))\n",
    "    c = Counter(_tokens(\" \".join(contexts)))\n",
    "    if not a:\n",
    "        return 0.0\n",
    "    inter = sum(min(a[t], c.get(t,0)) for t in a)\n",
    "    total = sum(a.values())\n",
    "    return inter / max(1, total)\n",
    "def _nums(s: str) -> List[float]:\n",
    "    out = []\n",
    "    for m in _NUM_RE.findall(s or \"\"):\n",
    "        try:\n",
    "            out.append(float(m))\n",
    "        except Exception:\n",
    "            pass\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 276,
   "id": "b3a27ce7-fcd1-4514-a455-5c2bd12860f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers.utils import logging as hf_logging\n",
    "hf_logging.set_verbosity_error()  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 277,
   "id": "04ddaf01-7ad6-4e0a-a279-2145f24192f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def numeric_consistency(answer: str, contexts: List[str], eps: float = 1e-6, rel: float = 1e-3) -> float:\n",
    "    \"\"\"\n",
    "    Score  [0,1]: 1.0 if all numbers in answer appear (approximately) in contexts.\n",
    "    Allows either absolute eps or relative tolerance rel.\n",
    "    \"\"\"\n",
    "    ans_nums = _nums(answer)\n",
    "    ctx_nums = _nums(\" \".join(contexts))\n",
    "    if not ans_nums:\n",
    "        return 1.0  \n",
    "    ok = 0\n",
    "    for a in ans_nums:\n",
    "        match = any(abs(a - c) <= max(eps, rel * max(1.0, abs(c))) for c in ctx_nums)\n",
    "        if match:\n",
    "            ok += 1\n",
    "    return ok / len(ans_nums)\n",
    "_nli_loaded = False\n",
    "try:\n",
    "    from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "    import torch\n",
    "    _tok_nli = AutoTokenizer.from_pretrained(\"roberta-large-mnli\")\n",
    "    _mdl_nli = AutoModelForSequenceClassification.from_pretrained(\"roberta-large-mnli\")\n",
    "    _mdl_nli.eval()\n",
    "    _device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    _mdl_nli.to(_device)\n",
    "    _nli_loaded = True\n",
    "except Exception:\n",
    "    _nli_loaded = False\n",
    "def nli_entailment_prob(premise: str, hypothesis: str) -> Optional[float]:\n",
    "    \"\"\"Return P(entailment) using MNLI head if available, else None.\"\"\"\n",
    "    if not _nli_loaded:\n",
    "        return None\n",
    "    import torch\n",
    "    with torch.no_grad():\n",
    "        batch = _tok_nli(premise, hypothesis, truncation=True, max_length=512, return_tensors=\"pt\").to(_device)\n",
    "        logits = _mdl_nli(**batch).logits\n",
    "        probs = torch.softmax(logits, dim=-1)[0].tolist()\n",
    "        return float(probs[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 278,
   "id": "60cdc5ef-b1ed-471b-9117-032496bca5c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def verify_faithfulness(answer: str, contexts: List[str]) -> Dict[str, float]:\n",
    "    \"\"\"\n",
    "    Lightweight verifier: span overlap + numeric consistency  proxy faithfulness.\n",
    "    \"\"\"\n",
    "    so = span_overlap(answer, contexts)\n",
    "    num = numeric_consistency(answer, contexts)\n",
    "    faith = 0.5*so + 0.5*num\n",
    "    return {\"faithfulness\": float(faith), \"entailment_prob\": float(faith), \"mode\": \"overlap+numeric\"}\n",
    "def verify_bridge(answer: str, contexts: List[str], use_nli: bool = True) -> Dict[str, float]:\n",
    "    \"\"\"\n",
    "    Bridge verifier: if use_nli and model available  combine NLI with overlap/numeric;\n",
    "    else falls back to verify_faithfulness().\n",
    "    \"\"\"\n",
    "    base = verify_faithfulness(answer, contexts)\n",
    "    if not use_nli or not _nli_loaded or not contexts:\n",
    "        return base\n",
    "    K = 2\n",
    "    ctx = \" \".join(contexts[:K])\n",
    "    p1 = nli_entailment_prob(ctx, answer)   \n",
    "    p2 = nli_entailment_prob(answer, ctx)   \n",
    "    if p1 is None or p2 is None:\n",
    "        return base\n",
    "    nli = 0.5*(p1+p2)\n",
    "    faith = 0.6*nli + 0.2*base[\"faithfulness\"] + 0.2*base[\"entailment_prob\"]\n",
    "    return {\"faithfulness\": float(faith), \"entailment_prob\": float(nli), \"mode\": \"mnli+overlap\"}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 279,
   "id": "bd4d1872-8bcc-416e-a9ba-a8f9056bfe18",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Router v1 predictions ===\n",
      "- select age, income from dataset where balance > 10  SQL       conf=0.96  dist={np.str_('factual'): 0.01, np.str_('math'): 0.02, np.str_('policy'): 0.01, np.str_('sql'): 0.96}\n",
      "- 2+2*10 - 3                                          MATH      conf=0.88  dist={np.str_('factual'): 0.03, np.str_('math'): 0.88, np.str_('policy'): 0.02, np.str_('sql'): 0.07}\n",
      "- Do we comply with PII masking in logs?              POLICY    conf=0.85  dist={np.str_('factual'): 0.02, np.str_('math'): 0.02, np.str_('policy'): 0.85, np.str_('sql'): 0.11}\n",
      "- What are common notes or patterns?                  FACTUAL   conf=0.91  dist={np.str_('factual'): 0.91, np.str_('math'): 0.04, np.str_('policy'): 0.01, np.str_('sql'): 0.05}\n",
      "\n",
      "=== Verifier tests ===\n",
      "\n",
      "Q: select age, income from dataset where balance > 1000 limit 5\n",
      "A: Here are 5 rows with age and income where balance > 1000.\n",
      "ctx: We selected 5 results with columns age and income from dataset where bal...\n",
      " - verify_faithfulness : {'faithfulness': 0.8636363636363636, 'entailment_prob': 0.8636363636363636, 'mode': 'overlap+numeric'}\n",
      " - verify_bridge (NLI) : {'faithfulness': 0.6951192842288451, 'entailment_prob': 0.5827745646238327, 'mode': 'mnli+overlap'}   [NLI loaded?  True ]\n",
      " - verify_bridge (off) : {'faithfulness': 0.8636363636363636, 'entailment_prob': 0.8636363636363636, 'mode': 'overlap+numeric'}\n",
      "\n",
      "Q: 2+2*10 - 3\n",
      "A: Result is 19\n",
      "ctx: Compute multiplication before addition: 2 + (2*10) - 3 = 19...\n",
      " - verify_faithfulness : {'faithfulness': 0.6666666666666666, 'entailment_prob': 0.6666666666666666, 'mode': 'overlap+numeric'}\n",
      " - verify_bridge (NLI) : {'faithfulness': 0.5635628099242845, 'entailment_prob': 0.49482690542936325, 'mode': 'mnli+overlap'}   [NLI loaded?  True ]\n",
      " - verify_bridge (off) : {'faithfulness': 0.6666666666666666, 'entailment_prob': 0.6666666666666666, 'mode': 'overlap+numeric'}\n",
      "\n",
      "Q: Do we comply with PII masking in logs?\n",
      "A: YesPII like email and SSN are redacted in logs; DML is denied and queries are limited.\n",
      "ctx: Policy: redact PII (emails, SSNs) in logs. Enforce SELECT-only and LIMIT...\n",
      " - verify_faithfulness : {'faithfulness': 0.6176470588235294, 'entailment_prob': 0.6176470588235294, 'mode': 'overlap+numeric'}\n",
      " - verify_bridge (NLI) : {'faithfulness': 0.6887674002086415, 'entailment_prob': 0.7361809611320496, 'mode': 'mnli+overlap'}   [NLI loaded?  True ]\n",
      " - verify_bridge (off) : {'faithfulness': 0.6176470588235294, 'entailment_prob': 0.6176470588235294, 'mode': 'overlap+numeric'}\n",
      "\n",
      "Q: What are common notes or patterns?\n",
      "A: Common phrases include 'late fee', 'chargeback risk', and 'manual review scheduled'.\n",
      "ctx: Many notes mention late fee reversal, chargeback risk due to suspicious ...\n",
      " - verify_faithfulness : {'faithfulness': 0.8636363636363636, 'entailment_prob': 0.8636363636363636, 'mode': 'overlap+numeric'}\n",
      " - verify_bridge (NLI) : {'faithfulness': 0.6416266979141669, 'entailment_prob': 0.49362025409936905, 'mode': 'mnli+overlap'}   [NLI loaded?  True ]\n",
      " - verify_bridge (off) : {'faithfulness': 0.8636363636363636, 'entailment_prob': 0.8636363636363636, 'mode': 'overlap+numeric'}\n"
     ]
    }
   ],
   "source": [
    "samples = [\n",
    "    (\"select age, income from dataset where balance > 1000 limit 5\",\n",
    "     \"Here are 5 rows with age and income where balance > 1000.\",\n",
    "     [\"We selected 5 results with columns age and income from dataset where balance > 1000.\"]),\n",
    "    (\"2+2*10 - 3\",\n",
    "     \"Result is 19\",\n",
    "     [\"Compute multiplication before addition: 2 + (2*10) - 3 = 19\"]),\n",
    "    (\"Do we comply with PII masking in logs?\",\n",
    "     \"YesPII like email and SSN are redacted in logs; DML is denied and queries are limited.\",\n",
    "     [\"Policy: redact PII (emails, SSNs) in logs. Enforce SELECT-only and LIMIT <= 100.\"]),\n",
    "    (\"What are common notes or patterns?\",\n",
    "     \"Common phrases include 'late fee', 'chargeback risk', and 'manual review scheduled'.\",\n",
    "     [\"Many notes mention late fee reversal, chargeback risk due to suspicious transaction, and manual review scheduled.\"])\n",
    "]\n",
    "print(\"=== Router v1 predictions ===\")\n",
    "for q, _, _ in samples:\n",
    "    route, conf, dist = route_query(q)\n",
    "    print(f\"- {q[:50]:<50}  {route.upper():8}  conf={conf:.2f}  dist={ {k:round(v,2) for k,v in dist.items()} }\")\n",
    "print(\"\\n=== Verifier tests ===\")\n",
    "for q, a, ctx in samples:\n",
    "    v1 = verify_faithfulness(a, ctx)\n",
    "    v2_on  = verify_bridge(a, ctx, use_nli=True)\n",
    "    v2_off = verify_bridge(a, ctx, use_nli=False)\n",
    "    print(f\"\\nQ: {q}\\nA: {a}\\nctx: {ctx[0][:72]}...\")\n",
    "    print(\" - verify_faithfulness :\", v1)\n",
    "    print(\" - verify_bridge (NLI) :\", v2_on, \"  [NLI loaded? \", _nli_loaded, \"]\")\n",
    "    print(\" - verify_bridge (off) :\", v2_off)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 280,
   "id": "0b10f028-6503-47f6-855f-0c15eddac824",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re, math, random, time, json\n",
    "from dataclasses import dataclass\n",
    "from typing import List, Dict, Tuple, Optional\n",
    "from collections import Counter\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 281,
   "id": "ed7ab781-6015-42a8-a8e0-c031ad3e9992",
   "metadata": {},
   "outputs": [],
   "source": [
    "CHAOS = {\n",
    "    \"disable_ce\": False,   \n",
    "    \"force_bm25\": False,   \n",
    "    \"verifier_v2\": False,  \n",
    "}\n",
    "def chaos_disable_ce_reranker(flag: bool = True):\n",
    "    CHAOS[\"disable_ce\"] = bool(flag)\n",
    "    print(f\"Chaos: CE reranker disabled={CHAOS['disable_ce']}\")\n",
    "def chaos_force_bm25(flag: bool = True):\n",
    "    CHAOS[\"force_bm25\"] = bool(flag)\n",
    "    print(f\"Chaos: force BM25 fallback={CHAOS['force_bm25']}\")\n",
    "def chaos_set_verifier_v2(flag: bool = True):\n",
    "    CHAOS[\"verifier_v2\"] = bool(flag)\n",
    "    print(f\"Chaos: verifier v2 (NLI)={CHAOS['verifier_v2']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 282,
   "id": "9d26fbe8-cc94-4c9c-a76a-64be9ef11a0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "_SQL_DDL_DML_BLOCKLIST = {\n",
    "    \"DROP\", \"TRUNCATE\", \"DELETE\", \"UPDATE\", \"INSERT\",\n",
    "    \"ALTER\", \"CREATE\", \"RENAME\", \"GRANT\", \"REVOKE\", \"MERGE\",\n",
    "}\n",
    "_SQL_SELECT_RE = re.compile(r\"(?is)\\bselect\\b.*?(?:;|$)\")\n",
    "_SQL_COMMENT_LINE = re.compile(r\"(?m)^[ \\t]*--.*?$\")\n",
    "_SQL_COMMENT_BLOCK = re.compile(r\"/\\*.*?\\*/\", re.DOTALL)\n",
    "def _strip_sql_comments(sql: str) -> str:\n",
    "    if not sql: return \"\"\n",
    "    sql = _SQL_COMMENT_BLOCK.sub(\"\", sql)\n",
    "    sql = _SQL_COMMENT_LINE.sub(\"\", sql)\n",
    "    return sql\n",
    "def extract_sql(text: str) -> Optional[str]:\n",
    "    \"\"\"\n",
    "    Extract the first SELECT statement from free-form text / code fences.\n",
    "    Returns None if no SQL-ish SELECT is found.\n",
    "    \"\"\"\n",
    "    if not text or not isinstance(text, str): \n",
    "        return None\n",
    "    s = _strip_sql_comments(text)\n",
    "    m = re.search(r\"```(?:sql)?\\s*(.*?)```\", s, flags=re.IGNORECASE | re.DOTALL)\n",
    "    if m:\n",
    "        s = m.group(1)\n",
    "    m = _SQL_SELECT_RE.search(s or \"\")\n",
    "    return m.group(0).strip(\" ;\") if m else None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 283,
   "id": "530182f1-c13f-4a10-bc7a-667de363f4e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sanitize_select_limit(sql: str, max_limit: int = 100) -> str:\n",
    "    \"\"\"\n",
    "    Enforce:\n",
    "      - Single SELECT statement only (no extra ';' commands).\n",
    "      - No DDL/DML keywords from blocklist.\n",
    "      - Ensure LIMIT exists and  max_limit (append or clamp).\n",
    "    Raises ValueError for violations.\n",
    "    \"\"\"\n",
    "    if not sql: \n",
    "        raise ValueError(\"empty SQL\")\n",
    "    s = _strip_sql_comments(sql).strip()\n",
    "    parts = [p.strip() for p in s.split(\";\") if p.strip()]\n",
    "    if len(parts) != 1:\n",
    "        raise ValueError(\"multiple statements not allowed\")\n",
    "    s = parts[0]\n",
    "    if not re.match(r\"(?is)^\\s*select\\b\", s):\n",
    "        raise ValueError(\"only SELECT statements allowed\")\n",
    "    upper = re.sub(r\"'[^']*'|\\\"[^\\\"]*\\\"\", \"\", s).upper()\n",
    "    for bad in _SQL_DDL_DML_BLOCKLIST:\n",
    "        if re.search(rf\"\\b{bad}\\b\", upper):\n",
    "            raise ValueError(f\"disallowed keyword: {bad}\")\n",
    "    m = re.search(r\"(?is)\\blimit\\s+(\\d+)\\b\", s)\n",
    "    if m:\n",
    "        lim = int(m.group(1))\n",
    "        if lim > max_limit:\n",
    "            s = re.sub(r\"(?is)\\blimit\\s+\\d+\\b\", f\"LIMIT {max_limit}\", s)\n",
    "    else:\n",
    "        s = s.rstrip() + f\" LIMIT {max_limit}\"\n",
    "    return s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 284,
   "id": "f202b341-43d8-4229-a7b6-9e58c8ed7de6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " SELECT-only enforced with LIMIT :: select age, income from dataset where balance > 1000 limit 5  OK\n",
      " LIMIT appended :: SELECT * FROM dataset  OK\n",
      " LIMIT clamped :: select * from t limit 1000  OK\n",
      " multi-statement rejected :: select * from users; DROP TABLE users;  BLOCK\n",
      " blocklist DELETE blocked :: select * from u where 1=1; -- harmless\n",
      " delete from u  BLOCK\n",
      " no SQL extracted :: Show me top 10 rows  NONE\n",
      "\n",
      "SQL guard unit tests passed 6/6\n"
     ]
    }
   ],
   "source": [
    "def _test_sql_guard():\n",
    "    tests = [\n",
    "        (\"select age, income from dataset where balance > 1000 limit 5\",\n",
    "         True, \"SELECT-only enforced with LIMIT\"),\n",
    "        (\"SELECT * FROM dataset\", True, \"LIMIT appended\"),\n",
    "        (\"select * from t limit 1000\", True, \"LIMIT clamped\"),\n",
    "        (\"select * from users; DROP TABLE users;\", False, \"multi-statement rejected\"),\n",
    "        (\"select * from u where 1=1; -- harmless\\n delete from u\", False, \"blocklist DELETE blocked\"),\n",
    "        (\"Show me top 10 rows\", None, \"no SQL extracted\"),\n",
    "    ]\n",
    "    ok = 0\n",
    "    for q, expect, note in tests:\n",
    "        try:\n",
    "            sql = extract_sql(q)\n",
    "            if expect is None:\n",
    "                assert sql is None, f\"Expected no SQL. Got: {sql}\"\n",
    "            else:\n",
    "                assert sql is not None, \"Expected SQL extraction\"\n",
    "                s2 = sanitize_select_limit(sql, max_limit=100)\n",
    "                assert re.search(r\"(?is)^\\s*select\\b\", s2), \"Must start with SELECT\"\n",
    "                assert re.search(r\"(?is)\\blimit\\s+\\d+\\b\", s2), \"Must have LIMIT\"\n",
    "            print(f\" {note} :: {q[:60]}  {('OK' if expect else 'BLOCK' if expect is False else 'NONE')}\")\n",
    "            ok += 1\n",
    "        except Exception as e:\n",
    "            if expect is False:  \n",
    "                print(f\" BLOCKED ({note}) :: {q[:60]}  {e}\")\n",
    "                ok += 1\n",
    "            else:\n",
    "                print(f\" FAIL ({note}) :: {q[:60]}  {e}\")\n",
    "    print(f\"\\nSQL guard unit tests passed {ok}/{len(tests)}\")\n",
    "_test_sql_guard()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 285,
   "id": "9d8274b8-d0b7-4f76-933c-73c65637eb0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "DEFAULT_DOCS = [\n",
    "    {\"id\": \"d1\", \"text\": \"Rust pipelines are fast and memory-safe using async with Tokio and Axum.\"},\n",
    "    {\"id\": \"d2\", \"text\": \"RAG pipelines retrieve relevant chunks and augment the prompt for the LLM.\"},\n",
    "    {\"id\": \"d3\", \"text\": \"Hybrid retrieval with BM25 and vector kNN improves recall and faithfulness.\"},\n",
    "    {\"id\": \"d4\", \"text\": \"LangChain helps prototype chains, tools, and agents quickly in Python.\"},\n",
    "    {\"id\": \"d5\", \"text\": \"Tantivy provides BM25 in Rust; pgvector or FAISS provide dense similarity.\"},\n",
    "]\n",
    "TOK = re.compile(r\"[A-Za-z0-9]+\")\n",
    "def _toks(s: str) -> List[str]:\n",
    "    return [t.lower() for t in TOK.findall(s or \"\")]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 286,
   "id": "8ca8d42f-1915-46a9-a33f-398b70821ee0",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TinyDense:\n",
    "    def __init__(self, docs: List[Dict[str,str]], dim: int = 384):\n",
    "        self.ids = [d[\"id\"] for d in docs]\n",
    "        self.txt = [d[\"text\"] for d in docs]\n",
    "        self.dim = dim\n",
    "        self.embs = self._hash_embed(self.txt).astype(np.float32)\n",
    "        self.embs /= (np.linalg.norm(self.embs, axis=1, keepdims=True) + 1e-9)\n",
    "    def _hash_embed(self, texts: List[str]) -> np.ndarray:\n",
    "        def vec_for_token(tok: str) -> np.ndarray:\n",
    "            rng = np.random.default_rng(abs(hash(tok)) % (2**32))\n",
    "            return rng.standard_normal(self.dim)\n",
    "        mats = []\n",
    "        for s in texts:\n",
    "            toks = _toks(s)\n",
    "            if not toks: \n",
    "                mats.append(np.zeros(self.dim)); continue\n",
    "            v = np.sum([vec_for_token(t) for t in toks], axis=0)\n",
    "            mats.append(v)\n",
    "        return np.vstack(mats)\n",
    "    def encode_query(self, q: str) -> np.ndarray:\n",
    "        v = self._hash_embed([q])[0]\n",
    "        v = v / (np.linalg.norm(v) + 1e-9)\n",
    "        return v.astype(np.float32)\n",
    "    def topk(self, q: str, k: int = 5) -> List[Dict]:\n",
    "        qv = self.encode_query(q)\n",
    "        sims = (self.embs @ qv).ravel()\n",
    "        order = np.argsort(-sims)[:k]\n",
    "        return [{\"id\": self.ids[i], \"text\": self.txt[i], \"score\": float(sims[i])} for i in order]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 287,
   "id": "fad7113c-559f-45e3-9436-dca997af1df1",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TinyBM25:\n",
    "    def __init__(self, docs: List[Dict[str,str]]):\n",
    "        self.ids = [d[\"id\"] for d in docs]\n",
    "        self.txt = [d[\"text\"] for d in docs]\n",
    "        self.tokens = [_toks(t) for t in self.txt]\n",
    "        self.df = Counter()\n",
    "        for toks in self.tokens:\n",
    "            for t in set(toks):\n",
    "                self.df[t] += 1\n",
    "        self.avgdl = np.mean([len(t) for t in self.tokens]) if self.tokens else 1.0\n",
    "        self.N = len(self.tokens)\n",
    "        self.k1, self.b = 1.5, 0.75\n",
    "    def _score(self, qtok: List[str], idx: int) -> float:\n",
    "        toks = self.tokens[idx]; dl = len(toks)\n",
    "        tf = Counter(toks); score = 0.0\n",
    "        for t in qtok:\n",
    "            n_q = self.df.get(t, 0)\n",
    "            if n_q == 0: \n",
    "                continue\n",
    "            idf = math.log(1 + (self.N - n_q + 0.5) / (n_q + 0.5))\n",
    "            f = tf.get(t, 0)\n",
    "            denom = f + self.k1 * (1 - self.b + self.b * dl / self.avgdl)\n",
    "            score += idf * (f * (self.k1 + 1)) / (denom if denom else 1.0)\n",
    "        return score\n",
    "    def topk(self, q: str, k: int = 5) -> List[Dict]:\n",
    "        qtok = _toks(q)\n",
    "        scores = [self._score(qtok, i) for i in range(self.N)]\n",
    "        order = np.argsort(-np.array(scores))[:k]\n",
    "        return [{\"id\": self.ids[i], \"text\": self.txt[i], \"score\": float(scores[i])} for i in order]\n",
    "DENSE = TinyDense(DEFAULT_DOCS)\n",
    "BM25 = TinyBM25(DEFAULT_DOCS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 288,
   "id": "10bc2f43-913b-453e-9064-56cf3286ab08",
   "metadata": {},
   "outputs": [],
   "source": [
    "def rrf_fuse(a: List[Dict], b: List[Dict], k: int = 5, k_rrf: float = 60.0) -> List[Dict]:\n",
    "    ids = {x[\"id\"] for x in a} | {x[\"id\"] for x in b}\n",
    "    ra = {d[\"id\"]: i+1 for i, d in enumerate(a)}\n",
    "    rb = {d[\"id\"]: i+1 for i, d in enumerate(b)}\n",
    "    fused = []\n",
    "    for id_ in ids:\n",
    "        sa = 1.0 / (k_rrf + ra.get(id_, 1e9))\n",
    "        sb = 1.0 / (k_rrf + rb.get(id_, 1e9))\n",
    "        txt = next((d[\"text\"] for d in a if d[\"id\"] == id_), None) or next((d[\"text\"] for d in b if d[\"id\"] == id_), \"\")\n",
    "        fused.append({\"id\": id_, \"text\": txt, \"score\": float(sa + sb)})\n",
    "    fused.sort(key=lambda x: -x[\"score\"])\n",
    "    return fused[:k]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 289,
   "id": "48b750a3-f1c3-4a94-bd8e-0087d57c16b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "    from sklearn.linear_model import LogisticRegression\n",
    "    _HAVE_SK = True\n",
    "except Exception:\n",
    "    _HAVE_SK = False\n",
    "ROUTES = [\"sql\",\"math\",\"policy\",\"factual\"]\n",
    "_train = [\n",
    "    (\"select age, income from dataset where balance > 1000 limit 5\", \"sql\"),\n",
    "    (\"show me top 10 rows from dataset\", \"sql\"),\n",
    "    (\"SELECT * FROM dataset LIMIT 3\", \"sql\"),\n",
    "    (\"avg(income) by tenure where default=1\", \"sql\"),\n",
    "    (\"2+2*10 - 3\", \"math\"),\n",
    "    (\"compute 1000/25 + 17 - 3\", \"math\"),\n",
    "    (\"what is 3.1415 * 2?\", \"math\"),\n",
    "    (\"(12+8)/5 * 3 - 4\", \"math\"),\n",
    "    (\"Do we comply with PII masking in logs?\", \"policy\"),\n",
    "    (\"Outline SQL guardrails and DML restrictions\", \"policy\"),\n",
    "    (\"What are the logging redaction rules for SSNs?\", \"policy\"),\n",
    "    (\"Explain how we prevent unsafe SQL\", \"policy\"),\n",
    "    (\"What are common notes or patterns?\", \"factual\"),\n",
    "    (\"How many records mention chargeback risk?\", \"factual\"),\n",
    "    (\"Summarize key trends in notes\", \"factual\"),\n",
    "    (\"List frequent phrases about late fees\", \"factual\"),\n",
    "]\n",
    "if _HAVE_SK:\n",
    "    _vec = TfidfVectorizer(analyzer=\"char_wb\", ngram_range=(3,5), max_features=8000)\n",
    "    X = _vec.fit_transform([t for t,_ in _train])\n",
    "    y = [c for _,c in _train]\n",
    "    _clf = LogisticRegression(max_iter=500).fit(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 290,
   "id": "4666dd79-51b2-4073-98c2-61d4675bb19e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def route_query(q: str) -> Tuple[str, float, Dict[str,float]]:\n",
    "    if _HAVE_SK:\n",
    "        Xq = _vec.transform([q])\n",
    "        proba = _clf.predict_proba(Xq)[0]\n",
    "        classes = _clf.classes_\n",
    "        pred = classes[int(np.argmax(proba))]\n",
    "        return str(pred), float(np.max(proba)), {c: float(p) for c,p in zip(classes, proba)}\n",
    "    s = q.lower()\n",
    "    if re.search(r\"\\bselect\\b|\\bfrom\\b|\\blimit\\b\", s): return \"sql\", 0.8, {}\n",
    "    if re.search(r\"[\\d\\)\\(][\\+\\-\\*/^][\\d\\(\\)]\", s):    return \"math\", 0.7, {}\n",
    "    if \"policy\" in s or \"comply\" in s or \"pii\" in s:   return \"policy\", 0.6, {}\n",
    "    return \"factual\", 0.5, {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 291,
   "id": "28076b73-fca7-478f-a330-e235ed1c223a",
   "metadata": {},
   "outputs": [],
   "source": [
    "_NUM_RE = re.compile(r\"[-+]?\\d*\\.?\\d+(?:[eE][-+]?\\d+)?\")\n",
    "def _nums(s: str) -> List[float]:\n",
    "    out = []\n",
    "    for m in _NUM_RE.findall(s or \"\"):\n",
    "        try: out.append(float(m))\n",
    "        except: pass\n",
    "    return out\n",
    "def span_overlap(answer: str, contexts: List[str]) -> float:\n",
    "    a = Counter(_toks(answer)); c = Counter(_toks(\" \".join(contexts)))\n",
    "    if not a: return 0.0\n",
    "    inter = sum(min(a[t], c.get(t,0)) for t in a)\n",
    "    return inter / max(1, sum(a.values()))\n",
    "def numeric_consistency(answer: str, contexts: List[str], eps=1e-6, rel=1e-3) -> float:\n",
    "    ans_nums = _nums(answer); ctx_nums = _nums(\" \".join(contexts))\n",
    "    if not ans_nums: return 1.0\n",
    "    ok = 0\n",
    "    for a in ans_nums:\n",
    "        if any(abs(a-c) <= max(eps, rel*max(1.0, abs(c))) for c in ctx_nums):\n",
    "            ok += 1\n",
    "    return ok / len(ans_nums)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 292,
   "id": "8b2357f0-0962-467a-9181-f8ef17e418af",
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "    import torch\n",
    "    _tok_nli = AutoTokenizer.from_pretrained(\"roberta-large-mnli\")\n",
    "    _mdl_nli = AutoModelForSequenceClassification.from_pretrained(\"roberta-large-mnli\")\n",
    "    _mdl_nli.eval()\n",
    "    _dev = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    _mdl_nli.to(_dev)\n",
    "    _HAVE_NLI = True\n",
    "except Exception:\n",
    "    _HAVE_NLI = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 293,
   "id": "f9258542-ad6f-4df3-b7e0-ad5a82ef3106",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _nli_entail(prem: str, hyp: str) -> Optional[float]:\n",
    "    if not _HAVE_NLI: return None\n",
    "    import torch\n",
    "    with torch.no_grad():\n",
    "        batch = _tok_nli(prem, hyp, truncation=True, max_length=512, return_tensors=\"pt\").to(_dev)\n",
    "        logits = _mdl_nli(**batch).logits\n",
    "        probs = torch.softmax(logits, dim=-1)[0].tolist()  \n",
    "        return float(probs[2])\n",
    "def verify_faithfulness(answer: str, ctxs: List[str]) -> Dict[str, float]:\n",
    "    so = span_overlap(answer, ctxs)\n",
    "    num = numeric_consistency(answer, ctxs)\n",
    "    faith = 0.5*so + 0.5*num\n",
    "    return {\"faithfulness\": float(faith), \"entailment_prob\": float(faith), \"mode\": \"overlap+numeric\"}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 294,
   "id": "42d6f410-60f4-4170-991c-63af99815efa",
   "metadata": {},
   "outputs": [],
   "source": [
    "def verify_bridge(answer: str, ctxs: List[str]) -> Dict[str, float]:\n",
    "    base = verify_faithfulness(answer, ctxs)\n",
    "    if not CHAOS[\"verifier_v2\"] or not _HAVE_NLI or not ctxs:\n",
    "        return base\n",
    "    K = 2\n",
    "    ctx = \" \".join(ctxs[:K])\n",
    "    p1 = _nli_entail(ctx, answer)\n",
    "    p2 = _nli_entail(answer, ctx)\n",
    "    if p1 is None or p2 is None:\n",
    "        return base\n",
    "    nli = 0.5*(p1+p2)\n",
    "    faith = 0.6*nli + 0.2*base[\"faithfulness\"] + 0.2*base[\"entailment_prob\"]\n",
    "    return {\"faithfulness\": float(faith), \"entailment_prob\": float(nli), \"mode\": \"mnli+overlap\"}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 295,
   "id": "e24b15e3-aade-43b9-9ab0-1575f874954d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import ast\n",
    "_ALLOWED = (ast.Expression, ast.BinOp, ast.UnaryOp, ast.Num, ast.Load,\n",
    "            ast.Add, ast.Sub, ast.Mult, ast.Div, ast.Pow, ast.Mod, ast.USub, ast.UAdd, ast.FloorDiv, ast.Constant, ast.Call, ast.Name)\n",
    "def _safe_eval(expr: str) -> float:\n",
    "    \"\"\"\n",
    "    Evaluate arithmetic-only expressions safely.\n",
    "    Supports + - * / // % ** and parentheses. No names/calls.\n",
    "    \"\"\"\n",
    "    tree = ast.parse(expr, mode=\"eval\")\n",
    "    for node in ast.walk(tree):\n",
    "        if isinstance(node, ast.Call) or isinstance(node, ast.Name):\n",
    "            raise ValueError(\"names/calls not allowed\")\n",
    "        if not isinstance(node, _ALLOWED):\n",
    "            raise ValueError(f\"disallowed: {type(node).__name__}\")\n",
    "    return float(eval(compile(tree, \"<math>\", \"eval\"), {\"__builtins__\": {}}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 296,
   "id": "d769e033-1232-4acb-ba7d-e5d3b57779d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def retrieve(query: str, top_k: int = 3) -> List[Dict]:\n",
    "    bm = BM25.topk(query, top_k)\n",
    "    if CHAOS[\"force_bm25\"]:\n",
    "        return bm\n",
    "    dn = DENSE.topk(query, top_k)\n",
    "    fused = rrf_fuse(dn, bm, k=top_k)\n",
    "    return fused\n",
    "def pipeline(query: str, max_sql_limit: int = 100) -> Dict:\n",
    "    route, conf, _ = route_query(query)\n",
    "    if route == \"sql\":\n",
    "        sql = extract_sql(query)\n",
    "        if not sql:\n",
    "            return {\"action\": \"clarify\", \"payload\": {\"reason\": \"no_sql_detected\"}, \"citations\": []}\n",
    "        try:\n",
    "            sql2 = sanitize_select_limit(sql, max_limit=max_sql_limit)\n",
    "            return {\"action\": \"run_sql\", \"payload\": {\"sql\": sql2}, \"citations\": [], \"confidence\": conf}\n",
    "        except Exception as e:\n",
    "            return {\"action\": \"clarify\", \"payload\": {\"reason\": \"sql_blocked\", \"error\": str(e)}, \"citations\": [], \"confidence\": conf}\n",
    "    elif route == \"math\":\n",
    "        try:\n",
    "            val = _safe_eval(query)\n",
    "            return {\"action\": \"respond\", \"payload\": {\"numeric\": val}, \"citations\": [], \"confidence\": conf}\n",
    "        except Exception as e:\n",
    "            return {\"action\": \"clarify\", \"payload\": {\"reason\": \"bad_math\", \"error\": str(e)}, \"citations\": [], \"confidence\": conf}\n",
    "    elif route == \"factual\":\n",
    "        ctx = retrieve(query, top_k=3)\n",
    "        ans = f\"Based on the retrieved notes, key themes emerge across {len(ctx)} snippets.\"\n",
    "        ctx_texts = [c[\"text\"] for c in ctx]\n",
    "        ver = verify_bridge(ans, ctx_texts)\n",
    "        return {\"action\": \"respond\", \"payload\": {\"text\": ans, \"verify\": ver}, \"citations\": [{\"id\": c[\"id\"]} for c in ctx], \"confidence\": conf}\n",
    "    else:  \n",
    "        return {\"action\": \"clarify\", \"payload\": {\"reason\": \"policy_requires_confirmation\"}, \"citations\": [], \"confidence\": conf}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 297,
   "id": "5168c00b-de00-4b12-b8a1-63b72641215c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== GOLDEN (baseline) ===\n",
      " sql       run_sql    :: must be SELECT ... LIMIT\n",
      " math      respond    :: must yield numeric\n",
      " factual   respond    :: must respond with citation\n",
      " policy    clarify    :: must clarify/ticket\n",
      "\n",
      "Golden pass rate: 100.0%  (4/4)\n"
     ]
    }
   ],
   "source": [
    "GOLDEN = [\n",
    "    {\"route\": \"sql\",     \"query\": \"select age, income and balance where default = 1\",     \"expect\": \"run_sql\"},\n",
    "    {\"route\": \"math\",    \"query\": \"2+2*10 - 3\",                                            \"expect\": \"respond\"},\n",
    "    {\"route\": \"factual\", \"query\": \"What are common notes for customers with late fees?\",   \"expect\": \"respond_with_citation\"},\n",
    "    {\"route\": \"policy\",  \"query\": \"Do we comply with PII masking in logs?\",               \"expect\": \"clarify_or_ticket\"},\n",
    "]\n",
    "def run_golden(verbose: bool = True) -> Tuple[float, List[Dict]]:\n",
    "    rows = []\n",
    "    passed = 0\n",
    "    for g in GOLDEN:\n",
    "        out = pipeline(g[\"query\"])\n",
    "        ok = False\n",
    "        reason = \"\"\n",
    "        if g[\"route\"] == \"sql\":\n",
    "            ok = (out[\"action\"] == \"run_sql\" and\n",
    "                  re.search(r\"(?is)^\\s*select\\b\", out[\"payload\"].get(\"sql\",\"\")) and\n",
    "                  re.search(r\"(?is)\\blimit\\s+\\d+\\b\", out[\"payload\"].get(\"sql\",\"\")))\n",
    "            reason = \"must be SELECT ... LIMIT\"\n",
    "        elif g[\"route\"] == \"math\":\n",
    "            ok = (out[\"action\"] == \"respond\" and isinstance(out[\"payload\"].get(\"numeric\", None), float))\n",
    "            reason = \"must yield numeric\"\n",
    "        elif g[\"route\"] == \"factual\":\n",
    "            ok = (out[\"action\"] == \"respond\" and len(out.get(\"citations\", [])) > 0)\n",
    "            reason = \"must respond with citation\"\n",
    "        elif g[\"route\"] == \"policy\":\n",
    "            ok = (out[\"action\"] in (\"clarify\",\"ticket\"))\n",
    "            reason = \"must clarify/ticket\"\n",
    "        rows.append({\"route\": g[\"route\"], \"query\": g[\"query\"], \"action\": out[\"action\"], \"ok\": bool(ok), \"reason\": reason, \"out\": out})\n",
    "        if ok: passed += 1\n",
    "        if verbose:\n",
    "            badge = \"\" if ok else \"\"\n",
    "            print(f\"{badge} {g['route']:<8}  {out['action']:<10} :: {reason}\")\n",
    "    rate = 100.0 * passed / max(1, len(GOLDEN))\n",
    "    print(f\"\\nGolden pass rate: {rate:.1f}%  ({passed}/{len(GOLDEN)})\")\n",
    "    assert rate >= 95.0, f\"Golden pass rate below threshold: {rate:.1f}%\"\n",
    "    return rate, rows\n",
    "print(\"\\n=== GOLDEN (baseline) ===\")\n",
    "baseline_rate, baseline_rows = run_golden(verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 298,
   "id": "d55c2413-640c-4d20-bce6-fcf5bf859f8c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== CHAOS: Disable CE reranker ===\n",
      "Chaos: CE reranker disabled=True\n",
      "\n",
      "Golden pass rate: 100.0%  (4/4)\n",
      "Pass rate (CE off): 100.0%\n",
      "\n",
      "=== CHAOS: Force BM25 fallback ===\n",
      "Chaos: force BM25 fallback=True\n",
      "\n",
      "Golden pass rate: 100.0%  (4/4)\n",
      "Pass rate (BM25 only): 100.0%\n",
      "\n",
      "=== CHAOS: Verifier v2 ON (NLI if available) ===\n",
      "Chaos: verifier v2 (NLI)=True\n",
      "\n",
      "Golden pass rate: 100.0%  (4/4)\n",
      "Pass rate (verifier v2): 100.0%\n",
      "Chaos: CE reranker disabled=False\n",
      "Chaos: force BM25 fallback=False\n",
      "Chaos: verifier v2 (NLI)=False\n",
      "\n",
      "Chaos toggles reset.\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n=== CHAOS: Disable CE reranker ===\")\n",
    "chaos_disable_ce_reranker(True)\n",
    "rate_ce_off, _ = run_golden(verbose=False)\n",
    "print(f\"Pass rate (CE off): {rate_ce_off:.1f}%\")\n",
    "print(\"\\n=== CHAOS: Force BM25 fallback ===\")\n",
    "chaos_force_bm25(True)\n",
    "rate_bm25_only, _ = run_golden(verbose=False)\n",
    "print(f\"Pass rate (BM25 only): {rate_bm25_only:.1f}%\")\n",
    "print(\"\\n=== CHAOS: Verifier v2 ON (NLI if available) ===\")\n",
    "chaos_set_verifier_v2(True)\n",
    "rate_v2, _ = run_golden(verbose=False)\n",
    "print(f\"Pass rate (verifier v2): {rate_v2:.1f}%\")\n",
    "chaos_disable_ce_reranker(False)\n",
    "chaos_force_bm25(False)\n",
    "chaos_set_verifier_v2(False)\n",
    "print(\"\\nChaos toggles reset.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 299,
   "id": "e663f71d-1722-481c-8893-b2826e5fcafe",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time, math, random, statistics, concurrent.futures as cf\n",
    "from typing import List, Dict, Optional\n",
    "import requests\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 300,
   "id": "e1251c07-179a-452e-bc64-ea1277aeabfe",
   "metadata": {},
   "outputs": [],
   "source": [
    "def rrf_fuse(dense: List[Dict], bm25: List[Dict], k: int = 5, k_rrf: float = 60.0) -> List[Dict]:\n",
    "    ids = {x[\"id\"] for x in dense} | {x[\"id\"] for x in bm25}\n",
    "    rd = {d[\"id\"]: i+1 for i, d in enumerate(dense)}\n",
    "    rb = {d[\"id\"]: i+1 for i, d in enumerate(bm25)}\n",
    "    out = []\n",
    "    for _id in ids:\n",
    "        sd = 1.0 / (k_rrf + rd.get(_id, 1e9))\n",
    "        sb = 1.0 / (k_rrf + rb.get(_id, 1e9))\n",
    "        txt = next((d[\"text\"] for d in dense if d[\"id\"] == _id), None) or \\\n",
    "              next((d[\"text\"] for d in bm25 if d[\"id\"] == _id), \"\")\n",
    "        out.append({\"id\": _id, \"text\": txt, \"score\": float(sd + sb)})\n",
    "    out.sort(key=lambda x: -x[\"score\"])\n",
    "    return out[:k]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 301,
   "id": "064bc342-5406-45fc-a1f5-ff1e070dc829",
   "metadata": {},
   "outputs": [],
   "source": [
    "def one_request(base: str, q: str, top_k: int = 5, use_rerank: bool = True, timeout: float = 8.0) -> Dict:\n",
    "    s = requests.Session()\n",
    "    t0 = time.time()\n",
    "    t = time.time()\n",
    "    d_dense = s.post(f\"{base}/retrieve_dense\", json={\"query\": q, \"top_k\": top_k}, timeout=timeout).json()\n",
    "    dense_ms = int((time.time() - t) * 1000)\n",
    "    t = time.time()\n",
    "    d_bm25 = s.post(f\"{base}/retrieve_bm25\", json={\"query\": q, \"top_k\": top_k}, timeout=timeout).json()\n",
    "    bm25_ms = int((time.time() - t) * 1000)\n",
    "    fused = rrf_fuse(d_dense[\"results\"], d_bm25[\"results\"], k=top_k)\n",
    "    if use_rerank:\n",
    "        t = time.time()\n",
    "        d_rer = s.post(f\"{base}/rerank\", json={\"query\": q, \"candidates\": fused}, timeout=timeout).json()\n",
    "        rerank_ms = int((time.time() - t) * 1000)\n",
    "        contexts = d_rer[\"results\"]\n",
    "    else:\n",
    "        rerank_ms = 0\n",
    "        contexts = fused\n",
    "    t = time.time()\n",
    "    d_comp = s.post(f\"{base}/complete\", json={\"query\": q, \"contexts\": contexts}, timeout=timeout).json()\n",
    "    complete_ms = int((time.time() - t) * 1000)\n",
    "    total_ms = int((time.time() - t0) * 1000)\n",
    "    return {\n",
    "        \"ok\": True,\n",
    "        \"latency_ms\": total_ms,\n",
    "        \"dense_ms\": dense_ms,\n",
    "        \"bm25_ms\": bm25_ms,\n",
    "        \"rerank_ms\": rerank_ms,\n",
    "        \"complete_ms\": complete_ms,\n",
    "        \"answer\": d_comp.get(\"answer\", \"\"),\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 302,
   "id": "6a31fa63-a61d-4a92-8048-2b708b6e0050",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Summary: {'sent': 240, 'ok': 240, 'errors': 0, 'error_rate': 0.0, 'qps_observed': 7.690540873401241, 'p50_ms': 544.0, 'p95_ms': 591.3499999999999, 'p99_ms': 1201.44, 'step_p95': {'dense_ms': 211.0, 'bm25_ms': 31.0, 'rerank_ms': 400.04999999999995, 'complete_ms': 19.0}}\n"
     ]
    }
   ],
   "source": [
    "def load_test(base: str = \"http://127.0.0.1:8001\", qps: int = 8, duration_s: int = 30,\n",
    "              top_k: int = 5, use_rerank: bool = True,\n",
    "              queries: Optional[List[str]] = None) -> Dict:\n",
    "    if not queries:\n",
    "        queries = [\n",
    "            \"What are common notes for customers with late fees?\",\n",
    "            \"select age, income and balance where default = 1 limit 5\",\n",
    "            \"2+2*10 - 3\",\n",
    "            \"Do we comply with PII masking in logs?\",\n",
    "            \"How do BM25 and vector search differ?\",\n",
    "        ]\n",
    "    latencies = []\n",
    "    per_step = {\"dense_ms\": [], \"bm25_ms\": [], \"rerank_ms\": [], \"complete_ms\": []}\n",
    "    errors = 0\n",
    "    sent = 0\n",
    "    start = time.time()\n",
    "    deadline = start + duration_s\n",
    "    while time.time() < deadline:\n",
    "        tick = time.time()\n",
    "        with cf.ThreadPoolExecutor(max_workers=qps) as ex:\n",
    "            futs = []\n",
    "            for _ in range(qps):\n",
    "                q = random.choice(queries)\n",
    "                futs.append(ex.submit(one_request, base, q, top_k, use_rerank))\n",
    "            for f in cf.as_completed(futs):\n",
    "                sent += 1\n",
    "                try:\n",
    "                    r = f.result()\n",
    "                    if r[\"ok\"]:\n",
    "                        latencies.append(r[\"latency_ms\"])\n",
    "                        for k in per_step:\n",
    "                            per_step[k].append(r[k])\n",
    "                    else:\n",
    "                        errors += 1\n",
    "                except Exception:\n",
    "                    errors += 1\n",
    "        elapsed = time.time() - tick\n",
    "        time.sleep(max(0.0, 1.0 - elapsed))\n",
    "    if latencies:\n",
    "        p50 = float(np.percentile(latencies, 50))\n",
    "        p95 = float(np.percentile(latencies, 95))\n",
    "        p99 = float(np.percentile(latencies, 99))\n",
    "    else:\n",
    "        p50 = p95 = p99 = float(\"nan\")\n",
    "    plt.figure()\n",
    "    plt.hist(latencies, bins=40)\n",
    "    plt.title(\"Loadgen latencies (ms)\")\n",
    "    plt.xlabel(\"Latency (ms)\")\n",
    "    plt.ylabel(\"Count\")\n",
    "    plt.show()\n",
    "    summary = {\n",
    "        \"sent\": sent,\n",
    "        \"ok\": len(latencies),\n",
    "        \"errors\": errors,\n",
    "        \"error_rate\": errors / max(1, sent),\n",
    "        \"qps_observed\": sent / max(0.001, (time.time() - start)),\n",
    "        \"p50_ms\": p50, \"p95_ms\": p95, \"p99_ms\": p99,\n",
    "        \"step_p95\": {k: float(np.percentile(v, 95)) if v else None for k, v in per_step.items()},\n",
    "    }\n",
    "    print(\"Summary:\", summary)\n",
    "    assert (math.isnan(p95) or p95 <= 1800), f\"p95 over SLO: {p95:.0f} ms\"\n",
    "    return summary\n",
    "summary = load_test(base=\"http://127.0.0.1:8001\", qps=8, duration_s=30, top_k=5, use_rerank=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 303,
   "id": "d0ee4e13-f961-4f4a-9a22-26336b5e0c89",
   "metadata": {},
   "outputs": [],
   "source": [
    "from prometheus_client import start_http_server, Counter, Histogram\n",
    "import socket\n",
    "LOADGEN_REQS   = Counter(\"loadgen_requests_total\", \"Total loadgen requests\")\n",
    "LOADGEN_ERRORS = Counter(\"loadgen_errors_total\", \"Total loadgen errors\")\n",
    "LAT_MS         = Histogram(\"loadgen_latency_ms\", \"End-to-end latency (ms)\", buckets=(\n",
    "    50, 100, 150, 250, 400, 600, 800, 1000, 1200, 1500, 1800, 2500, 4000\n",
    "))\n",
    "DENSE_MS   = Histogram(\"rag_dense_ms\", \"Dense retrieval latency (ms)\")\n",
    "BM25_MS    = Histogram(\"rag_bm25_ms\", \"BM25 retrieval latency (ms)\")\n",
    "RERANK_MS  = Histogram(\"rag_rerank_ms\", \"Rerank latency (ms)\")\n",
    "COMPLETE_MS= Histogram(\"rag_complete_ms\", \"Complete latency (ms)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 304,
   "id": "e4cb401c-e482-4178-a7e0-76304084bc97",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prometheus exporter up on :8001    curl -s http://127.0.0.1:8001/metrics | head -n 25\n"
     ]
    }
   ],
   "source": [
    "def _bind_port(prefer=8001, tries=5):\n",
    "    port = prefer\n",
    "    for _ in range(tries):\n",
    "        try:\n",
    "            start_http_server(port)\n",
    "            return port\n",
    "        except OSError:\n",
    "            port += 1\n",
    "    raise RuntimeError(\"Could not bind Prometheus exporter on 8001-{}.\".format(8001+tries-1))\n",
    "EXPORTER_PORT = _bind_port(8001, tries=5)\n",
    "print(f\"Prometheus exporter up on :{EXPORTER_PORT}    curl -s http://127.0.0.1:{EXPORTER_PORT}/metrics | head -n 25\")\n",
    "def one_request_with_metrics(base, q, top_k=5, use_rerank=True, timeout=8.0):\n",
    "    LOADGEN_REQS.inc()\n",
    "    try:\n",
    "        r = one_request(base, q, top_k, use_rerank, timeout)\n",
    "        LAT_MS.observe(r[\"latency_ms\"] / 1.0)    \n",
    "        DENSE_MS.observe(r[\"dense_ms\"] / 1.0)\n",
    "        BM25_MS.observe(r[\"bm25_ms\"] / 1.0)\n",
    "        if r[\"rerank_ms\"] > 0:\n",
    "            RERANK_MS.observe(r[\"rerank_ms\"] / 1.0)\n",
    "        COMPLETE_MS.observe(r[\"complete_ms\"] / 1.0)\n",
    "        return r\n",
    "    except Exception:\n",
    "        LOADGEN_ERRORS.inc()\n",
    "        raise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 305,
   "id": "c61d81c3-7879-442b-add7-1fb4d33f9d36",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# HELP python_gc_objects_collected_total Objects collected during gc\n",
      "# TYPE python_gc_objects_collected_total counter\n",
      "python_gc_objects_collected_total{generation=\"0\"} 62853.0\n",
      "python_gc_objects_collected_total{generation=\"1\"} 17022.0\n",
      "python_gc_objects_collected_total{generation=\"2\"} 39556.0\n",
      "# HELP python_gc_objects_uncollectable_total Uncollectable objects found during GC\n",
      "# TYPE python_gc_objects_uncollectable_total counter\n",
      "python_gc_objects_uncollectable_total{generation=\"0\"} 0.0\n",
      "python_gc_obje\n"
     ]
    }
   ],
   "source": [
    "one_request = one_request_with_metrics\n",
    "import requests\n",
    "print(requests.get(f\"http://127.0.0.1:{EXPORTER_PORT}/metrics\", timeout=5).text[:500])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 306,
   "id": "3db6d35d-700b-460a-bc7f-f2d3a0955456",
   "metadata": {},
   "outputs": [],
   "source": [
    "from textwrap import dedent\n",
    "from pathlib import Path\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 307,
   "id": "f29fb90f-4636-4572-bb85-51d332c53e6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "API_SNIPPETS = dedent(r\"\"\"\n",
    "# RAG Microservice  quick curl\n",
    "BASE=http://127.0.0.1:8001\n",
    "# Health\n",
    "curl -s $BASE/up | jq .\n",
    "# Retrieve (BM25 + Dense)\n",
    "curl -s -X POST $BASE/retrieve_bm25 -H \"Content-Type: application/json\" \\\n",
    "  -d '{\"query\":\"How do BM25 and vector search differ?\",\"top_k\":3}' | jq .\n",
    "curl -s -X POST $BASE/retrieve_dense -H \"Content-Type: application/json\" \\\n",
    "  -d '{\"query\":\"How do BM25 and vector search differ?\",\"top_k\":3}' | jq .\n",
    "# Rerank (optional)\n",
    "curl -s -X POST $BASE/rerank -H \"Content-Type: application/json\" \\\n",
    "  -d '{\"query\":\"How do BM25 and vector search differ?\",\"candidates\":[{\"id\":\"d1\",\"text\":\"...\"}]}' | jq .\n",
    "# Complete (compose answer with contexts)\n",
    "curl -s -X POST $BASE/complete -H \"Content-Type: application/json\" \\\n",
    "  -d '{\"query\":\"How do BM25 and vector search differ?\",\"contexts\":[{\"id\":\"d1\",\"text\":\"...\"}]}' | jq .\n",
    "\"\"\").strip()\n",
    "OVERSEER_SCHEMA = {\n",
    "  \"input_example\": {\n",
    "    \"error_bundle\": {\n",
    "      \"node\": \"query_hot_path\",\n",
    "      \"stack\": \"Traceback ...\",\n",
    "      \"code_context\": [\n",
    "        {\"path\": \"services/retrieval/router.py\", \"start\": 40, \"end\": 95}\n",
    "      ],\n",
    "      \"failing_input\": {\"query\": \"SELECT * FROM users; DROP TABLE users;\"},\n",
    "      \"last_green_sha\": \"abc123def\",\n",
    "      \"allow_list\": [\"services/*\", \"prompts/*.json\", \"configs/*.yaml\"]\n",
    "    }\n",
    "  },\n",
    "  \"output_schema\": {\n",
    "    \"patch_type\": \"unified_diff|config\",\n",
    "    \"patch\": \"<diff or JSON>\",\n",
    "    \"notes\": \"<<=120 chars>>\",\n",
    "    \"tests\": [\"tests/...::...\"]\n",
    "  }\n",
    "}\n",
    "DISABLE_CANARY = dedent(r\"\"\"\n",
    "# Disable canary (route 100% to champion)\n",
    "BASE=http://127.0.0.1:8000\n",
    "# Option A: Update RL weights to A=1.0, B=0.0\n",
    "curl -s -X POST \"$BASE/rl/update\" \\\n",
    "  -H \"Content-Type: application/json\" \\\n",
    "  -d '{\"weights\":{\"A\":1.0,\"B\":0.0}}' | jq .\n",
    "# Option B: Hard toggle governor/adapters to cheapest path (if exposed)\n",
    "curl -s -X POST \"$BASE/governor/mode?enable_ce=true&enable_adapters=true\" | jq .\n",
    "# Option C: Disable verifier v2 if its the regression\n",
    "curl -s -X POST \"$BASE/verify/mode?set_v2=false\" | jq .\n",
    "\"\"\").strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 308,
   "id": "0b0197ed-dc37-4103-8a58-27aac0e18183",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wrote: ['local_snippets\\\\API_QUICK_REFERENCE_SNIPPETS.md', 'local_snippets\\\\OVERSEER_SCHEMA_SNIPPET.json', 'local_snippets\\\\RUNBOOK_DISABLE_CANARY.md', 'local_snippets\\\\SQL_SANITIZE.md', 'local_snippets\\\\VERIFIER_NOTES.md']\n"
     ]
    }
   ],
   "source": [
    "out_dir = Path(\"local_snippets\"); out_dir.mkdir(exist_ok=True)\n",
    "(Path(out_dir / \"API_QUICK_REFERENCE_SNIPPETS.md\")).write_text(API_SNIPPETS + \"\\n\", encoding=\"utf-8\")\n",
    "(Path(out_dir / \"OVERSEER_SCHEMA_SNIPPET.json\")).write_text(json.dumps(OVERSEER_SCHEMA, indent=2) + \"\\n\", encoding=\"utf-8\")\n",
    "(Path(out_dir / \"RUNBOOK_DISABLE_CANARY.md\")).write_text(DISABLE_CANARY + \"\\n\", encoding=\"utf-8\")\n",
    "print(\"Wrote:\", [str(p) for p in out_dir.iterdir()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 309,
   "id": "c4535e85-f684-4749-a783-21ff44cbaa46",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, re, time, math, random, threading, json, pathlib, sys\n",
    "from typing import List, Dict, Optional, Tuple\n",
    "from dataclasses import dataclass\n",
    "from functools import lru_cache\n",
    "from collections import Counter\n",
    "import numpy as np\n",
    "from fastapi import FastAPI, HTTPException\n",
    "from pydantic import BaseModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 310,
   "id": "d26b5e2a-bd11-4dbb-b6db-2b46ff0b12df",
   "metadata": {},
   "outputs": [],
   "source": [
    "CHAOS = {\n",
    "    \"verifier_v2\": False,  \n",
    "}\n",
    "def chaos_set_verifier_v2(flag: bool = True):\n",
    "    CHAOS[\"verifier_v2\"] = bool(flag)\n",
    "    print(f\"Chaos: verifier_v2={CHAOS['verifier_v2']}\")\n",
    "try:\n",
    "    import faiss  \n",
    "    _HAVE_FAISS = True\n",
    "except Exception:\n",
    "    faiss = None\n",
    "    _HAVE_FAISS = False\n",
    "try:\n",
    "    from rank_bm25 import BM25Okapi\n",
    "    _HAVE_BM25 = True\n",
    "except Exception:\n",
    "    BM25Okapi = None\n",
    "    _HAVE_BM25 = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 311,
   "id": "ef8a3bfa-a649-4e22-b499-0d476316e15e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _try_load_sentence_transformers():\n",
    "    try:\n",
    "        from sentence_transformers import SentenceTransformer\n",
    "        return SentenceTransformer\n",
    "    except Exception:\n",
    "        return None\n",
    "def _try_load_cross_encoder():\n",
    "    try:\n",
    "        from sentence_transformers import CrossEncoder\n",
    "        return CrossEncoder\n",
    "    except Exception:\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 312,
   "id": "4c275047-334c-437f-b545-e3564091caf9",
   "metadata": {},
   "outputs": [],
   "source": [
    "_hf_nli_ready = False\n",
    "_tok_nli = None\n",
    "_mdl_nli = None\n",
    "_dev_nli = None\n",
    "def _try_load_nli():\n",
    "    global _hf_nli_ready, _tok_nli, _mdl_nli, _dev_nli\n",
    "    if _hf_nli_ready:\n",
    "        return True\n",
    "    try:\n",
    "        from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "        import torch\n",
    "        _tok_nli = AutoTokenizer.from_pretrained(\"roberta-large-mnli\")\n",
    "        _mdl_nli = AutoModelForSequenceClassification.from_pretrained(\"roberta-large-mnli\")\n",
    "        _mdl_nli.eval()\n",
    "        _dev_nli = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "        _mdl_nli.to(_dev_nli)\n",
    "        _hf_nli_ready = True\n",
    "    except Exception:\n",
    "        _hf_nli_ready = False\n",
    "    return _hf_nli_ready"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 313,
   "id": "80046e85-3097-4bda-ac1e-5885dd35ac60",
   "metadata": {},
   "outputs": [],
   "source": [
    "TOKEN_RE = re.compile(r\"[A-Za-z0-9]+\")\n",
    "def tokenize(s: str) -> List[str]:\n",
    "    return [t.lower() for t in TOKEN_RE.findall(s or \"\")]\n",
    "def normalize(vec: np.ndarray) -> np.ndarray:\n",
    "    denom = np.linalg.norm(vec, axis=1, keepdims=True) + 1e-9\n",
    "    return vec / denom\n",
    "DEFAULT_DOCS = [\n",
    "    {\"id\": \"d1\", \"text\": \"Rust pipelines are fast and memory-safe using async with Tokio and Axum.\"},\n",
    "    {\"id\": \"d2\", \"text\": \"RAG pipelines retrieve relevant chunks and augment the prompt for the LLM.\"},\n",
    "    {\"id\": \"d3\", \"text\": \"Hybrid retrieval with BM25 and vector kNN improves recall and faithfulness.\"},\n",
    "    {\"id\": \"d4\", \"text\": \"LangChain helps prototype chains, tools, and agents quickly in Python.\"},\n",
    "    {\"id\": \"d5\", \"text\": \"Tantivy provides BM25 in Rust; pgvector or FAISS provide dense similarity.\"},\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 314,
   "id": "b2dab755-1f10-4d6a-81c4-60752e0dc63e",
   "metadata": {},
   "outputs": [],
   "source": [
    "_ST_EMB_MODEL_NAME = os.getenv(\"EMB_MODEL\", \"intfloat/e5-base\")\n",
    "@dataclass\n",
    "class DenseIndexMeta:\n",
    "    dim: int\n",
    "    use_faiss: bool\n",
    "    count: int\n",
    "class DenseIndex:\n",
    "    \"\"\"\n",
    "    Encoder priority:\n",
    "      1) sentence-transformers (E5) if available\n",
    "      2) hashing fallback\n",
    "    Index priority:\n",
    "      1) FAISS IndexFlatIP if available\n",
    "      2) NumPy cosine\n",
    "    \"\"\"\n",
    "    def __init__(self, docs: List[Dict[str, str]]):\n",
    "        self.ids = [d[\"id\"] for d in docs]\n",
    "        self.texts = [d[\"text\"] for d in docs]\n",
    "        ST = _try_load_sentence_transformers()\n",
    "        if ST:\n",
    "            try:\n",
    "                self.emb_model = ST(_ST_EMB_MODEL_NAME)\n",
    "                embs = self.emb_model.encode(self.texts, normalize_embeddings=True, convert_to_numpy=True)\n",
    "                self.embs = embs.astype(np.float32)\n",
    "                self.dim = int(self.embs.shape[1])\n",
    "            except Exception:\n",
    "                self.emb_model = None\n",
    "                self.embs, self.dim = self._hash_embed(self.texts)\n",
    "        else:\n",
    "            self.emb_model = None\n",
    "            self.embs, self.dim = self._hash_embed(self.texts)\n",
    "        self.faiss_index = None\n",
    "        self.use_faiss = False\n",
    "        if _HAVE_FAISS:\n",
    "            try:\n",
    "                idx = faiss.IndexFlatIP(self.dim)\n",
    "                idx.add(self.embs)\n",
    "                self.faiss_index = idx\n",
    "                self.use_faiss = True\n",
    "            except Exception:\n",
    "                self.faiss_index = None\n",
    "                self.use_faiss = False\n",
    "        self.meta = DenseIndexMeta(dim=self.dim, use_faiss=self.use_faiss, count=len(self.ids))\n",
    "    def _hash_embed(self, texts: List[str], dim: int = 384) -> Tuple[np.ndarray, int]:\n",
    "        def proj(tok):\n",
    "            rng2 = np.random.default_rng(abs(hash(tok)) % (2**32))\n",
    "            return rng2.standard_normal(dim)\n",
    "        mat = []\n",
    "        for t in texts:\n",
    "            toks = tokenize(t)\n",
    "            if not toks:\n",
    "                mat.append(np.zeros(dim))\n",
    "                continue\n",
    "            v = np.sum([proj(tok) for tok in toks], axis=0)\n",
    "            mat.append(v)\n",
    "        arr = np.vstack(mat).astype(np.float32)\n",
    "        arr = normalize(arr)\n",
    "        return arr, dim\n",
    "    def encode_query(self, q: str) -> np.ndarray:\n",
    "        if self.emb_model:\n",
    "            try:\n",
    "                v = self.emb_model.encode([q], normalize_embeddings=True, convert_to_numpy=True)[0].astype(np.float32)\n",
    "                return v\n",
    "            except Exception:\n",
    "                pass\n",
    "        embs, _ = self._hash_embed([q])\n",
    "        return embs[0].astype(np.float32)\n",
    "    def _adapt_dim(self, qv: np.ndarray) -> np.ndarray:\n",
    "        \"\"\"Pad/truncate query vector to match index dim and re-normalize.\"\"\"\n",
    "        if qv.ndim == 1:\n",
    "            qv = qv.reshape(1, -1)\n",
    "        qdim = int(qv.shape[1])\n",
    "        if qdim == self.dim:\n",
    "            pass\n",
    "        elif qdim > self.dim:\n",
    "            qv = qv[:, :self.dim]\n",
    "        else:\n",
    "            pad = np.zeros((qv.shape[0], self.dim - qdim), dtype=qv.dtype)\n",
    "            qv = np.hstack([qv, pad])\n",
    "        qv /= (np.linalg.norm(qv, axis=1, keepdims=True) + 1e-9)\n",
    "        return qv.astype(np.float32)\n",
    "    def _numpy_search(self, qv: np.ndarray, k: int) -> List[Dict]:\n",
    "        sims = (self.embs @ qv.T).ravel()\n",
    "        order = np.argsort(-sims)[:k]\n",
    "        return [{\"id\": self.ids[i], \"text\": self.texts[i], \"score\": float(sims[i])} for i in order]\n",
    "    def topk(self, q: str, k: int = 5) -> List[Dict]:\n",
    "        k = max(1, min(k, 50))\n",
    "        qv = self._adapt_dim(self.encode_query(q))\n",
    "        if self.use_faiss and self.faiss_index is not None:\n",
    "            try:\n",
    "                if hasattr(self.faiss_index, \"d\") and int(self.faiss_index.d) != self.dim:\n",
    "                    self.use_faiss = False\n",
    "                    raise AssertionError(\"faiss index dim drift\")\n",
    "                sims, idx = self.faiss_index.search(qv, k)\n",
    "                sims, idx = sims[0], idx[0]\n",
    "                out = []\n",
    "                for i, sc in zip(idx, sims):\n",
    "                    if i < 0: continue\n",
    "                    out.append({\"id\": self.ids[i], \"text\": self.texts[i], \"score\": float(sc)})\n",
    "                return out\n",
    "            except Exception:\n",
    "                self.use_faiss = False\n",
    "        return self._numpy_search(qv, k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 315,
   "id": "ede3dc20-48ed-40b2-8fca-8b293a525c94",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BM25Index:\n",
    "    def __init__(self, docs: List[Dict[str, str]]):\n",
    "        self.ids = [d[\"id\"] for d in docs]\n",
    "        self.texts = [d[\"text\"] for d in docs]\n",
    "        self.tokens = [tokenize(t) for t in self.texts]\n",
    "        self.have_bm25 = _HAVE_BM25 and BM25Okapi is not None\n",
    "        if self.have_bm25:\n",
    "            self.bm25 = BM25Okapi(self.tokens)\n",
    "        else:\n",
    "            self.bm25 = None\n",
    "            self.df = Counter()\n",
    "            for toks in self.tokens:\n",
    "                for t in set(toks): self.df[t] += 1\n",
    "            self.avgdl = np.mean([len(t) for t in self.tokens]) if self.tokens else 1.0\n",
    "            self.N = len(self.tokens)\n",
    "            self.k1, self.b = 1.5, 0.75\n",
    "    def _score(self, qtok: List[str], i: int) -> float:\n",
    "        if self.have_bm25:\n",
    "            raise RuntimeError(\"should not call _score when using rank_bm25\")\n",
    "        toks = self.tokens[i]; dl = len(toks)\n",
    "        tf = Counter(toks); score = 0.0\n",
    "        for t in qtok:\n",
    "            n_q = self.df.get(t, 0)\n",
    "            if n_q == 0: continue\n",
    "            idf = math.log(1 + (self.N - n_q + 0.5) / (n_q + 0.5))\n",
    "            f = tf.get(t, 0)\n",
    "            denom = f + self.k1 * (1 - self.b + self.b * dl / self.avgdl)\n",
    "            score += idf * (f * (self.k1 + 1)) / (denom if denom else 1.0)\n",
    "        return score\n",
    "    def topk(self, q: str, k: int = 5) -> List[Dict]:\n",
    "        k = max(1, min(k, 50))\n",
    "        qtok = tokenize(q)\n",
    "        if self.have_bm25:\n",
    "            scores = self.bm25.get_scores(qtok)\n",
    "            order = np.argsort(-scores)[:k]\n",
    "            return [{\"id\": self.ids[i], \"text\": self.texts[i], \"score\": float(scores[i])} for i in order]\n",
    "        scores = [self._score(qtok, i) for i in range(len(self.tokens))]\n",
    "        order = np.argsort(-np.array(scores))[:k]\n",
    "        return [{\"id\": self.ids[i], \"text\": self.texts[i], \"score\": float(scores[i])} for i in order]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 316,
   "id": "7ed285de-b1ec-4ee2-8657-8ed85ab3ef12",
   "metadata": {},
   "outputs": [],
   "source": [
    "_ST_RERANK_MODEL_NAME = os.getenv(\"RERANK_MODEL\", \"BAAI/bge-reranker-v2-m3\")\n",
    "@lru_cache(maxsize=1)\n",
    "def _get_cross_encoder():\n",
    "    CE = _try_load_cross_encoder()\n",
    "    if not CE: return None\n",
    "    try:\n",
    "        return CE(_ST_RERANK_MODEL_NAME)\n",
    "    except Exception:\n",
    "        return None\n",
    "def rerank_cross_encoder(query: str, cands: List[Dict]) -> List[Dict]:\n",
    "    ce = _get_cross_encoder()\n",
    "    if ce is None or not cands:\n",
    "        return cands\n",
    "    try:\n",
    "        pairs = [(query, c[\"text\"]) for c in cands]\n",
    "        scores = ce.predict(pairs).tolist()\n",
    "        out = []\n",
    "        for c, s in zip(cands, scores):\n",
    "            d = dict(c); d[\"score\"] = float(s); out.append(d)\n",
    "        return sorted(out, key=lambda x: -x[\"score\"])\n",
    "    except Exception:\n",
    "        return cands"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 317,
   "id": "b2523b02-c898-4f94-b8ac-af5b6d624613",
   "metadata": {},
   "outputs": [],
   "source": [
    "_NUM_RE = re.compile(r\"[-+]?\\d*\\.?\\d+(?:[eE][-+]?\\d+)?\")\n",
    "def _nums(s: str) -> List[float]:\n",
    "    out = []\n",
    "    for m in _NUM_RE.findall(s or \"\"):\n",
    "        try: out.append(float(m))\n",
    "        except Exception: pass\n",
    "    return out\n",
    "def span_overlap(answer: str, contexts: List[str]) -> float:\n",
    "    a = Counter(tokenize(answer)); c = Counter(tokenize(\" \".join(contexts)))\n",
    "    if not a: return 0.0\n",
    "    inter = sum(min(a[t], c.get(t, 0)) for t in a)\n",
    "    return inter / max(1, sum(a.values()))\n",
    "def numeric_consistency(answer: str, contexts: List[str], eps: float = 1e-6, rel: float = 1e-3) -> float:\n",
    "    ans_nums = _nums(answer); ctx_nums = _nums(\" \".join(contexts))\n",
    "    if not ans_nums: return 1.0\n",
    "    ok = 0\n",
    "    for a in ans_nums:\n",
    "        if any(abs(a - c) <= max(eps, rel * max(1.0, abs(c))) for c in ctx_nums):\n",
    "            ok += 1\n",
    "    return ok / len(ans_nums)\n",
    "def nli_entailment_prob(prem: str, hyp: str) -> Optional[float]:\n",
    "    if not _try_load_nli(): return None\n",
    "    import torch\n",
    "    with torch.no_grad():\n",
    "        batch = _tok_nli(prem, hyp, truncation=True, max_length=512, return_tensors=\"pt\").to(_dev_nli)\n",
    "        logits = _mdl_nli(**batch).logits\n",
    "        probs = torch.softmax(logits, dim=-1)[0].tolist()\n",
    "        return float(probs[2])  \n",
    "def verify_faithfulness(answer: str, contexts: List[str]) -> Dict[str, float]:\n",
    "    so = span_overlap(answer, contexts)\n",
    "    num = numeric_consistency(answer, contexts)\n",
    "    faith = 0.5 * so + 0.5 * num\n",
    "    return {\"faithfulness\": float(faith), \"entailment_prob\": float(faith), \"mode\": \"overlap+numeric\"}\n",
    "def verify_bridge(answer: str, contexts: List[str]) -> Dict[str, float]:\n",
    "    base = verify_faithfulness(answer, contexts)\n",
    "    if not CHAOS[\"verifier_v2\"] or not contexts:\n",
    "        return base\n",
    "    p1 = nli_entailment_prob(\" \".join(contexts[:2]), answer)\n",
    "    p2 = nli_entailment_prob(answer, \" \".join(contexts[:2]))\n",
    "    if p1 is None or p2 is None:\n",
    "        return base\n",
    "    nli = 0.5 * (p1 + p2)\n",
    "    faith = 0.6 * nli + 0.2 * base[\"faithfulness\"] + 0.2 * base[\"entailment_prob\"]\n",
    "    return {\"faithfulness\": float(faith), \"entailment_prob\": float(nli), \"mode\": \"mnli+overlap\"}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 318,
   "id": "cc219f36-450b-4eee-b6c5-7a56d22c9214",
   "metadata": {},
   "outputs": [],
   "source": [
    "_SQL_DDL_DML_BLOCKLIST = {\n",
    "    \"DROP\",\"TRUNCATE\",\"DELETE\",\"UPDATE\",\"INSERT\",\"ALTER\",\"CREATE\",\"RENAME\",\"GRANT\",\"REVOKE\",\"MERGE\",\n",
    "}\n",
    "_SQL_SELECT_RE = re.compile(r\"(?is)\\bselect\\b.*?(?:;|$)\")\n",
    "_SQL_COMMENT_LINE  = re.compile(r\"(?m)^[ \\t]*--.*?$\")\n",
    "_SQL_COMMENT_BLOCK = re.compile(r\"/\\*.*?\\*/\", re.DOTALL)\n",
    "def _strip_sql_comments(sql: str) -> str:\n",
    "    if not sql: return \"\"\n",
    "    sql = _SQL_COMMENT_BLOCK.sub(\"\", sql)\n",
    "    sql = _SQL_COMMENT_LINE.sub(\"\", sql)\n",
    "    return sql\n",
    "def extract_sql(text: str) -> Optional[str]:\n",
    "    if not text or not isinstance(text, str): \n",
    "        return None\n",
    "    s = _strip_sql_comments(text)\n",
    "    m = re.search(r\"```(?:sql)?\\s*(.*?)```\", s, flags=re.IGNORECASE | re.DOTALL)\n",
    "    if m:\n",
    "        s = m.group(1)\n",
    "    m = _SQL_SELECT_RE.search(s or \"\")\n",
    "    return m.group(0).strip(\" ;\") if m else None\n",
    "def sanitize_select_limit(sql: str, max_limit: int = 100) -> str:\n",
    "    if not sql: \n",
    "        raise ValueError(\"empty SQL\")\n",
    "    s = _strip_sql_comments(sql).strip()\n",
    "    parts = [p.strip() for p in s.split(\";\") if p.strip()]\n",
    "    if len(parts) != 1:\n",
    "        raise ValueError(\"multiple statements not allowed\")\n",
    "    s = parts[0]\n",
    "    if not re.match(r\"(?is)^\\s*select\\b\", s):\n",
    "        raise ValueError(\"only SELECT statements allowed\")\n",
    "    upper = re.sub(r\"'[^']*'|\\\"[^\\\"]*\\\"\", \"\", s).upper()\n",
    "    for bad in _SQL_DDL_DML_BLOCKLIST:\n",
    "        if re.search(rf\"\\b{bad}\\b\", upper):\n",
    "            raise ValueError(f\"disallowed keyword: {bad}\")\n",
    "    m = re.search(r\"(?is)\\blimit\\s+(\\d+)\\b\", s)\n",
    "    if m:\n",
    "        lim = int(m.group(1))\n",
    "        if lim > max_limit:\n",
    "            s = re.sub(r\"(?is)\\blimit\\s+\\d+\\b\", f\"LIMIT {max_limit}\", s)\n",
    "    else:\n",
    "        s = s.rstrip() + f\" LIMIT {max_limit}\"\n",
    "    return s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 319,
   "id": "258fa181-b09b-4952-8fda-23c60c192e29",
   "metadata": {},
   "outputs": [],
   "source": [
    "class AppState:\n",
    "    def __init__(self, docs: Optional[List[Dict[str,str]]] = None):\n",
    "        self.docs = docs or DEFAULT_DOCS\n",
    "        self.dense = DenseIndex(self.docs)\n",
    "        self.bm25 = BM25Index(self.docs)\n",
    "        self.created_ts = int(time.time())\n",
    "    def reload(self, docs: List[Dict[str,str]]):\n",
    "        self.docs = docs\n",
    "        self.dense = DenseIndex(self.docs)\n",
    "        self.bm25 = BM25Index(self.docs)\n",
    "STATE = AppState()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 320,
   "id": "9e74a1ab-ccf3-4dc1-ba12-866e224640c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "app = FastAPI(title=\"RAG Microservice (Notebook)\", version=\"0.3\")\n",
    "_INSTRUMENTED = globals().get(\"_INSTRUMENTED\", False)\n",
    "try:\n",
    "    from prometheus_fastapi_instrumentator import Instrumentator\n",
    "    if not _INSTRUMENTED:\n",
    "        Instrumentator().instrument(app).expose(app, endpoint=\"/metrics\")\n",
    "        _INSTRUMENTED = True\n",
    "        globals()[\"_INSTRUMENTED\"] = True\n",
    "except Exception as e:\n",
    "    print(\"Prometheus instrumentator not available:\", e)\n",
    "from prometheus_client import Histogram, REGISTRY\n",
    "def _get_or_create_hist(name: str, doc: str, buckets=None):\n",
    "    if buckets is None:\n",
    "        buckets = (50,100,150,250,400,600,800,1000,1200,1500,1800,2500,4000)\n",
    "    try:\n",
    "        return Histogram(name, doc, buckets=buckets)\n",
    "    except ValueError:\n",
    "        coll = REGISTRY._names_to_collectors.get(name)\n",
    "        if isinstance(coll, Histogram):\n",
    "            return coll\n",
    "        raise\n",
    "RAG_DENSE_MS    = _get_or_create_hist(\"rag_dense_ms\",    \"Dense retrieval latency (ms)\")\n",
    "RAG_BM25_MS     = _get_or_create_hist(\"rag_bm25_ms\",     \"BM25 retrieval latency (ms)\")\n",
    "RAG_RERANK_MS   = _get_or_create_hist(\"rag_rerank_ms\",   \"Rerank latency (ms)\")\n",
    "RAG_COMPLETE_MS = _get_or_create_hist(\"rag_complete_ms\", \"Complete latency (ms)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 321,
   "id": "5ee171ac-1c03-48de-84c7-8925f93d0942",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LoadDocsReq(BaseModel):\n",
    "    docs: List[Dict[str, str]]  \n",
    "class RetrieveReq(BaseModel):\n",
    "    query: str\n",
    "    top_k: Optional[int] = 5\n",
    "class RerankReq(BaseModel):\n",
    "    query: str\n",
    "    candidates: List[Dict]\n",
    "class CompleteReq(BaseModel):\n",
    "    query: str\n",
    "    contexts: List[Dict]\n",
    "class SchemaProfileReq(BaseModel):\n",
    "    sample: List[Dict] = []\n",
    "class SchemaValidateReq(BaseModel):\n",
    "    sample: List[Dict] = []\n",
    "class SqlSanitizeReq(BaseModel):\n",
    "    text: str\n",
    "    max_limit: Optional[int] = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 322,
   "id": "ddf917f0-62a2-44e0-86e2-8b9560863bd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "@app.get(\"/up\")\n",
    "def up():\n",
    "    return {\n",
    "        \"ok\": True,\n",
    "        \"created_ts\": STATE.created_ts,\n",
    "        \"docs\": len(STATE.docs),\n",
    "        \"dense\": {\"dim\": STATE.dense.meta.dim, \"faiss\": STATE.dense.meta.use_faiss, \"count\": STATE.dense.meta.count},\n",
    "        \"bm25\": {\"impl\": \"rank_bm25\" if STATE.bm25.have_bm25 else \"tf_fallback\", \"count\": len(STATE.bm25.ids)},\n",
    "        \"verifier_v2\": CHAOS[\"verifier_v2\"],\n",
    "    }\n",
    "@app.post(\"/load_docs\")\n",
    "def load_docs(req: LoadDocsReq):\n",
    "    if not req.docs:\n",
    "        raise HTTPException(400, \"docs required\")\n",
    "    for d in req.docs:\n",
    "        if \"id\" not in d or \"text\" not in d:\n",
    "            raise HTTPException(400, \"each doc must have id and text\")\n",
    "    STATE.reload(req.docs)\n",
    "    return {\"ok\": True, \"count\": len(req.docs)}\n",
    "@app.post(\"/retrieve_dense\")\n",
    "def retrieve_dense(req: RetrieveReq):\n",
    "    if not req.query.strip():\n",
    "        raise HTTPException(400, \"query required\")\n",
    "    k = int(req.top_k or 5)\n",
    "    t0 = time.time()\n",
    "    res = STATE.dense.topk(req.query, k)\n",
    "    dt = int((time.time() - t0) * 1000)\n",
    "    RAG_DENSE_MS.observe(dt / 1.0)\n",
    "    return {\"results\": res, \"latency_ms\": dt, \"index\": {\"faiss\": STATE.dense.meta.use_faiss, \"dim\": STATE.dense.meta.dim}}\n",
    "@app.post(\"/retrieve_bm25\")\n",
    "def retrieve_bm25(req: RetrieveReq):\n",
    "    if not req.query.strip():\n",
    "        raise HTTPException(400, \"query required\")\n",
    "    k = int(req.top_k or 5)\n",
    "    t0 = time.time()\n",
    "    res = STATE.bm25.topk(req.query, k)\n",
    "    dt = int((time.time() - t0) * 1000)\n",
    "    RAG_BM25_MS.observe(dt / 1.0)\n",
    "    return {\"results\": res, \"latency_ms\": dt, \"impl\": \"rank_bm25\" if STATE.bm25.have_bm25 else \"tf_fallback\"}\n",
    "@app.post(\"/rerank\")\n",
    "def rerank(req: RerankReq):\n",
    "    if not req.candidates:\n",
    "        return {\"results\": [], \"latency_ms\": 0, \"impl\": \"passthrough\"}\n",
    "    t0 = time.time()\n",
    "    out = rerank_cross_encoder(req.query, list(req.candidates))\n",
    "    dt = int((time.time() - t0) * 1000)\n",
    "    RAG_RERANK_MS.observe(dt / 1.0)\n",
    "    return {\"results\": out, \"latency_ms\": dt, \"impl\": \"cross_encoder\" if _get_cross_encoder() else \"passthrough\"}\n",
    "@app.post(\"/complete\")\n",
    "def complete(req: CompleteReq):\n",
    "    ctx_texts = [c.get(\"text\",\"\") for c in (req.contexts or [])]\n",
    "    ctx_str = \"\\n\".join(f\"- {t}\" for t in ctx_texts[:4])\n",
    "    t0 = time.time()\n",
    "    ans = f\"Q: {req.query}\\nA: Based on the retrieved contexts, here's a concise answer.\\n\\nContext:\\n{ctx_str}\"\n",
    "    ver = verify_bridge(ans, ctx_texts)\n",
    "    dt = int((time.time() - t0) * 1000)\n",
    "    RAG_COMPLETE_MS.observe(dt / 1.0)\n",
    "    return {\"answer\": ans, \"verify\": ver, \"latency_ms\": dt}\n",
    "@app.post(\"/sql/sanitize\")\n",
    "def sql_sanitize(req: SqlSanitizeReq):\n",
    "    try:\n",
    "        found = extract_sql(req.text)\n",
    "        if not found:\n",
    "            return {\"ok\": False, \"error\": \"no_sql_detected\"}\n",
    "        sanitized = sanitize_select_limit(found, max_limit=int(req.max_limit or 100))\n",
    "        return {\"ok\": True, \"sql\": sanitized}\n",
    "    except Exception as e:\n",
    "        return {\"ok\": False, \"error\": str(e)}\n",
    "@app.post(\"/schema/profile\")\n",
    "def schema_profile(req: SchemaProfileReq):\n",
    "    sample = req.sample or []\n",
    "    cols = {}\n",
    "    for row in sample[:200]:\n",
    "        for k, v in row.items():\n",
    "            col = cols.setdefault(k, {\"nulls\": 0, \"num_like\": 0, \"count\": 0})\n",
    "            col[\"count\"] += 1\n",
    "            if v in (None, \"\", \"null\", \"NaN\"): col[\"nulls\"] += 1\n",
    "            try:\n",
    "                float(v)\n",
    "                col[\"num_like\"] += 1\n",
    "            except Exception:\n",
    "                pass\n",
    "    stats = {k: {\"null_rate\": (v[\"nulls\"] / max(1, v[\"count\"])), \"numeric_likelihood\": (v[\"num_like\"] / max(1, v[\"count\"]))} for k, v in cols.items()}\n",
    "    pii_flags = [k for k in cols if any(t in k.lower() for t in (\"name\",\"email\",\"phone\",\"ssn\",\"address\",\"dob\"))]\n",
    "    return {\"schema_contract\": {\"columns\": list(cols.keys())}, \"stats\": stats, \"pii_flags\": pii_flags, \"confidence\": 0.92}\n",
    "@app.post(\"/schema/validate\")\n",
    "def schema_validate(req: SchemaValidateReq):\n",
    "    sample = req.sample or []\n",
    "    if not sample:\n",
    "        return {\"passes\": True, \"cast_error_rate\": 0.0}\n",
    "    keys = list(sample[0].keys())\n",
    "    numericish = set()\n",
    "    for k in keys:\n",
    "        vals = [r.get(k) for r in sample[:100] if k in r]\n",
    "        hits = 0\n",
    "        for v in vals:\n",
    "            try: float(v); hits += 1\n",
    "            except Exception: pass\n",
    "        if hits >= max(3, int(0.6 * len(vals))):\n",
    "            numericish.add(k)\n",
    "    total, bad = 0, 0\n",
    "    for row in sample[:500]:\n",
    "        for k in numericish:\n",
    "            total += 1\n",
    "            try:\n",
    "                float(row.get(k))\n",
    "            except Exception:\n",
    "                bad += 1\n",
    "    err = (bad / max(1, total))\n",
    "    return {\"passes\": err < 0.05, \"cast_error_rate\": err}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 323,
   "id": "05753475-9fbf-47f9-88b5-dc6aa47f18d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "_srv_thread: Optional[threading.Thread] = None\n",
    "def start_server(host: str = \"127.0.0.1\", prefer_port: int = 8001, tries: int = 10, log_level: str = \"warning\"):\n",
    "    \"\"\"Start uvicorn in background; on 'address in use', auto-increment port.\"\"\"\n",
    "    global _srv_thread\n",
    "    import uvicorn, socket\n",
    "    if _srv_thread and _srv_thread.is_alive():\n",
    "        print(\"Server already running.\")\n",
    "        return\n",
    "    port = prefer_port\n",
    "    for _ in range(tries):\n",
    "        s = socket.socket()\n",
    "        try:\n",
    "            s.bind((host, port))\n",
    "            s.close()\n",
    "            break\n",
    "        except OSError:\n",
    "            s.close()\n",
    "            port += 1\n",
    "    config = uvicorn.Config(app, host=host, port=port, log_level=log_level)\n",
    "    server = uvicorn.Server(config)\n",
    "    def _run():\n",
    "        _ = STATE.dense.topk(\"warmup\", 3)\n",
    "        _ = STATE.bm25.topk(\"warmup\", 3)\n",
    "        server.run()\n",
    "    _srv_thread = threading.Thread(target=_run, daemon=True)\n",
    "    _srv_thread.start()\n",
    "    time.sleep(0.6)\n",
    "    print(f\" RAG microservice up at http://{host}:{port}  (endpoints: /up, /retrieve_*, /rerank, /complete, /sql/sanitize, /metrics)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 324,
   "id": "4007b6ee-d244-44c0-8d9b-49009502b8dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _synth_docs(n: int) -> List[Dict[str, str]]:\n",
    "    docs = []\n",
    "    for i in range(n):\n",
    "        docs.append({\"id\": f\"d{i}\", \"text\": f\"doc {i} about bm25 vector search and rag patterns {i%97} {i%53}\"})\n",
    "    return docs\n",
    "def faiss_scale_probe(ns: List[int] = [5000, 20000, 50000], top_k: int = 10, bm25_prefilter: int = 200,\n",
    "                      out_csv: str = \"faiss_scale.csv\"):\n",
    "    rows = []\n",
    "    for N in ns:\n",
    "        docs = _synth_docs(N)\n",
    "        t0 = time.time()\n",
    "        dense = DenseIndex(docs)\n",
    "        build_s = time.time() - t0\n",
    "        qs = [f\"vector search bm25 {i%101}\" for i in range(25)]\n",
    "        lats = []\n",
    "        for q in qs:\n",
    "            t = time.time()\n",
    "            bm = BM25Index(docs).topk(q, bm25_prefilter)\n",
    "            cands = bm[:bm25_prefilter]\n",
    "            _ = dense.topk(q, top_k)\n",
    "            dt = (time.time() - t) * 1000\n",
    "            lats.append(dt)\n",
    "        p95 = float(np.percentile(lats, 95)) if lats else float(\"nan\")\n",
    "        rows.append({\"N\": N, \"build_s\": round(build_s, 3), \"q_p95_ms\": round(p95, 1), \"faiss\": dense.use_faiss})\n",
    "        print(f\"[FAISS] N={N:>6} build={build_s:>5.2f}s p95={p95:>6.1f}ms faiss={dense.use_faiss}\")\n",
    "    import csv\n",
    "    with open(out_csv, \"w\", newline=\"\", encoding=\"utf-8\") as f:\n",
    "        w = csv.DictWriter(f, fieldnames=[\"N\",\"build_s\",\"q_p95_ms\",\"faiss\"])\n",
    "        w.writeheader(); w.writerows(rows)\n",
    "    print(\" wrote\", out_csv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 325,
   "id": "210f3698-998e-45c7-97ea-4b2e67bcb6cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def bm25_scale_probe(ns: List[int] = [5000, 20000, 50000], top_k: int = 200, out_csv: str = \"bm25_scale.csv\"):\n",
    "    rows = []\n",
    "    for N in ns:\n",
    "        docs = _synth_docs(N)\n",
    "        t0 = time.time()\n",
    "        bm = BM25Index(docs)\n",
    "        build_s = time.time() - t0\n",
    "        qs = [f\"bm25 retrieval {i%101}\" for i in range(25)]\n",
    "        lats = []\n",
    "        for q in qs:\n",
    "            t = time.time()\n",
    "            _ = bm.topk(q, top_k)\n",
    "            dt = (time.time() - t) * 1000\n",
    "            lats.append(dt)\n",
    "        p95 = float(np.percentile(lats, 95)) if lats else float(\"nan\")\n",
    "        rows.append({\"N\": N, \"build_s\": round(build_s, 3), \"q_p95_ms\": round(p95, 1), \"impl\": \"rank_bm25\" if bm.have_bm25 else \"tf_fallback\"})\n",
    "        print(f\"[BM25 ] N={N:>6} build={build_s:>5.2f}s p95={p95:>6.1f}ms impl={'rank_bm25' if bm.have_bm25 else 'tf_fallback'}\")\n",
    "    import csv\n",
    "    with open(out_csv, \"w\", newline=\"\", encoding=\"utf-8\") as f:\n",
    "        w = csv.DictWriter(f, fieldnames=[\"N\",\"build_s\",\"q_p95_ms\",\"impl\"])\n",
    "        w.writeheader(); w.writerows(rows)\n",
    "    print(\" wrote\", out_csv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 326,
   "id": "ae7c6700-ba35-49e1-b6ce-4cbd852b13d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_router_v1_and_save(art_dir: str = \"artifacts\", fname: str = \"router_v1.joblib\"):\n",
    "    os.makedirs(art_dir, exist_ok=True)\n",
    "    try:\n",
    "        from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "        from sklearn.linear_model import LogisticRegression\n",
    "        import joblib\n",
    "    except Exception as e:\n",
    "        print(\"sklearn or joblib not available:\", e)\n",
    "        return None\n",
    "    train_data = [\n",
    "        (\"select age, income from dataset where balance > 1000 limit 5\", \"sql\"),\n",
    "        (\"show me top 10 rows from dataset\", \"sql\"),\n",
    "        (\"SELECT * FROM dataset LIMIT 3\", \"sql\"),\n",
    "        (\"avg(income) by tenure where default=1\", \"sql\"),\n",
    "        (\"2+2*10 - 3\", \"math\"),\n",
    "        (\"compute 1000/25 + 17 - 3\", \"math\"),\n",
    "        (\"what is 3.1415 * 2?\", \"math\"),\n",
    "        (\"(12+8)/5 * 3 - 4\", \"math\"),\n",
    "        (\"Do we comply with PII masking in logs?\", \"policy\"),\n",
    "        (\"Outline SQL guardrails and DML restrictions\", \"policy\"),\n",
    "        (\"What are the logging redaction rules for SSNs?\", \"policy\"),\n",
    "        (\"Explain how we prevent unsafe SQL\", \"policy\"),\n",
    "        (\"What are common notes or patterns?\", \"factual\"),\n",
    "        (\"How many records mention chargeback risk?\", \"factual\"),\n",
    "        (\"Summarize key trends in notes\", \"factual\"),\n",
    "        (\"List frequent phrases about late fees\", \"factual\"),\n",
    "    ]\n",
    "    X_txt, y_txt = zip(*train_data)\n",
    "    vec = TfidfVectorizer(analyzer=\"char_wb\", ngram_range=(3,5), max_features=8000)\n",
    "    X = vec.fit_transform(X_txt)\n",
    "    clf = LogisticRegression(max_iter=500).fit(X, y_txt)\n",
    "    path = os.path.join(art_dir, fname)\n",
    "    joblib.dump({\"vec\": vec, \"clf\": clf}, path)\n",
    "    print(\" saved\", path)\n",
    "    bundle = joblib.load(path)\n",
    "    vec2, clf2 = bundle[\"vec\"], bundle[\"clf\"]\n",
    "    qs = [\n",
    "        \"select * from dataset limit 5\",\n",
    "        \"2+2*10 - 3\",\n",
    "        \"Do we comply with PII masking in logs?\",\n",
    "        \"Summarize key trends in notes\",\n",
    "    ]\n",
    "    for q in qs:\n",
    "        p1 = clf.predict(vec.transform([q]))[0]\n",
    "        p2 = clf2.predict(vec2.transform([q]))[0]\n",
    "        assert p1 == p2, f\"Mismatch after reload on '{q}': {p1} vs {p2}\"\n",
    "    print(\" reload round-trip OK (identical predictions)\")\n",
    "    return path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 327,
   "id": "0718bac2-1e58-443f-94ec-e984bff5f63c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " RAG microservice up at http://127.0.0.1:8002  (endpoints: /up, /retrieve_*, /rerank, /complete, /sql/sanitize, /metrics)\n"
     ]
    }
   ],
   "source": [
    "def curl_examples():\n",
    "    base = \"http://127.0.0.1:8001\"\n",
    "    print(f\"\"\"\n",
    "# Health\n",
    "curl -s {base}/up | jq .\n",
    "# SQL sanitize\n",
    "curl -s -X POST {base}/sql/sanitize -H \"Content-Type: application/json\" \\\\\n",
    "  -d '{{\"text\":\"SELECT * FROM dataset;\"}}' | jq .\n",
    "curl -s -X POST {base}/sql/sanitize -H \"Content-Type: application/json\" \\\\\n",
    "  -d '{{\"text\":\"SELECT * FROM users; DROP TABLE users;\"}}' | jq .\n",
    "curl -s -X POST {base}/sql/sanitize -H \"Content-Type: application/json\" \\\\\n",
    "  -d '{{\"text\":\"this is not sql\"}}' | jq .\n",
    "\"\"\".strip())\n",
    "start_server(prefer_port=8001, tries=10, log_level=\"warning\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 328,
   "id": "ac0140dd-3d27-4100-90de-6507d46a7b87",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chaos: verifier_v2=True\n",
      "[FAISS] N=  1000 build= 3.05s p95=  44.7ms faiss=True\n",
      "[FAISS] N=  5000 build=11.14s p95=  86.9ms faiss=True\n",
      " wrote faiss_scale.csv\n",
      "[BM25 ] N=  1000 build= 0.01s p95=   2.0ms impl=rank_bm25\n",
      "[BM25 ] N=  5000 build= 0.03s p95=  13.0ms impl=rank_bm25\n",
      " wrote bm25_scale.csv\n",
      " saved artifacts\\router_v1.joblib\n",
      " reload round-trip OK (identical predictions)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'artifacts\\\\router_v1.joblib'"
      ]
     },
     "execution_count": 328,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chaos_set_verifier_v2(True)\n",
    "faiss_scale_probe(ns=[1000, 5000], top_k=5, bm25_prefilter=200, out_csv=\"faiss_scale.csv\")\n",
    "bm25_scale_probe(ns=[1000, 5000], top_k=100, out_csv=\"bm25_scale.csv\")\n",
    "train_router_v1_and_save()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 329,
   "id": "029a891d-3e5b-4b2a-83c2-85f70c8223dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, io, json, time, math, random, socket, threading, textwrap\n",
    "from pathlib import Path\n",
    "from typing import List, Dict, Tuple, Optional\n",
    "from dataclasses import dataclass\n",
    "import numpy as np\n",
    "from collections import Counter as CCounter         \n",
    "from prometheus_client import CollectorRegistry\n",
    "from prometheus_client import start_http_server as prom_start_http_server\n",
    "from prometheus_client import Counter as PromCounter\n",
    "from prometheus_client import Histogram as PromHistogram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 330,
   "id": "ddd9d597-c0cb-41ac-ad31-db2341cf230b",
   "metadata": {},
   "outputs": [],
   "source": [
    "HEAVY = False                          \n",
    "QPS = 8 if HEAVY else 4\n",
    "DUR = 30 if HEAVY else 12\n",
    "TOPK = 5\n",
    "BASE_PORT = 8001\n",
    "EXPORTER_PORT = 8051\n",
    "ROOT = Path(\".\")\n",
    "ART_LOCAL = ROOT/\"artifacts\"/\"local\"\n",
    "SNIPS = ROOT/\"local_snippets\"\n",
    "ART_LOCAL.mkdir(parents=True, exist_ok=True)\n",
    "SNIPS.mkdir(parents=True, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 331,
   "id": "8bf5b6f4-f94b-4ae7-bd1f-b35e5b1e6c94",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "def _try_server_up(port=BASE_PORT) -> bool:\n",
    "    try:\n",
    "        j = requests.get(f\"http://127.0.0.1:{port}/up\", timeout=2).json()\n",
    "        return bool(j.get(\"ok\"))\n",
    "    except Exception:\n",
    "        return False\n",
    "def ensure_server() -> int:\n",
    "    \"\"\"\n",
    "    Assumes you already defined start_server(host, prefer_port, tries, ...)\n",
    "    in earlier cells. We'll reuse that to spawn the FastAPI app if not up.\n",
    "    \"\"\"\n",
    "    global BASE_PORT\n",
    "    if _try_server_up(BASE_PORT):\n",
    "        return BASE_PORT\n",
    "    try:\n",
    "        start_server(prefer_port=BASE_PORT, tries=10, log_level=\"warning\")\n",
    "    except NameError:\n",
    "        raise RuntimeError(\"start_server(...) is not defined in this kernel.\")\n",
    "    for p in range(BASE_PORT, BASE_PORT+10):\n",
    "        if _try_server_up(p):\n",
    "            BASE_PORT = p\n",
    "            (ART_LOCAL/\"PORT.txt\").write_text(str(p), encoding=\"utf-8\")\n",
    "            return p\n",
    "    raise RuntimeError(\"Could not detect a running microservice on 8001..8010\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 332,
   "id": "3540b799-7227-4f46-8276-446910dfa3e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _rrf_fuse(dense: List[Dict], bm25: List[Dict], k: int = 5, k_rrf: float = 60.0) -> List[Dict]:\n",
    "    ids = {x[\"id\"] for x in dense} | {x[\"id\"] for x in bm25}\n",
    "    rd = {d[\"id\"]: i+1 for i, d in enumerate(dense)}\n",
    "    rb = {d[\"id\"]: i+1 for i, d in enumerate(bm25)}\n",
    "    out = []\n",
    "    for _id in ids:\n",
    "        sd = 1.0/(k_rrf + rd.get(_id, 10**9))\n",
    "        sb = 1.0/(k_rrf + rb.get(_id, 10**9))\n",
    "        txt = next((d[\"text\"] for d in dense if d[\"id\"] == _id), None) or \\\n",
    "              next((d[\"text\"] for d in bm25 if d[\"id\"] == _id), \"\")\n",
    "        out.append({\"id\": _id, \"text\": txt, \"score\": float(sd+sb)})\n",
    "    out.sort(key=lambda x: -x[\"score\"])\n",
    "    return out[:k]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 333,
   "id": "4e8d12aa-0efd-4b8e-b835-a14c2b80abde",
   "metadata": {},
   "outputs": [],
   "source": [
    "def one_request(base: str, q: str, top_k: int = 5, use_rerank: bool = True, timeout: float = 8.0) -> Dict:\n",
    "    s = requests.Session()\n",
    "    t0 = time.time()\n",
    "    t = time.time()\n",
    "    d_dense = s.post(f\"{base}/retrieve_dense\", json={\"query\": q, \"top_k\": top_k}, timeout=timeout).json()\n",
    "    dense_ms = int((time.time() - t) * 1000)\n",
    "    t = time.time()\n",
    "    d_bm25 = s.post(f\"{base}/retrieve_bm25\", json={\"query\": q, \"top_k\": top_k}, timeout=timeout).json()\n",
    "    bm25_ms = int((time.time() - t) * 1000)\n",
    "    fused = _rrf_fuse(d_dense.get(\"results\", []), d_bm25.get(\"results\", []), k=top_k)\n",
    "    if use_rerank:\n",
    "        t = time.time()\n",
    "        d_rer = s.post(f\"{base}/rerank\", json={\"query\": q, \"candidates\": fused}, timeout=timeout).json()\n",
    "        rerank_ms = int((time.time() - t) * 1000)\n",
    "        contexts = d_rer.get(\"results\", fused)\n",
    "    else:\n",
    "        rerank_ms = 0\n",
    "        contexts = fused\n",
    "    t = time.time()\n",
    "    d_comp = s.post(f\"{base}/complete\", json={\"query\": q, \"contexts\": contexts}, timeout=timeout).json()\n",
    "    complete_ms = int((time.time() - t) * 1000)\n",
    "    total_ms = int((time.time() - t0) * 1000)\n",
    "    return {\n",
    "        \"ok\": True,\n",
    "        \"latency_ms\": total_ms,\n",
    "        \"dense_ms\": dense_ms,\n",
    "        \"bm25_ms\": bm25_ms,\n",
    "        \"rerank_ms\": rerank_ms,\n",
    "        \"complete_ms\": complete_ms,\n",
    "        \"answer\": d_comp.get(\"answer\", \"\"),\n",
    "        \"verify\": d_comp.get(\"verify\", {}),\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 334,
   "id": "30036abc-d98d-4421-8e0b-8c015557f559",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _chaos_apply(toggle: Dict[str, bool]):\n",
    "    if \"disable_ce\" in toggle and 'chaos_disable_ce_reranker' in globals():\n",
    "        chaos_disable_ce_reranker(bool(toggle[\"disable_ce\"]))\n",
    "    if \"force_bm25\" in toggle and 'chaos_force_bm25' in globals():\n",
    "        chaos_force_bm25(bool(toggle[\"force_bm25\"]))\n",
    "    if \"verifier_v2\" in toggle and 'chaos_set_verifier_v2' in globals():\n",
    "        chaos_set_verifier_v2(bool(toggle[\"verifier_v2\"]))\n",
    "if 'GOLDEN' not in globals():\n",
    "    GOLDEN = [\n",
    "        {\"route\": \"sql\",     \"query\": \"select age, income and balance where default = 1\",     \"expect\": \"run_sql\"},\n",
    "        {\"route\": \"math\",    \"query\": \"2+2*10 - 3\",                                            \"expect\": \"respond\"},\n",
    "        {\"route\": \"factual\", \"query\": \"What are common notes for customers with late fees?\",   \"expect\": \"respond_with_citation\"},\n",
    "        {\"route\": \"policy\",  \"query\": \"Do we comply with PII masking in logs?\",               \"expect\": \"clarify_or_ticket\"},\n",
    "    ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 335,
   "id": "7fcb7f55-8695-4853-ab89-cfb69fe35288",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _golden_eval_case(g: Dict) -> Tuple[bool, str, Dict]:\n",
    "    out = pipeline(g[\"query\"])\n",
    "    ok = False\n",
    "    reason = \"\"\n",
    "    if g[\"route\"] == \"sql\":\n",
    "        ok = (out[\"action\"] == \"run_sql\" and\n",
    "              isinstance(out.get(\"payload\",{}).get(\"sql\",\"\"), str) and\n",
    "              (\"select\" in out[\"payload\"][\"sql\"].lower()) and\n",
    "              (\"limit\" in out[\"payload\"][\"sql\"].lower()))\n",
    "        reason = \"must be SELECT ... LIMIT\"\n",
    "    elif g[\"route\"] == \"math\":\n",
    "        ok = (out[\"action\"] == \"respond\" and isinstance(out.get(\"payload\",{}).get(\"numeric\", None), float))\n",
    "        reason = \"must yield numeric\"\n",
    "    elif g[\"route\"] == \"factual\":\n",
    "        ok = (out[\"action\"] == \"respond\" and len(out.get(\"citations\", [])) > 0)\n",
    "        reason = \"must respond with citation\"\n",
    "    elif g[\"route\"] == \"policy\":\n",
    "        ok = (out[\"action\"] in (\"clarify\",\"ticket\"))\n",
    "        reason = \"must clarify/ticket\"\n",
    "    return ok, reason, out\n",
    "def _json_write(p: Path, obj):\n",
    "    p.parent.mkdir(parents=True, exist_ok=True)\n",
    "    p.write_text(json.dumps(obj, indent=2) + \"\\n\", encoding=\"utf-8\")\n",
    "def _md_write(p: Path, s: str):\n",
    "    p.parent.mkdir(parents=True, exist_ok=True)\n",
    "    p.write_text(s.strip()+\"\\n\", encoding=\"utf-8\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 336,
   "id": "b5782e63-cb6b-4c81-afee-f036c11fc5f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_golden_once(tag: str,\n",
    "                    toggle: Dict[str, bool],\n",
    "                    md_out: Path,\n",
    "                    json_out: Path,\n",
    "                    expect_baseline_pass_ge: float = 95.0) -> float:\n",
    "    _chaos_apply(toggle)\n",
    "    rows = []\n",
    "    passed = 0\n",
    "    for g in GOLDEN:\n",
    "        ok, reason, out = _golden_eval_case(g)\n",
    "        rows.append({\n",
    "            \"tag\": tag,\n",
    "            \"route\": g[\"route\"],\n",
    "            \"query\": g[\"query\"],\n",
    "            \"ok\": bool(ok),\n",
    "            \"expect\": g[\"expect\"],\n",
    "            \"reason\": reason,\n",
    "            \"action\": out.get(\"action\"),\n",
    "            \"out\": out\n",
    "        })\n",
    "        passed += int(ok)\n",
    "    pass_rate = 100.0 * passed / max(1, len(GOLDEN))\n",
    "    _json_write(json_out, {\"tag\": tag, \"pass_rate\": pass_rate, \"rows\": rows})\n",
    "    md = [f\"# Golden Report  {tag}\",\n",
    "          f\"- Pass rate: **{pass_rate:.1f}%** ({passed}/{len(GOLDEN)})\",\n",
    "          f\"- Toggles: `{json.dumps(toggle)}`\",\n",
    "          \"\"]\n",
    "    for r in rows:\n",
    "        badge = \"\" if r[\"ok\"] else \"\"\n",
    "        md.append(f\"{badge} **{r['route']}**  `{r['action']}` :: {r['reason']}\")\n",
    "    if tag != \"baseline\":\n",
    "        md.append(\"\\n_No hard gating on chaos; compare to baseline and ensure behavior is stable._\")\n",
    "    _md_write(md_out, \"\\n\".join(md))\n",
    "    if tag == \"baseline\" and pass_rate < expect_baseline_pass_ge:\n",
    "        print(f\"Baseline pass rate {pass_rate:.1f}% < {expect_baseline_pass_ge}% (not failing the run; see report).\")\n",
    "    return pass_rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 337,
   "id": "1f91f659-3d3c-447a-8464-9203d1891ad7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_golden_pack() -> Dict[str, float]:\n",
    "    res = {}\n",
    "    res[\"baseline\"] = run_golden_once(\n",
    "        tag=\"baseline\",\n",
    "        toggle={\"disable_ce\": False, \"force_bm25\": False, \"verifier_v2\": False},\n",
    "        md_out=ART_LOCAL/\"golden_report_baseline.md\",\n",
    "        json_out=ART_LOCAL/\"golden_report_baseline.json\",\n",
    "        expect_baseline_pass_ge=95.0\n",
    "    )\n",
    "    res[\"no_rerank\"] = run_golden_once(\n",
    "        tag=\"no_rerank\",\n",
    "        toggle={\"disable_ce\": True, \"force_bm25\": False, \"verifier_v2\": False},\n",
    "        md_out=ART_LOCAL/\"golden_report_no_rerank.md\",\n",
    "        json_out=ART_LOCAL/\"golden_report_no_rerank.json\"\n",
    "    )\n",
    "    res[\"bm25_only\"] = run_golden_once(\n",
    "        tag=\"bm25_only\",\n",
    "        toggle={\"disable_ce\": True, \"force_bm25\": True, \"verifier_v2\": False},\n",
    "        md_out=ART_LOCAL/\"golden_report_bm25_only.md\",\n",
    "        json_out=ART_LOCAL/\"golden_report_bm25_only.json\"\n",
    "    )\n",
    "    res[\"verifier_v2_on\"] = run_golden_once(\n",
    "        tag=\"verifier_v2_on\",\n",
    "        toggle={\"disable_ce\": False, \"force_bm25\": False, \"verifier_v2\": True},\n",
    "        md_out=ART_LOCAL/\"golden_report_verifier_v2_on.md\",\n",
    "        json_out=ART_LOCAL/\"golden_report_verifier_v2_on.json\"\n",
    "    )\n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 338,
   "id": "84566884-0c4b-4faf-ab4e-ad6b38fbf425",
   "metadata": {},
   "outputs": [],
   "source": [
    "_PROM = {\"server\": None, \"registry\": None, \"inited\": False}\n",
    "def _start_exporter_if_needed(port: int = EXPORTER_PORT):\n",
    "    if _PROM[\"inited\"]:\n",
    "        return\n",
    "    reg = CollectorRegistry()\n",
    "    _PROM[\"registry\"] = reg\n",
    "    _PROM[\"LOADGEN_REQS\"]   = PromCounter(\"loadgen_requests_total\", \"Total loadgen requests\", [\"scenario\"], registry=reg)\n",
    "    _PROM[\"LOADGEN_ERRORS\"] = PromCounter(\"loadgen_errors_total\", \"Total loadgen errors\",   [\"scenario\"], registry=reg)\n",
    "    _PROM[\"LAT_MS\"]         = PromHistogram(\"loadgen_latency_ms\", \"End-to-end latency (ms)\", [\"scenario\"],\n",
    "                              buckets=(50,100,150,250,400,600,800,1000,1200,1500,1800,2500,4000), registry=reg)\n",
    "    _PROM[\"DENSE_MS\"]       = PromHistogram(\"loadgen_dense_ms\", \"Dense retrieval latency (ms)\", [\"scenario\"], registry=reg)\n",
    "    _PROM[\"BM25_MS\"]        = PromHistogram(\"loadgen_bm25_ms\", \"BM25 retrieval latency (ms)\", [\"scenario\"], registry=reg)\n",
    "    _PROM[\"RERANK_MS\"]      = PromHistogram(\"loadgen_rerank_ms\", \"Rerank latency (ms)\", [\"scenario\"], registry=reg)\n",
    "    _PROM[\"COMPLETE_MS\"]    = PromHistogram(\"loadgen_complete_ms\",\"Complete latency (ms)\", [\"scenario\"], registry=reg)\n",
    "    try:\n",
    "        prom_start_http_server(port, registry=reg)\n",
    "        _PROM[\"inited\"] = True\n",
    "        print(f\"Prometheus loadgen exporter on :{port} (separate registry)\")\n",
    "    except OSError:\n",
    "        for p in range(port+1, port+10):\n",
    "            try:\n",
    "                prom_start_http_server(p, registry=reg)\n",
    "                _PROM[\"inited\"] = True\n",
    "                print(f\"Prometheus loadgen exporter on :{p} (separate registry)\")\n",
    "                break\n",
    "            except OSError:\n",
    "                continue\n",
    "        if not _PROM[\"inited\"]:\n",
    "            print(\"Could not start Prometheus exporter (ports busy). Continuing without exporter.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 339,
   "id": "9bffaa61-8223-4906-abf6-34232daa7807",
   "metadata": {},
   "outputs": [],
   "source": [
    "import concurrent.futures as cf\n",
    "import matplotlib.pyplot as plt\n",
    "def _queries_default():\n",
    "    return [\n",
    "        \"What are common notes for customers with late fees?\",\n",
    "        \"select age, income and balance where default = 1 limit 5\",\n",
    "        \"2+2*10 - 3\",\n",
    "        \"Do we comply with PII masking in logs?\",\n",
    "        \"How do BM25 and vector search differ?\",\n",
    "    ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 340,
   "id": "19fb7f0b-c9fd-458b-8bf9-b2d16a7a09a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_test_profile(base: str,\n",
    "                      scenario: str,\n",
    "                      qps: int = QPS,\n",
    "                      duration_s: int = DUR,\n",
    "                      top_k: int = TOPK,\n",
    "                      use_rerank: bool = True,\n",
    "                      force_bm25: bool = False,\n",
    "                      save_png: Optional[Path] = None) -> Dict:\n",
    "    _chaos_apply({\"disable_ce\": (not use_rerank), \"force_bm25\": force_bm25})\n",
    "\n",
    "    latencies = []\n",
    "    per_step = {\"dense_ms\": [], \"bm25_ms\": [], \"rerank_ms\": [], \"complete_ms\": []}\n",
    "    errors = 0\n",
    "    sent = 0\n",
    "    start = time.time()\n",
    "    deadline = start + duration_s\n",
    "    queries = _queries_default()\n",
    "    while time.time() < deadline:\n",
    "        tick = time.time()\n",
    "        with cf.ThreadPoolExecutor(max_workers=qps) as ex:\n",
    "            futs = []\n",
    "            for _ in range(qps):\n",
    "                q = random.choice(queries)\n",
    "                futs.append(ex.submit(one_request, base, q, top_k, use_rerank))\n",
    "            for f in cf.as_completed(futs):\n",
    "                sent += 1\n",
    "                try:\n",
    "                    r = f.result()\n",
    "                    latencies.append(r[\"latency_ms\"])\n",
    "                    for k in per_step:\n",
    "                        per_step[k].append(r[k])\n",
    "                    if _PROM[\"inited\"]:\n",
    "                        _PROM[\"LOADGEN_REQS\"].labels(scenario).inc()\n",
    "                        _PROM[\"LAT_MS\"].labels(scenario).observe(r[\"latency_ms\"] / 1.0)\n",
    "                        _PROM[\"DENSE_MS\"].labels(scenario).observe(r[\"dense_ms\"] / 1.0)\n",
    "                        _PROM[\"BM25_MS\"].labels(scenario).observe(r[\"bm25_ms\"] / 1.0)\n",
    "                        if r[\"rerank_ms\"] > 0:\n",
    "                            _PROM[\"RERANK_MS\"].labels(scenario).observe(r[\"rerank_ms\"] / 1.0)\n",
    "                        _PROM[\"COMPLETE_MS\"].labels(scenario).observe(r[\"complete_ms\"] / 1.0)\n",
    "                except Exception:\n",
    "                    errors += 1\n",
    "                    if _PROM[\"inited\"]:\n",
    "                        _PROM[\"LOADGEN_ERRORS\"].labels(scenario).inc()\n",
    "        elapsed = time.time() - tick\n",
    "        time.sleep(max(0.0, 1.0 - elapsed))\n",
    "    p50 = float(np.percentile(latencies, 50)) if latencies else float(\"nan\")\n",
    "    p95 = float(np.percentile(latencies, 95)) if latencies else float(\"nan\")\n",
    "    p99 = float(np.percentile(latencies, 99)) if latencies else float(\"nan\")\n",
    "    if save_png is not None:\n",
    "        plt.figure()\n",
    "        plt.hist(latencies, bins=40)\n",
    "        plt.title(f\"{scenario} latencies (ms)\")\n",
    "        plt.xlabel(\"Latency (ms)\")\n",
    "        plt.ylabel(\"Count\")\n",
    "        save_png.parent.mkdir(parents=True, exist_ok=True)\n",
    "        plt.savefig(save_png, bbox_inches=\"tight\")\n",
    "        plt.close()\n",
    "    summary = {\n",
    "        \"scenario\": scenario,\n",
    "        \"sent\": sent,\n",
    "        \"ok\": len(latencies),\n",
    "        \"errors\": errors,\n",
    "        \"error_rate\": errors / max(1, sent),\n",
    "        \"qps_observed\": sent / max(0.001, (time.time() - start)),\n",
    "        \"p50_ms\": p50, \"p95_ms\": p95, \"p99_ms\": p99,\n",
    "        \"step_p95\": {k: float(np.percentile(v, 95)) if v else None for k, v in per_step.items()},\n",
    "        \"use_rerank\": use_rerank,\n",
    "        \"force_bm25\": force_bm25,\n",
    "        \"top_k\": top_k,\n",
    "        \"duration_s\": duration_s,\n",
    "        \"qps_target\": qps\n",
    "    }\n",
    "    return summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 341,
   "id": "60ad31ca-0367-4d45-9a54-910ce56d576f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_loadgen_pack(base: str) -> Dict[str, Dict]:\n",
    "    _start_exporter_if_needed(EXPORTER_PORT)\n",
    "    res = {}\n",
    "    res[\"baseline\"] = load_test_profile(\n",
    "        base=base, scenario=\"baseline\", qps=QPS, duration_s=DUR, top_k=TOPK, use_rerank=True,\n",
    "        save_png=ART_LOCAL/\"hist_baseline.png\"\n",
    "    )\n",
    "    _json_write(ART_LOCAL/\"loadgen_baseline.json\", res[\"baseline\"])\n",
    "\n",
    "    res[\"no_rerank\"] = load_test_profile(\n",
    "        base=base, scenario=\"no_rerank\", qps=QPS, duration_s=DUR, top_k=TOPK, use_rerank=False,\n",
    "        save_png=ART_LOCAL/\"hist_no_rerank.png\"\n",
    "    )\n",
    "    _json_write(ART_LOCAL/\"loadgen_no_rerank.json\", res[\"no_rerank\"])\n",
    "\n",
    "    res[\"bm25_only\"] = load_test_profile(\n",
    "        base=base, scenario=\"bm25_only\", qps=QPS, duration_s=DUR, top_k=TOPK, use_rerank=False, force_bm25=True,\n",
    "        save_png=ART_LOCAL/\"hist_bm25_only.png\"\n",
    "    )\n",
    "    _json_write(ART_LOCAL/\"loadgen_bm25_only.json\", res[\"bm25_only\"])\n",
    "    slo = res[\"baseline\"][\"p95_ms\"]\n",
    "    if not math.isnan(slo) and slo > 1800:\n",
    "        print(f\"Baseline p95 {slo:.0f}ms > 1800ms SLO (see hist & JSON).\")\n",
    "    else:\n",
    "        print(f\" Baseline p95 {slo:.0f}ms within SLO.\")\n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 342,
   "id": "fa18b79b-a097-4201-951a-368879d1172a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_extra_snippets():\n",
    "    (SNIPS/\"SQL_SANITIZE.md\").write_text(textwrap.dedent(\"\"\"\n",
    "    # SQL_SANITIZE examples\n",
    "    ## Clean SELECT\n",
    "    POST /sql/sanitize\n",
    "    {\"text\":\"SELECT * FROM dataset\"}\n",
    "     {\"ok\": true, \"sql\": \"SELECT * FROM dataset LIMIT 100\"}\n",
    "    ## Malicious (DROP)\n",
    "    {\"text\":\"SELECT * FROM users; DROP TABLE users;\"}\n",
    "     {\"ok\": false, \"error\": \"multiple statements not allowed\"}\n",
    "    ## No SQL\n",
    "    {\"text\":\"this is not sql\"}\n",
    "     {\"ok\": false, \"error\": \"no_sql_detected\"}\n",
    "    \"\"\").strip()+\"\\n\", encoding=\"utf-8\")\n",
    "    (SNIPS/\"VERIFIER_NOTES.md\").write_text(textwrap.dedent(\"\"\"\n",
    "    # Verifier Notes\n",
    "    - **v1 (default):** span overlap + numeric consistency  proxy faithfulness.\n",
    "    - **v2 (optional):** if `verifier_v2` chaos flag ON and MNLI available, blend MNLI entailment with v1.\n",
    "    Toggle in notebook:\n",
    "    ```python\n",
    "    chaos_set_verifier_v2(True)   # on\n",
    "    chaos_set_verifier_v2(False)  # off\n",
    "    ```\n",
    "    \"\"\").strip()+\"\\n\", encoding=\"utf-8\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 343,
   "id": "825eb541-f9d0-4ce5-a88c-6ab91e770ba6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def bundle_local_artifacts() -> Dict:\n",
    "    files = []\n",
    "    for p in [\n",
    "        ART_LOCAL/\"golden_report_baseline.md\",\n",
    "        ART_LOCAL/\"golden_report_baseline.json\",\n",
    "        ART_LOCAL/\"golden_report_no_rerank.md\",\n",
    "        ART_LOCAL/\"golden_report_no_rerank.json\",\n",
    "        ART_LOCAL/\"golden_report_bm25_only.md\",\n",
    "        ART_LOCAL/\"golden_report_bm25_only.json\",\n",
    "        ART_LOCAL/\"golden_report_verifier_v2_on.md\",\n",
    "        ART_LOCAL/\"golden_report_verifier_v2_on.json\",\n",
    "        ART_LOCAL/\"loadgen_baseline.json\",\n",
    "        ART_LOCAL/\"loadgen_no_rerank.json\",\n",
    "        ART_LOCAL/\"loadgen_bm25_only.json\",\n",
    "        ART_LOCAL/\"hist_baseline.png\",\n",
    "        ART_LOCAL/\"hist_no_rerank.png\",\n",
    "        ART_LOCAL/\"hist_bm25_only.png\",\n",
    "        ART_LOCAL/\"PORT.txt\",\n",
    "        ROOT/\"faiss_scale.csv\",\n",
    "        ROOT/\"bm25_scale.csv\",\n",
    "        ROOT/\"artifacts\"/\"router_v1.joblib\",\n",
    "        SNIPS/\"API_QUICK_REFERENCE_SNIPPETS.md\",\n",
    "        SNIPS/\"OVERSEER_SCHEMA_SNIPPET.json\",\n",
    "        SNIPS/\"RUNBOOK_DISABLE_CANARY.md\",\n",
    "        SNIPS/\"SQL_SANITIZE.md\",\n",
    "        SNIPS/\"VERIFIER_NOTES.md\",\n",
    "    ]:\n",
    "        if p.exists():\n",
    "            files.append(str(p))\n",
    "    (ART_LOCAL/\"MANIFEST.json\").write_text(json.dumps({\"files\": files}, indent=2)+\"\\n\", encoding=\"utf-8\")\n",
    "    return {\"count\": len(files), \"root\": str(ART_LOCAL)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 344,
   "id": "34e714bb-2eef-4e7c-b871-56314f6bb6df",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_golden_pack_and_smoke():\n",
    "    port = ensure_server()\n",
    "    base = f\"http://127.0.0.1:{port}\"\n",
    "    up = requests.get(f\"{base}/up\", timeout=5).json()\n",
    "    print(\"UP:\", up)\n",
    "    res = run_golden_pack()\n",
    "    print(\" Golden reports written under artifacts/local/\")\n",
    "    _start_exporter_if_needed(EXPORTER_PORT)\n",
    "    quick = load_test_profile(base, scenario=\"smoke\", qps=QPS, duration_s=10, top_k=TOPK, use_rerank=True,\n",
    "                              save_png=ART_LOCAL/\"hist_smoke.png\")\n",
    "    _json_write(ART_LOCAL/\"loadgen_smoke.json\", quick)\n",
    "    print(\" Smoke loadgen done (10s). p95(ms) =\", quick[\"p95_ms\"])\n",
    "    return {\"golden\": res, \"smoke\": quick}\n",
    "def run_all_reports_and_bundle():\n",
    "    port = ensure_server()\n",
    "    base = f\"http://127.0.0.1:{port}\"\n",
    "    _ = run_golden_pack()\n",
    "    print(\" Golden reports done.\")\n",
    "    profiles = run_loadgen_pack(base)\n",
    "    print(\" Loadgen profiles done.\")\n",
    "    write_extra_snippets()\n",
    "    print(\" Docs snippets written.\")\n",
    "    manifest = bundle_local_artifacts()\n",
    "    print(\" Bundle ready:\", manifest)\n",
    "    return {\"profiles\": profiles, \"manifest\": manifest}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 345,
   "id": "3235460c-ddb3-4eee-ae2c-5ef33e69559f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chaos: CE reranker disabled=False\n",
      "Chaos: force BM25 fallback=False\n",
      "Chaos: verifier_v2=False\n",
      "Chaos: CE reranker disabled=True\n",
      "Chaos: force BM25 fallback=False\n",
      "Chaos: verifier_v2=False\n",
      "Chaos: CE reranker disabled=True\n",
      "Chaos: force BM25 fallback=True\n",
      "Chaos: verifier_v2=False\n",
      "Chaos: CE reranker disabled=False\n",
      "Chaos: force BM25 fallback=False\n",
      "Chaos: verifier_v2=True\n",
      " Golden reports done.\n",
      "Prometheus loadgen exporter on :8051 (separate registry)\n",
      "Chaos: CE reranker disabled=False\n",
      "Chaos: force BM25 fallback=False\n",
      "Chaos: CE reranker disabled=True\n",
      "Chaos: force BM25 fallback=False\n",
      "Chaos: CE reranker disabled=True\n",
      "Chaos: force BM25 fallback=True\n",
      " Baseline p95 284ms within SLO.\n",
      " Loadgen profiles done.\n",
      " Docs snippets written.\n",
      " Bundle ready: {'count': 22, 'root': 'artifacts\\\\local'}\n",
      "UP: {'ok': True, 'base': 'http://127.0.0.1:8001'}\n",
      "Chaos: CE reranker disabled=False\n",
      "Chaos: force BM25 fallback=False\n",
      "Chaos: verifier_v2=False\n",
      "Chaos: CE reranker disabled=True\n",
      "Chaos: force BM25 fallback=False\n",
      "Chaos: verifier_v2=False\n",
      "Chaos: CE reranker disabled=True\n",
      "Chaos: force BM25 fallback=True\n",
      "Chaos: verifier_v2=False\n",
      "Chaos: CE reranker disabled=False\n",
      "Chaos: force BM25 fallback=False\n",
      "Chaos: verifier_v2=True\n",
      " Golden reports written under artifacts/local/\n",
      "Chaos: CE reranker disabled=False\n",
      "Chaos: force BM25 fallback=False\n",
      " Smoke loadgen done (10s). p95(ms) = 455.0\n",
      "\n",
      "ALL GREEN \n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\" or True:\n",
    "    port = ensure_server()\n",
    "    res = run_all_reports_and_bundle()\n",
    "    res_smoke = run_golden_pack_and_smoke()\n",
    "    print(\"\\nALL GREEN \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 346,
   "id": "47e9a280-4407-4b94-a4a7-35c6d70708ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, sys, json, time, textwrap, subprocess, socket, shutil\n",
    "from pathlib import Path\n",
    "ROOT = Path(\"rag_rust_gateway\").resolve()\n",
    "SRC = ROOT / \"src\"\n",
    "SRC.mkdir(parents=True, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 347,
   "id": "76d44916-b6b0-44cc-968c-3f68bc57f424",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "12276"
      ]
     },
     "execution_count": 347,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(ROOT / \"Cargo.toml\").write_text(\"\"\"\\\n",
    "[package]\n",
    "name = \"rag_rust_gateway\"\n",
    "version = \"0.3.0\"\n",
    "edition = \"2021\"\n",
    "\n",
    "[dependencies]\n",
    "axum = { version = \"0.7\", features = [\"json\"] }\n",
    "tokio = { version = \"1\", features = [\"rt-multi-thread\", \"macros\"] }\n",
    "serde = { version = \"1\", features = [\"derive\"] }\n",
    "serde_json = \"1\"\n",
    "reqwest = { version = \"0.12\", default-features = false, features = [\"json\", \"rustls-tls\"] }\n",
    "tracing = \"0.1\"\n",
    "tracing-subscriber = { version = \"0.3\", features = [\"env-filter\", \"fmt\"] }\n",
    "anyhow = \"1\"\n",
    "prometheus = \"0.13\"\n",
    "\"\"\", encoding=\"utf-8\")\n",
    "\n",
    "(ROOT / \"rust-toolchain.toml\").write_text(\"\"\"\\\n",
    "[toolchain]\n",
    "channel = \"stable\"\n",
    "components = [\"rustfmt\", \"clippy\"]\n",
    "\"\"\", encoding=\"utf-8\")\n",
    "\n",
    "(SRC / \"main.rs\").write_text(r'''\n",
    "use axum::{routing::{get, post}, Router, http::StatusCode, Json};\n",
    "use serde::{Deserialize, Serialize};\n",
    "use std::{env, fs, path::PathBuf, time::{Duration, Instant}, net::SocketAddr, sync::Arc};\n",
    "use prometheus::{Encoder, TextEncoder, Registry, IntCounter, IntCounterVec, Histogram, HistogramOpts, HistogramVec};\n",
    "use tracing_subscriber::{layer::SubscriberExt, util::SubscriberInitExt};\n",
    "\n",
    "#[derive(Clone)]\n",
    "struct Cfg {\n",
    "    rag_base: String,\n",
    "    top_k_default: usize,\n",
    "    use_rerank_default: bool,\n",
    "    timeout_ms: u64,\n",
    "    insecure: bool,\n",
    "}\n",
    "\n",
    "impl Cfg {\n",
    "    fn from_env() -> Self {\n",
    "        let rag_base = env::var(\"RAG_BASE\").unwrap_or_else(|_| \"http://127.0.0.1:8001\".into());\n",
    "        let top_k_default = env::var(\"TOP_K\").ok().and_then(|s| s.parse().ok()).unwrap_or(5usize);\n",
    "        let use_rerank_default = env::var(\"USE_RERANK\").ok().and_then(|s| s.parse::<u8>().ok()).map(|v| v != 0).unwrap_or(true);\n",
    "        let timeout_ms = env::var(\"GATEWAY_TIMEOUT_MS\").ok().and_then(|s| s.parse::<u64>().ok()).unwrap_or(1500);\n",
    "        let insecure = env::var(\"INSECURE\").ok().and_then(|s| s.parse::<u8>().ok()).map(|v| v != 0).unwrap_or(false);\n",
    "        Self { rag_base, top_k_default, use_rerank_default, timeout_ms, insecure }\n",
    "    }\n",
    "}\n",
    "\n",
    "#[derive(Serialize)]\n",
    "struct UpResp {\n",
    "    ok: bool,\n",
    "    gateway: serde_json::Value,\n",
    "    microservice: serde_json::Value,\n",
    "}\n",
    "\n",
    "#[derive(Deserialize)]\n",
    "struct AnswerReq { query: String, top_k: Option<usize>, use_rerank: Option<bool> }\n",
    "\n",
    "#[derive(Serialize, Deserialize, Debug, Clone)]\n",
    "struct Doc { id: String, text: String, score: f32 }\n",
    "\n",
    "#[derive(Serialize)]\n",
    "struct AnswerMeta { pipeline: String, latency_ms: u128 }\n",
    "\n",
    "#[derive(Serialize)]\n",
    "struct AnswerResp { answer: String, contexts: Vec<Doc>, meta: AnswerMeta }\n",
    "\n",
    "#[derive(Serialize, Deserialize)]\n",
    "struct RetrieveReq { query: String, top_k: usize }\n",
    "\n",
    "#[derive(Serialize, Deserialize)]\n",
    "struct RetrieveResp { results: Vec<Doc>, latency_ms: Option<i64> }\n",
    "\n",
    "#[derive(Serialize, Deserialize)]\n",
    "struct RerankReq { query: String, candidates: Vec<Doc> }\n",
    "\n",
    "#[derive(Serialize, Deserialize)]\n",
    "struct RerankResp { results: Vec<Doc>, latency_ms: Option<i64> }\n",
    "\n",
    "#[derive(Serialize, Deserialize)]\n",
    "struct CompleteReq { query: String, contexts: Vec<Doc> }\n",
    "\n",
    "#[derive(Serialize, Deserialize)]\n",
    "struct CompleteResp { answer: String, verify: serde_json::Value, latency_ms: Option<i64> }\n",
    "\n",
    "struct Metrics {\n",
    "    registry: Registry,\n",
    "    bind_attempts: IntCounter,\n",
    "    answer_reqs: IntCounter,\n",
    "    answer_errors: IntCounterVec,\n",
    "    answer_latency: Histogram,\n",
    "    step_latency: HistogramVec,\n",
    "}\n",
    "\n",
    "impl Metrics {\n",
    "    fn new() -> Self {\n",
    "        let registry = Registry::new();\n",
    "\n",
    "        let bind_attempts = IntCounter::new(\"rust_gateway_bind_attempts_total\", \"Bind attempts\").unwrap();\n",
    "        let answer_reqs = IntCounter::new(\"gateway_answer_requests_total\", \"Answer requests\").unwrap();\n",
    "        let answer_errors = IntCounterVec::new(\n",
    "            prometheus::Opts::new(\"gateway_answer_errors_total\", \"Answer errors\"),\n",
    "            &[\"stage\"]\n",
    "        ).unwrap();\n",
    "        let answer_latency = Histogram::with_opts(HistogramOpts::new(\n",
    "            \"gateway_answer_latency_ms\", \"End-to-end latency ms\"\n",
    "        )).unwrap();\n",
    "        let step_latency = HistogramVec::new(\n",
    "            HistogramOpts::new(\"gateway_step_latency_ms\", \"Fan-out step latency ms\"),\n",
    "            &[\"stage\"]\n",
    "        ).unwrap();\n",
    "\n",
    "        registry.register(Box::new(bind_attempts.clone())).unwrap();\n",
    "        registry.register(Box::new(answer_reqs.clone())).unwrap();\n",
    "        registry.register(Box::new(answer_errors.clone())).unwrap();\n",
    "        registry.register(Box::new(answer_latency.clone())).unwrap();\n",
    "        registry.register(Box::new(step_latency.clone())).unwrap();\n",
    "\n",
    "        Self { registry, bind_attempts, answer_reqs, answer_errors, answer_latency, step_latency }\n",
    "    }\n",
    "}\n",
    "\n",
    "#[tokio::main]\n",
    "async fn main() -> anyhow::Result<()> {\n",
    "    tracing_subscriber::registry()\n",
    "        .with(tracing_subscriber::EnvFilter::new(\n",
    "            env::var(\"RUST_LOG\").unwrap_or_else(|_| \"rag_rust_gateway=info,axum=warn\".into()),\n",
    "        ))\n",
    "        .with(tracing_subscriber::fmt::layer())\n",
    "        .init();\n",
    "\n",
    "    let cfg = Arc::new(Cfg::from_env());\n",
    "    let metrics = Arc::new(Metrics::new());\n",
    "\n",
    "    // Build reqwest client\n",
    "    let mut client = reqwest::Client::builder()\n",
    "        .timeout(Duration::from_millis(cfg.timeout_ms));\n",
    "    if cfg.insecure {\n",
    "        client = client.danger_accept_invalid_certs(true);\n",
    "    }\n",
    "    let client = Arc::new(client.build()?);\n",
    "\n",
    "    // Wait for microservice /up (bounded backoff)\n",
    "    wait_for_microservice(&client, &cfg.rag_base).await?;\n",
    "\n",
    "    // Routes\n",
    "    let app = Router::new()\n",
    "        .route(\"/up\", get({\n",
    "            let cfg = cfg.clone(); let client = client.clone();\n",
    "            move || up_handler(cfg.clone(), client.clone())\n",
    "        }))\n",
    "        .route(\"/answer\", post({\n",
    "            let cfg = cfg.clone(); let client = client.clone(); let m = metrics.clone();\n",
    "            move |payload| answer_handler(payload, cfg.clone(), client.clone(), m.clone())\n",
    "        }))\n",
    "        .route(\"/metrics\", get({\n",
    "            let m = metrics.clone();\n",
    "            move || async move {\n",
    "                let encoder = TextEncoder::new();\n",
    "                let metric_families = m.registry.gather();\n",
    "                let mut buf = Vec::new();\n",
    "                encoder.encode(&metric_families, &mut buf).unwrap();\n",
    "                (StatusCode::OK, String::from_utf8(buf).unwrap())\n",
    "            }\n",
    "        }));\n",
    "\n",
    "    // Bind: prefer GATEWAY_PORT else ephemeral; write PORT.txt\n",
    "    let port_env = env::var(\"GATEWAY_PORT\").ok().and_then(|s| s.parse::<u16>().ok());\n",
    "    let listener = match try_bind(port_env, &metrics).await {\n",
    "        Ok(lst) => lst,\n",
    "        Err(e) => {\n",
    "            tracing::error!(\"Failed to bind: {}\", e);\n",
    "            return Err(e);\n",
    "        }\n",
    "    };\n",
    "    let bound = listener.local_addr()?;\n",
    "    write_port_txt(bound.port())?;\n",
    "\n",
    "    tracing::info!(\"Gateway listening on {}\", bound);\n",
    "    axum::serve(listener, app).await?;\n",
    "    Ok(())\n",
    "}\n",
    "\n",
    "async fn try_bind(pref: Option<u16>, metrics: &Metrics) -> anyhow::Result<tokio::net::TcpListener> {\n",
    "    let start = Instant::now();\n",
    "    let desired = pref.unwrap_or(0);\n",
    "    metrics.bind_attempts.inc();\n",
    "\n",
    "    let listener = if desired != 0 {\n",
    "        match tokio::net::TcpListener::bind(SocketAddr::from(([127,0,0,1], desired))).await {\n",
    "            Ok(l) => l,\n",
    "            Err(_) => tokio::net::TcpListener::bind(SocketAddr::from(([127,0,0,1], 0))).await?,\n",
    "        }\n",
    "    } else {\n",
    "        tokio::net::TcpListener::bind(SocketAddr::from(([127,0,0,1], 0))).await?\n",
    "    };\n",
    "    let elapsed = start.elapsed();\n",
    "    if elapsed > Duration::from_millis(500) {\n",
    "        tracing::warn!(\"Bind exceeded 500ms SLO: {:?}\", elapsed);\n",
    "    }\n",
    "    Ok(listener)\n",
    "}\n",
    "\n",
    "fn write_port_txt(port: u16) -> anyhow::Result<()> {\n",
    "    let cwd_port = PathBuf::from(\"PORT.txt\");\n",
    "    if fs::write(&cwd_port, port.to_string()).is_err() {\n",
    "        let mut tmp = std::env::temp_dir();\n",
    "        tmp.push(\"PORT.txt\");\n",
    "        fs::write(&tmp, port.to_string())?;\n",
    "    }\n",
    "    Ok(())\n",
    "}\n",
    "\n",
    "async fn up_handler(cfg: Arc<Cfg>, client: Arc<reqwest::Client>)\n",
    " -> Result<Json<UpResp>, StatusCode> {\n",
    "    let gw = serde_json::json!({\n",
    "        \"rag_base\": cfg.rag_base,\n",
    "        \"timeout_ms\": cfg.timeout_ms,\n",
    "        \"top_k_default\": cfg.top_k_default,\n",
    "        \"use_rerank_default\": cfg.use_rerank_default,\n",
    "        \"insecure\": cfg.insecure\n",
    "    });\n",
    "    let ms = match client.get(format!(\"{}/up\", cfg.rag_base)).send().await {\n",
    "        Ok(r) => r.json::<serde_json::Value>().await.unwrap_or(serde_json::json!({\"ok\": false})),\n",
    "        Err(_) => serde_json::json!({\"ok\": false}),\n",
    "    };\n",
    "    Ok(Json(UpResp { ok: true, gateway: gw, microservice: ms }))\n",
    "}\n",
    "\n",
    "async fn answer_handler(\n",
    "    axum::Json(req): axum::Json<AnswerReq>,\n",
    "    cfg: Arc<Cfg>,\n",
    "    client: Arc<reqwest::Client>,\n",
    "    metrics: Arc<Metrics>,\n",
    ") -> Result<Json<AnswerResp>, StatusCode> {\n",
    "    metrics.answer_reqs.inc();\n",
    "    let t0 = Instant::now();\n",
    "\n",
    "    let top_k = req.top_k.unwrap_or(cfg.top_k_default).max(1).min(50);\n",
    "    let use_rerank = req.use_rerank.unwrap_or(cfg.use_rerank_default);\n",
    "    let base = &cfg.rag_base;\n",
    "\n",
    "    // Fan-out: retrieve_dense + retrieve_bm25 (parallel)\n",
    "    let t_retrieve = Instant::now();\n",
    "    let (dense_res, bm25_res) = tokio::join!(\n",
    "        client.post(format!(\"{}/retrieve_dense\", base))\n",
    "              .json(&RetrieveReq { query: req.query.clone(), top_k }).send(),\n",
    "        client.post(format!(\"{}/retrieve_bm25\", base))\n",
    "              .json(&RetrieveReq { query: req.query.clone(), top_k }).send()\n",
    "    );\n",
    "\n",
    "    let dense: RetrieveResp = match dense_res {\n",
    "        Ok(r) => r.json().await.map_err(|_| StatusCode::BAD_GATEWAY).unwrap_or(RetrieveResp{results: vec![], latency_ms: None}),\n",
    "        Err(_) => {\n",
    "            metrics.answer_errors.with_label_values(&[\"retrieve_dense\"]).inc();\n",
    "            return Err(StatusCode::BAD_GATEWAY);\n",
    "        }\n",
    "    };\n",
    "    let bm25: RetrieveResp = match bm25_res {\n",
    "        Ok(r) => r.json().await.map_err(|_| StatusCode::BAD_GATEWAY).unwrap_or(RetrieveResp{results: vec![], latency_ms: None}),\n",
    "        Err(_) => {\n",
    "            metrics.answer_errors.with_label_values(&[\"retrieve_bm25\"]).inc();\n",
    "            return Err(StatusCode::BAD_GATEWAY);\n",
    "        }\n",
    "    };\n",
    "    metrics.step_latency.with_label_values(&[\"retrieve\"]).observe(t_retrieve.elapsed().as_millis() as f64);\n",
    "\n",
    "    // RRF fusion (if CE off) or call rerank\n",
    "    let contexts: Vec<Doc>;\n",
    "    if use_rerank {\n",
    "        let t = Instant::now();\n",
    "        let rer = client.post(format!(\"{}/rerank\", base))\n",
    "            .json(&RerankReq { query: req.query.clone(), candidates: rrf_merge(&dense.results, &bm25.results, top_k) })\n",
    "            .send().await.map_err(|_| StatusCode::BAD_GATEWAY)?;\n",
    "        let reresp: RerankResp = rer.json().await.map_err(|_| StatusCode::BAD_GATEWAY)?;\n",
    "        contexts = reresp.results;\n",
    "        metrics.step_latency.with_label_values(&[\"rerank\"]).observe(t.elapsed().as_millis() as f64);\n",
    "    } else {\n",
    "        contexts = rrf_merge(&dense.results, &bm25.results, top_k);\n",
    "    }\n",
    "\n",
    "    // Complete\n",
    "    let t = Instant::now();\n",
    "    let comp = client.post(format!(\"{}/complete\", base))\n",
    "        .json(&CompleteReq { query: req.query.clone(), contexts: contexts.clone() })\n",
    "        .send().await.map_err(|_| StatusCode::BAD_GATEWAY)?;\n",
    "    let compresp: CompleteResp = comp.json().await.map_err(|_| StatusCode::BAD_GATEWAY)?;\n",
    "    metrics.step_latency.with_label_values(&[\"complete\"]).observe(t.elapsed().as_millis() as f64);\n",
    "\n",
    "    let ms = t0.elapsed().as_millis();\n",
    "    metrics.answer_latency.observe(ms as f64);\n",
    "\n",
    "    Ok(Json(AnswerResp {\n",
    "        answer: compresp.answer,\n",
    "        contexts,\n",
    "        meta: AnswerMeta { pipeline: if use_rerank { \"rerank\".into() } else { \"rrf\".into() }, latency_ms: ms }\n",
    "    }))\n",
    "}\n",
    "\n",
    "fn rrf_merge(dense: &Vec<Doc>, bm25: &Vec<Doc>, k: usize) -> Vec<Doc> {\n",
    "    use std::collections::{HashMap, HashSet};\n",
    "    let mut rd: HashMap<&str, usize> = HashMap::new();\n",
    "    let mut rb: HashMap<&str, usize> = HashMap::new();\n",
    "    for (i, d) in dense.iter().enumerate() { rd.insert(d.id.as_str(), i+1); }\n",
    "    for (i, d) in bm25.iter().enumerate() { rb.insert(d.id.as_str(), i+1); }\n",
    "\n",
    "    let mut ids: HashSet<&str> = HashSet::new();\n",
    "    for d in dense { ids.insert(d.id.as_str()); }\n",
    "    for d in bm25 { ids.insert(d.id.as_str()); }\n",
    "\n",
    "    let k_rrf: f32 = 60.0;\n",
    "    let mut fused: Vec<Doc> = ids.into_iter().map(|id| {\n",
    "        let sd = rd.get(id).map(|r| 1.0 / (k_rrf + *r as f32)).unwrap_or(0.0);\n",
    "        let sb = rb.get(id).map(|r| 1.0 / (k_rrf + *r as f32)).unwrap_or(0.0);\n",
    "        let text = dense.iter().find(|d| d.id == id)\n",
    "            .or_else(|| bm25.iter().find(|d| d.id == id))\n",
    "            .map(|d| d.text.clone()).unwrap_or_default();\n",
    "        Doc { id: id.to_string(), text, score: sd + sb }\n",
    "    }).collect();\n",
    "\n",
    "    fused.sort_by(|a,b| b.score.partial_cmp(&a.score).unwrap());\n",
    "    fused.truncate(k);\n",
    "    fused\n",
    "}\n",
    "\n",
    "async fn wait_for_microservice(client: &reqwest::Client, base: &str) -> anyhow::Result<()> {\n",
    "    let mut backoff = 100u64;\n",
    "    for _ in 0..8 {\n",
    "        match client.get(format!(\"{}/up\", base)).send().await {\n",
    "            Ok(r) if r.status().is_success() => return Ok(()),\n",
    "            _ => {\n",
    "                tokio::time::sleep(Duration::from_millis(backoff)).await;\n",
    "                backoff = (backoff * 2).min(1200);\n",
    "            }\n",
    "        }\n",
    "    }\n",
    "    anyhow::bail!(\"microservice {} not ready\", base)\n",
    "}\n",
    "''', encoding=\"utf-8\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 348,
   "id": "bd5b97cc-6387-458e-8d0b-546e713b9fb2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " C:\\Users\\aniru\\.cargo\\bin\\cargo.EXE build --release --message-format=json\n",
      " Built binary: C:\\Users\\aniru\\rag_rust_gateway\\target\\x86_64-pc-windows-gnu\\release\\rag_rust_gateway.exe\n",
      "\n",
      " Rust gateway up on http://127.0.0.1:58350\n",
      "Smoke /up failed: HTTPConnectionPool(host='127.0.0.1', port=58350): Max retries exceeded with url: /up (Caused by NewConnectionError('<urllib3.connection.HTTPConnection object at 0x000001B996977620>: Failed to establish a new connection: [WinError 10061] No connection could be made because the target machine actively refused it'))\n",
      "\n",
      "Examples:\n",
      "  curl -s http://127.0.0.1:58350/up | jq .\n",
      "  curl -s -X POST http://127.0.0.1:58350/answer -H 'Content-Type: application/json' -d '{\\\"query\\\":\\\"How do BM25 and vector search differ?\\\",\\\"top_k\\\":5,\\\"use_rerank\\\":true}' | jq .\n"
     ]
    }
   ],
   "source": [
    "import os, sys, json, time, subprocess, shutil\n",
    "from pathlib import Path\n",
    "ROOT = Path(\"rag_rust_gateway\").resolve()\n",
    "if not (ROOT / \"Cargo.toml\").exists():\n",
    "    raise SystemExit(\"rust project folder 'rag_rust_gateway' not found. Re-run the previous setup cell first.\")\n",
    "cargo = shutil.which(\"cargo\")\n",
    "if not cargo:\n",
    "    raise SystemExit(\"Cargo not found in PATH. Install Rust from https://rustup.rs and restart the kernel.\")\n",
    "def run_build_and_find_exe():\n",
    "    cmd = [cargo, \"build\", \"--release\", \"--message-format=json\"]\n",
    "    print(\"\", \" \".join(cmd))\n",
    "    proc = subprocess.Popen(cmd, cwd=str(ROOT), stdout=subprocess.PIPE, stderr=subprocess.PIPE, text=True)\n",
    "    exe_paths = []\n",
    "    json_lines = []\n",
    "    while True:\n",
    "        line = proc.stdout.readline()\n",
    "        if not line and proc.poll() is not None:\n",
    "            break\n",
    "        if not line:\n",
    "            continue\n",
    "        line = line.strip()\n",
    "        if line.startswith(\"{\") and line.endswith(\"}\"):\n",
    "            try:\n",
    "                msg = json.loads(line)\n",
    "                json_lines.append(msg)\n",
    "                if msg.get(\"reason\") == \"compiler-artifact\":\n",
    "                    exe = msg.get(\"executable\")\n",
    "                    if exe:\n",
    "                        exe_paths.append(Path(exe))\n",
    "            except Exception:\n",
    "                pass\n",
    "        else:\n",
    "            pass\n",
    "    out, err = proc.communicate(timeout=60)\n",
    "    if proc.returncode != 0:\n",
    "        print(\"=== cargo stderr ===\")\n",
    "        print(err)\n",
    "        raise SystemExit(f\"cargo build failed with code {proc.returncode}\")\n",
    "    if exe_paths:\n",
    "        exe = exe_paths[-1]\n",
    "        if exe.exists():\n",
    "            return exe\n",
    "    exe = ROOT / \"target\" / \"release\" / (\"rag_rust_gateway.exe\" if os.name == \"nt\" else \"rag_rust_gateway\")\n",
    "    if exe.exists():\n",
    "        return exe\n",
    "    rel = ROOT / \"target\" / \"release\"\n",
    "    if rel.exists():\n",
    "        for p in rel.glob(\"rag_rust_gateway*\"):\n",
    "            if p.is_file():\n",
    "                return p\n",
    "    print(\"Could not locate built binary. Last cargo messages:\")\n",
    "    for m in json_lines[-5:]:\n",
    "        print(json.dumps(m, indent=2)[:600])\n",
    "    raise SystemExit(f\"Built binary not found. Looked for {exe}\")\n",
    "exe_path = run_build_and_find_exe()\n",
    "print(f\" Built binary: {exe_path}\")\n",
    "env = os.environ.copy()\n",
    "env.setdefault(\"RAG_BASE\", env.get(\"RAG_BASE\", \"http://127.0.0.1:8001\"))\n",
    "env.setdefault(\"GATEWAY_PORT\", env.get(\"GATEWAY_PORT\", \"8002\"))\n",
    "env.setdefault(\"GATEWAY_TIMEOUT_MS\", env.get(\"GATEWAY_TIMEOUT_MS\", \"1500\"))\n",
    "env.setdefault(\"TOP_K\", env.get(\"TOP_K\", \"5\"))\n",
    "env.setdefault(\"USE_RERANK\", env.get(\"USE_RERANK\", \"1\"))\n",
    "env.setdefault(\"INSECURE\", env.get(\"INSECURE\", \"0\"))\n",
    "env.setdefault(\"RUST_LOG\", env.get(\"RUST_LOG\", \"rag_rust_gateway=info,axum=warn\"))\n",
    "proc = subprocess.Popen([str(exe_path)], cwd=str(ROOT), env=env)\n",
    "port_file = ROOT / \"PORT.txt\"\n",
    "for _ in range(60):\n",
    "    if port_file.exists():\n",
    "        break\n",
    "    time.sleep(0.1)\n",
    "if not port_file.exists():\n",
    "    proc.terminate()\n",
    "    raise SystemExit(\"Gateway failed to start (no PORT.txt). Check the gateway logs above.\")\n",
    "PORT = port_file.read_text(encoding=\"utf-8\").strip()\n",
    "base = f\"http://127.0.0.1:{PORT}\"\n",
    "print(f\"\\n Rust gateway up on {base}\")\n",
    "try:\n",
    "    import requests\n",
    "    up = requests.get(f\"{base}/up\", timeout=5).json()\n",
    "    print(\"/up  OK\", json.dumps(up, indent=2)[:500], \"...\")\n",
    "except Exception as e:\n",
    "    print(\"Smoke /up failed:\", e)\n",
    "print(\"\\nExamples:\")\n",
    "print(f\"  curl -s {base}/up | jq .\")\n",
    "print(f\"  curl -s -X POST {base}/answer -H 'Content-Type: application/json' \"\n",
    "      r\"-d '{\\\"query\\\":\\\"How do BM25 and vector search differ?\\\",\\\"top_k\\\":5,\\\"use_rerank\\\":true}' | jq .\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 349,
   "id": "92c9a944-36a7-45dd-bab6-6e6235e0e967",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, json, hashlib, time\n",
    "from pathlib import Path\n",
    "ROOT = Path(\".\")\n",
    "OUT = ROOT/\"artifacts\"/\"local\"/\"n8n\"\n",
    "OUT.mkdir(parents=True, exist_ok=True)\n",
    "def _write(name, obj):\n",
    "    p = OUT/f\"{name}.json\"\n",
    "    p.write_text(json.dumps(obj, indent=2) + \"\\n\", encoding=\"utf-8\")\n",
    "    print(\"wrote\", p)\n",
    "    return p\n",
    "def _sig(s: str) -> str:\n",
    "    return hashlib.sha256(s.encode(\"utf-8\")).hexdigest()[:16]\n",
    "ENV_GATEWAY = \"{{ $env.RAG_GATEWAY || ('http://127.0.0.1:' + ($env.GATEWAY_PORT || '8002')) }}\"\n",
    "ENV_RAGBASE = \"{{ $env.RAG_BASE || 'http://127.0.0.1:8001' }}\"\n",
    "SLO_BUDGET_MS = 1800"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 350,
   "id": "062a528c-d7e1-4812-a61a-f30d0872b6c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "wrote artifacts\\local\\n8n\\query_hot_path.json\n",
      "wrote artifacts\\local\\n8n\\upload_pipeline.json\n",
      "wrote artifacts\\local\\n8n\\global_error_to_overseer.json\n",
      "wrote artifacts\\local\\n8n\\slo_monitor.json\n",
      "wrote artifacts\\local\\n8n\\credentials_placeholders.json\n",
      "\n",
      "Import tips:\n",
      "- In n8n, create an Environment Variable for RAG_GATEWAY or set GATEWAY_PORT.\n",
      "- Set OVERSEER_URL and SLO_ALERT_WEBHOOK if you want error/SLO routing to real destinations.\n",
      "- Mark 'global_error_to_overseer' as your global error workflow in Settings > Error Workflow.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "hot_path = {\n",
    "  \"name\": \"query_hot_path\",\n",
    "  \"nodes\": [\n",
    "    {\n",
    "      \"id\": \"1\",\n",
    "      \"name\": \"Manual Trigger\",\n",
    "      \"type\": \"n8n-nodes-base.manualTrigger\",\n",
    "      \"typeVersion\": 2.1,\n",
    "      \"position\": [200, 300]\n",
    "    },\n",
    "    {\n",
    "      \"id\": \"2\",\n",
    "      \"name\": \"Set Query\",\n",
    "      \"type\": \"n8n-nodes-base.set\",\n",
    "      \"typeVersion\": 2.1,\n",
    "      \"position\": [430, 300],\n",
    "      \"parameters\": {\n",
    "        \"values\": {\n",
    "          \"string\": [\n",
    "            {\"name\": \"query\", \"value\": \"How do BM25 and vector search differ?\"},\n",
    "          ],\n",
    "          \"number\": [\n",
    "            {\"name\": \"top_k\", \"value\": 5}\n",
    "          ],\n",
    "          \"boolean\": [\n",
    "            {\"name\": \"use_rerank\", \"value\": True}\n",
    "          ]\n",
    "        },\n",
    "        \"options\": {}\n",
    "      }\n",
    "    },\n",
    "    {\n",
    "      \"id\": \"3\",\n",
    "      \"name\": \"HTTP /answer (Rust)\",\n",
    "      \"type\": \"n8n-nodes-base.httpRequest\",\n",
    "      \"typeVersion\": 4.2,\n",
    "      \"position\": [680, 300],\n",
    "      \"parameters\": {\n",
    "        \"url\": f\"={ENV_GATEWAY}/answer\",\n",
    "        \"method\": \"POST\",\n",
    "        \"responseFormat\": \"json\",\n",
    "        \"jsonParameters\": True,\n",
    "        \"options\": {\"timeout\": 1500, \"ignoreResponseCode\": True},\n",
    "        \"bodyParametersJson\": \"\"\"{\n",
    "          \"query\": \"={{$json.query}}\",\n",
    "          \"top_k\": \"={{$json.top_k}}\",\n",
    "          \"use_rerank\": \"={{$json.use_rerank}}\"\n",
    "        }\"\"\"\n",
    "      }\n",
    "    },\n",
    "    {\n",
    "      \"id\": \"4\",\n",
    "      \"name\": \"IF Error?\",\n",
    "      \"type\": \"n8n-nodes-base.if\",\n",
    "      \"typeVersion\": 2.1,\n",
    "      \"position\": [920, 300],\n",
    "      \"parameters\": {\n",
    "        \"conditions\": {\n",
    "          \"boolean\": [],\n",
    "          \"number\": [\n",
    "            {\n",
    "              \"value1\": \"={{$json[\\\"statusCode\\\"] || 200}}\",\n",
    "              \"operation\": \"notEqual\",\n",
    "              \"value2\": 200\n",
    "            }\n",
    "          ]\n",
    "        }\n",
    "      }\n",
    "    },\n",
    "    {\n",
    "      \"id\": \"5\",\n",
    "      \"name\": \"Compute SLO flag\",\n",
    "      \"type\": \"n8n-nodes-base.function\",\n",
    "      \"typeVersion\": 2.1,\n",
    "      \"position\": [1160, 180],\n",
    "      \"parameters\": {\n",
    "        \"functionCode\": f\"\"\"\n",
    "const ms = $json?.body?.meta?.latency_ms ?? $json?.body?.latency_ms ?? 0;\n",
    "return [{{ ...$json, slo_budget_ms: {SLO_BUDGET_MS}, latency_ms: ms, slo_violation: (ms > {SLO_BUDGET_MS}) }}];\n",
    "\"\"\"\n",
    "      }\n",
    "    },\n",
    "    {\n",
    "      \"id\": \"6\",\n",
    "      \"name\": \"IF SLO violated?\",\n",
    "      \"type\": \"n8n-nodes-base.if\",\n",
    "      \"typeVersion\": 2.1,\n",
    "      \"position\": [1390, 180],\n",
    "      \"parameters\": {\n",
    "        \"conditions\": {\n",
    "          \"boolean\": [\n",
    "            {\"value1\": \"={{$json.slo_violation}}\", \"operation\": \"isTrue\"}\n",
    "          ]\n",
    "        }\n",
    "      }\n",
    "    },\n",
    "    {\n",
    "      \"id\": \"7\",\n",
    "      \"name\": \"SLO Alert (Webhook/Slack/etc.)\",\n",
    "      \"type\": \"n8n-nodes-base.httpRequest\",\n",
    "      \"typeVersion\": 4.2,\n",
    "      \"position\": [1620, 80],\n",
    "      \"parameters\": {\n",
    "        \"url\": \"={{$env.SLO_ALERT_WEBHOOK || 'http://127.0.0.1:8080/slo-alert'}}\",\n",
    "        \"method\": \"POST\",\n",
    "        \"jsonParameters\": True,\n",
    "        \"responseFormat\": \"json\",\n",
    "        \"bodyParametersJson\": \"\"\"{\n",
    "          \"type\":\"slo_violation\",\n",
    "          \"latency_ms\":\"={{$json.latency_ms}}\",\n",
    "          \"budget_ms\":\"={{$json.slo_budget_ms}}\",\n",
    "          \"query\":\"={{$node['Set Query'].json['query']}}\"\n",
    "        }\"\"\"\n",
    "      }\n",
    "    },\n",
    "    {\n",
    "      \"id\": \"8\",\n",
    "      \"name\": \"Return Answer\",\n",
    "      \"type\": \"n8n-nodes-base.function\",\n",
    "      \"typeVersion\": 2.1,\n",
    "      \"position\": [1620, 280],\n",
    "      \"parameters\": {\n",
    "        \"functionCode\": \"\"\"\n",
    "// Normalize gateway -> nice shape\n",
    "const b = $json.body || {};\n",
    "return [{ answer: b.answer, contexts: b.contexts || [], verify: b.verify || b.body?.verify || null, meta: b.meta || {} }];\n",
    "\"\"\"\n",
    "      }\n",
    "    },\n",
    "    {\n",
    "      \"id\": \"9\",\n",
    "      \"name\": \"Overseer on Error\",\n",
    "      \"type\": \"n8n-nodes-base.httpRequest\",\n",
    "      \"typeVersion\": 4.2,\n",
    "      \"position\": [1160, 420],\n",
    "      \"parameters\": {\n",
    "        \"url\": \"={{$env.OVERSEER_URL || 'http://127.0.0.1:7000/overseer/ingest'}}\",\n",
    "        \"method\": \"POST\",\n",
    "        \"responseFormat\": \"json\",\n",
    "        \"jsonParameters\": True,\n",
    "        \"options\": { \"ignoreResponseCode\": True },\n",
    "        \"headerParametersJson\": \"\"\"{\n",
    "          \"Content-Type\": \"application/json\",\n",
    "          \"Idempotency-Key\": \"={{$json.error_signature}}\"\n",
    "        }\"\"\",\n",
    "        \"bodyParametersJson\": \"\"\"={{$json.error_bundle}}\"\"\"\n",
    "      }\n",
    "    },\n",
    "    {\n",
    "      \"id\": \"10\",\n",
    "      \"name\": \"Build error_bundle\",\n",
    "      \"type\": \"n8n-nodes-base.function\",\n",
    "      \"typeVersion\": 2.1,\n",
    "      \"position\": [1160, 560],\n",
    "      \"parameters\": {\n",
    "        \"functionCode\": \"\"\"\n",
    "// Build a stable error signature & bundle; Overseer dedupes on Idempotency-Key\n",
    "const status = $json.statusCode || 0;\n",
    "const payload = $node['Set Query'].json;\n",
    "const sigSrc = `answer:${status}:${payload.query || ''}:${(payload.top_k||'')}:${(payload.use_rerank?'1':'0')}`;\n",
    "const sig = require('crypto').createHash('sha256').update(sigSrc).digest('hex').slice(0,16);\n",
    "return [{\n",
    "  error_signature: sig,\n",
    "  error_bundle: {\n",
    "    node: \"query_hot_path\",\n",
    "    status_code: status,\n",
    "    failing_input: payload,\n",
    "    gateway_body: $json.body || null,\n",
    "    last_green_sha: process.env.LAST_GREEN_SHA || \"unknown\",\n",
    "    allow_list: [\"rag_rust_gateway/*\", \"services/*\", \"prompts/*.json\", \"configs/*.yaml\"]\n",
    "  }\n",
    "}];\n",
    "\"\"\"\n",
    "      }\n",
    "    }\n",
    "  ],\n",
    "  \"connections\": {\n",
    "    \"Set Query\": {\"main\": [[{\"node\": \"HTTP /answer (Rust)\", \"type\": \"main\", \"index\": 0}]]},\n",
    "    \"Manual Trigger\": {\"main\": [[{\"node\": \"Set Query\", \"type\": \"main\", \"index\": 0}]]},\n",
    "    \"HTTP /answer (Rust)\": {\"main\": [[{\"node\": \"IF Error?\", \"type\": \"main\", \"index\": 0}]]},\n",
    "    \"IF Error?\": {\n",
    "      \"main\": [\n",
    "        [{\"node\": \"Compute SLO flag\", \"type\": \"main\", \"index\": 0}],\n",
    "        [{\"node\": \"Build error_bundle\", \"type\": \"main\", \"index\": 0}]\n",
    "      ]\n",
    "    },\n",
    "    \"Compute SLO flag\": {\"main\": [[{\"node\": \"IF SLO violated?\", \"type\": \"main\", \"index\": 0}]]},\n",
    "    \"IF SLO violated?\": {\n",
    "      \"main\": [\n",
    "        [{\"node\": \"SLO Alert (Webhook/Slack/etc.)\", \"type\": \"main\", \"index\": 0}],\n",
    "        [{\"node\": \"Return Answer\", \"type\": \"main\", \"index\": 0}]\n",
    "      ]\n",
    "    },\n",
    "    \"Build error_bundle\": {\"main\": [[{\"node\": \"Overseer on Error\", \"type\": \"main\", \"index\": 0}]]}\n",
    "  },\n",
    "  \"meta\": {\"workflowId\": _sig(\"query_hot_path\")}\n",
    "}\n",
    "_write(\"query_hot_path\", hot_path)\n",
    "upload_flow = {\n",
    "  \"name\": \"upload_pipeline\",\n",
    "  \"nodes\": [\n",
    "    {\"id\":\"1\",\"name\":\"Webhook (Upload)\",\"type\":\"n8n-nodes-base.webhook\",\"typeVersion\":1.4,\"position\":[220,260],\n",
    "     \"parameters\":{\"path\":\"upload/basic\",\"httpMethod\":\"POST\",\"responseMode\":\"lastNode\",\"responseData\":\"allEntries\"}},\n",
    "    {\"id\":\"2\",\"name\":\"Schema Profile\",\"type\":\"n8n-nodes-base.httpRequest\",\"typeVersion\":4.2,\"position\":[460,160],\n",
    "     \"parameters\":{\"url\":f\"={ENV_RAGBASE}/schema/profile\",\"method\":\"POST\",\"responseFormat\":\"json\",\"jsonParameters\":True,\n",
    "                   \"bodyParametersJson\":\"={{$json}}\",\"options\":{\"ignoreResponseCode\":True}}},\n",
    "    {\"id\":\"3\",\"name\":\"Schema Validate\",\"type\":\"n8n-nodes-base.httpRequest\",\"typeVersion\":4.2,\"position\":[460,360],\n",
    "     \"parameters\":{\"url\":f\"={ENV_RAGBASE}/schema/validate\",\"method\":\"POST\",\"responseFormat\":\"json\",\"jsonParameters\":True,\n",
    "                   \"bodyParametersJson\":\"={{$json}}\",\"options\":{\"ignoreResponseCode\":True}}},\n",
    "    {\"id\":\"4\",\"name\":\"IF Cast OK?\",\"type\":\"n8n-nodes-base.if\",\"typeVersion\":2.1,\"position\":[700,360],\n",
    "     \"parameters\":{\"conditions\":{\"boolean\":[{\"value1\":\"={{$json.body.passes || false}}\",\"operation\":\"isTrue\"}]}}},\n",
    "    {\"id\":\"5\",\"name\":\"Auto-Fix LLM (stub)\",\"type\":\"n8n-nodes-base.httpRequest\",\"typeVersion\":4.2,\"position\":[940,520],\n",
    "     \"parameters\":{\"url\":f\"={ENV_RAGBASE}/complete\",\"method\":\"POST\",\"responseFormat\":\"json\",\"jsonParameters\":True,\n",
    "                   \"bodyParametersJson\":\"\"\"{\n",
    "                     \"query\":\"Fix schema/casts for upload\",\n",
    "                     \"contexts\":[{\"id\":\"ctx\",\"text\":\"={{$json.body | toString}}\"}]\n",
    "                   }\"\"\",\"options\":{\"ignoreResponseCode\":True}}},\n",
    "    {\"id\":\"6\",\"name\":\"Trainer (Advanced)\",\"type\":\"n8n-nodes-base.httpRequest\",\"typeVersion\":4.2,\"position\":[940,200],\n",
    "     \"parameters\":{\"url\":f\"={ENV_RAGBASE}/load_docs\",\"method\":\"POST\",\"responseFormat\":\"json\",\"jsonParameters\":True,\n",
    "                   \"bodyParametersJson\":\"={{$json.body.sample ? JSON.stringify({docs:$json.body.sample}) : JSON.stringify({docs:[]})}}\",\n",
    "                   \"options\":{\"ignoreResponseCode\":True}}},\n",
    "    {\"id\":\"7\",\"name\":\"TTL/GC (stub)\",\"type\":\"n8n-nodes-base.httpRequest\",\"typeVersion\":4.2,\"position\":[1180,200],\n",
    "     \"parameters\":{\"url\":\"={{$env.TTL_GC_ENDPOINT || 'http://127.0.0.1:8080/gc'}}\",\"method\":\"POST\",\"responseFormat\":\"json\",\n",
    "                   \"jsonParameters\":True,\"bodyParametersJson\":\"={{$json}}\",\"options\":{\"ignoreResponseCode\":True}}},\n",
    "    {\"id\":\"8\",\"name\":\"Global Error Hook\",\"type\":\"n8n-nodes-base.errorTrigger\",\"typeVersion\":1,\"position\":[220,520]}\n",
    "  ],\n",
    "  \"connections\": {\n",
    "    \"Webhook (Upload)\": {\"main\":[[{\"node\":\"Schema Profile\",\"type\":\"main\",\"index\":0},\n",
    "                                  {\"node\":\"Schema Validate\",\"type\":\"main\",\"index\":0}]]},\n",
    "    \"Schema Validate\": {\"main\":[[{\"node\":\"IF Cast OK?\",\"type\":\"main\",\"index\":0}]]},\n",
    "    \"IF Cast OK?\": {\"main\":[[{\"node\":\"Trainer (Advanced)\",\"type\":\"main\",\"index\":0}],\n",
    "                             [{\"node\":\"Auto-Fix LLM (stub)\",\"type\":\"main\",\"index\":0}]]},\n",
    "    \"Trainer (Advanced)\": {\"main\":[[{\"node\":\"TTL/GC (stub)\",\"type\":\"main\",\"index\":0}]]}\n",
    "  },\n",
    "  \"meta\": {\"workflowId\": _sig(\"upload_pipeline\")}\n",
    "}\n",
    "_write(\"upload_pipeline\", upload_flow)\n",
    "global_error = {\n",
    "  \"name\": \"global_error_to_overseer\",\n",
    "  \"nodes\": [\n",
    "    {\"id\":\"1\",\"name\":\"Error Trigger\",\"type\":\"n8n-nodes-base.errorTrigger\",\"typeVersion\":1,\"position\":[220,260]},\n",
    "    {\"id\":\"2\",\"name\":\"Bundle & Sign\",\"type\":\"n8n-nodes-base.function\",\"typeVersion\":2.1,\"position\":[460,260],\n",
    "     \"parameters\":{\"functionCode\":\"\"\"\n",
    "const flow = $json.workflow || {};\n",
    "const exec = $json.execution || {};\n",
    "const err  = $json.error || {};\n",
    "const sigSrc = `${flow.name||'wf'}:${err.node?.name||'node'}:${err.message||''}`;\n",
    "const sig = require('crypto').createHash('sha256').update(sigSrc).digest('hex').slice(0,16);\n",
    "return [{\n",
    "  error_signature: sig,\n",
    "  error_bundle: {\n",
    "    node: err.node?.name || 'unknown',\n",
    "    workflow: flow.name || 'unknown',\n",
    "    stack: err.stack || err.message || 'unknown',\n",
    "    code_context: [],  // optional\n",
    "    failing_input: exec.data || {},\n",
    "    last_green_sha: process.env.LAST_GREEN_SHA || 'unknown',\n",
    "    allow_list: [\"services/*\",\"prompts/*.json\",\"configs/*.yaml\"]\n",
    "  }\n",
    "}];\n",
    "\"\"\"}},\n",
    "    {\"id\":\"3\",\"name\":\"POST Overseer\",\"type\":\"n8n-nodes-base.httpRequest\",\"typeVersion\":4.2,\"position\":[700,260],\n",
    "     \"parameters\":{\"url\":\"={{$env.OVERSEER_URL || 'http://127.0.0.1:7000/overseer/ingest'}}\",\n",
    "       \"method\":\"POST\",\"jsonParameters\":True,\"responseFormat\":\"json\",\"options\":{\"ignoreResponseCode\":True},\n",
    "       \"headerParametersJson\":\"={\\\"Idempotency-Key\\\":\\\"{{$json.error_signature}}\\\"}\",\n",
    "       \"bodyParametersJson\":\"={{$json.error_bundle}}\"\n",
    "     }}\n",
    "  ],\n",
    "  \"connections\": { \"Error Trigger\": {\"main\":[[{\"node\":\"Bundle & Sign\",\"type\":\"main\",\"index\":0}]]},\n",
    "                   \"Bundle & Sign\": {\"main\":[[{\"node\":\"POST Overseer\",\"type\":\"main\",\"index\":0}]]} },\n",
    "  \"meta\": {\"workflowId\": _sig(\"global_error_to_overseer\")}\n",
    "}\n",
    "_write(\"global_error_to_overseer\", global_error)\n",
    "slo_monitor = {\n",
    "  \"name\": \"slo_monitor\",\n",
    "  \"nodes\": [\n",
    "    {\"id\":\"1\",\"name\":\"Cron (1m)\",\"type\":\"n8n-nodes-base.cron\",\"typeVersion\":1.1,\"position\":[220,180],\n",
    "     \"parameters\":{\"triggerTimes\":[{\"mode\":\"everyMinute\"}]}},\n",
    "    {\"id\":\"2\",\"name\":\"GET /up (gateway)\",\"type\":\"n8n-nodes-base.httpRequest\",\"typeVersion\":4.2,\"position\":[460,180],\n",
    "     \"parameters\":{\"url\":f\"={ENV_GATEWAY}/up\",\"method\":\"GET\",\"responseFormat\":\"json\",\"options\":{\"ignoreResponseCode\":True}}},\n",
    "    {\"id\":\"3\",\"name\":\"Check fields\",\"type\":\"n8n-nodes-base.function\",\"typeVersion\":2.1,\"position\":[700,180],\n",
    "     \"parameters\":{\"functionCode\":f\"\"\"\n",
    "const ok = !!($json.body?.microservice?.ok);\n",
    "const dt = $json.body?.microservice?.docs ?? 0;\n",
    "const slo = {SLO_BUDGET_MS};\n",
    "return [{{ ok, docs: dt, slo }}];\n",
    "\"\"\"}},\n",
    "    {\"id\":\"4\",\"name\":\"IF unhealthy?\",\"type\":\"n8n-nodes-base.if\",\"typeVersion\":2.1,\"position\":[940,180],\n",
    "     \"parameters\":{\"conditions\":{\"boolean\":[{\"value1\":\"={{$json.ok}}\",\"operation\":\"isFalse\"}]}}},\n",
    "    {\"id\":\"5\",\"name\":\"Notify\",\"type\":\"n8n-nodes-base.httpRequest\",\"typeVersion\":4.2,\"position\":[1180,100],\n",
    "     \"parameters\":{\"url\":\"={{$env.SLO_ALERT_WEBHOOK || 'http://127.0.0.1:8080/slo-alert'}}\",\"method\":\"POST\",\"jsonParameters\":True,\n",
    "                   \"responseFormat\":\"json\",\"bodyParametersJson\":\"={{$json}}\"}}\n",
    "  ],\n",
    "  \"connections\": {\n",
    "    \"Cron (1m)\": {\"main\":[[{\"node\":\"GET /up (gateway)\",\"type\":\"main\",\"index\":0}]]},\n",
    "    \"GET /up (gateway)\": {\"main\":[[{\"node\":\"Check fields\",\"type\":\"main\",\"index\":0}]]},\n",
    "    \"Check fields\": {\"main\":[[{\"node\":\"IF unhealthy?\",\"type\":\"main\",\"index\":0}]]},\n",
    "    \"IF unhealthy?\": {\"main\":[[{\"node\":\"Notify\",\"type\":\"main\",\"index\":0}],[ ]]}\n",
    "  },\n",
    "  \"meta\": {\"workflowId\": _sig(\"slo_monitor\")}\n",
    "}\n",
    "_write(\"slo_monitor\", slo_monitor)\n",
    "creds = {\n",
    "  \"note\": \"Create these in n8n > Credentials; nodes reference $env.* by expression so creds can inject env at runtime.\",\n",
    "  \"credentials\": {\n",
    "    \"gateway_api\": {\"base\": ENV_GATEWAY},\n",
    "    \"rag_base\": {\"base\": ENV_RAGBASE},\n",
    "    \"hf_token\": {\"env\": \"HF_TOKEN\"},\n",
    "    \"s3_minio\": {\"env\": [\"S3_ENDPOINT\",\"S3_ACCESS_KEY\",\"S3_SECRET_KEY\",\"S3_BUCKET\"]},\n",
    "    \"postgres_dsn\": {\"env\": \"PG_DSN\"}\n",
    "  }\n",
    "}\n",
    "_write(\"credentials_placeholders\", creds)\n",
    "print(\"\\nImport tips:\\n- In n8n, create an Environment Variable for RAG_GATEWAY or set GATEWAY_PORT.\\n\"\n",
    "      \"- Set OVERSEER_URL and SLO_ALERT_WEBHOOK if you want error/SLO routing to real destinations.\\n\"\n",
    "      \"- Mark 'global_error_to_overseer' as your global error workflow in Settings > Error Workflow.\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 351,
   "id": "c787e7c7-1f03-4601-8922-1dacb8ca07e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, re, time, math, json, random, threading, socket, atexit, sys\n",
    "from typing import List, Dict, Optional, Tuple\n",
    "from dataclasses import dataclass\n",
    "from functools import lru_cache\n",
    "from collections import Counter\n",
    "from contextlib import closing\n",
    "try:\n",
    "    import numpy as np\n",
    "except ImportError as e:\n",
    "    raise SystemExit(\"This cell needs numpy: pip install numpy\") from e\n",
    "try:\n",
    "    from fastapi import FastAPI, HTTPException\n",
    "    from pydantic import BaseModel\n",
    "except ImportError as e:\n",
    "    raise SystemExit(\"This cell needs fastapi & pydantic: pip install fastapi pydantic uvicorn\") from e\n",
    "try:\n",
    "    import uvicorn\n",
    "except ImportError as e:\n",
    "    raise SystemExit(\"This cell needs uvicorn: pip install uvicorn\") from e\n",
    "try:\n",
    "    import requests\n",
    "except ImportError as e:\n",
    "    raise SystemExit(\"This cell needs requests: pip install requests\") from e"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 352,
   "id": "bb55756a-305e-42cc-9eb9-ec73dc98877f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _find_free_port(preferred: Optional[int] = None, tries: int = 20) -> int:\n",
    "    if preferred:\n",
    "        with closing(socket.socket(socket.AF_INET, socket.SOCK_STREAM)) as s:\n",
    "            try:\n",
    "                s.bind((\"127.0.0.1\", preferred))\n",
    "                return preferred\n",
    "            except OSError:\n",
    "                pass\n",
    "    for _ in range(tries):\n",
    "        with closing(socket.socket(socket.AF_INET, socket.SOCK_STREAM)) as s:\n",
    "            s.bind((\"127.0.0.1\", 0))\n",
    "            return s.getsockname()[1]\n",
    "    raise RuntimeError(\"Could not find a free port\")\n",
    "class ServerThread:\n",
    "    \"\"\"Run a uvicorn Server in a background thread with a clean stop()\"\"\"\n",
    "    def __init__(self, app: FastAPI, preferred_port: Optional[int] = None, name: str = \"srv\", log_level: str = \"warning\"):\n",
    "        self.app = app\n",
    "        self.port = _find_free_port(preferred_port)\n",
    "        self.name = name\n",
    "        self._server = uvicorn.Server(uvicorn.Config(app=self.app, host=\"127.0.0.1\", port=self.port, log_level=log_level, loop=\"asyncio\"))\n",
    "        self._thread = threading.Thread(target=self._server.run, name=f\"{name}-uvicorn\", daemon=True)\n",
    "        self._started = False\n",
    "    def start(self):\n",
    "        if self._started:\n",
    "            return\n",
    "        self._thread.start()\n",
    "        for _ in range(60):\n",
    "            if getattr(self._server, \"started\", False):\n",
    "                break\n",
    "            time.sleep(0.05)\n",
    "        self._started = True\n",
    "    def stop(self, join_timeout: float = 2.0):\n",
    "        try:\n",
    "            self._server.should_exit = True\n",
    "        except Exception:\n",
    "            pass\n",
    "        if self._thread.is_alive():\n",
    "            self._thread.join(timeout=join_timeout)\n",
    "    @property\n",
    "    def base(self) -> str:\n",
    "        return f\"http://127.0.0.1:{self.port}\"\n",
    "RUNNING = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 353,
   "id": "2934e358-3a52-40e7-a29c-e9ffb5dd0950",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<function __main__.stop_all()>"
      ]
     },
     "execution_count": 353,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def stop_all():\n",
    "    for k, s in list(RUNNING.items()):\n",
    "        try:\n",
    "            s.stop()\n",
    "        except Exception:\n",
    "            pass\n",
    "        RUNNING.pop(k, None)\n",
    "def status():\n",
    "    info = {name: srv.base for name, srv in RUNNING.items()}\n",
    "    print(json.dumps(info or {\"status\":\"no servers running\"}, indent=2))\n",
    "atexit.register(stop_all)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 354,
   "id": "749ad450-2a6a-4849-86d8-cbf692b1332e",
   "metadata": {},
   "outputs": [],
   "source": [
    "TOKEN_RE = re.compile(r\"[A-Za-z0-9]+\")\n",
    "def tokenize(s: str) -> List[str]:\n",
    "    return [t.lower() for t in TOKEN_RE.findall(s or \"\")]\n",
    "def normalize(vec: np.ndarray) -> np.ndarray:\n",
    "    denom = np.linalg.norm(vec, axis=1, keepdims=True) + 1e-9\n",
    "    return vec / denom\n",
    "DEFAULT_DOCS = [\n",
    "    {\"id\": \"d1\", \"text\": \"Rust pipelines are fast and memory-safe using async with Tokio and Axum.\"},\n",
    "    {\"id\": \"d2\", \"text\": \"RAG pipelines retrieve relevant chunks and augment the prompt for the LLM.\"},\n",
    "    {\"id\": \"d3\", \"text\": \"Hybrid retrieval with BM25 and vector kNN improves recall and faithfulness.\"},\n",
    "    {\"id\": \"d4\", \"text\": \"LangChain helps prototype chains, tools, and agents quickly in Python.\"},\n",
    "    {\"id\": \"d5\", \"text\": \"Tantivy provides BM25 in Rust; pgvector or FAISS provide dense similarity.\"},\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 355,
   "id": "aaf99bfb-3629-4d3d-8c31-9aa61b777443",
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class DenseIndexMeta:\n",
    "    dim: int\n",
    "    use_faiss: bool\n",
    "    count: int\n",
    "class DenseIndex:\n",
    "    \"\"\"Tiny hash-based encoder (deterministic; no external models)\"\"\"\n",
    "    def __init__(self, docs: List[Dict[str, str]], dim: int = 384):\n",
    "        self.ids = [d[\"id\"] for d in docs]\n",
    "        self.texts = [d[\"text\"] for d in docs]\n",
    "        self.dim = dim\n",
    "        self.embs = self._hash_embed(self.texts, dim)\n",
    "        self.meta = DenseIndexMeta(dim=self.dim, use_faiss=False, count=len(self.ids))\n",
    "    def _hash_embed(self, texts: List[str], dim: int) -> np.ndarray:\n",
    "        def proj(tok):\n",
    "            rng = np.random.default_rng(abs(hash(tok)) % (2**32))\n",
    "            return rng.standard_normal(dim)\n",
    "        mat = []\n",
    "        for t in texts:\n",
    "            toks = tokenize(t)\n",
    "            v = np.sum([proj(tok) for tok in toks], axis=0) if toks else np.zeros(dim)\n",
    "            mat.append(v)\n",
    "        arr = np.vstack(mat).astype(np.float32)\n",
    "        return normalize(arr)\n",
    "    def encode(self, text: str) -> np.ndarray:\n",
    "        return self._hash_embed([text], self.dim)[0]\n",
    "    def topk(self, q: str, k: int = 5) -> List[Dict]:\n",
    "        qv = self.encode(q).astype(np.float32)\n",
    "        sims = self.embs @ qv\n",
    "        order = np.argsort(-sims)[:k]\n",
    "        return [{\"id\": self.ids[i], \"text\": self.texts[i], \"score\": float(sims[i])} for i in order]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 356,
   "id": "5b52b991-370d-4c08-bf22-82a372af339d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BM25Index:\n",
    "    \"\"\"Very small BM25-like scorer (no external deps)\"\"\"\n",
    "    def __init__(self, docs: List[Dict[str, str]]):\n",
    "        self.ids = [d[\"id\"] for d in docs]\n",
    "        self.texts = [d[\"text\"] for d in docs]\n",
    "        self.tokens = [tokenize(t) for t in self.texts]\n",
    "        self.df = Counter()\n",
    "        for toks in self.tokens:\n",
    "            for t in set(toks):\n",
    "                self.df[t] += 1\n",
    "        self.N = len(self.tokens)\n",
    "        self.avgdl = np.mean([len(t) for t in self.tokens]) if self.tokens else 1.0\n",
    "        self.k1, self.b = 1.5, 0.75\n",
    "    def _score(self, qtok: List[str], i: int) -> float:\n",
    "        toks = self.tokens[i]; dl = len(toks)\n",
    "        tf = Counter(toks); score = 0.0\n",
    "        for t in qtok:\n",
    "            n_q = self.df.get(t, 0)\n",
    "            if n_q == 0: \n",
    "                continue\n",
    "            idf = math.log(1 + (self.N - n_q + 0.5) / (n_q + 0.5))\n",
    "            f = tf.get(t, 0)\n",
    "            denom = f + self.k1 * (1 - self.b + self.b * dl / (self.avgdl or 1.0))\n",
    "            score += idf * (f * (self.k1 + 1)) / (denom or 1.0)\n",
    "        return score\n",
    "    def topk(self, q: str, k: int = 5) -> List[Dict]:\n",
    "        qtok = tokenize(q)\n",
    "        scores = [self._score(qtok, i) for i in range(len(self.tokens))]\n",
    "        order = np.argsort(-np.array(scores))[:k]\n",
    "        return [{\"id\": self.ids[i], \"text\": self.texts[i], \"score\": float(scores[i])} for i in order]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 357,
   "id": "9ae5c457-11a4-4d5f-a413-a1692f51e651",
   "metadata": {},
   "outputs": [],
   "source": [
    "def rrf_fuse(dense: List[Dict], bm25: List[Dict], k: int = 5, k_rrf: float = 60.0) -> List[Dict]:\n",
    "    ids = {x[\"id\"] for x in dense} | {x[\"id\"] for x in bm25}\n",
    "    rd = {d[\"id\"]: i+1 for i, d in enumerate(dense)}\n",
    "    rb = {d[\"id\"]: i+1 for i, d in enumerate(bm25)}\n",
    "    out = []\n",
    "    for _id in ids:\n",
    "        sd = 1.0/(k_rrf + rd.get(_id, 10**9))\n",
    "        sb = 1.0/(k_rrf + rb.get(_id, 10**9))\n",
    "        txt = next((d[\"text\"] for d in dense if d[\"id\"] == _id), None) or next((d[\"text\"] for d in bm25 if d[\"id\"] == _id), \"\")\n",
    "        out.append({\"id\": _id, \"text\": txt, \"score\": float(sd+sb)})\n",
    "    out.sort(key=lambda x: -x[\"score\"])\n",
    "    return out[:k]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 358,
   "id": "be7d2fc9-97db-4b7c-b9cd-cae93791cf7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_hf_stub_app():\n",
    "    app = FastAPI(title=\"HF Stubs\", version=\"0.1\")\n",
    "    @app.get(\"/\")\n",
    "    def root():\n",
    "        return {\"ok\": True, \"endpoints\": [\"GET /up\", \"POST /embeddings\", \"POST /reranker\", \"POST /generate\", \"POST /fix\"]}\n",
    "    @app.get(\"/up\")\n",
    "    def up():\n",
    "        return {\"ok\": True, \"models\": {\"e5\": \"hash-embed-384\", \"reranker\": \"lexical\", \"slm\": \"toy\", \"fix\": \"toy\"}}\n",
    "    class EmbReq(BaseModel):\n",
    "        inputs: List[str]\n",
    "    @app.post(\"/embeddings\")\n",
    "    def embeddings(req: EmbReq):\n",
    "        dim = 384\n",
    "        idx = DenseIndex([{\"id\":\"x\",\"text\": t} for t in req.inputs], dim=dim)\n",
    "        embs = [idx.encode(t).tolist() for t in req.inputs]\n",
    "        return {\"ok\": True, \"model\": \"hash-embed-384\", \"dim\": dim, \"embeddings\": embs}\n",
    "    class RerankReq(BaseModel):\n",
    "        query: str\n",
    "        candidates: List[str]\n",
    "    @app.post(\"/reranker\")\n",
    "    def reranker(req: RerankReq):\n",
    "        qtok = set(tokenize(req.query))\n",
    "        scores = []\n",
    "        for c in req.candidates:\n",
    "            ctok = set(tokenize(c))\n",
    "            j = (len(qtok & ctok) / max(1, len(qtok | ctok))) if (qtok or ctok) else 0.0\n",
    "            scores.append(j)\n",
    "        return {\"ok\": True, \"model\": \"lexical\", \"scores\": scores}\n",
    "    class GenReq(BaseModel):\n",
    "        prompt: str\n",
    "        max_tokens: Optional[int] = 64\n",
    "    @app.post(\"/generate\")\n",
    "    def generate(req: GenReq):\n",
    "        words = tokenize(req.prompt)[:12]\n",
    "        rnd = \" \".join(random.choice([\"alpha\",\"beta\",\"gamma\",\"delta\",\"omega\",\"rust\",\"vector\",\"bm25\",\"faithful\",\"cache\"]) for _ in range(20))\n",
    "        return {\"ok\": True, \"model\": \"slm\", \"outputs\": [(\" \".join(words) + \" | \" + rnd)[:max(10, req.max_tokens or 64)]]}\n",
    "    class FixReq(BaseModel):\n",
    "        text: str\n",
    "    @app.post(\"/fix\")\n",
    "    def fix(req: FixReq):\n",
    "        sample = req.text\n",
    "        sample = re.sub(r'\"\\$?(\\d[\\d,\\.]*)\"', r'{\"value\": \\1}', sample)\n",
    "        return {\"ok\": True, \"model\": \"slm\", \"outputs\": [sample]}\n",
    "    return app"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 359,
   "id": "a57e7ca0-c120-4fa8-addd-efdd3210293d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_rag_app():\n",
    "    app = FastAPI(title=\"RAG Microservice\", version=\"0.3\")\n",
    "    class RetrieveReq(BaseModel):\n",
    "        query: str\n",
    "        top_k: Optional[int] = 5\n",
    "    class RerankReq(BaseModel):\n",
    "        query: str\n",
    "        candidates: List[Dict]  \n",
    "    class CompleteReq(BaseModel):\n",
    "        query: str\n",
    "        contexts: List[Dict]\n",
    "    class SqlSanitizeReq(BaseModel):\n",
    "        text: str\n",
    "        max_limit: Optional[int] = 100\n",
    "    class SchemaProfileReq(BaseModel):\n",
    "        sample: List[Dict] = []\n",
    "    state = {\"dense\": DenseIndex(DEFAULT_DOCS), \"bm25\": BM25Index(DEFAULT_DOCS), \"created\": int(time.time())}\n",
    "    @app.get(\"/\")\n",
    "    def root():\n",
    "        return {\"ok\": True, \"endpoints\": [\"GET /up\", \"POST /retrieve_dense\", \"POST /retrieve_bm25\", \"POST /rerank\", \"POST /complete\", \"POST /sql/sanitize\"]}\n",
    "    @app.get(\"/up\")\n",
    "    def up():\n",
    "        return {\"ok\": True, \"created_ts\": state[\"created\"], \"docs\": len(state[\"dense\"].ids),\n",
    "                \"dense\": {\"dim\": state[\"dense\"].meta.dim, \"count\": state[\"dense\"].meta.count},\n",
    "                \"bm25\": {\"impl\": \"tf_fallback\", \"count\": len(state[\"bm25\"].ids)}}\n",
    "    @app.post(\"/retrieve_dense\")\n",
    "    def retrieve_dense(req: RetrieveReq):\n",
    "        if not req.query.strip():\n",
    "            raise HTTPException(400, \"query required\")\n",
    "        k = max(1, min(int(req.top_k or 5), 50))\n",
    "        t0 = time.time()\n",
    "        res = state[\"dense\"].topk(req.query, k)\n",
    "        ms = int((time.time() - t0) * 1000)\n",
    "        return {\"results\": res, \"latency_ms\": ms, \"index\": {\"dim\": state[\"dense\"].meta.dim}}\n",
    "    @app.post(\"/retrieve_bm25\")\n",
    "    def retrieve_bm25(req: RetrieveReq):\n",
    "        if not req.query.strip():\n",
    "            raise HTTPException(400, \"query required\")\n",
    "        k = max(1, min(int(req.top_k or 5), 50))\n",
    "        t0 = time.time()\n",
    "        res = state[\"bm25\"].topk(req.query, k)\n",
    "        ms = int((time.time() - t0) * 1000)\n",
    "        return {\"results\": res, \"latency_ms\": ms, \"impl\": \"tf_fallback\"}\n",
    "    @app.post(\"/rerank\")\n",
    "    def rerank(req: RerankReq):\n",
    "        pairs = [(req.query, c.get(\"text\",\"\")) for c in req.candidates]\n",
    "        scored = []\n",
    "        for c in req.candidates:\n",
    "            qtok = set(tokenize(req.query))\n",
    "            ctok = set(tokenize(c.get(\"text\",\"\")))\n",
    "            j = (len(qtok & ctok) / max(1, len(qtok | ctok))) if (qtok or ctok) else 0.0\n",
    "            d = dict(c); d[\"score\"] = float(j); scored.append(d)\n",
    "        scored.sort(key=lambda x: -x[\"score\"])\n",
    "        return {\"results\": scored, \"latency_ms\": 1, \"impl\": \"lexical\"}\n",
    "    def _nums(s: str) -> List[float]:\n",
    "        out = []\n",
    "        for m in re.findall(r\"[-+]?\\d*\\.?\\d+(?:[eE][-+]?\\d+)?\", s or \"\"):\n",
    "            try:\n",
    "                out.append(float(m))\n",
    "            except Exception:\n",
    "                pass\n",
    "        return out\n",
    "    def verify(answer: str, ctxs: List[str]) -> Dict[str,float]:\n",
    "        a = tokenize(answer); c = tokenize(\" \".join(ctxs))\n",
    "        inter = sum(1 for t in a if t in set(c))\n",
    "        so = inter / max(1, len(a))\n",
    "        ans_nums = _nums(answer); ctx_nums = _nums(\" \".join(ctxs))\n",
    "        if not ans_nums:\n",
    "            num = 1.0\n",
    "        else:\n",
    "            ok = 0\n",
    "            for x in ans_nums:\n",
    "                if any(abs(x-y) <= max(1e-6, 1e-3*max(1.0,abs(y))) for y in ctx_nums):\n",
    "                    ok+=1\n",
    "            num = ok / len(ans_nums)\n",
    "        faith = 0.5*so + 0.5*num\n",
    "        return {\"faithfulness\": float(faith), \"entailment_prob\": float(faith)}\n",
    "    @app.post(\"/complete\")\n",
    "    def complete(req: CompleteReq):\n",
    "        ctx_texts = [c.get(\"text\",\"\") for c in req.contexts[:5]]\n",
    "        t0 = time.time()\n",
    "        ans = f\"Q: {req.query}\\nA: Based on the contexts, here is a concise answer.\\n\\nContext:\\n- \" + \"\\n- \".join(ctx_texts[:4])\n",
    "        ver = verify(ans, ctx_texts)\n",
    "        ms = int((time.time() - t0) * 1000)\n",
    "        return {\"answer\": ans, \"verify\": ver, \"latency_ms\": ms}\n",
    "    _SQL_DDL_DML_BLOCKLIST = {\"DROP\",\"TRUNCATE\",\"DELETE\",\"UPDATE\",\"INSERT\",\"ALTER\",\"CREATE\",\"RENAME\",\"GRANT\",\"REVOKE\",\"MERGE\"}\n",
    "    _SQL_SELECT_RE = re.compile(r\"(?is)\\bselect\\b.*?(?:;|$)\")\n",
    "    _SQL_COMMENT_LINE  = re.compile(r\"(?m)^[ \\t]*--.*?$\")\n",
    "    _SQL_COMMENT_BLOCK = re.compile(r\"/\\*.*?\\*/\", re.DOTALL)\n",
    "    def _strip_sql_comments(sql: str) -> str:\n",
    "        sql = _SQL_COMMENT_BLOCK.sub(\"\", sql or \"\")\n",
    "        sql = _SQL_COMMENT_LINE.sub(\"\", sql)\n",
    "        return sql\n",
    "    def extract_sql(text: str) -> Optional[str]:\n",
    "        if not text or not isinstance(text, str):\n",
    "            return None\n",
    "        s = _strip_sql_comments(text)\n",
    "        m = re.search(r\"```(?:sql)?\\s*(.*?)```\", s, flags=re.IGNORECASE | re.DOTALL)\n",
    "        if m: s = m.group(1)\n",
    "        m = _SQL_SELECT_RE.search(s or \"\")\n",
    "        return m.group(0).strip(\" ;\") if m else None\n",
    "    def sanitize_select_limit(sql: str, max_limit: int = 100) -> str:\n",
    "        s = _strip_sql_comments(sql).strip()\n",
    "        parts = [p.strip() for p in s.split(\";\") if p.strip()]\n",
    "        if len(parts) != 1:\n",
    "            raise ValueError(\"multiple statements not allowed\")\n",
    "        s = parts[0]\n",
    "        if not re.match(r\"(?is)^\\s*select\\b\", s):\n",
    "            raise ValueError(\"only SELECT statements allowed\")\n",
    "        upper = re.sub(r\"'[^']*'|\\\"[^\\\"]*\\\"\", \"\", s).upper()\n",
    "        for bad in _SQL_DDL_DML_BLOCKLIST:\n",
    "            if re.search(rf\"\\b{bad}\\b\", upper):\n",
    "                raise ValueError(f\"disallowed keyword: {bad}\")\n",
    "        m = re.search(r\"(?is)\\blimit\\s+(\\d+)\\b\", s)\n",
    "        if m:\n",
    "            lim = int(m.group(1))\n",
    "            if lim > max_limit:\n",
    "                s = re.sub(r\"(?is)\\blimit\\s+\\d+\\b\", f\"LIMIT {max_limit}\", s)\n",
    "        else:\n",
    "            s = s.rstrip() + f\" LIMIT {max_limit}\"\n",
    "        return s\n",
    "    @app.post(\"/sql/sanitize\")\n",
    "    def sql_sanitize(req: SqlSanitizeReq):\n",
    "        try:\n",
    "            found = extract_sql(req.text)\n",
    "            if not found:\n",
    "                return {\"ok\": False, \"error\": \"no_sql_detected\"}\n",
    "            sanitized = sanitize_select_limit(found, max_limit=int(req.max_limit or 100))\n",
    "            return {\"ok\": True, \"sql\": sanitized}\n",
    "        except Exception as e:\n",
    "            return {\"ok\": False, \"error\": str(e)}\n",
    "    return app"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 360,
   "id": "f2a3359d-943a-4873-9141-f80ce297d2a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_gateway_app(rag_base: str):\n",
    "    app = FastAPI(title=\"RAG Python Gateway\", version=\"0.1\")\n",
    "    class AnswerReq(BaseModel):\n",
    "        query: str\n",
    "        top_k: Optional[int] = 5\n",
    "        use_rerank: Optional[bool] = True\n",
    "    @app.get(\"/\")\n",
    "    def root():\n",
    "        return {\"ok\": True, \"endpoints\": [\"GET /up\", \"POST /answer\"], \"rag_base\": rag_base}\n",
    "    @app.get(\"/up\")\n",
    "    def up():\n",
    "        try:\n",
    "            ms = requests.get(f\"{rag_base}/up\", timeout=3).json()\n",
    "        except Exception as e:\n",
    "            ms = {\"ok\": False, \"error\": str(e)}\n",
    "        return {\"ok\": True, \"gateway\": {\"rag_base\": rag_base}, \"microservice\": ms}\n",
    "    @app.post(\"/answer\")\n",
    "    def answer(req: AnswerReq):\n",
    "        t0 = time.time()\n",
    "        try:\n",
    "            d = requests.post(f\"{rag_base}/retrieve_dense\", json={\"query\": req.query, \"top_k\": req.top_k}, timeout=3).json()\n",
    "            b = requests.post(f\"{rag_base}/retrieve_bm25\", json={\"query\": req.query, \"top_k\": req.top_k}, timeout=3).json()\n",
    "            fused = rrf_fuse(d.get(\"results\", []), b.get(\"results\", []), k=int(req.top_k or 5))\n",
    "            contexts = fused\n",
    "            if req.use_rerank:\n",
    "                rr = requests.post(f\"{rag_base}/rerank\", json={\"query\": req.query, \"candidates\": fused}, timeout=3).json()\n",
    "                contexts = rr.get(\"results\", fused)\n",
    "            comp = requests.post(f\"{rag_base}/complete\", json={\"query\": req.query, \"contexts\": contexts}, timeout=5).json()\n",
    "            ms = int((time.time()-t0)*1000)\n",
    "            return {\"answer\": comp.get(\"answer\",\"\"), \"contexts\": contexts, \"meta\": {\"pipeline\": \"rerank\" if req.use_rerank else \"rrf\", \"latency_ms\": ms}, \"verify\": comp.get(\"verify\",{})}\n",
    "        except requests.RequestException as e:\n",
    "            return {\"error\": \"bad_gateway\", \"detail\": str(e), \"latency_ms\": int((time.time()-t0)*1000)}\n",
    "    return app"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 361,
   "id": "39a0d041-322b-4794-9aee-3340054f73c6",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "class KeepAlive:\n",
    "    def __init__(self, hf_base: str, period_s: int = 60):\n",
    "        self.hf_base = hf_base\n",
    "        self.period_s = max(5, int(period_s))\n",
    "        self._stop = threading.Event()\n",
    "        self._t = None\n",
    "    def _tick(self):\n",
    "        if self._stop.is_set():\n",
    "            return\n",
    "        try:\n",
    "            requests.get(f\"{self.hf_base}/up\", timeout=2)\n",
    "            requests.post(f\"{self.hf_base}/embeddings\", json={\"inputs\": [\"warm\", \"up\"]}, timeout=2)\n",
    "            requests.post(f\"{self.hf_base}/reranker\", json={\"query\":\"warm\",\"candidates\":[\"a\",\"b\"]}, timeout=2)\n",
    "            if int(time.time()) % (self.period_s*2) < self.period_s:\n",
    "                requests.post(f\"{self.hf_base}/generate\", json={\"prompt\":\"warm up\", \"max_tokens\": 32}, timeout=2)\n",
    "                requests.post(f\"{self.hf_base}/fix\", json={\"text\": 'age:\"30\", income:\"$4000\"'}, timeout=2)\n",
    "        except Exception:\n",
    "            pass\n",
    "        self._t = threading.Timer(self.period_s, self._tick)\n",
    "        self._t.daemon = True\n",
    "        self._t.start()\n",
    "    def start(self):\n",
    "        if self._t and self._t.is_alive():\n",
    "            return\n",
    "        self._stop.clear()\n",
    "        self._t = threading.Timer(1.0, self._tick)\n",
    "        self._t.daemon = True\n",
    "        self._t.start()\n",
    "    def stop(self):\n",
    "        self._stop.set()\n",
    "        if self._t:\n",
    "            self._t.cancel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 362,
   "id": "958fe428-487b-4588-9a90-370aceeaf563",
   "metadata": {},
   "outputs": [],
   "source": [
    "def start_all(auto_keepalive: bool = False, log_level: str = \"warning\"):\n",
    "    stop_all()  \n",
    "    hf = ServerThread(build_hf_stub_app(), preferred_port=None, name=\"hf\", log_level=log_level); hf.start()\n",
    "    RUNNING[\"hf\"] = hf\n",
    "    rag = ServerThread(build_rag_app(), preferred_port=None, name=\"rag\", log_level=log_level); rag.start()\n",
    "    RUNNING[\"rag\"] = rag\n",
    "    gw = ServerThread(build_gateway_app(rag.base), preferred_port=None, name=\"gw\", log_level=log_level); gw.start()\n",
    "    RUNNING[\"gateway\"] = gw\n",
    "    if auto_keepalive:\n",
    "        ka = KeepAlive(hf.base, period_s=60)\n",
    "        ka.start()\n",
    "        RUNNING[\"keepalive\"] = ka  \n",
    "    print(\" Services up:\")\n",
    "    status()\n",
    "    print(\"Quick curl tips:\")\n",
    "    print(f\"  curl -s {RUNNING['gateway'].base}/up\")\n",
    "    print(f\"  curl -s -X POST {RUNNING['gateway'].base}/answer -H 'Content-Type: application/json' -d '{{\\\"query\\\":\\\"bm25 vs dense\\\",\\\"top_k\\\":4,\\\"use_rerank\\\":true}}'\")\n",
    "def quick_sanity():\n",
    "    if not RUNNING:\n",
    "        print(\"No servers running. Call start_all().\")\n",
    "        return\n",
    "    gw = RUNNING[\"gateway\"].base\n",
    "    rag = RUNNING[\"rag\"].base\n",
    "    hf  = RUNNING[\"hf\"].base\n",
    "    try:\n",
    "        print(\"== /up ==\")\n",
    "        print(\"GET\", gw+\"/up\", \"\", requests.get(gw+\"/up\", timeout=3).status_code)\n",
    "        print(\"GET\", rag+\"/up\", \"\", requests.get(rag+\"/up\", timeout=3).status_code)\n",
    "        print(\"GET\", hf +\"/up\", \"\", requests.get(hf +\"/up\", timeout=3).status_code)\n",
    "\n",
    "        print(\"\\n== /answer ==\")\n",
    "        r = requests.post(gw+\"/answer\", json={\"query\":\"How do BM25 and vector search differ?\",\"top_k\":5,\"use_rerank\":True}, timeout=6)\n",
    "        print(\"POST\", gw+\"/answer\", \"\", r.status_code)\n",
    "        print(json.dumps(r.json(), indent=2)[:800])\n",
    "    except Exception as e:\n",
    "        print(\"Sanity check error:\", e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 363,
   "id": "46c83e92-dbe8-4349-bc72-c1b319a9732c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Services up:\n",
      "{\n",
      "  \"hf\": \"http://127.0.0.1:54361\",\n",
      "  \"rag\": \"http://127.0.0.1:54364\",\n",
      "  \"gateway\": \"http://127.0.0.1:54367\"\n",
      "}\n",
      "Quick curl tips:\n",
      "  curl -s http://127.0.0.1:54367/up\n",
      "  curl -s -X POST http://127.0.0.1:54367/answer -H 'Content-Type: application/json' -d '{\"query\":\"bm25 vs dense\",\"top_k\":4,\"use_rerank\":true}'\n",
      "== /up ==\n",
      "GET http://127.0.0.1:54367/up  200\n",
      "GET http://127.0.0.1:54364/up  200\n",
      "GET http://127.0.0.1:54361/up  200\n",
      "\n",
      "== /answer ==\n",
      "POST http://127.0.0.1:54367/answer  422\n",
      "{\n",
      "  \"detail\": [\n",
      "    {\n",
      "      \"type\": \"missing\",\n",
      "      \"loc\": [\n",
      "        \"query\",\n",
      "        \"req\"\n",
      "      ],\n",
      "      \"msg\": \"Field required\",\n",
      "      \"input\": null\n",
      "    }\n",
      "  ]\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "AUTO_START = True\n",
    "if AUTO_START:\n",
    "    start_all(auto_keepalive=False, log_level=\"warning\")\n",
    "    quick_sanity()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 364,
   "id": "fb14d6a9-4033-4384-b025-6307e1b8df99",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, re, time, json, platform, subprocess, threading, socket\n",
    "from contextlib import closing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 365,
   "id": "c50f2e09-b41b-4590-bf58-6dbb38aa12d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _current_pid() -> int:\n",
    "    return os.getpid()\n",
    "def _pid_on_port(port:int) -> int | None:\n",
    "    try:\n",
    "        if platform.system() == \"Windows\":\n",
    "            out = subprocess.check_output(\n",
    "                [\"cmd\",\"/c\", f\"netstat -ano | findstr LISTENING | findstr :{port}\"],\n",
    "                text=True, stderr=subprocess.DEVNULL\n",
    "            )\n",
    "            for line in out.splitlines():\n",
    "                parts = re.split(r\"\\s+\", line.strip())\n",
    "                if parts and parts[-1].isdigit():\n",
    "                    return int(parts[-1])\n",
    "        else:\n",
    "            out = subprocess.check_output(\n",
    "                [\"bash\",\"-lc\", f\"lsof -nP -iTCP:{port} -sTCP:LISTEN | awk 'NR>1 {{print $2; exit}}'\"],\n",
    "                text=True, stderr=subprocess.DEVNULL\n",
    "            ).strip()\n",
    "            return int(out) if out.isdigit() else None\n",
    "    except Exception:\n",
    "        return None\n",
    "    return None\n",
    "def _kill_external_pid(pid:int):\n",
    "    if not pid or pid == _current_pid():\n",
    "        return\n",
    "    try:\n",
    "        if platform.system() == \"Windows\":\n",
    "            subprocess.run([\"taskkill\",\"/PID\",str(pid),\"/F\",\"/T\"],\n",
    "                           check=False, stdout=subprocess.DEVNULL, stderr=subprocess.DEVNULL)\n",
    "        else:\n",
    "            subprocess.run([\"kill\",\"-9\",str(pid)],\n",
    "                           check=False, stdout=subprocess.DEVNULL, stderr=subprocess.DEVNULL)\n",
    "    except Exception:\n",
    "        pass\n",
    "def _free_port_or_kill_external(port:int):\n",
    "    \"\"\"Ensure port is free by killing only external owners (not this kernel).\"\"\"\n",
    "    pid = _pid_on_port(port)\n",
    "    if pid and pid != _current_pid():\n",
    "        _kill_external_pid(pid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 366,
   "id": "db18ec35-d89e-468a-83e7-c75c9e115ddb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_hf_stub_app():  \n",
    "    app = FastAPI(title=\"HF Stubs\", version=\"0.2\")\n",
    "    @app.get(\"/\")\n",
    "    def root():\n",
    "        return {\"ok\": True, \"try_docs\": \"/docs\",\n",
    "                \"endpoints\": {\"embeddings\":\"POST\",\"reranker\":\"POST\",\"generate\":\"POST\",\"fix\":\"POST\"}}\n",
    "    @app.get(\"/up\")\n",
    "    def up():\n",
    "        return {\"ok\": True, \"models\": {\"e5\": \"hash-embed-384\", \"reranker\": \"lexical\", \"slm\": \"toy\", \"fix\": \"toy\"}}\n",
    "    @app.get(\"/warmup\")\n",
    "    @app.post(\"/warmup\")\n",
    "    def warmup():\n",
    "        return {\"ok\": True, \"warmed\": True}\n",
    "    @app.get(\"/embeddings\")\n",
    "    def embeddings_get():\n",
    "        return {\"ok\": False, \"hint\": \"Use POST /embeddings with JSON\",\n",
    "                \"example\": {\"inputs\": [\"hello\", \"world\"]}}\n",
    "    @app.get(\"/reranker\")\n",
    "    def reranker_get():\n",
    "        return {\"ok\": False, \"hint\": \"Use POST /reranker with JSON\",\n",
    "                \"example\": {\"query\": \"rust vs python\", \"candidates\": [\"tokio async\", \"pyo3 bridge\"]}}\n",
    "    @app.get(\"/generate\")\n",
    "    def generate_get():\n",
    "        return {\"ok\": False, \"hint\": \"Use POST /generate with JSON\",\n",
    "                \"example\": {\"prompt\": \"say hi\", \"max_tokens\": 32}}\n",
    "    @app.get(\"/fix\")\n",
    "    def fix_get():\n",
    "        return {\"ok\": False, \"hint\": \"Use POST /fix with JSON\",\n",
    "                \"example\": {\"text\": \"age:\\\"30\\\", income:\\\"$4000\\\"\"}}\n",
    "    class EmbReq(BaseModel):\n",
    "        inputs: List[str]\n",
    "    @app.post(\"/embeddings\")\n",
    "    def embeddings(req: EmbReq):\n",
    "        dim = 384\n",
    "        idx = DenseIndex([{\"id\":\"x\",\"text\": t} for t in req.inputs], dim=dim)\n",
    "        embs = [idx.encode(t).tolist() for t in req.inputs]\n",
    "        return {\"ok\": True, \"model\": \"hash-embed-384\", \"dim\": dim, \"embeddings\": embs}\n",
    "    class RerankReq(BaseModel):\n",
    "        query: str\n",
    "        candidates: List[str]\n",
    "    @app.post(\"/reranker\")\n",
    "    def reranker(req: RerankReq):\n",
    "        qtok = set(tokenize(req.query))\n",
    "        scores = []\n",
    "        for c in req.candidates:\n",
    "            ctok = set(tokenize(c))\n",
    "            j = (len(qtok & ctok) / max(1, len(qtok | ctok))) if (qtok or ctok) else 0.0\n",
    "            scores.append(j)\n",
    "        return {\"ok\": True, \"model\": \"lexical\", \"scores\": scores}\n",
    "    class GenReq(BaseModel):\n",
    "        prompt: str\n",
    "        max_tokens: Optional[int] = 64\n",
    "    @app.post(\"/generate\")\n",
    "    def generate(req: GenReq):\n",
    "        words = tokenize(req.prompt)[:12]\n",
    "        rnd = \" \".join(random.choice(\n",
    "            [\"alpha\",\"beta\",\"gamma\",\"delta\",\"omega\",\"rust\",\"vector\",\"bm25\",\"faithful\",\"cache\"]\n",
    "        ) for _ in range(20))\n",
    "        return {\"ok\": True, \"model\": \"slm\",\n",
    "                \"outputs\": [(\" \".join(words) + \" | \" + rnd)[:max(10, req.max_tokens or 64)]]}\n",
    "    class FixReq(BaseModel):\n",
    "        text: str\n",
    "    @app.post(\"/fix\")\n",
    "    def fix(req: FixReq):\n",
    "        sample = req.text\n",
    "        sample = re.sub(r'\"\\$?(\\d[\\d,\\.]*)\"', r'{\"value\": \\1}', sample)\n",
    "        return {\"ok\": True, \"model\": \"slm\", \"outputs\": [sample]}\n",
    "    return app"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 367,
   "id": "5032cd20-cfc0-4d35-b490-8a443fafbca7",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SafeKeepAlive:\n",
    "    def __init__(self, hf_base:str, period_s:int=60, fail_open_after:int=4):\n",
    "        self.hf_base = hf_base.rstrip(\"/\")\n",
    "        self.period_s = max(5, int(period_s))\n",
    "        self.fail_open_after = max(1, int(fail_open_after))\n",
    "        self._stop = threading.Event()\n",
    "        self._t: threading.Thread | None = None\n",
    "        self._fails = 0\n",
    "        self._open_until = 0.0  \n",
    "    def _wait(self, seconds:float):\n",
    "        end = time.time() + seconds\n",
    "        while not self._stop.is_set() and time.time() < end:\n",
    "            time.sleep(0.1)\n",
    "    def _tick(self):\n",
    "        import requests\n",
    "        while not self._stop.is_set():\n",
    "            now = time.time()\n",
    "            if now < self._open_until:\n",
    "                self._wait(self.period_s) \n",
    "                continue\n",
    "            try:\n",
    "                requests.get(f\"{self.hf_base}/up\", timeout=2)\n",
    "                requests.post(f\"{self.hf_base}/embeddings\", json={\"inputs\": [\"warm\", \"up\"]}, timeout=2)\n",
    "                requests.post(f\"{self.hf_base}/reranker\", json={\"query\":\"warm\",\"candidates\":[\"a\",\"b\"]}, timeout=2)\n",
    "                try:\n",
    "                    requests.post(f\"{self.hf_base}/generate\", json={\"prompt\":\"warm up\",\"max_tokens\":16}, timeout=2)\n",
    "                    requests.post(f\"{self.hf_base}/fix\", json={\"text\": 'age:\"30\", income:\"$4000\"'}, timeout=2)\n",
    "                except Exception:\n",
    "                    pass\n",
    "                self._fails = 0\n",
    "                self._wait(self.period_s)\n",
    "            except Exception:\n",
    "                self._fails += 1\n",
    "                if self._fails >= self.fail_open_after:\n",
    "                    self._open_until = time.time() + 5*self.period_s\n",
    "                    self._fails = 0\n",
    "                self._wait(min(2*self.period_s, 60))\n",
    "    def start(self):\n",
    "        if self._t and self._t.is_alive(): return\n",
    "        self._stop.clear()\n",
    "        self._t = threading.Thread(target=self._tick, name=\"safe-keepalive\", daemon=True)\n",
    "        self._t.start()\n",
    "    def stop(self):\n",
    "        self._stop.set()\n",
    "        if self._t and self._t.is_alive():\n",
    "            self._t.join(timeout=2.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 368,
   "id": "dfba3e8a-7380-4a6c-87c2-d3c454e4917a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def start_stack(auto_keepalive:bool=False, log_level:str=\"warning\"):\n",
    "    \"\"\"Stop any existing stack, free ports 7070/8001 (externals only), then start clean.\"\"\"\n",
    "    stop_stack(force=False)\n",
    "    _free_port_or_kill_external(7070)\n",
    "    _free_port_or_kill_external(8001)\n",
    "    hf_srv = ServerThread(build_hf_stub_app(), preferred_port=7070, name=\"hf\", log_level=log_level); hf_srv.start()\n",
    "    RUNNING[\"hf\"] = hf_srv\n",
    "    rag_srv = ServerThread(build_rag_app(), preferred_port=8001, name=\"rag\", log_level=log_level); rag_srv.start()\n",
    "    RUNNING[\"rag\"] = rag_srv\n",
    "    gw_srv = ServerThread(build_gateway_app(rag_srv.base), preferred_port=None, name=\"gw\", log_level=log_level); gw_srv.start()\n",
    "    RUNNING[\"gateway\"] = gw_srv\n",
    "    if auto_keepalive:\n",
    "        ka = SafeKeepAlive(hf_srv.base, period_s=60, fail_open_after=4); ka.start()\n",
    "        RUNNING[\"keepalive\"] = ka\n",
    "    print(\" Stack up\")\n",
    "    print(\"  HF   :\", hf_srv.base, \" /docs\")\n",
    "    print(\"  RAG  :\", rag_srv.base, \" /docs\")\n",
    "    print(\"  GW   :\", gw_srv.base,  \" /docs\")\n",
    "    print(\"  Try  : curl -s\", gw_srv.base, \"/up\")\n",
    "    return RUNNING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 369,
   "id": "c8b76c05-67f6-4c61-bfdf-f3c8f04d39e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def stop_stack(force:bool=True):\n",
    "    \"\"\"Gracefully stop servers & keep-alive. Only kill external PIDs if force=True.\"\"\"\n",
    "    ka = RUNNING.pop(\"keepalive\", None)\n",
    "    if ka and hasattr(ka, \"stop\"):\n",
    "        try: ka.stop()\n",
    "        except Exception: pass\n",
    "    for name in [\"gateway\",\"rag\",\"hf\"]:\n",
    "        srv = RUNNING.pop(name, None)\n",
    "        if srv and hasattr(srv, \"stop\"):\n",
    "            try: srv.stop()\n",
    "            except Exception: pass\n",
    "    time.sleep(0.2)\n",
    "    if force:\n",
    "        for port in (7070, 8001):\n",
    "            _free_port_or_kill_external(port)\n",
    "    print(\" Stack stopped\")\n",
    "def sanity():\n",
    "    \"\"\"Robust self-check that won't crash on non-JSON responses.\"\"\"\n",
    "    if not RUNNING:\n",
    "        print(\"Stack is down. Call start_stack() first.\")\n",
    "        return\n",
    "    import requests\n",
    "    gw = RUNNING[\"gateway\"].base\n",
    "    rag = RUNNING[\"rag\"].base\n",
    "    hf  = RUNNING[\"hf\"].base\n",
    "    print(\"== /up ==\")\n",
    "    for name, url in [(\"GW\", gw), (\"RAG\", rag), (\"HF\", hf)]:\n",
    "        try:\n",
    "            r = requests.get(url + \"/up\", timeout=3)\n",
    "            ct = r.headers.get(\"content-type\",\"\")\n",
    "            body = (r.json() if \"json\" in ct else r.text)\n",
    "            print(f\"{name} {url}/up \", r.status_code, body)\n",
    "        except Exception as e:\n",
    "            print(f\"{name} error:\", e)\n",
    "    print(\"\\n== /answer ==\")\n",
    "    try:\n",
    "        r = requests.post(gw+\"/answer\",\n",
    "                          json={\"query\":\"bm25 vs dense\",\"top_k\":4,\"use_rerank\":True},\n",
    "                          timeout=8)\n",
    "        print(\"POST /answer \", r.status_code)\n",
    "        try:\n",
    "            print(json.dumps(r.json(), indent=2)[:900])\n",
    "        except Exception:\n",
    "            print((r.text or \"\")[:400])\n",
    "    except Exception as e:\n",
    "        print(\"request error:\", e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 370,
   "id": "0cc31139-5a7c-4e04-b7e4-da2cc1147436",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded safe stack v2. Use:\n",
      "  stop_stack(); start_stack(auto_keepalive=False); sanity()\n"
     ]
    }
   ],
   "source": [
    "def HARD_RESET():\n",
    "    \"\"\"One-liner reset that won't kill the kernel.\"\"\"\n",
    "    stop_stack(force=True)\n",
    "    print(\"Hard reset complete. Now call: start_stack(auto_keepalive=False)\")\n",
    "print(\"Loaded safe stack v2. Use:\\n  stop_stack(); start_stack(auto_keepalive=False); sanity()\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 371,
   "id": "cf6ec166-a664-488d-b7c2-c37d185b1f85",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time, json, re, threading, os\n",
    "from pathlib import Path\n",
    "from typing import Optional, Dict, Tuple, Any\n",
    "from collections import OrderedDict, defaultdict\n",
    "from fastapi import FastAPI, Request, HTTPException\n",
    "from fastapi.responses import PlainTextResponse, JSONResponse\n",
    "try:\n",
    "    from prometheus_client import Counter as PromCounter, Histogram as PromHistogram, Gauge as PromGauge\n",
    "    from prometheus_client import generate_latest, CONTENT_TYPE_LATEST\n",
    "    _HAVE_PROM = True\n",
    "except Exception:\n",
    "    _HAVE_PROM = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 372,
   "id": "dd139dd1-5991-4a06-a4c7-9e237afcd355",
   "metadata": {},
   "outputs": [],
   "source": [
    "ART = Path(\"artifacts\") / \"local\"\n",
    "ART.mkdir(parents=True, exist_ok=True)\n",
    "AUDIT_DIR = ART / \"audit\"\n",
    "AUDIT_DIR.mkdir(parents=True, exist_ok=True)\n",
    "REGISTRY_PATH = ART / \"artifact_registry.json\"\n",
    "_RE_EMAIL = re.compile(r'\\b([A-Za-z0-9._%+-])([A-Za-z0-9._%+-]*?)@([A-Za-z0-9.-]+\\.[A-Za-z]{2,})\\b')\n",
    "_RE_SSN   = re.compile(r'\\b(\\d{3})[-\\s]?(\\d{2})[-\\s]?(\\d{4})\\b')\n",
    "def redact(s: str) -> str:\n",
    "    if not isinstance(s, str): return s\n",
    "    s = _RE_EMAIL.sub(lambda m: f\"{m.group(1)}***@{m.group(3)}\", s)\n",
    "    s = _RE_SSN.sub(lambda m: f\"{m.group(1)}-**-****\", s)\n",
    "    return s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 373,
   "id": "9fccb422-30d9-4cbd-b673-5e943db8d782",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ArtifactRegistry:\n",
    "    def __init__(self, path: Path):\n",
    "        self.path = path\n",
    "        if not self.path.exists():\n",
    "            self._write({\"router_bundles\": [], \"faiss\": {}, \"bm25\": {}})\n",
    "        self._lock = threading.Lock()\n",
    "    def _read(self) -> Dict[str, Any]:\n",
    "        try:\n",
    "            return json.loads(self.path.read_text(encoding=\"utf-8\"))\n",
    "        except Exception:\n",
    "            return {\"router_bundles\": [], \"faiss\": {}, \"bm25\": {}}\n",
    "    def _write(self, obj: Dict[str, Any]):\n",
    "        self.path.parent.mkdir(parents=True, exist_ok=True)\n",
    "        self.path.write_text(json.dumps(obj, indent=2) + \"\\n\", encoding=\"utf-8\")\n",
    "    def register_router(self, path: str, version: Optional[str] = None):\n",
    "        with self._lock:\n",
    "            reg = self._read()\n",
    "            rec = {\"path\": path, \"version\": version or f\"v-{int(time.time())}\", \"ts\": int(time.time())}\n",
    "            reg[\"router_bundles\"].append(rec)\n",
    "            self._write(reg)\n",
    "            return rec\n",
    "    def latest_router(self) -> Optional[Dict[str, Any]]:\n",
    "        reg = self._read()\n",
    "        return reg[\"router_bundles\"][-1] if reg.get(\"router_bundles\") else None\n",
    "    def register_faiss(self, namespace: str, uri: str, tier: str = \"hot\"):\n",
    "        with self._lock:\n",
    "            reg = self._read()\n",
    "            reg[\"faiss\"].setdefault(namespace, {})[tier] = {\"uri\": uri, \"ts\": int(time.time())}\n",
    "            self._write(reg)\n",
    "            return reg[\"faiss\"][namespace][tier]\n",
    "    def register_bm25(self, namespace: str, corpus_version: str, uri: Optional[str] = None):\n",
    "        with self._lock:\n",
    "            reg = self._read()\n",
    "            reg[\"bm25\"][namespace] = {\"version\": corpus_version, \"uri\": uri, \"ts\": int(time.time())}\n",
    "            self._write(reg)\n",
    "            return reg[\"bm25\"][namespace]\n",
    "    def snapshot(self) -> Dict[str, Any]:\n",
    "        return self._read()\n",
    "REG = ArtifactRegistry(REGISTRY_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 374,
   "id": "d28a825f-632b-42a4-8f2a-e94008bd2db2",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TTLCache:\n",
    "    def __init__(self, capacity: int = 1000, ttl_s: int = 300):\n",
    "        self.capacity = int(capacity)\n",
    "        self.ttl = int(ttl_s)\n",
    "        self._store: \"OrderedDict[str, Tuple[float, Any]]\" = OrderedDict()\n",
    "        self._lock = threading.Lock()\n",
    "        self.hits = 0\n",
    "        self.misses = 0\n",
    "    def _prune(self, now: float):\n",
    "        keys = [k for k,(t,_) in self._store.items() if now - t > self.ttl]\n",
    "        for k in keys:\n",
    "            self._store.pop(k, None)\n",
    "        while len(self._store) > self.capacity:\n",
    "            self._store.popitem(last=False)\n",
    "    def get(self, key: str):\n",
    "        now = time.time()\n",
    "        with self._lock:\n",
    "            self._prune(now)\n",
    "            val = self._store.get(key)\n",
    "            if not val:\n",
    "                self.misses += 1\n",
    "                return None\n",
    "            ts, payload = val\n",
    "            if now - ts > self.ttl:\n",
    "                self._store.pop(key, None)\n",
    "                self.misses += 1\n",
    "                return None\n",
    "            self._store.move_to_end(key, last=True)\n",
    "            self.hits += 1\n",
    "            return payload\n",
    "    def set(self, key: str, value: Any):\n",
    "        now = time.time()\n",
    "        with self._lock:\n",
    "            self._store[key] = (now, value)\n",
    "            self._store.move_to_end(key, last=True)\n",
    "            self._prune(now)\n",
    "    def stats(self) -> Dict[str, Any]:\n",
    "        with self._lock:\n",
    "            return {\n",
    "                \"size\": len(self._store),\n",
    "                \"capacity\": self.capacity,\n",
    "                \"ttl_s\": self.ttl,\n",
    "                \"hits\": self.hits,\n",
    "                \"misses\": self.misses,\n",
    "            }\n",
    "    def clear(self):\n",
    "        with self._lock:\n",
    "            self._store.clear()\n",
    "            self.hits = 0\n",
    "            self.misses = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 375,
   "id": "dcdbd82d-4efb-49a1-858a-0703ebafd46e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class QuotaManager:\n",
    "    def __init__(self, default_per_minute: int = 300):\n",
    "        self.default = int(default_per_minute)\n",
    "        self.limits: Dict[str, int] = defaultdict(lambda: self.default)\n",
    "        self.window_counts: Dict[str, Tuple[int, int]] = {}  \n",
    "        self._lock = threading.Lock()\n",
    "    def set_limit(self, ns: str, per_minute: int):\n",
    "        with self._lock:\n",
    "            self.limits[ns] = max(1, int(per_minute))\n",
    "    def get_limit(self, ns: str) -> int:\n",
    "        with self._lock:\n",
    "            return self.limits[ns]\n",
    "    def allow(self, ns: str) -> bool:\n",
    "        now = int(time.time())\n",
    "        win = now - (now % 60)\n",
    "        with self._lock:\n",
    "            start, cnt = self.window_counts.get(ns, (win, 0))\n",
    "            if start != win:\n",
    "                start, cnt = win, 0\n",
    "            limit = self.limits[ns]\n",
    "            if cnt >= limit:\n",
    "                self.window_counts[ns] = (start, cnt)\n",
    "                return False\n",
    "            cnt += 1\n",
    "            self.window_counts[ns] = (start, cnt)\n",
    "            return True\n",
    "    def snapshot(self) -> Dict[str, Any]:\n",
    "        with self._lock:\n",
    "            return {\n",
    "                \"default_per_minute\": self.default,\n",
    "                \"limits\": dict(self.limits),\n",
    "                \"windows\": {k: {\"window_start\": v[0], \"count\": v[1]} for k,v in self.window_counts.items()}\n",
    "            }\n",
    "QUOTAS = QuotaManager(default_per_minute=300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 376,
   "id": "93ba4209-241b-4587-957f-4b5498426fbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Canary:\n",
    "    def __init__(self):\n",
    "        self.percent_by_ns: Dict[str, float] = defaultdict(lambda: 0.0)  \n",
    "        self._lock = threading.Lock()\n",
    "        self.events: Dict[str, Dict[str,int]] = defaultdict(lambda: {\"rerank\":0, \"rrf\":0})\n",
    "    def set_percent(self, ns: str, pct: float):\n",
    "        pct = max(0.0, min(100.0, pct))\n",
    "        with self._lock:\n",
    "            self.percent_by_ns[ns] = pct\n",
    "    def choose(self, ns: str) -> bool:\n",
    "        pct = self.percent_by_ns[ns]\n",
    "        pick_rerank = (100.0 * (time.time() % 1)) < pct  \n",
    "        self.events[ns][\"rerank\" if pick_rerank else \"rrf\"] += 1\n",
    "        return pick_rerank\n",
    "    def snapshot(self) -> Dict[str, Any]:\n",
    "        with self._lock:\n",
    "            return {\"percent_by_ns\": dict(self.percent_by_ns), \"events\": {k: dict(v) for k,v in self.events.items()}}\n",
    "CANARY = Canary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 377,
   "id": "f973d39e-2592-42cd-a751-a2e19bdae885",
   "metadata": {},
   "outputs": [],
   "source": [
    "if _HAVE_PROM:\n",
    "    GW_REQS   = PromCounter(\"py_gateway_answer_requests_total\", \"Gateway /answer requests\", [\"ns\", \"pipeline\", \"cache\"])\n",
    "    GW_ERRORS = PromCounter(\"py_gateway_answer_errors_total\",   \"Gateway errors\", [\"ns\", \"stage\"])\n",
    "    GW_LAT_MS = PromHistogram(\"py_gateway_answer_latency_ms\",   \"End-to-end latency (ms)\")\n",
    "    GW_CACHE_HITS = PromCounter(\"py_gateway_cache_hits_total\",  \"Cache hits\", [\"ns\"])\n",
    "    GW_CACHE_MISSES = PromCounter(\"py_gateway_cache_misses_total\",\"Cache misses\", [\"ns\"])\n",
    "else:\n",
    "    GW_REQS=GW_ERRORS=GW_LAT_MS=GW_CACHE_HITS=GW_CACHE_MISSES=None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 378,
   "id": "dc4bc389-e5b3-46c0-9bd8-938230d057d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "RESP_CACHE = TTLCache(capacity=1000, ttl_s=300)\n",
    "def _norm_query(q: str) -> str:\n",
    "    return re.sub(r\"\\s+\", \" \", (q or \"\").strip().lower())\n",
    "def _ns_from(req: Request) -> str:\n",
    "    return (req.headers.get(\"X-Namespace\") or req.query_params.get(\"ns\") or \"default\").strip().lower()\n",
    "def _audit_write(ns: str, record: Dict[str, Any]):\n",
    "    day = time.strftime(\"%Y%m%d\")\n",
    "    p = AUDIT_DIR / f\"answer_{day}.ndjson\"\n",
    "    rec = dict(record)\n",
    "    rec[\"namespace\"] = ns\n",
    "    try:\n",
    "        with p.open(\"a\", encoding=\"utf-8\") as f:\n",
    "            f.write(json.dumps(rec, ensure_ascii=False) + \"\\n\")\n",
    "    except Exception:\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 379,
   "id": "737e863d-ee40-4c33-a8bf-4ebaa2103a93",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_gateway_app_v2(rag_base: str) -> FastAPI:\n",
    "    app = FastAPI(title=\"RAG Python Gateway (feat)\", version=\"0.2\")\n",
    "    @app.get(\"/\")\n",
    "    def root():\n",
    "        return {\"ok\": True, \"endpoints\": [\"/up\", \"/answer\", \"/metrics\", \"/admin/*\", \"/canary/*\", \"/artifacts/*\"], \"rag_base\": rag_base}\n",
    "    @app.get(\"/up\")\n",
    "    def up():\n",
    "        try:\n",
    "            ms = requests.get(f\"{rag_base}/up\", timeout=3).json()\n",
    "        except Exception as e:\n",
    "            ms = {\"ok\": False, \"error\": str(e)}\n",
    "        return {\"ok\": True, \"gateway\": {\"rag_base\": rag_base}, \"microservice\": ms}\n",
    "    @app.get(\"/metrics\")\n",
    "    def metrics():\n",
    "        if not _HAVE_PROM:\n",
    "            return PlainTextResponse(\"prometheus_client not installed\", status_code=200)\n",
    "        return PlainTextResponse(generate_latest(), media_type=CONTENT_TYPE_LATEST)\n",
    "    @app.post(\"/answer\")\n",
    "    def answer(req: Request, body: Dict[str, Any]):\n",
    "        t0 = time.time()\n",
    "        ns = _ns_from(req)\n",
    "        query = body.get(\"query\", \"\")\n",
    "        top_k = int(body.get(\"top_k\", 5))\n",
    "        use_rerank = body.get(\"use_rerank\") \n",
    "        if not isinstance(query, str) or not query.strip():\n",
    "            raise HTTPException(400, \"query required\")\n",
    "        if not QUOTAS.allow(ns):\n",
    "            if _HAVE_PROM: GW_ERRORS.labels(ns, \"quota\").inc()\n",
    "            raise HTTPException(status_code=429, detail=\"quota_exceeded\")\n",
    "        if use_rerank is None:\n",
    "            use_rerank = CANARY.choose(ns)\n",
    "        key = json.dumps({\"ns\": ns, \"q\": _norm_query(query), \"k\": top_k, \"rr\": bool(use_rerank)}, sort_keys=True)\n",
    "        cached = RESP_CACHE.get(key)\n",
    "        if cached is not None:\n",
    "            if _HAVE_PROM:\n",
    "                GW_REQS.labels(ns, \"rerank\" if use_rerank else \"rrf\", \"hit\").inc()\n",
    "                GW_CACHE_HITS.labels(ns).inc()\n",
    "                GW_LAT_MS.observe((time.time()-t0)*1000.0)\n",
    "            _audit_write(ns, {\"query\": redact(query), \"pipeline\": \"rerank\" if use_rerank else \"rrf\",\n",
    "                              \"latency_ms\": int((time.time()-t0)*1000), \"cache\": \"hit\"})\n",
    "            return JSONResponse(cached)\n",
    "        try:\n",
    "            d = requests.post(f\"{rag_base}/retrieve_dense\", json={\"query\": query, \"top_k\": top_k}, timeout=3).json()\n",
    "            b = requests.post(f\"{rag_base}/retrieve_bm25\", json={\"query\": query, \"top_k\": top_k}, timeout=3).json()\n",
    "            fused = rrf_fuse(d.get(\"results\", []), b.get(\"results\", []), k=top_k)\n",
    "            contexts = fused\n",
    "            if use_rerank:\n",
    "                rr = requests.post(f\"{rag_base}/rerank\", json={\"query\": query, \"candidates\": fused}, timeout=3).json()\n",
    "                contexts = rr.get(\"results\", fused)\n",
    "            comp = requests.post(f\"{rag_base}/complete\", json={\"query\": query, \"contexts\": contexts}, timeout=5).json()\n",
    "        except requests.RequestException as e:\n",
    "            if _HAVE_PROM: GW_ERRORS.labels(ns, \"upstream\").inc()\n",
    "            raise HTTPException(502, f\"bad_gateway: {e}\")\n",
    "        resp = {\n",
    "            \"answer\": comp.get(\"answer\",\"\"),\n",
    "            \"contexts\": contexts,\n",
    "            \"verify\": comp.get(\"verify\", {}),\n",
    "            \"meta\": {\n",
    "                \"pipeline\": \"rerank\" if use_rerank else \"rrf\",\n",
    "                \"latency_ms\": int((time.time()-t0)*1000),\n",
    "                \"cache\": \"miss\"\n",
    "            }\n",
    "        }\n",
    "        RESP_CACHE.set(key, resp)\n",
    "        if _HAVE_PROM:\n",
    "            GW_REQS.labels(ns, resp[\"meta\"][\"pipeline\"], \"miss\").inc()\n",
    "            GW_CACHE_MISSES.labels(ns).inc()\n",
    "            GW_LAT_MS.observe(resp[\"meta\"][\"latency_ms\"])\n",
    "        _audit_write(ns, {\"query\": redact(query), \"pipeline\": resp[\"meta\"][\"pipeline\"],\n",
    "                          \"latency_ms\": resp[\"meta\"][\"latency_ms\"], \"cache\": \"miss\"})\n",
    "        return JSONResponse(resp)\n",
    "    @app.get(\"/admin/cache/stats\")\n",
    "    def cache_stats():\n",
    "        return {\"ok\": True, \"stats\": RESP_CACHE.stats()}\n",
    "    @app.post(\"/admin/cache/clear\")\n",
    "    def cache_clear():\n",
    "        RESP_CACHE.clear()\n",
    "        return {\"ok\": True}\n",
    "    @app.get(\"/admin/quotas\")\n",
    "    def quotas_get():\n",
    "        return {\"ok\": True, \"quotas\": QUOTAS.snapshot()}\n",
    "    @app.post(\"/admin/quotas/set\")\n",
    "    def quotas_set(payload: Dict[str, Any]):\n",
    "        ns = (payload.get(\"namespace\") or \"default\").strip().lower()\n",
    "        per_min = int(payload.get(\"per_minute\", 300))\n",
    "        QUOTAS.set_limit(ns, per_min)\n",
    "        return {\"ok\": True, \"namespace\": ns, \"per_minute\": per_min}\n",
    "    @app.get(\"/canary/state\")\n",
    "    def canary_state():\n",
    "        return {\"ok\": True, **CANARY.snapshot()}\n",
    "    @app.post(\"/canary/set\")\n",
    "    def canary_set(payload: Dict[str, Any]):\n",
    "        ns = (payload.get(\"namespace\") or \"default\").strip().lower()\n",
    "        pct = float(payload.get(\"percent\", 0.0))\n",
    "        if pct > 2.0:\n",
    "            raise HTTPException(400, \"percent must be  2.0 by default\")\n",
    "        CANARY.set_percent(ns, pct)\n",
    "        return {\"ok\": True, \"namespace\": ns, \"percent\": pct}\n",
    "    @app.post(\"/artifacts/register/router\")\n",
    "    def reg_router(payload: Dict[str, Any]):\n",
    "        rec = REG.register_router(path=str(payload.get(\"path\")), version=payload.get(\"version\"))\n",
    "        return {\"ok\": True, \"record\": rec}\n",
    "    @app.get(\"/artifacts/router/latest\")\n",
    "    def router_latest():\n",
    "        rec = REG.latest_router()\n",
    "        return {\"ok\": bool(rec), \"record\": rec}\n",
    "    @app.post(\"/artifacts/register/faiss\")\n",
    "    def reg_faiss(payload: Dict[str, Any]):\n",
    "        ns = (payload.get(\"namespace\") or \"default\").strip().lower()\n",
    "        uri = str(payload.get(\"uri\"))\n",
    "        tier = (payload.get(\"tier\") or \"hot\").strip().lower()\n",
    "        rec = REG.register_faiss(ns, uri, tier)\n",
    "        return {\"ok\": True, \"namespace\": ns, \"tier\": tier, \"record\": rec}\n",
    "    @app.post(\"/artifacts/register/bm25\")\n",
    "    def reg_bm25(payload: Dict[str, Any]):\n",
    "        ns = (payload.get(\"namespace\") or \"default\").strip().lower()\n",
    "        version = str(payload.get(\"version\") or \"\")\n",
    "        uri = payload.get(\"uri\")\n",
    "        rec = REG.register_bm25(ns, version, uri)\n",
    "        return {\"ok\": True, \"namespace\": ns, \"record\": rec}\n",
    "    @app.get(\"/artifacts/snapshot\")\n",
    "    def artifacts_snapshot():\n",
    "        return {\"ok\": True, \"registry\": REG.snapshot()}\n",
    "    return app"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 380,
   "id": "751f4193-74dc-4730-83fa-1efda47af4fd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Featureful gateway up at http://127.0.0.1:54375\n",
      "Try:\n",
      "  curl -s http://127.0.0.1:54375/up | jq .\n",
      "  curl -s http://127.0.0.1:54375/admin/cache/stats | jq .\n",
      "  curl -s -X POST http://127.0.0.1:54375/canary/set -H 'Content-Type: application/json' -d '{\"namespace\":\"default\",\"percent\":2}' | jq .\n",
      "  curl -s -X POST http://127.0.0.1:54375/answer -H 'X-Namespace: default' -H 'Content-Type: application/json' -d '{\"query\":\"bm25 vs dense\",\"top_k\":4}' | jq .\n"
     ]
    }
   ],
   "source": [
    "def restart_gateway_with_features():\n",
    "    if \"gateway\" in RUNNING:\n",
    "        try:\n",
    "            RUNNING[\"gateway\"].stop()\n",
    "        except Exception:\n",
    "            pass\n",
    "        RUNNING.pop(\"gateway\", None)\n",
    "    rag_base = RUNNING[\"rag\"].base if \"rag\" in RUNNING else \"http://127.0.0.1:8001\"\n",
    "    gw_srv = ServerThread(build_gateway_app_v2(rag_base), preferred_port=None, name=\"gw\", log_level=\"warning\")\n",
    "    gw_srv.start()\n",
    "    RUNNING[\"gateway\"] = gw_srv\n",
    "    print(\" Featureful gateway up at\", gw_srv.base)\n",
    "    print(\"Try:\")\n",
    "    print(f\"  curl -s {gw_srv.base}/up | jq .\")\n",
    "    print(f\"  curl -s {gw_srv.base}/admin/cache/stats | jq .\")\n",
    "    print(f\"  curl -s -X POST {gw_srv.base}/canary/set -H 'Content-Type: application/json' -d '{{\\\"namespace\\\":\\\"default\\\",\\\"percent\\\":2}}' | jq .\")\n",
    "    print(f\"  curl -s -X POST {gw_srv.base}/answer -H 'X-Namespace: default' -H 'Content-Type: application/json' -d '{{\\\"query\\\":\\\"bm25 vs dense\\\",\\\"top_k\\\":4}}' | jq .\")\n",
    "restart_gateway_with_features()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 381,
   "id": "56dcc02e-6f21-4fb6-a1ed-d7197d289247",
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import annotations\n",
    "import os, re, io, time, json, hashlib, subprocess, threading\n",
    "from pathlib import Path\n",
    "from typing import Any, Dict, List, Optional, Tuple, Literal\n",
    "from dataclasses import dataclass\n",
    "from fnmatch import fnmatch\n",
    "from fastapi import FastAPI, Request, HTTPException\n",
    "from fastapi.responses import JSONResponse\n",
    "from pydantic import BaseModel, Field, root_validator, validator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 382,
   "id": "eba7006e-4aa5-4282-852e-077a3a57e726",
   "metadata": {},
   "outputs": [],
   "source": [
    "OVERSEER_DIR = ART / \"overseer\"\n",
    "PATCH_DIR    = OVERSEER_DIR / \"patches\"\n",
    "LOG_DIR      = OVERSEER_DIR / \"logs\"\n",
    "for d in (OVERSEER_DIR, PATCH_DIR, LOG_DIR): d.mkdir(parents=True, exist_ok=True)\n",
    "ALLOW_PATHS_DEFAULT = [\"services/*\", \"prompts/*.json\", \"configs/*.yaml\", \"rag_rust_gateway/*\"]\n",
    "BLOCK_DEP_FILES     = {\n",
    "    \"requirements.txt\", \"pyproject.toml\", \"Pipfile\", \"Pipfile.lock\",\n",
    "    \"package.json\", \"package-lock.json\", \"yarn.lock\",\n",
    "    \"Cargo.toml\", \"Cargo.lock\",\n",
    "    \"setup.cfg\", \"setup.py\",\n",
    "}\n",
    "MAX_FILES  = 2\n",
    "MAX_LINES  = 30 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 383,
   "id": "1fbed5ed-1328-40ad-a52b-f8be0d10550c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CodeContext(BaseModel):\n",
    "    path: str\n",
    "    start: int\n",
    "    end: int\n",
    "class ErrorBundle(BaseModel):\n",
    "    node: str\n",
    "    status_code: Optional[int] = None\n",
    "    stack: Optional[str] = None\n",
    "    code_context: List[CodeContext] = []\n",
    "    failing_input: Dict[str, Any] = {}\n",
    "    last_green_sha: Optional[str] = \"unknown\"\n",
    "    allow_list: List[str] = []\n",
    "class IngestReq(BaseModel):\n",
    "    error_bundle: ErrorBundle\n",
    "class PatchReq(BaseModel):\n",
    "    patch_type: Literal[\"unified_diff\",\"config\"]\n",
    "    patch: str | Dict[str, Any]\n",
    "    notes: str = Field(..., max_length=120)\n",
    "    tests: List[str] = []\n",
    "    allow_paths: List[str] = []\n",
    "    dry_run: bool = True\n",
    "    @validator(\"allow_paths\", pre=True, always=True)\n",
    "    def _default_allow(cls, v):\n",
    "        return v or ALLOW_PATHS_DEFAULT\n",
    "class SandboxReq(BaseModel):\n",
    "    patch_id: Optional[str] = None   \n",
    "    run_golden: bool = True\n",
    "    run_linters: bool = True\n",
    "    run_security: bool = True\n",
    "class CanaryDecisionReq(BaseModel):\n",
    "    namespace: str = \"default\"\n",
    "    p95_ms: Optional[float] = None\n",
    "    faithfulness_mean: Optional[float] = None\n",
    "    ndcg10: Optional[float] = None\n",
    "    error_rate: Optional[float] = None\n",
    "    window_n: Optional[int] = None\n",
    "    promote_threshold_p95_ms: float = 1800.0\n",
    "    promote_threshold_error: float = 0.03\n",
    "    min_window_n: int = 200\n",
    "    rollback_p95_ms: float = 2000.0\n",
    "    rollback_error: float = 0.07\n",
    "    desired_percent: float = 2.0 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 384,
   "id": "73d79101-80d5-484f-bad7-f2ef6b3f3010",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _ts() -> str:\n",
    "    return time.strftime(\"%Y%m%d-%H%M%S\")\n",
    "def _write_jsonl(path: Path, obj: Dict[str, Any]):\n",
    "    with path.open(\"a\", encoding=\"utf-8\") as f:\n",
    "        f.write(json.dumps(obj, ensure_ascii=False) + \"\\n\")\n",
    "def _sha16(s: str) -> str:\n",
    "    return hashlib.sha256(s.encode(\"utf-8\")).hexdigest()[:16]\n",
    "def _diff_stats(udiff: str) -> Tuple[List[str], int]:\n",
    "    \"\"\"Return (files_touched, +/- line count) from unified diff text.\"\"\"\n",
    "    files: List[str] = []\n",
    "    lines = 0\n",
    "    cur_file = None\n",
    "    for ln in udiff.splitlines():\n",
    "        if ln.startswith(\"+++ \") or ln.startswith(\"--- \") or ln.startswith(\"diff --git \"):\n",
    "            m = re.search(r\"\\+\\+\\+\\s+[ab]/(.+)$\", ln) or re.search(r\"---\\s+[ab]/(.+)$\", ln)\n",
    "            if not m:\n",
    "                m = re.search(r\"diff --git a/(.+?) b/\\1\", ln)\n",
    "            if m:\n",
    "                cur_file = m.group(1)\n",
    "                if cur_file not in files:\n",
    "                    files.append(cur_file)\n",
    "            continue\n",
    "        if ln.startswith(\"+\") and not ln.startswith(\"+++ \"):\n",
    "            lines += 1\n",
    "        elif ln.startswith(\"-\") and not ln.startswith(\"--- \"):\n",
    "            lines += 1\n",
    "    return files, lines\n",
    "def _path_allowed(path: str, allow_globs: List[str]) -> bool:\n",
    "    return any(fnmatch(path, pat) for pat in allow_globs)\n",
    "def _violates_dep_policy(files: List[str]) -> Optional[str]:\n",
    "    for f in files:\n",
    "        name = Path(f).name\n",
    "        if name in BLOCK_DEP_FILES:\n",
    "            return f\"dependency file change not allowed: {name}\"\n",
    "    return None\n",
    "def _store_patch(patch_text: str, meta: Dict[str, Any]) -> str:\n",
    "    pid = f\"{_ts()}-{_sha16(patch_text)[:8]}\"\n",
    "    dest = PATCH_DIR / f\"{pid}.diff\"\n",
    "    meta_path = PATCH_DIR / f\"{pid}.meta.json\"\n",
    "    dest.write_text(patch_text, encoding=\"utf-8\")\n",
    "    meta_path.write_text(json.dumps(meta, indent=2) + \"\\n\", encoding=\"utf-8\")\n",
    "    return pid\n",
    "def _load_patch(pid: str) -> Tuple[str, Dict[str,Any]]:\n",
    "    diff_path = PATCH_DIR / f\"{pid}.diff\"\n",
    "    meta_path = PATCH_DIR / f\"{pid}.meta.json\"\n",
    "    if not diff_path.exists():\n",
    "        raise FileNotFoundError(f\"patch {pid} not found\")\n",
    "    text = diff_path.read_text(encoding=\"utf-8\")\n",
    "    meta = json.loads(meta_path.read_text(encoding=\"utf-8\")) if meta_path.exists() else {}\n",
    "    return text, meta\n",
    "def _maybe_run(cmd: List[str], cwd: Optional[str]=None, timeout: int=60) -> Tuple[int, str]:\n",
    "    \"\"\"Run a tool if present; return (rc, output). If not installed, rc=0 and note skipped.\"\"\"\n",
    "    exe = cmd[0]\n",
    "    if shutil.which(exe) is None:\n",
    "        return 0, f\"{exe}: skipped (not installed)\"\n",
    "    try:\n",
    "        out = subprocess.check_output(cmd, cwd=cwd, stderr=subprocess.STDOUT, text=True, timeout=timeout)\n",
    "        return 0, out\n",
    "    except subprocess.CalledProcessError as e:\n",
    "        return e.returncode, (e.output or \"\")\n",
    "    except Exception as e:\n",
    "        return 1, str(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 385,
   "id": "9256c2bc-2b5d-4326-886b-ba44ee532235",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Overseer up at http://127.0.0.1:7000    /overseer/up  /overseer/ingest  /overseer/patch/*  /overseer/sandbox/run  /overseer/canary/decide\n"
     ]
    }
   ],
   "source": [
    "def build_overseer_app() -> FastAPI:\n",
    "    app = FastAPI(title=\"LLM Overseer\", version=\"0.2\")\n",
    "    @app.get(\"/overseer/up\")\n",
    "    def up():\n",
    "        return {\"ok\": True, \"policy\": {\n",
    "            \"allow_paths\": ALLOW_PATHS_DEFAULT, \"max_files\": MAX_FILES, \"max_lines\": MAX_LINES,\n",
    "            \"block_dep_files\": sorted(BLOCK_DEP_FILES)\n",
    "        }}\n",
    "    @app.post(\"/overseer/ingest\")\n",
    "    def ingest(req: IngestReq, request: Request):\n",
    "        idem = request.headers.get(\"Idempotency-Key\") or _sha16(json.dumps(req.dict(), sort_keys=True))\n",
    "        rec = req.error_bundle.dict()\n",
    "        rec.update({\"received_ts\": int(time.time()), \"idempotency_key\": idem})\n",
    "        _write_jsonl(AUDIT_DIR / f\"overseer_ingest_{time.strftime('%Y%m%d')}.ndjson\", rec)\n",
    "        return {\"ok\": True, \"idempotency_key\": idem}\n",
    "    @app.post(\"/overseer/patch/validate\")\n",
    "    def patch_validate(body: PatchReq):\n",
    "        if body.patch_type == \"config\":\n",
    "            if isinstance(body.patch, str):\n",
    "                try:\n",
    "                    json.loads(body.patch)\n",
    "                except Exception as e:\n",
    "                    raise HTTPException(400, f\"config is not valid JSON: {e}\")\n",
    "            return {\"ok\": True, \"type\": \"config\", \"files\": [], \"lines\": 0}\n",
    "        if not isinstance(body.patch, str):\n",
    "            raise HTTPException(400, \"unified_diff requires text patch\")\n",
    "        files, lines = _diff_stats(body.patch)\n",
    "        if len(files) == 0:\n",
    "            raise HTTPException(400, \"no files detected in diff\")\n",
    "        if len(files) > MAX_FILES:\n",
    "            raise HTTPException(400, f\"touches {len(files)} files (> {MAX_FILES})\")\n",
    "        if lines > MAX_LINES:\n",
    "            raise HTTPException(400, f\"patch is {lines} changed lines (> {MAX_LINES})\")\n",
    "        dep_err = _violates_dep_policy(files)\n",
    "        if dep_err:\n",
    "            raise HTTPException(400, dep_err)\n",
    "        not_allowed = [f for f in files if not _path_allowed(f, body.allow_paths)]\n",
    "        if not_allowed:\n",
    "            raise HTTPException(400, f\"paths not allow-listed: {not_allowed}\")\n",
    "        return {\"ok\": True, \"type\": \"unified_diff\", \"files\": files, \"lines\": lines}\n",
    "    @app.post(\"/overseer/patch/submit\")\n",
    "    def patch_submit(body: PatchReq):\n",
    "        _ = patch_validate(body)\n",
    "        patch_text = body.patch if isinstance(body.patch, str) else json.dumps(body.patch, indent=2)\n",
    "        pid = _store_patch(patch_text, {\"notes\": body.notes, \"tests\": body.tests, \"allow_paths\": body.allow_paths})\n",
    "        return {\"ok\": True, \"patch_id\": pid, \"stored\": True}\n",
    "    @app.post(\"/overseer/sandbox/run\")\n",
    "    def sandbox_run(body: SandboxReq):\n",
    "        \"\"\"Run linters/security/test harness; best-effort, tools optional.\"\"\"\n",
    "        results: Dict[str, Any] = {\"ok\": True, \"steps\": []}\n",
    "        if body.patch_id:\n",
    "            try:\n",
    "                patch_text, meta = _load_patch(body.patch_id)\n",
    "                files, lines = _diff_stats(patch_text)\n",
    "                results[\"steps\"].append({\"step\": \"patch_load\", \"ok\": True, \"files\": files, \"lines\": lines})\n",
    "            except Exception as e:\n",
    "                results[\"steps\"].append({\"step\": \"patch_load\", \"ok\": False, \"error\": str(e)})\n",
    "                results[\"ok\"] = False\n",
    "        if body.run_linters:\n",
    "            for cmd in ([\"ruff\", \".\"] , [\"sqlfluff\", \"lint\", \".\"], [\"mypy\", \".\"], [\"semgrep\", \"--quiet\", \"--error\", \"--config\", \"auto\", \".\"]):\n",
    "                rc, out = _maybe_run(cmd, cwd=\".\")\n",
    "                results[\"steps\"].append({\"step\": \"lint\", \"tool\": cmd[0], \"ok\": (rc == 0), \"rc\": rc, \"out\": out[:8000]})\n",
    "                if rc != 0: results[\"ok\"] = False\n",
    "        if body.run_security:\n",
    "            rc, out = _maybe_run([\"bandit\", \"-q\", \"-r\", \".\"], cwd=\".\")\n",
    "            results[\"steps\"].append({\"step\": \"security\", \"tool\": \"bandit\", \"ok\": (rc == 0), \"rc\": rc, \"out\": out[:8000]})\n",
    "            if rc != 0: results[\"ok\"] = False\n",
    "        ran_golden = False\n",
    "        if body.run_golden:\n",
    "            try:\n",
    "                if \"run_golden_pack\" in globals():\n",
    "                    golden = run_golden_pack()  \n",
    "                    results[\"steps\"].append({\"step\": \"golden\", \"ok\": True, \"report\": golden})\n",
    "                    ran_golden = True\n",
    "                elif \"run_all_reports_and_bundle\" in globals():\n",
    "                    rep = run_all_reports_and_bundle()\n",
    "                    results[\"steps\"].append({\"step\": \"reports\", \"ok\": True, \"report\": rep})\n",
    "                    ran_golden = True\n",
    "                else:\n",
    "                    results[\"steps\"].append({\"step\": \"golden\", \"ok\": False, \"error\": \"runner not loaded\"})\n",
    "            except Exception as e:\n",
    "                results[\"steps\"].append({\"step\": \"golden\", \"ok\": False, \"error\": str(e)})\n",
    "                results[\"ok\"] = False\n",
    "        results[\"summary\"] = {\"ran_golden\": ran_golden}\n",
    "        return results\n",
    "    @app.post(\"/overseer/canary/decide\")\n",
    "    def canary_decide(body: CanaryDecisionReq):\n",
    "        \"\"\"Simple decision rule for promotion/rollback; caller supplies recent metrics window.\"\"\"\n",
    "        ns = body.namespace.strip().lower()\n",
    "        if (body.window_n or 0) < body.min_window_n:\n",
    "            return {\"ok\": True, \"decision\": \"hold\", \"reason\": f\"window_n<{body.min_window_n}\", \"namespace\": ns}\n",
    "        p95   = body.p95_ms or float(\"inf\")\n",
    "        error = body.error_rate or 1.0\n",
    "        if p95 <= body.promote_threshold_p95_ms and error <= body.promote_threshold_error:\n",
    "            try:\n",
    "                if \"RUNNING\" in globals() and \"gateway\" in RUNNING:\n",
    "                    gw = RUNNING[\"gateway\"].base\n",
    "                    _ = requests.post(f\"{gw}/canary/set\",\n",
    "                                      json={\"namespace\": ns, \"percent\": body.desired_percent},\n",
    "                                      timeout=3)\n",
    "                decision = \"promote\"\n",
    "            except Exception:\n",
    "                decision = \"promote_local\"\n",
    "            return {\"ok\": True, \"decision\": decision, \"target_percent\": min(body.desired_percent, 2.0), \"namespace\": ns}\n",
    "        if p95 > body.rollback_p95_ms or error > body.rollback_error:\n",
    "            try:\n",
    "                if \"RUNNING\" in globals() and \"gateway\" in RUNNING:\n",
    "                    gw = RUNNING[\"gateway\"].base\n",
    "                    _ = requests.post(f\"{gw}/canary/set\",\n",
    "                                      json={\"namespace\": ns, \"percent\": 0.0},\n",
    "                                      timeout=3)\n",
    "                decision = \"rollback\"\n",
    "            except Exception:\n",
    "                decision = \"rollback_local\"\n",
    "            return {\"ok\": True, \"decision\": decision, \"target_percent\": 0.0, \"namespace\": ns}\n",
    "        return {\"ok\": True, \"decision\": \"hold\", \"namespace\": ns}\n",
    "    return app\n",
    "def start_overseer(preferred_port: int = 7000, log_level: str = \"warning\"):\n",
    "    srv = ServerThread(build_overseer_app(), preferred_port=preferred_port, name=\"overseer\", log_level=log_level)\n",
    "    srv.start()\n",
    "    RUNNING[\"overseer\"] = srv\n",
    "    print(f\" Overseer up at {srv.base}    /overseer/up  /overseer/ingest  /overseer/patch/*  /overseer/sandbox/run  /overseer/canary/decide\")\n",
    "    return srv\n",
    "if \"overseer\" not in RUNNING:\n",
    "    start_overseer(preferred_port=7000, log_level=\"warning\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 386,
   "id": "16b94891-d206-4c2a-8489-dfd84bf135ce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overseer demo helper available: demo_overseer_flow()\n"
     ]
    }
   ],
   "source": [
    "def demo_overseer_flow():\n",
    "    base = RUNNING[\"overseer\"].base\n",
    "    eb = {\n",
    "        \"error_bundle\": {\n",
    "            \"node\": \"query_hot_path\",\n",
    "            \"status_code\": 502,\n",
    "            \"stack\": \"Traceback: ...\",\n",
    "            \"code_context\": [{\"path\":\"services/router.py\",\"start\":40,\"end\":95}],\n",
    "            \"failing_input\": {\"query\":\"SELECT * FROM users; DROP TABLE users;\"},\n",
    "            \"last_green_sha\": \"abc123\",\n",
    "            \"allow_list\": ALLOW_PATHS_DEFAULT,\n",
    "        }\n",
    "    }\n",
    "    r1 = requests.post(f\"{base}/overseer/ingest\", json=eb, timeout=5).json()\n",
    "    diff = \"\"\"\\\n",
    "diff --git a/services/router.py b/services/router.py\n",
    "--- a/services/router.py\n",
    "+++ b/services/router.py\n",
    "@@ -41,6 +41,10 @@\n",
    " def sanitize(q: str) -> str:\n",
    "+    # guard: single-statement only\n",
    "+    if ';' in q.strip().rstrip(';'):\n",
    "+        raise ValueError(\"multiple statements not allowed\")\n",
    "+    return q\n",
    "\"\"\"\n",
    "    r2 = requests.post(f\"{base}/overseer/patch/validate\",\n",
    "                       json={\"patch_type\":\"unified_diff\",\"patch\":diff,\"notes\":\"guard multi-statement\",\"tests\":[\"tests/test_sql_guard.py::test_multi_stmt\"]},\n",
    "                       timeout=5).json()\n",
    "    r3 = requests.post(f\"{base}/overseer/patch/submit\",\n",
    "                       json={\"patch_type\":\"unified_diff\",\"patch\":diff,\"notes\":\"guard multi-statement\",\"tests\":[\"tests/test_sql_guard.py::test_multi_stmt\"]},\n",
    "                       timeout=5).json()\n",
    "    r4 = requests.post(f\"{base}/overseer/sandbox/run\",\n",
    "                       json={\"patch_id\": r3.get(\"patch_id\"), \"run_golden\": True, \"run_linters\": True, \"run_security\": True},\n",
    "                       timeout=60).json()\n",
    "    return {\"ingest\": r1, \"validate\": r2, \"submit\": r3, \"sandbox\": r4}\n",
    "print(\"Overseer demo helper available: demo_overseer_flow()\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 387,
   "id": "b01ac2ef-124b-4312-8143-659dc13fbbcc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Minute pre-warm thread running (gateway/rag). Circuit breaker state checked via CB.open().\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Exception in thread prewarm-60s:\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\aniru\\AppData\\Local\\Temp\\ipykernel_17912\\1497256819.py\", line 29, in _loop\n",
      "AttributeError: 'CircuitBreaker' object has no attribute 'ok'\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Anaconda3\\Lib\\threading.py\", line 1075, in _bootstrap_inner\n",
      "    self.run()\n",
      "  File \"C:\\Anaconda3\\Lib\\site-packages\\ipykernel\\ipkernel.py\", line 766, in run_closure\n",
      "    _threading_Thread_run(self)\n",
      "  File \"C:\\Anaconda3\\Lib\\threading.py\", line 1012, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"C:\\Users\\aniru\\AppData\\Local\\Temp\\ipykernel_17912\\1497256819.py\", line 31, in _loop\n",
      "AttributeError: 'CircuitBreaker' object has no attribute 'hit'\n"
     ]
    }
   ],
   "source": [
    "class CircuitBreaker:\n",
    "    def __init__(self, thresh:int=5, cooldown_s:int=60):\n",
    "        self.fail = 0\n",
    "        self.thresh = max(1, thresh)\n",
    "        self.cooldown_until = 0.0\n",
    "        self.cooldown_s = cooldown_s\n",
    "        self._lock = threading.Lock()\n",
    "    def ok(self):\n",
    "        with self._lock:\n",
    "            self.fail = 0\n",
    "    def hit(self):\n",
    "        with self._lock:\n",
    "            self.fail += 1\n",
    "            if self.fail >= self.thresh:\n",
    "                self.cooldown_until = time.time() + self.cooldown_s\n",
    "                self.fail = 0\n",
    "    def open(self) -> bool:\n",
    "        with self._lock:\n",
    "            return time.time() < self.cooldown_until\n",
    "CB = CircuitBreaker(thresh=4, cooldown_s=90)\n",
    "def start_minute_prewarm():\n",
    "    def _loop():\n",
    "        while True:\n",
    "            try:\n",
    "                if RUNNING.get(\"rag\"):\n",
    "                    _ = requests.get(RUNNING[\"rag\"].base + \"/up\", timeout=2).json()\n",
    "                if RUNNING.get(\"gateway\"):\n",
    "                    _ = requests.get(RUNNING[\"gateway\"].base + \"/up\", timeout=2).json()\n",
    "                CB.ok()\n",
    "            except Exception:\n",
    "                CB.hit()\n",
    "            time.sleep(60)\n",
    "    t = threading.Thread(target=_loop, name=\"prewarm-60s\", daemon=True)\n",
    "    t.start()\n",
    "    return t\n",
    "_ = start_minute_prewarm()\n",
    "print(\"Minute pre-warm thread running (gateway/rag). Circuit breaker state checked via CB.open().\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 388,
   "id": "fae94517-5537-4192-be0f-9673a6b8335e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, json, time, math, threading, statistics, shutil\n",
    "from pathlib import Path\n",
    "from typing import Any, Dict, Optional, List, Tuple\n",
    "_PG = {\"dsn\": os.getenv(\"PG_DSN\"), \"conn\": None, \"lib\": None}\n",
    "try:\n",
    "    import psycopg  \n",
    "    _PG[\"lib\"] = \"psycopg3\"\n",
    "except Exception:\n",
    "    try:\n",
    "        import psycopg2  \n",
    "        _PG[\"lib\"] = \"psycopg2\"\n",
    "    except Exception:\n",
    "        _PG[\"lib\"] = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 389,
   "id": "1d87d8fc-ea16-4b62-8d35-ef26aba60720",
   "metadata": {},
   "outputs": [],
   "source": [
    "_PG_DDL = \"\"\"\n",
    "-- Core tables (idempotent)\n",
    "CREATE TABLE IF NOT EXISTS jobs (\n",
    "  job_id BIGSERIAL PRIMARY KEY,\n",
    "  ns TEXT NOT NULL DEFAULT 'default',\n",
    "  route TEXT NOT NULL,\n",
    "  status TEXT NOT NULL,\n",
    "  payload JSONB DEFAULT '{}'::jsonb,\n",
    "  created_at TIMESTAMPTZ NOT NULL DEFAULT now(),\n",
    "  updated_at TIMESTAMPTZ NOT NULL DEFAULT now()\n",
    ");\n",
    "CREATE TABLE IF NOT EXISTS artifacts (\n",
    "  artifact_id BIGSERIAL PRIMARY KEY,\n",
    "  ns TEXT NOT NULL DEFAULT 'default',\n",
    "  art_type TEXT NOT NULL,\n",
    "  uri TEXT NOT NULL,\n",
    "  version TEXT,\n",
    "  meta JSONB DEFAULT '{}'::jsonb,\n",
    "  created_at TIMESTAMPTZ NOT NULL DEFAULT now()\n",
    ");\n",
    "CREATE TABLE IF NOT EXISTS metrics_rollup (\n",
    "  day DATE NOT NULL,\n",
    "  ns TEXT NOT NULL,\n",
    "  p50_ms NUMERIC,\n",
    "  p95_ms NUMERIC,\n",
    "  p99_ms NUMERIC,\n",
    "  error_rate NUMERIC,\n",
    "  faithfulness_mean NUMERIC,\n",
    "  ndcg10 NUMERIC,\n",
    "  cost_per_req NUMERIC,\n",
    "  cache_hit_rate NUMERIC,\n",
    "  PRIMARY KEY (day, ns)\n",
    ");\n",
    "CREATE TABLE IF NOT EXISTS patch_audit (\n",
    "  patch_id TEXT PRIMARY KEY,\n",
    "  notes TEXT,\n",
    "  tests JSONB,\n",
    "  decision TEXT,\n",
    "  rationale TEXT,\n",
    "  canary_kpis JSONB,\n",
    "  created_at TIMESTAMPTZ NOT NULL DEFAULT now()\n",
    ");\n",
    "CREATE TABLE IF NOT EXISTS canary_releases (\n",
    "  release_id BIGSERIAL PRIMARY KEY,\n",
    "  ns TEXT NOT NULL,\n",
    "  percent NUMERIC NOT NULL,\n",
    "  status TEXT NOT NULL,               -- scheduled|active|rolled_back|promoted|closed\n",
    "  promoter TEXT,\n",
    "  window JSONB,\n",
    "  start_ts TIMESTAMPTZ NOT NULL DEFAULT now(),\n",
    "  end_ts TIMESTAMPTZ\n",
    ");\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 390,
   "id": "85eaf145-c626-4a84-829e-8bbbb7077c2e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[pg] skipped init (psycopg not installed or PG_DSN not set)\n"
     ]
    }
   ],
   "source": [
    "def _pg_connect():\n",
    "    if not _PG[\"dsn\"] or not _PG[\"lib\"]:\n",
    "        return None\n",
    "    if _PG[\"conn\"] is not None:\n",
    "        return _PG[\"conn\"]\n",
    "    try:\n",
    "        if _PG[\"lib\"] == \"psycopg3\":\n",
    "            _PG[\"conn\"] = psycopg.connect(_PG[\"dsn\"])\n",
    "        else:\n",
    "            _PG[\"conn\"] = psycopg2.connect(_PG[\"dsn\"])\n",
    "        _PG[\"conn\"].autocommit = True\n",
    "    except Exception as e:\n",
    "        print(f\"[pg] connect failed: {e}\")\n",
    "        _PG[\"conn\"] = None\n",
    "    return _PG[\"conn\"]\n",
    "def pg_init():\n",
    "    conn = _pg_connect()\n",
    "    if not conn:\n",
    "        print(\"[pg] skipped init (psycopg not installed or PG_DSN not set)\")\n",
    "        return False\n",
    "    try:\n",
    "        cur = conn.cursor()\n",
    "        cur.execute(_PG_DDL)\n",
    "        cur.close()\n",
    "        print(\"[pg] schema ensured (jobs, artifacts, metrics_rollup, patch_audit, canary_releases)\")\n",
    "        return True\n",
    "    except Exception as e:\n",
    "        print(\"[pg] ddl error:\", e)\n",
    "        return False\n",
    "def pg_insert_artifact(ns: str, art_type: str, uri: str, version: Optional[str], meta: Dict[str,Any]):\n",
    "    conn = _pg_connect()\n",
    "    if not conn: return False\n",
    "    try:\n",
    "        cur = conn.cursor()\n",
    "        cur.execute(\n",
    "            \"INSERT INTO artifacts(ns, art_type, uri, version, meta) VALUES (%s,%s,%s,%s,%s)\",\n",
    "            (ns, art_type, uri, version, json.dumps(meta))\n",
    "        )\n",
    "        cur.close(); return True\n",
    "    except Exception as e:\n",
    "        print(\"[pg] insert artifacts:\", e); return False\n",
    "def pg_record_canary(ns: str, percent: float, status: str, promoter: Optional[str], window: Dict[str,Any]):\n",
    "    conn = _pg_connect()\n",
    "    if not conn: return False\n",
    "    try:\n",
    "        cur = conn.cursor()\n",
    "        cur.execute(\n",
    "            \"INSERT INTO canary_releases(ns, percent, status, promoter, window) VALUES (%s,%s,%s,%s,%s)\",\n",
    "            (ns, percent, status, promoter, json.dumps(window))\n",
    "        )\n",
    "        cur.close(); return True\n",
    "    except Exception as e:\n",
    "        print(\"[pg] insert canary:\", e); return False\n",
    "def pg_upsert_rollup(day: str, ns: str, row: Dict[str,Any]):\n",
    "    conn = _pg_connect()\n",
    "    if not conn: return False\n",
    "    try:\n",
    "        cur = conn.cursor()\n",
    "        cur.execute(\"\"\"\n",
    "        INSERT INTO metrics_rollup(day, ns, p50_ms, p95_ms, p99_ms, error_rate, faithfulness_mean, ndcg10, cost_per_req, cache_hit_rate)\n",
    "        VALUES (%s,%s,%s,%s,%s,%s,%s,%s,%s,%s)\n",
    "        ON CONFLICT (day, ns) DO UPDATE SET\n",
    "          p50_ms=EXCLUDED.p50_ms, p95_ms=EXCLUDED.p95_ms, p99_ms=EXCLUDED.p99_ms,\n",
    "          error_rate=EXCLUDED.error_rate, faithfulness_mean=EXCLUDED.faithfulness_mean,\n",
    "          ndcg10=EXCLUDED.ndcg10, cost_per_req=EXCLUDED.cost_per_req, cache_hit_rate=EXCLUDED.cache_hit_rate\n",
    "        \"\"\", (day, ns, row.get(\"p50_ms\"), row.get(\"p95_ms\"), row.get(\"p99_ms\"),\n",
    "              row.get(\"error_rate\"), row.get(\"faithfulness_mean\"), row.get(\"ndcg10\"),\n",
    "              row.get(\"cost_per_req\"), row.get(\"cache_hit_rate\")))\n",
    "        cur.close(); return True\n",
    "    except Exception as e:\n",
    "        print(\"[pg] upsert rollup:\", e); return False\n",
    "_ = pg_init()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 391,
   "id": "34d49d96-f7c6-4d29-a366-808e43b8bc23",
   "metadata": {},
   "outputs": [],
   "source": [
    "_S3 = {\"ok\": False, \"client\": None, \"bucket\": os.getenv(\"S3_BUCKET\", \"\"), \"endpoint\": os.getenv(\"S3_ENDPOINT\")}\n",
    "try:\n",
    "    import boto3\n",
    "    _S3[\"client\"] = boto3.client(\n",
    "        \"s3\",\n",
    "        endpoint_url=_S3[\"endpoint\"] or None,\n",
    "        aws_access_key_id=os.getenv(\"S3_ACCESS_KEY\"),\n",
    "        aws_secret_access_key=os.getenv(\"S3_SECRET_KEY\"),\n",
    "        region_name=os.getenv(\"AWS_REGION\") or \"us-east-1\",\n",
    "    )\n",
    "    _S3[\"ok\"] = bool(_S3[\"bucket\"])\n",
    "except Exception:\n",
    "    _S3[\"ok\"] = False\n",
    "def s3_put(local_path: Path, prefix: str) -> str:\n",
    "    \"\"\"\n",
    "    Upload local file to S3 with given prefix (e.g., 'indices/hot/').\n",
    "    Returns a URI like s3://bucket/prefix/filename or file:// fallback.\n",
    "    \"\"\"\n",
    "    p = Path(local_path)\n",
    "    if _S3[\"ok\"] and p.exists():\n",
    "        key = f\"{prefix.rstrip('/')}/{p.name}\"\n",
    "        try:\n",
    "            _S3[\"client\"].upload_file(str(p), _S3[\"bucket\"], key)\n",
    "            return f\"s3://{_S3['bucket']}/{key}\"\n",
    "        except Exception as e:\n",
    "            print(\"[s3] upload failed, falling back:\", e)\n",
    "    return f\"file://{p.resolve()}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 392,
   "id": "253e11ae-e28f-4e17-b016-73416c3f1d8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RouteQuota:\n",
    "    def __init__(self, default_per_minute: int = 120):\n",
    "        self.default = int(default_per_minute)\n",
    "        self.limits: Dict[Tuple[str,str], int] = {}  \n",
    "        self.window: Dict[Tuple[str,str], Tuple[int,int]] = {}  \n",
    "        self._lock = threading.Lock()\n",
    "    def set(self, ns: str, route: str, per_minute: int):\n",
    "        with self._lock:\n",
    "            self.limits[(ns, route)] = max(1, int(per_minute))\n",
    "    def get(self, ns: str, route: str) -> int:\n",
    "        with self._lock:\n",
    "            return self.limits.get((ns, route), self.default)\n",
    "    def allow(self, ns: str, route: str) -> bool:\n",
    "        now = int(time.time()); win = now - (now % 60)\n",
    "        key = (ns, route)\n",
    "        with self._lock:\n",
    "            start, cnt = self.window.get(key, (win, 0))\n",
    "            if start != win:\n",
    "                start, cnt = win, 0\n",
    "            limit = self.get(ns, route)\n",
    "            if cnt >= limit:\n",
    "                self.window[key] = (start, cnt)\n",
    "                return False\n",
    "            cnt += 1\n",
    "            self.window[key] = (start, cnt)\n",
    "            return True\n",
    "    def snapshot(self):\n",
    "        with self._lock:\n",
    "            return {\n",
    "                \"default_per_minute\": self.default,\n",
    "                \"limits\": {f\"{k[0]}::{k[1]}\": v for k,v in self.limits.items()},\n",
    "                \"windows\": {f\"{k[0]}::{k[1]}\": {\"window_start\": v[0], \"count\": v[1]} for k,v in self.window.items()},\n",
    "            }\n",
    "ROUTE_QUOTAS = RouteQuota(default_per_minute=int(os.getenv(\"ROUTE_QPM_DEFAULT\", \"120\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 393,
   "id": "1ae7d9be-eea6-4635-a519-2bc2b9d20ff6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gateway admin extension skipped: Invalid args for response field! Hint: check that <class 'requests.models.Request'> is a valid Pydantic field type. If you are using a return type annotation that is not a valid Pydantic field (e.g. Union[Response, dict, None]) you can disable generating the response model from the type annotation with the path operation decorator parameter response_model=None. Read more: https://fastapi.tiangolo.com/tutorial/response-model/\n"
     ]
    }
   ],
   "source": [
    "if \"gateway\" in RUNNING:\n",
    "    try:\n",
    "        from fastapi import FastAPI\n",
    "        import requests as _rq  \n",
    "        old_srv = RUNNING[\"gateway\"]\n",
    "        rag_base = RUNNING[\"rag\"].base if \"rag\" in RUNNING else \"http://127.0.0.1:8001\"\n",
    "        old_srv.stop(); RUNNING.pop(\"gateway\", None)\n",
    "        def extend_gateway_with_admin(base: str) -> FastAPI:\n",
    "            app = build_gateway_app_v2(base)  \n",
    "            @app.get(\"/admin/route-quotas\")\n",
    "            def route_quotas_get():\n",
    "                return {\"ok\": True, \"route_quotas\": ROUTE_QUOTAS.snapshot()}\n",
    "            @app.post(\"/admin/route-quotas/set\")\n",
    "            def route_quotas_set(payload: Dict[str,Any]):\n",
    "                ns = (payload.get(\"namespace\") or \"default\").strip().lower()\n",
    "                route = (payload.get(\"route\") or \"answer\").strip().lower()\n",
    "                per_min = int(payload.get(\"per_minute\", ROUTE_QUOTAS.default))\n",
    "                ROUTE_QUOTAS.set(ns, route, per_min)\n",
    "                return {\"ok\": True, \"namespace\": ns, \"route\": route, \"per_minute\": per_min}\n",
    "            orig_answer = app.routes[[r.path for r in app.routes].index(\"/answer\")].endpoint\n",
    "            @app.post(\"/answer\")  \n",
    "            def answer_wrapped(req: _rq.Request=None, body: Dict[str,Any]=None):  \n",
    "                raise RuntimeError(\"shadow route placeholder\")  \n",
    "            app.routes.pop()  \n",
    "            from fastapi import Request as _FRequest, HTTPException as _FHTTP\n",
    "            @app.post(\"/answer\")\n",
    "            def _answer_proxy(request: _FRequest, body: Dict[str,Any]):\n",
    "                ns = (request.headers.get(\"X-Namespace\") or request.query_params.get(\"ns\") or \"default\").strip().lower()\n",
    "                if not ROUTE_QUOTAS.allow(ns, \"answer\"):\n",
    "                    raise _FHTTP(429, \"route_quota_exceeded\")\n",
    "                return app.dependency_overrides.get(orig_answer, orig_answer)(request, body) \n",
    "            return app\n",
    "        srv = ServerThread(extend_gateway_with_admin(rag_base), preferred_port=None, name=\"gw\", log_level=\"warning\")\n",
    "        srv.start(); RUNNING[\"gateway\"] = srv\n",
    "        print(\" Gateway reloaded with /admin/route-quotas endpoints at\", srv.base)\n",
    "    except Exception as e:\n",
    "        print(\"gateway admin extension skipped:\", e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 394,
   "id": "7e74d357-ef47-4e88-b6ff-1da0e2ac7a1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _histogram(tokens: List[str], topk: int = 50) -> Dict[str,float]:\n",
    "    from collections import Counter\n",
    "    c = Counter(tokens)\n",
    "    total = sum(c.values()) or 1\n",
    "    most = c.most_common(topk)\n",
    "    keep = dict(most)\n",
    "    other = total - sum(keep.values())\n",
    "    h = {k: v/total for k,v in keep.items()}\n",
    "    if other > 0:\n",
    "        h[\"__other__\"] = other/total\n",
    "    return h\n",
    "def psi(p_base: Dict[str,float], p_curr: Dict[str,float], eps: float = 1e-6) -> float:\n",
    "    keys = set(p_base) | set(p_curr)\n",
    "    s = 0.0\n",
    "    for k in keys:\n",
    "        e = max(eps, p_base.get(k, 0.0))\n",
    "        a = max(eps, p_curr.get(k, 0.0))\n",
    "        s += (a - e) * math.log(a / e)\n",
    "    return s\n",
    "def drift_psi_from_audit(day: Optional[str]=None, ns: str=\"default\", baseline_days: int=3) -> Optional[float]:\n",
    "    \"\"\"\n",
    "    Compute PSI for query token distribution against last <baseline_days> days as baseline.\n",
    "    \"\"\"\n",
    "    day = day or time.strftime(\"%Y%m%d\")\n",
    "    def _load_queries(d: str) -> List[str]:\n",
    "        p = AUDIT_DIR / f\"answer_{d}.ndjson\"\n",
    "        qs = []\n",
    "        if p.exists():\n",
    "            with p.open() as f:\n",
    "                for ln in f:\n",
    "                    try:\n",
    "                        obj = json.loads(ln)\n",
    "                        if (obj.get(\"namespace\") or ns) == ns and \"query\" in obj:\n",
    "                            qs.append(obj[\"query\"])\n",
    "                    except Exception:\n",
    "                        pass\n",
    "        return qs\n",
    "    base_tokens: List[str] = []\n",
    "    for i in range(1, baseline_days+1):\n",
    "        t = time.time() - 86400*i\n",
    "        base_tokens += \" \".join(_load_queries(time.strftime(\"%Y%m%d\", time.localtime(t)))).lower().split()\n",
    "    curr_tokens = \" \".join(_load_queries(day)).lower().split()\n",
    "    if not base_tokens or not curr_tokens:\n",
    "        return None\n",
    "    hb = _histogram(base_tokens); hc = _histogram(curr_tokens)\n",
    "    return psi(hb, hc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 395,
   "id": "b5e38fc0-be93-4d85-8bb2-8d36c34bafca",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _percentile(xs: List[float], p: float) -> float:\n",
    "    if not xs: return float(\"nan\")\n",
    "    xs2 = sorted(xs)\n",
    "    k = (len(xs2) - 1) * (p / 100.0)\n",
    "    f = math.floor(k); c = math.ceil(k)\n",
    "    if f == c: return xs2[int(k)]\n",
    "    d0 = xs2[f] * (c - k)\n",
    "    d1 = xs2[c] * (k - f)\n",
    "    return d0 + d1\n",
    "def build_rollup_from_audit(day: Optional[str]=None, ns: str=\"default\") -> Dict[str,Any]:\n",
    "    day = day or time.strftime(\"%Y%m%d\")\n",
    "    p = AUDIT_DIR / f\"answer_{day}.ndjson\"\n",
    "    lats: List[float] = []\n",
    "    faiths: List[float] = []\n",
    "    cache_hits = 0\n",
    "    total = 0\n",
    "    if p.exists():\n",
    "        with p.open() as f:\n",
    "            for ln in f:\n",
    "                try:\n",
    "                    obj = json.loads(ln)\n",
    "                    if obj.get(\"namespace\", \"default\") != ns: \n",
    "                        continue\n",
    "                    total += 1\n",
    "                    lats.append(float(obj.get(\"latency_ms\", 0)))\n",
    "                    if \"verify\" in obj:\n",
    "                        pass\n",
    "                    if obj.get(\"cache\") == \"hit\":\n",
    "                        cache_hits += 1\n",
    "                except Exception:\n",
    "                    pass\n",
    "    roll = {\n",
    "        \"day\": f\"{day[:4]}-{day[4:6]}-{day[6:]}\",\n",
    "        \"ns\": ns,\n",
    "        \"p50_ms\": _percentile(lats, 50) if lats else None,\n",
    "        \"p95_ms\": _percentile(lats, 95) if lats else None,\n",
    "        \"p99_ms\": _percentile(lats, 99) if lats else None,\n",
    "        \"error_rate\": 0.0,                    \n",
    "        \"faithfulness_mean\": None,            \n",
    "        \"ndcg10\": None,                     \n",
    "        \"cost_per_req\": None,                 \n",
    "        \"cache_hit_rate\": (cache_hits / total) if total else None,\n",
    "        \"psi\": drift_psi_from_audit(day, ns)\n",
    "    }\n",
    "    pg_upsert_rollup(roll[\"day\"], ns, roll)\n",
    "    return roll\n",
    "def schedule_daily_rollups(ns_list: List[str]=[\"default\"], when_hhmm: str=\"00:05\"):\n",
    "    hh,mm = map(int, when_hhmm.split(\":\"))\n",
    "    def _loop():\n",
    "        while True:\n",
    "            now = time.localtime()\n",
    "            target = time.mktime((now.tm_year, now.tm_mon, now.tm_mday, hh, mm, 0, now.tm_wday, now.tm_yday, now.tm_isdst))\n",
    "            if target <= time.mktime(now):\n",
    "                target += 86400\n",
    "            time.sleep(max(0, target - time.mktime(now)))\n",
    "            day = time.strftime(\"%Y%m%d\")\n",
    "            for ns in ns_list:\n",
    "                try:\n",
    "                    r = build_rollup_from_audit(day, ns)\n",
    "                    (ART / \"reports\" / f\"rollup_{ns}_{day}.json\").parent.mkdir(parents=True, exist_ok=True)\n",
    "                    (ART / \"reports\" / f\"rollup_{ns}_{day}.json\").write_text(json.dumps(r, indent=2), encoding=\"utf-8\")\n",
    "                except Exception as e:\n",
    "                    print(\"[rollup] error:\", e)\n",
    "    t = threading.Thread(target=_loop, name=\"daily-rollups\", daemon=True)\n",
    "    t.start()\n",
    "    return t\n",
    "_ = schedule_daily_rollups()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 396,
   "id": "2c8fe8ec-0d35-4781-9b0b-74fa8cf05f38",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_router_bundle_local(path: str) -> str:\n",
    "    p = Path(path)\n",
    "    if not p.exists():\n",
    "        raise FileNotFoundError(path)\n",
    "    dest_dir = ART / \"artifacts\"\n",
    "    dest_dir.mkdir(parents=True, exist_ok=True)\n",
    "    dest = dest_dir / p.name\n",
    "    shutil.copy2(p, dest)\n",
    "    uri = s3_put(dest, \"artifacts/\")\n",
    "    REG.register_router(uri, version=os.getenv(\"ROUTER_VERSION\"))\n",
    "    pg_insert_artifact(\"default\", \"router_bundle\", uri, os.getenv(\"ROUTER_VERSION\"), {\"source\": \"local\"})\n",
    "    return uri\n",
    "def register_faiss_uri(namespace: str, uri: str, tier: str=\"hot\"):\n",
    "    rec = REG.register_faiss(namespace, uri, tier)\n",
    "    pg_insert_artifact(namespace, f\"faiss_{tier}\", uri, None, rec)\n",
    "    return rec\n",
    "def register_bm25_version(namespace: str, version: str, uri: Optional[str]=None):\n",
    "    rec = REG.register_bm25(namespace, version, uri)\n",
    "    if uri:\n",
    "        pg_insert_artifact(namespace, \"bm25\", uri, version, rec)\n",
    "    return rec\n",
    "def record_canary_active(ns: str, percent: float, promoter: str=\"auto\", window: Optional[Dict[str,Any]]=None):\n",
    "    window = window or {}\n",
    "    pg_record_canary(ns, percent, \"active\", promoter, window)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 397,
   "id": "fcdb4df1-528f-40d0-b57d-dde576aebbd6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Services/storage continuation loaded: PG(optional), S3(optional), route quotas, rollups, PSI, GC.\n",
      "Quick tips:\n",
      "  - POST /admin/route-quotas/set  {\"namespace\":\"default\",\"route\":\"answer\",\"per_minute\":60}\n",
      "  - Python: build_rollup_from_audit(); drift_psi_from_audit()\n",
      "  - Save router bundle: save_router_bundle_local('artifacts/router_v1.joblib')  # then check /artifacts/snapshot\n"
     ]
    }
   ],
   "source": [
    "ART_TTL_DAYS = int(os.getenv(\"ART_TTL_DAYS\", \"14\"))\n",
    "def _too_old(p: Path, days: int) -> bool:\n",
    "    try:\n",
    "        return (time.time() - p.stat().st_mtime) > days*86400\n",
    "    except Exception:\n",
    "        return False\n",
    "def start_ttl_gc():\n",
    "    def _loop():\n",
    "        while True:\n",
    "            try:\n",
    "                removed = []\n",
    "                for root in [ART, ART / \"artifacts\", ART / \"local\"]:\n",
    "                    if not root.exists(): continue\n",
    "                    for p in root.rglob(\"*\"):\n",
    "                        if p.is_file() and _too_old(p, ART_TTL_DAYS):\n",
    "                            try:\n",
    "                                p.unlink()\n",
    "                                removed.append(str(p))\n",
    "                            except Exception:\n",
    "                                pass\n",
    "                if removed:\n",
    "                    rpt = {\"ts\": int(time.time()), \"removed\": removed[:200]}\n",
    "                    (ART / \"reports\" / \"gc_last.json\").parent.mkdir(parents=True, exist_ok=True)\n",
    "                    (ART / \"reports\" / \"gc_last.json\").write_text(json.dumps(rpt, indent=2), encoding=\"utf-8\")\n",
    "                time.sleep(86400)\n",
    "            except Exception:\n",
    "                time.sleep(3600)\n",
    "    t = threading.Thread(target=_loop, name=\"ttl-gc\", daemon=True)\n",
    "    t.start()\n",
    "    return t\n",
    "_ = start_ttl_gc()\n",
    "print(\"Services/storage continuation loaded: PG(optional), S3(optional), route quotas, rollups, PSI, GC.\")\n",
    "print(\"Quick tips:\")\n",
    "print(\"  - POST /admin/route-quotas/set  {\\\"namespace\\\":\\\"default\\\",\\\"route\\\":\\\"answer\\\",\\\"per_minute\\\":60}\")\n",
    "print(\"  - Python: build_rollup_from_audit(); drift_psi_from_audit()\")\n",
    "print(\"  - Save router bundle: save_router_bundle_local('artifacts/router_v1.joblib')  # then check /artifacts/snapshot\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 398,
   "id": "f9f5c505-ef4e-463e-b35e-6021fa74134b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, time, json, threading, math, re\n",
    "from pathlib import Path\n",
    "from typing import Any, Dict, Optional, Tuple, List\n",
    "from fastapi import FastAPI, Request, HTTPException\n",
    "from fastapi.responses import JSONResponse, PlainTextResponse\n",
    "COST_LEDGER = ART / \"reports\" / \"cost_ledger.ndjson\"\n",
    "COST_LEDGER.parent.mkdir(parents=True, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 399,
   "id": "9ae98cb4-46a9-46ec-9cff-8a5c2eb07ea6",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CostCaps:\n",
    "    \"\"\"\n",
    "    Tracks per-namespace, per-route daily budgets & spend, enforces caps.\n",
    "    \"\"\"\n",
    "    def __init__(self, default_daily_usd: float = float(os.getenv(\"BUDGET_DEFAULT_USD\", \"5.0\"))):\n",
    "        self.default = float(default_daily_usd)\n",
    "        self._state: Dict[Tuple[str, str], Dict[str, float | str]] = {}\n",
    "        self._lock = threading.Lock()\n",
    "    def _key(self, ns: str, route: str) -> Tuple[str, str]:\n",
    "        return (ns.strip().lower() or \"default\", route.strip().lower())\n",
    "    def _today(self) -> str:\n",
    "        return time.strftime(\"%Y%m%d\")\n",
    "    def set_budget(self, ns: str, route: str, daily_usd: float):\n",
    "        k = self._key(ns, route)\n",
    "        with self._lock:\n",
    "            st = self._state.get(k, {\"day\": self._today(), \"budget\": self.default, \"spent\": 0.0})\n",
    "            st[\"budget\"] = float(daily_usd)\n",
    "            if st[\"day\"] != self._today():\n",
    "                st[\"day\"] = self._today()\n",
    "                st[\"spent\"] = 0.0\n",
    "            self._state[k] = st\n",
    "    def snapshot(self) -> Dict[str, Any]:\n",
    "        with self._lock:\n",
    "            out = {}\n",
    "            for (ns, route), st in self._state.items():\n",
    "                out.setdefault(ns, {})[route] = dict(st)  \n",
    "            return {\"default_daily_usd\": self.default, \"by_ns_route\": out}\n",
    "    def reset_day(self, ns: str, route: str):\n",
    "        k = self._key(ns, route)\n",
    "        with self._lock:\n",
    "            st = self._state.get(k, {\"day\": self._today(), \"budget\": self.default, \"spent\": 0.0})\n",
    "            st[\"day\"] = self._today()\n",
    "            st[\"spent\"] = 0.0\n",
    "            self._state[k] = st\n",
    "    def allow_charge(self, ns: str, route: str, cost_usd: float, commit: bool = True) -> bool:\n",
    "        \"\"\"\n",
    "        Returns True if charge can proceed; if commit, increments spent.\n",
    "        Rotates day window automatically.\n",
    "        \"\"\"\n",
    "        if cost_usd <= 0:\n",
    "            return True\n",
    "        k = self._key(ns, route)\n",
    "        with self._lock:\n",
    "            st = self._state.get(k)\n",
    "            if not st:\n",
    "                st = {\"day\": self._today(), \"budget\": self.default, \"spent\": 0.0}\n",
    "                self._state[k] = st\n",
    "            if st[\"day\"] != self._today():\n",
    "                st[\"day\"] = self._today()\n",
    "                st[\"spent\"] = 0.0\n",
    "            budget = float(st[\"budget\"])\n",
    "            spent = float(st[\"spent\"])\n",
    "            if spent + cost_usd > budget:\n",
    "                return False\n",
    "            if commit:\n",
    "                st[\"spent\"] = spent + cost_usd\n",
    "                try:\n",
    "                    with COST_LEDGER.open(\"a\", encoding=\"utf-8\") as f:\n",
    "                        f.write(json.dumps({\n",
    "                            \"ts\": int(time.time()),\n",
    "                            \"ns\": k[0],\n",
    "                            \"route\": k[1],\n",
    "                            \"cost_usd\": round(cost_usd, 8),\n",
    "                            \"day\": st[\"day\"],\n",
    "                            \"budget\": budget,\n",
    "                            \"spent_after\": st[\"spent\"],\n",
    "                        }) + \"\\n\")\n",
    "                except Exception:\n",
    "                    pass\n",
    "            return True\n",
    "COSTS = CostCaps()\n",
    "PRICE_RERANK_PER_DOC = float(os.getenv(\"PRICE_RERANK_PER_DOC\", \"0.00001\"))   \n",
    "PRICE_GENERATE_PER_100 = float(os.getenv(\"PRICE_GENERATE_PER_100\", \"0.002\")) \n",
    "PRICE_FIX_PER_CALL = float(os.getenv(\"PRICE_FIX_PER_CALL\", \"0.0005\"))        \n",
    "def _estimate_generate_cost(max_tokens: int = 128) -> float:\n",
    "    blocks = max(1, int(math.ceil(max_tokens / 100)))\n",
    "    return blocks * PRICE_GENERATE_PER_100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 400,
   "id": "cbf12441-e841-4a19-85f9-fba305c4012b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _post_json(url: str, payload: Dict[str, Any], timeout: int = 5):\n",
    "    import requests as _r\n",
    "    try:\n",
    "        _r.post(url, json=payload, timeout=timeout)\n",
    "    except Exception:\n",
    "        pass\n",
    "def start_slo_alert_watcher(\n",
    "    namespace: str = \"default\",\n",
    "    check_every_s: int = 90,\n",
    "    slo_ms: float = float(os.getenv(\"SLO_BUDGET_MS\", \"1800\")),\n",
    "    min_requests: int = int(os.getenv(\"SLO_MIN_REQS\", \"100\")),\n",
    "):\n",
    "    \"\"\"\n",
    "    Periodically inspects latest rollup and alerts if SLOs are violated.\n",
    "    \"\"\"\n",
    "    webhook = os.getenv(\"SLO_ALERT_WEBHOOK\")  \n",
    "    def _loop():\n",
    "        last_alert_day = None\n",
    "        while True:\n",
    "            try:\n",
    "                roll = build_rollup_from_audit(ns=namespace)\n",
    "                if roll and webhook:\n",
    "                    p95 = (roll.get(\"p95_ms\") or 0)\n",
    "                    cache_hit = roll.get(\"cache_hit_rate\") or 0.0\n",
    "                    if p95 and p95 > slo_ms:\n",
    "                        _post_json(webhook, {\n",
    "                            \"type\": \"slo_violation\",\n",
    "                            \"ns\": namespace,\n",
    "                            \"p95_ms\": p95,\n",
    "                            \"budget_ms\": slo_ms,\n",
    "                            \"cache_hit_rate\": cache_hit,\n",
    "                            \"day\": roll.get(\"day\"),\n",
    "                        })\n",
    "                        last_alert_day = roll.get(\"day\")\n",
    "                time.sleep(check_every_s)\n",
    "            except Exception:\n",
    "                time.sleep(check_every_s)\n",
    "    t = threading.Thread(target=_loop, name=\"slo-alerts\", daemon=True)\n",
    "    t.start()\n",
    "    return t\n",
    "_ = start_slo_alert_watcher()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 401,
   "id": "4cfe7d20-5a24-4559-8ad9-e0df972b228d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extend_gateway_with_cost_caps(base: str) -> FastAPI:\n",
    "    \"\"\"\n",
    "    Wraps the already-featureful gateway with:\n",
    "      - cost budgets per namespace+route\n",
    "      - cost-aware rerank fallback to RRF\n",
    "      - optional Auto-Fix (HF /fix or /generate) when faithfulness is low, budget permitting\n",
    "      - admin endpoints to configure budgets and inspect spend\n",
    "    \"\"\"\n",
    "    app = build_gateway_app_v2(base)\n",
    "    @app.get(\"/admin/costs\")\n",
    "    def costs_get():\n",
    "        return {\"ok\": True, \"budgets\": COSTS.snapshot(), \"prices\": {\n",
    "            \"rerank_per_doc\": PRICE_RERANK_PER_DOC,\n",
    "            \"generate_per_100\": PRICE_GENERATE_PER_100,\n",
    "            \"fix_per_call\": PRICE_FIX_PER_CALL\n",
    "        }}\n",
    "    @app.post(\"/admin/costs/set\")\n",
    "    def costs_set(payload: Dict[str, Any]):\n",
    "        ns = (payload.get(\"namespace\") or \"default\").strip().lower()\n",
    "        route = (payload.get(\"route\") or \"answer\").strip().lower()\n",
    "        daily = float(payload.get(\"daily_usd\", COSTS.default))\n",
    "        COSTS.set_budget(ns, route, daily)\n",
    "        return {\"ok\": True, \"namespace\": ns, \"route\": route, \"daily_usd\": daily}\n",
    "    @app.post(\"/admin/costs/reset_day\")\n",
    "    def costs_reset(payload: Dict[str, Any]):\n",
    "        ns = (payload.get(\"namespace\") or \"default\").strip().lower()\n",
    "        route = (payload.get(\"route\") or \"answer\").strip().lower()\n",
    "        COSTS.reset_day(ns, route)\n",
    "        return {\"ok\": True, \"namespace\": ns, \"route\": route, \"reset\": True}\n",
    "    from fastapi import Request as _FRequest, HTTPException as _FHTTP\n",
    "    import requests as _rq\n",
    "    paths = [r.path for r in app.routes]\n",
    "    idx = paths.index(\"/answer\")\n",
    "    orig_answer = app.routes[idx].endpoint\n",
    "    app.routes.pop(idx)\n",
    "    @app.post(\"/answer\")\n",
    "    def answer_costed(request: _FRequest, body: Dict[str, Any]):\n",
    "        ns = (request.headers.get(\"X-Namespace\") or request.query_params.get(\"ns\") or \"default\").strip().lower()\n",
    "        query = body.get(\"query\", \"\")\n",
    "        if not isinstance(query, str) or not query.strip():\n",
    "            raise _FHTTP(400, \"query required\")\n",
    "        top_k = int(body.get(\"top_k\", 5))\n",
    "        use_rerank = body.get(\"use_rerank\")  \n",
    "        try:\n",
    "            d = _rq.post(f\"{base}/retrieve_dense\", json={\"query\": query, \"top_k\": top_k}, timeout=3).json()\n",
    "            b = _rq.post(f\"{base}/retrieve_bm25\", json={\"query\": query, \"top_k\": top_k}, timeout=3).json()\n",
    "            fused = rrf_fuse(d.get(\"results\", []), b.get(\"results\", []), k=top_k)\n",
    "        except Exception as e:\n",
    "            if _HAVE_PROM: GW_ERRORS.labels(ns, \"preflight\").inc()\n",
    "            raise _FHTTP(502, f\"preflight_failed: {e}\")\n",
    "        pipeline_for_budget = \"answer\"  \n",
    "        ce_cost = (len(fused) * PRICE_RERANK_PER_DOC) if (use_rerank is not False) else 0.0\n",
    "        ce_allowed = COSTS.allow_charge(ns, \"ce_rerank\", ce_cost, commit=False)  \n",
    "        force_rrf = (use_rerank is False) or (not ce_allowed)\n",
    "        body2 = dict(body)\n",
    "        if force_rrf:\n",
    "            body2[\"use_rerank\"] = False\n",
    "        resp: JSONResponse = orig_answer(request, body2)  \n",
    "        payload = json.loads(resp.body.decode(\"utf-8\")) if isinstance(resp.body, (bytes, bytearray)) else resp.body\n",
    "        pipeline = (payload.get(\"meta\") or {}).get(\"pipeline\")\n",
    "        if pipeline == \"rerank\" and use_rerank is not False:\n",
    "            if COSTS.allow_charge(ns, \"ce_rerank\", ce_cost, commit=True) and _HAVE_PROM:\n",
    "                try:\n",
    "                    GW_REQS.labels(ns, \"rerank_billed\", \"n/a\").inc()\n",
    "                except Exception:\n",
    "                    pass\n",
    "        elif not ce_allowed and pipeline != \"rerank\":\n",
    "            payload.setdefault(\"meta\", {})[\"budget_blocked_ce\"] = True\n",
    "        hf_base = RUNNING.get(\"hf\").base if RUNNING.get(\"hf\") else None\n",
    "        faith = ((payload.get(\"verify\") or {}).get(\"faithfulness\") or 1.0)\n",
    "        need_fix = faith < float(os.getenv(\"FAITH_MIN\", \"0.85\"))\n",
    "        want_escalate = bool(body.get(\"escalate\", False)) or need_fix\n",
    "        if hf_base and want_escalate:\n",
    "            fix_cost = PRICE_FIX_PER_CALL\n",
    "            if COSTS.allow_charge(ns, pipeline_for_budget, fix_cost, commit=True):\n",
    "                try:\n",
    "                    fx = _rq.post(f\"{hf_base}/fix\", json={\"text\": payload.get(\"answer\",\"\")}, timeout=3).json()\n",
    "                    if fx.get(\"ok\"):\n",
    "                        payload[\"answer_refined\"] = (fx.get(\"outputs\") or [\"\"])[0]\n",
    "                        payload.setdefault(\"meta\", {})[\"escalation\"] = {\"type\": \"fix\", \"charged\": fix_cost}\n",
    "                except Exception:\n",
    "                    gen_cost = _estimate_generate_cost(128)\n",
    "                    if COSTS.allow_charge(ns, pipeline_for_budget, gen_cost, commit=True):\n",
    "                        try:\n",
    "                            gx = _rq.post(f\"{hf_base}/generate\", json={\"prompt\": payload.get(\"answer\",\"\"), \"max_tokens\": 128}, timeout=4).json()\n",
    "                            if gx.get(\"ok\"):\n",
    "                                payload[\"answer_refined\"] = (gx.get(\"outputs\") or [\"\"])[0]\n",
    "                                payload.setdefault(\"meta\", {})[\"escalation\"] = {\"type\": \"generate\", \"charged\": gen_cost}\n",
    "                        except Exception:\n",
    "                            pass\n",
    "            else:\n",
    "                payload.setdefault(\"meta\", {})[\"budget_blocked_escalation\"] = True\n",
    "        return JSONResponse(payload)\n",
    "    return app"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 402,
   "id": "704ab50a-b273-4e14-9fe0-346c1c94c00f",
   "metadata": {},
   "outputs": [],
   "source": [
    "if \"gateway\" in RUNNING:\n",
    "    try:\n",
    "        old = RUNNING.pop(\"gateway\")\n",
    "        old.stop()\n",
    "    except Exception:\n",
    "        pass\n",
    "    rag_base = RUNNING[\"rag\"].base if \"rag\" in RUNNING else \"http://127.0.0.1:8001\"\n",
    "    gw_srv = ServerThread(extend_gateway_with_cost_caps(rag_base), preferred_port=None, name=\"gw\", log_level=\"warning\")\n",
    "    gw_srv.start()\n",
    "    RUNNING[\"gateway\"] = gw_srv\n",
    "    print(\" Gateway extended with cost caps & escalation at\", gw_srv.base)\n",
    "    print(\"Try budgets:\")\n",
    "    print(f\"  curl -s -X POST {gw_srv.base}/admin/costs/set -H 'Content-Type: application/json' -d '{{\\\"namespace\\\":\\\"default\\\",\\\"route\\\":\\\"answer\\\",\\\"daily_usd\\\":1.5}}' | jq .\")\n",
    "    print(f\"  curl -s -X POST {gw_srv.base}/answer -H 'X-Namespace: default' -H 'Content-Type: application/json' -d '{{\\\"query\\\":\\\"bm25 vs dense\\\",\\\"top_k\\\":5}}' | jq .\")\n",
    "SQL_AUDIT = ART / \"reports\" / \"sql_exec_audit.ndjson\"\n",
    "SQL_AUDIT.parent.mkdir(parents=True, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 403,
   "id": "d42fd888-8da5-4a03-a018-9479d4564d50",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " RAG microservice extended with /sql/execute (dry-run audit) at http://127.0.0.1:54386\n",
      "Continuation loaded: budgets, cost-aware /answer, optional Auto-Fix, SLO alerts, and SQL audit stub.\n",
      "Handy curls:\n",
      "  # dry-run SQL exec audit\n",
      "  curl -s -X POST http://127.0.0.1:54386/sql/execute -H 'Content-Type: application/json' -d '{\"text\":\"SELECT * FROM t\"}' | jq .\n"
     ]
    }
   ],
   "source": [
    "def _sql_redact_literals(sql: str) -> str:\n",
    "    sql2 = re.sub(r\"'[^']*'\", \"'***'\", sql)\n",
    "    sql2 = re.sub(r\"\\\"[^\\\"]*\\\"\", \"\\\"***\\\"\", sql2)\n",
    "    sql2 = _RE_EMAIL.sub(lambda m: f\"{m.group(1)}***@{m.group(3)}\", sql2)\n",
    "    return sql2\n",
    "if \"rag\" in RUNNING:\n",
    "    from fastapi import FastAPI as _FA\n",
    "    def _extend_rag_sql_exec(app: _FA) -> _FA:\n",
    "        @app.post(\"/sql/execute\")\n",
    "        def sql_execute(body: Dict[str, Any]):\n",
    "            \"\"\"\n",
    "            Dry-run only: validates with /sql/sanitize logic, stores an audit record, returns sanitized SQL.\n",
    "            \"\"\"\n",
    "            txt = (body.get(\"text\") or \"\").strip()\n",
    "            ns  = (body.get(\"namespace\") or \"default\").strip().lower()\n",
    "            try:\n",
    "                found = re.search(r\"(?is)\\bselect\\b.*?(?:;|$)\", txt or \"\")\n",
    "                if not found:\n",
    "                    return {\"ok\": False, \"error\": \"no_sql_detected\"}\n",
    "                s = re.sub(r\"/\\*.*?\\*/\", \"\", txt, flags=re.DOTALL)\n",
    "                s = re.sub(r\"(?m)^[ \\t]*--.*?$\", \"\", s)\n",
    "                s = s.strip()\n",
    "                parts = [p.strip() for p in s.split(\";\") if p.strip()]\n",
    "                if len(parts) != 1:\n",
    "                    return {\"ok\": False, \"error\": \"multiple statements not allowed\"}\n",
    "                if not re.match(r\"(?is)^\\s*select\\b\", parts[0]):\n",
    "                    return {\"ok\": False, \"error\": \"only SELECT statements allowed\"}\n",
    "                upper = re.sub(r\"'[^']*'|\\\"[^\\\"]*\\\"\", \"\", parts[0]).upper()\n",
    "                for bad in {\"DROP\",\"TRUNCATE\",\"DELETE\",\"UPDATE\",\"INSERT\",\"ALTER\",\"CREATE\",\"RENAME\",\"GRANT\",\"REVOKE\",\"MERGE\"}:\n",
    "                    if re.search(rf\"\\b{bad}\\b\", upper):\n",
    "                        return {\"ok\": False, \"error\": f\"disallowed keyword: {bad}\"}\n",
    "                if not re.search(r\"(?is)\\blimit\\s+\\d+\\b\", parts[0]):\n",
    "                    parts[0] = parts[0].rstrip() + \" LIMIT 100\"\n",
    "                sanitized = parts[0]\n",
    "                rec = {\n",
    "                    \"ts\": int(time.time()),\n",
    "                    \"namespace\": ns,\n",
    "                    \"sql\": _sql_redact_literals(sanitized),\n",
    "                    \"raw_hash\": hashlib.sha256(s.encode(\"utf-8\")).hexdigest()[:16],\n",
    "                }\n",
    "                with SQL_AUDIT.open(\"a\", encoding=\"utf-8\") as f:\n",
    "                    f.write(json.dumps(rec) + \"\\n\")\n",
    "                return {\"ok\": True, \"sql\": sanitized, \"dry_run\": True}\n",
    "            except Exception as e:\n",
    "                return {\"ok\": False, \"error\": str(e)}\n",
    "        return app\n",
    "    try:\n",
    "        old_rag = RUNNING.pop(\"rag\")\n",
    "        old_rag.stop()\n",
    "        rag_srv = ServerThread(_extend_rag_sql_exec(build_rag_app()), preferred_port=None, name=\"rag\", log_level=\"warning\")\n",
    "        rag_srv.start()\n",
    "        RUNNING[\"rag\"] = rag_srv\n",
    "        print(\" RAG microservice extended with /sql/execute (dry-run audit) at\", rag_srv.base)\n",
    "    except Exception as e:\n",
    "        print(\"RAG extension skipped:\", e)\n",
    "print(\"Continuation loaded: budgets, cost-aware /answer, optional Auto-Fix, SLO alerts, and SQL audit stub.\")\n",
    "print(\"Handy curls:\")\n",
    "if \"gateway\" in RUNNING:\n",
    "    gwb = RUNNING[\"gateway\"].base\n",
    "    print(f\"  # set cost budget for /answer\")\n",
    "    print(f\"  curl -s -X POST {gwb}/admin/costs/set -H 'Content-Type: application/json' -d '{{\\\"namespace\\\":\\\"default\\\",\\\"route\\\":\\\"answer\\\",\\\"daily_usd\\\":0.25}}' | jq .\")\n",
    "    print(f\"  # answer with implicit canary + cost-aware rerank\")\n",
    "    print(f\"  curl -s -X POST {gwb}/answer -H 'Content-Type: application/json' -d '{{\\\"query\\\":\\\"bm25 vs dense\\\",\\\"top_k\\\":6}}' | jq .\")\n",
    "if \"rag\" in RUNNING:\n",
    "    rb = RUNNING[\"rag\"].base\n",
    "    print(f\"  # dry-run SQL exec audit\")\n",
    "    print(f\"  curl -s -X POST {rb}/sql/execute -H 'Content-Type: application/json' -d '{{\\\"text\\\":\\\"SELECT * FROM t\\\"}}' | jq .\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 404,
   "id": "86e5c2d5-b2ae-4f61-bfa7-16dc0303b6c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, re, json, time, math, random, threading, statistics\n",
    "from pathlib import Path\n",
    "from typing import Any, Dict, List, Optional, Tuple\n",
    "from collections import defaultdict, Counter\n",
    "from fastapi import FastAPI, HTTPException\n",
    "from fastapi.responses import PlainTextResponse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 405,
   "id": "e2b5b7b4-5055-4c40-8609-a986e605bf2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _infer_type(v: Any) -> str:\n",
    "    if v is None: return \"null\"\n",
    "    if isinstance(v, bool): return \"bool\"\n",
    "    if isinstance(v, int): return \"int\"\n",
    "    if isinstance(v, float): return \"float\"\n",
    "    if isinstance(v, str): return \"string\"\n",
    "    if isinstance(v, list): return \"array\"\n",
    "    if isinstance(v, dict): return \"object\"\n",
    "    return \"unknown\"\n",
    "_RE_EMAIL = re.compile(r\"\\b[A-Za-z0-9._%+-]+@[A-Za-z0-9.-]+\\.[A-Za-z]{2,}\\b\")\n",
    "_RE_SSN   = re.compile(r\"\\b\\d{3}-?\\d{2}-?\\d{4}\\b\")\n",
    "_RE_PHONE = re.compile(r\"\\b(?:\\+?\\d{1,3}[-.\\s]?)?(?:\\(?\\d{3}\\)?[-.\\s]?)?\\d{3}[-.\\s]?\\d{4}\\b\")\n",
    "_RE_CCARD = re.compile(r\"\\b(?:\\d[ -]*?){13,16}\\b\")\n",
    "def _pii_flags(s: str) -> Dict[str, bool]:\n",
    "    s = s or \"\"\n",
    "    return {\n",
    "        \"email\": bool(_RE_EMAIL.search(s)),\n",
    "        \"ssn\": bool(_RE_SSN.search(s)),\n",
    "        \"phone\": bool(_RE_PHONE.search(s)),\n",
    "        \"ccard\": bool(_RE_CCARD.search(s)),\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 406,
   "id": "0165c0c9-179c-46cd-829f-ac90987c14cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _profile_docs(docs: List[Dict[str, Any]]) -> Dict[str, Any]:\n",
    "    fields = defaultdict(lambda: {\"types\": Counter(), \"nulls\": 0, \"examples\": [], \"pii\": Counter()})\n",
    "    n = max(1, len(docs))\n",
    "    for d in docs:\n",
    "        if not isinstance(d, dict): \n",
    "            continue\n",
    "        for k, v in d.items():\n",
    "            info = fields[k]\n",
    "            info[\"types\"][_infer_type(v)] += 1\n",
    "            if v is None: \n",
    "                info[\"nulls\"] += 1\n",
    "            if len(info[\"examples\"]) < 5:\n",
    "                if isinstance(v, (str, int, float, bool)) and v is not None:\n",
    "                    info[\"examples\"].append(v)\n",
    "            if isinstance(v, str):\n",
    "                pf = _pii_flags(v)\n",
    "                for fk, fv in pf.items():\n",
    "                    if fv: info[\"pii\"][fk] += 1\n",
    "    prof = {}\n",
    "    for k, v in fields.items():\n",
    "        prof[k] = {\n",
    "            \"type_dist\": dict(v[\"types\"]),\n",
    "            \"null_pct\": v[\"nulls\"] / n,\n",
    "            \"examples\": v[\"examples\"],\n",
    "            \"pii_flags\": {pk: int(pc) for pk, pc in v[\"pii\"].items()},\n",
    "        }\n",
    "    return {\"count\": len(docs), \"fields\": prof}\n",
    "def _cast_sample(docs: List[Dict[str, Any]]) -> List[Dict[str, Any]]:\n",
    "    out = []\n",
    "    for d in docs[:20]:\n",
    "        if not isinstance(d, dict):\n",
    "            continue\n",
    "        dd = {}\n",
    "        for k, v in d.items():\n",
    "            if isinstance(v, str):\n",
    "                vv = v.strip()\n",
    "                m = re.fullmatch(r\"[-+]?\\d+(?:\\.\\d+)?\", vv.replace(\",\", \"\"))\n",
    "                if m:\n",
    "                    try:\n",
    "                        num = float(vv.replace(\",\", \"\"))\n",
    "                        if abs(num - round(num)) < 1e-9:\n",
    "                            dd[k] = int(round(num))\n",
    "                        else:\n",
    "                            dd[k] = num\n",
    "                    except Exception:\n",
    "                        dd[k] = vv\n",
    "                else:\n",
    "                    dd[k] = vv\n",
    "            elif isinstance(v, float) and abs(v - round(v)) < 1e-9:\n",
    "                dd[k] = int(round(v))\n",
    "            else:\n",
    "                dd[k] = v\n",
    "        out.append(dd)\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 407,
   "id": "5bd3e9ed-7ea5-488b-afc1-500d5eea9308",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " RAG extended with /schema/profile and /schema/validate\n"
     ]
    }
   ],
   "source": [
    "def extend_rag_with_schema_endpoints():\n",
    "    if \"rag\" not in RUNNING:\n",
    "        print(\"[schema] RAG service not running; skip extension.\")\n",
    "        return\n",
    "    app = RUNNING[\"rag\"].app\n",
    "    existing = {getattr(r, \"path\", \"\") for r in app.routes}\n",
    "    if \"/schema/profile\" in existing and \"/schema/validate\" in existing:\n",
    "        print(\"RAG already has /schema/profile & /schema/validate\")\n",
    "        return\n",
    "    @app.post(\"/schema/profile\")\n",
    "    def schema_profile(payload: Dict[str, Any]):\n",
    "        sample = payload.get(\"sample\")\n",
    "        if not isinstance(sample, list) or not all(isinstance(x, dict) for x in sample):\n",
    "            raise HTTPException(400, \"payload must have 'sample': List[Object]\")\n",
    "        prof = _profile_docs(sample)\n",
    "        return {\"ok\": True, \"profile\": prof}\n",
    "    @app.post(\"/schema/validate\")\n",
    "    def schema_validate(payload: Dict[str, Any]):\n",
    "        sample = payload.get(\"sample\")\n",
    "        if not isinstance(sample, list) or not all(isinstance(x, dict) for x in sample):\n",
    "            raise HTTPException(400, \"payload must have 'sample': List[Object]\")\n",
    "        prof = _profile_docs(sample)\n",
    "        deny_pii = os.getenv(\"PII_DENY_LIST\", \"email,ssn,ccard\").split(\",\")\n",
    "        deny_pii = [x.strip().lower() for x in deny_pii if x.strip()]\n",
    "        blocks = []\n",
    "        for fname, finfo in (prof.get(\"fields\") or {}).items():\n",
    "            for flag, count in (finfo.get(\"pii_flags\") or {}).items():\n",
    "                if flag.lower() in deny_pii and count > 0:\n",
    "                    blocks.append({\"field\": fname, \"flag\": flag, \"count\": count})\n",
    "        passes = (len(blocks) == 0)\n",
    "        casted = _cast_sample(sample)\n",
    "        return {\"ok\": True, \"passes\": passes, \"blocks\": blocks, \"profile\": prof, \"sample\": casted}\n",
    "    print(\" RAG extended with /schema/profile and /schema/validate\")\n",
    "extend_rag_with_schema_endpoints()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 408,
   "id": "136f411f-550b-46b1-95f4-681df1881753",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Reports bundler ready: run_all_reports_and_bundle()\n"
     ]
    }
   ],
   "source": [
    "def _tail_lines(p: Path, n: int = 200) -> List[str]:\n",
    "    if not p.exists(): return []\n",
    "    with p.open(\"r\", encoding=\"utf-8\") as f:\n",
    "        lines = f.readlines()\n",
    "    return [ln.rstrip(\"\\n\") for ln in lines[-n:]]\n",
    "def run_all_reports_and_bundle(ns: str = \"default\") -> Dict[str, Any]:\n",
    "    day = time.strftime(\"%Y%m%d\")\n",
    "    bundle = {\n",
    "        \"ts\": int(time.time()),\n",
    "        \"day\": day,\n",
    "        \"namespace\": ns,\n",
    "        \"artifacts\": REG.snapshot(),\n",
    "        \"rollup\": build_rollup_from_audit(day=day, ns=ns),\n",
    "        \"psi\": drift_psi_from_audit(day=day, ns=ns),\n",
    "        \"cache\": RESP_CACHE.stats(),\n",
    "        \"quotas\": QUOTAS.snapshot(),\n",
    "        \"route_quotas\": ROUTE_QUOTAS.snapshot(),\n",
    "        \"costs\": COSTS.snapshot(),\n",
    "        \"canary\": CANARY.snapshot(),\n",
    "        \"sql_audit_tail\": _tail_lines(SQL_AUDIT, 200),\n",
    "        \"cost_ledger_tail\": _tail_lines(COST_LEDGER, 200),\n",
    "    }\n",
    "    out_dir = ART / \"reports\"\n",
    "    out_dir.mkdir(parents=True, exist_ok=True)\n",
    "    out_path = out_dir / f\"summary_{ns}_{day}.json\"\n",
    "    out_path.write_text(json.dumps(bundle, indent=2), encoding=\"utf-8\")\n",
    "    return {\"ok\": True, \"path\": str(out_path), \"size\": out_path.stat().st_size}\n",
    "print(\" Reports bundler ready: run_all_reports_and_bundle()\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 409,
   "id": "11b0d948-7f60-4352-b121-0a7afc6b41c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    from prometheus_client import Counter as _LCounter, Histogram as _LHist, Gauge as _LGauge\n",
    "    from prometheus_client import CollectorRegistry as _LReg, generate_latest as _lg_latest, CONTENT_TYPE_LATEST as _lg_ct\n",
    "    _HAVE_LG_PROM = True\n",
    "except Exception:\n",
    "    _HAVE_LG_PROM = False\n",
    "class LoadGen:\n",
    "    def __init__(self, gateway_base: Optional[str] = None, qps: float = 1.0, namespace: str = \"default\"):\n",
    "        self.base = gateway_base or (RUNNING[\"gateway\"].base if \"gateway\" in RUNNING else None)\n",
    "        self.qps = max(0.1, float(qps))\n",
    "        self.ns = namespace\n",
    "        self._stop = threading.Event()\n",
    "        self._t: Optional[threading.Thread] = None\n",
    "        if _HAVE_LG_PROM:\n",
    "            self.reg = _LReg()\n",
    "            self.req = _LCounter(\"loadgen_requests_total\", \"Requests sent\", [\"status\"], registry=self.reg)\n",
    "            self.lat = _LHist(\"loadgen_latency_ms\", \"Latency ms\", registry=self.reg, buckets=[25,50,100,200,400,800,1600,3200,6400])\n",
    "            self.err = _LCounter(\"loadgen_errors_total\", \"Errors\", [\"stage\"], registry=self.reg)\n",
    "            self.inflight = _LGauge(\"loadgen_inflight\", \"Inflight requests\", registry=self.reg)\n",
    "        else:\n",
    "            self.reg = None\n",
    "        self._last = {\"ok\": 0, \"err\": 0, \"p95\": None}\n",
    "    def _next_query(self) -> Dict[str, Any]:\n",
    "        qs = [\n",
    "            \"bm25 vs dense\",\n",
    "            \"what is reranking\",\n",
    "            \"rust tokio axum overview\",\n",
    "            \"faithfulness vs hallucination\",\n",
    "            \"faiss vs pgvector\",\n",
    "        ]\n",
    "        return {\"query\": random.choice(qs), \"top_k\": random.choice([3,4,5,6]), \"use_rerank\": random.choice([True, False])}\n",
    "    def _loop(self):\n",
    "        import requests as rq\n",
    "        per = 1.0 / self.qps\n",
    "        lat_buf: List[float] = []\n",
    "        while not self._stop.is_set():\n",
    "            t0 = time.time()\n",
    "            try:\n",
    "                payload = self._next_query()\n",
    "                if _HAVE_LG_PROM: self.inflight.inc()\n",
    "                r = rq.post(f\"{self.base}/answer\", headers={\"X-Namespace\": self.ns}, json=payload, timeout=6)\n",
    "                if _HAVE_LG_PROM:\n",
    "                    self.req.labels(str(r.status_code)).inc()\n",
    "                if r.status_code == 200:\n",
    "                    meta = (r.json().get(\"meta\") or {})\n",
    "                    ms = float(meta.get(\"latency_ms\") or 0.0)\n",
    "                    lat_buf.append(ms)\n",
    "                    if _HAVE_LG_PROM: self.lat.observe(ms)\n",
    "                else:\n",
    "                    if _HAVE_LG_PROM: self.err.labels(\"http\").inc()\n",
    "            except Exception:\n",
    "                if _HAVE_LG_PROM: self.err.labels(\"exc\").inc()\n",
    "            finally:\n",
    "                if _HAVE_LG_PROM: self.inflight.dec()\n",
    "                dt = time.time() - t0\n",
    "                time.sleep(max(0.0, per - dt))\n",
    "            if len(lat_buf) > 100:\n",
    "                lat_buf = lat_buf[-100:]\n",
    "                self._last[\"p95\"] = sorted(lat_buf)[int(0.95 * (len(lat_buf)-1))]\n",
    "        if _HAVE_LG_PROM:\n",
    "            try:\n",
    "                self._last[\"ok\"] = int(self.req._value.get())\n",
    "            except Exception:\n",
    "                pass\n",
    "    def start(self):\n",
    "        if self._t and self._t.is_alive(): return\n",
    "        self._stop.clear()\n",
    "        self._t = threading.Thread(target=self._loop, name=\"loadgen\", daemon=True)\n",
    "        self._t.start()\n",
    "    def stop(self):\n",
    "        self._stop.set()\n",
    "        if self._t and self._t.is_alive():\n",
    "            self._t.join(timeout=2.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 410,
   "id": "7d3cfdb1-1100-4af4-8d72-a055e8c7a975",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Loadgen exporter up at http://127.0.0.1:9108    /start  /metrics\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<__main__.ServerThread at 0x1b90258f170>"
      ]
     },
     "execution_count": 410,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def build_loadgen_app(qps: float = 1.0, namespace: str = \"default\"):\n",
    "    app = FastAPI(title=\"Loadgen Exporter\", version=\"0.1\")\n",
    "    state = {\"lg\": LoadGen(qps=qps, namespace=namespace)}\n",
    "    @app.get(\"/\")\n",
    "    def root():\n",
    "        return {\"ok\": True, \"endpoints\": [\"/start\", \"/stop\", \"/status\", \"/metrics\"]}\n",
    "    @app.post(\"/start\")\n",
    "    def start():\n",
    "        state[\"lg\"].start()\n",
    "        return {\"ok\": True, \"running\": True}\n",
    "    @app.post(\"/stop\")\n",
    "    def stop():\n",
    "        state[\"lg\"].stop()\n",
    "        return {\"ok\": True, \"running\": False}\n",
    "    @app.get(\"/status\")\n",
    "    def status():\n",
    "        return {\"ok\": True, \"qps\": state[\"lg\"].qps, \"ns\": state[\"lg\"].ns, \"last\": state[\"lg\"]._last}\n",
    "    @app.get(\"/metrics\")\n",
    "    def metrics():\n",
    "        if not _HAVE_LG_PROM:\n",
    "            return PlainTextResponse(\"prometheus_client not installed\", status_code=200)\n",
    "        return PlainTextResponse(_lg_latest(state[\"lg\"].reg), media_type=_lg_ct)\n",
    "    return app\n",
    "def start_loadgen_exporter(preferred_port: int = 9108, qps: float = 1.0, namespace: str = \"default\"):\n",
    "    srv = ServerThread(build_loadgen_app(qps=qps, namespace=namespace), preferred_port=preferred_port, name=\"loadgen\", log_level=\"warning\")\n",
    "    srv.start()\n",
    "    RUNNING[\"loadgen\"] = srv\n",
    "    print(f\" Loadgen exporter up at {srv.base}    /start  /metrics\")\n",
    "    return srv\n",
    "start_loadgen_exporter(qps=0.5, namespace=\"default\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 411,
   "id": "4df110f3-b466-4545-9a17-5466e62b8904",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Probes up at http://127.0.0.1:9150    /probe\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<__main__.ServerThread at 0x1b996962cc0>"
      ]
     },
     "execution_count": 411,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def build_probes_app():\n",
    "    app = FastAPI(title=\"Probes\", version=\"0.1\")\n",
    "    @app.get(\"/probe\")\n",
    "    def probe():\n",
    "        import requests as rq\n",
    "        out = {}\n",
    "        for name in [\"gateway\", \"rag\", \"hf\", \"overseer\"]:\n",
    "            if name in RUNNING:\n",
    "                url = RUNNING[name].base + (\"/up\" if name != \"overseer\" else \"/overseer/up\")\n",
    "                try:\n",
    "                    t0 = time.time()\n",
    "                    r = rq.get(url, timeout=3)\n",
    "                    out[name] = {\"status\": r.status_code, \"latency_ms\": int((time.time()-t0)*1000)}\n",
    "                except Exception as e:\n",
    "                    out[name] = {\"status\": 0, \"error\": str(e)}\n",
    "            else:\n",
    "                out[name] = {\"status\": -1, \"error\": \"not_running\"}\n",
    "        return {\"ok\": True, \"services\": out, \"ts\": int(time.time())}\n",
    "\n",
    "    return app\n",
    "def start_probes(preferred_port: int = 9150):\n",
    "    srv = ServerThread(build_probes_app(), preferred_port=preferred_port, name=\"probes\", log_level=\"warning\")\n",
    "    srv.start()\n",
    "    RUNNING[\"probes\"] = srv\n",
    "    print(f\" Probes up at {srv.base}    /probe\")\n",
    "    return srv\n",
    "start_probes()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 412,
   "id": "cc9a6b4e-1f66-45be-8c69-3408ad7d0777",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[slack] webhook not set; skipping scheduler\n",
      " Continuation loaded: RAG schema endpoints, reports bundler, loadgen exporter, probes, daily Slack summaries.\n",
      "Tips:\n",
      "  # schema profile/validate\n",
      "  curl -s -X POST http://127.0.0.1:54386/schema/profile -H 'Content-Type: application/json' -d '{\"sample\":[{\"email\":\"x@y.com\",\"age\":\"30\"}]}' | jq .\n",
      "  curl -s -X POST http://127.0.0.1:54386/schema/validate -H 'Content-Type: application/json' -d '{\"sample\":[{\"email\":\"x@y.com\",\"age\":\"30\"}]}' | jq .\n",
      "  # bundle reports now  run_all_reports_and_bundle()\n",
      "  # start optional exporters  start_loadgen_exporter(); start_probes()\n"
     ]
    }
   ],
   "source": [
    "def _post_webhook(url: str, payload: Dict[str, Any]):\n",
    "    try:\n",
    "        import requests as _rq\n",
    "        _rq.post(url, json=payload, timeout=5)\n",
    "    except Exception:\n",
    "        pass\n",
    "def schedule_daily_slack_summary(ns: str = \"default\", when_hhmm: str = \"09:00\"):\n",
    "    webhook = os.getenv(\"SLACK_WEBHOOK\") or os.getenv(\"SLO_ALERT_WEBHOOK\")\n",
    "    if not webhook:\n",
    "        print(\"[slack] webhook not set; skipping scheduler\")\n",
    "        return None\n",
    "    hh, mm = map(int, when_hhmm.split(\":\"))\n",
    "    def _loop():\n",
    "        while True:\n",
    "            now = time.localtime()\n",
    "            target = time.mktime((now.tm_year, now.tm_mon, now.tm_mday, hh, mm, 0, now.tm_wday, now.tm_yday, now.tm_isdst))\n",
    "            if target <= time.mktime(now):\n",
    "                target += 86400\n",
    "            time.sleep(max(0, target - time.mktime(now)))\n",
    "            try:\n",
    "                roll = build_rollup_from_audit(ns=ns)\n",
    "                gc_path = ART / \"reports\" / \"gc_last.json\"\n",
    "                gc = json.loads(gc_path.read_text()) if gc_path.exists() else {\"removed\": []}\n",
    "                msg = {\n",
    "                    \"type\": \"daily_summary\",\n",
    "                    \"ns\": ns,\n",
    "                    \"p50_ms\": roll.get(\"p50_ms\"),\n",
    "                    \"p95_ms\": roll.get(\"p95_ms\"),\n",
    "                    \"cache_hit_rate\": roll.get(\"cache_hit_rate\"),\n",
    "                    \"psi\": roll.get(\"psi\"),\n",
    "                    \"removed_files\": len(gc.get(\"removed\", [])),\n",
    "                    \"day\": roll.get(\"day\"),\n",
    "                }\n",
    "                _post_webhook(webhook, msg)\n",
    "            except Exception:\n",
    "                pass\n",
    "    t = threading.Thread(target=_loop, name=\"slack-daily\", daemon=True)\n",
    "    t.start()\n",
    "    print(f\" Slack daily summary scheduled at {when_hhmm} (ns={ns})\")\n",
    "    return t\n",
    "schedule_daily_slack_summary(ns=\"default\", when_hhmm=\"09:00\")\n",
    "print(\" Continuation loaded: RAG schema endpoints, reports bundler, loadgen exporter, probes, daily Slack summaries.\")\n",
    "print(\"Tips:\")\n",
    "if \"rag\" in RUNNING:\n",
    "    print(f\"  # schema profile/validate\")\n",
    "    print(f\"  curl -s -X POST {RUNNING['rag'].base}/schema/profile -H 'Content-Type: application/json' -d '{{\\\"sample\\\":[{{\\\"email\\\":\\\"x@y.com\\\",\\\"age\\\":\\\"30\\\"}}]}}' | jq .\")\n",
    "    print(f\"  curl -s -X POST {RUNNING['rag'].base}/schema/validate -H 'Content-Type: application/json' -d '{{\\\"sample\\\":[{{\\\"email\\\":\\\"x@y.com\\\",\\\"age\\\":\\\"30\\\"}}]}}' | jq .\")\n",
    "print(\"  # bundle reports now  run_all_reports_and_bundle()\")\n",
    "print(\"  # start optional exporters  start_loadgen_exporter(); start_probes()\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 413,
   "id": "799a5e07-d67c-4faf-9944-4497ed263af4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, json, time, random, re, hashlib\n",
    "from dataclasses import dataclass\n",
    "from typing import Any, Dict, List, Optional, Tuple\n",
    "from pathlib import Path\n",
    "import requests\n",
    "REPORTS_DIR = ART / \"reports\"\n",
    "REPORTS_DIR.mkdir(parents=True, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 414,
   "id": "d6354f45-bacf-43a1-9178-2dfd08eb15e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def pg_insert_patch_audit(patch_id: str, notes: str, tests: List[str],\n",
    "                          decision: str, rationale: str, canary_kpis: Dict[str, Any]) -> bool:\n",
    "    conn = _pg_connect()\n",
    "    if not conn:\n",
    "        return False\n",
    "    try:\n",
    "        cur = conn.cursor()\n",
    "        cur.execute(\n",
    "            \"\"\"\n",
    "            INSERT INTO patch_audit(patch_id, notes, tests, decision, rationale, canary_kpis)\n",
    "            VALUES (%s,%s,%s,%s,%s,%s)\n",
    "            ON CONFLICT (patch_id) DO UPDATE SET\n",
    "              notes=EXCLUDED.notes,\n",
    "              tests=EXCLUDED.tests,\n",
    "              decision=EXCLUDED.decision,\n",
    "              rationale=EXCLUDED.rationale,\n",
    "              canary_kpis=EXCLUDED.canary_kpis\n",
    "            \"\"\",\n",
    "            (patch_id, notes, json.dumps(tests), decision, rationale, json.dumps(canary_kpis))\n",
    "        )\n",
    "        cur.close()\n",
    "        return True\n",
    "    except Exception as e:\n",
    "        print(\"[pg] patch_audit insert/upsert failed:\", e)\n",
    "        return False\n",
    "def finalize_patch_audit(patch_id: str, notes: str, tests: List[str],\n",
    "                         decision: str, rationale: str, canary_kpis: Dict[str, Any]) -> Dict[str, Any]:\n",
    "    ok_pg = pg_insert_patch_audit(patch_id, notes, tests, decision, rationale, canary_kpis)\n",
    "    local = {\n",
    "        \"patch_id\": patch_id,\n",
    "        \"notes\": notes,\n",
    "        \"tests\": tests,\n",
    "        \"decision\": decision,\n",
    "        \"rationale\": rationale,\n",
    "        \"canary_kpis\": canary_kpis,\n",
    "        \"ts\": int(time.time())\n",
    "    }\n",
    "    (AUDIT_DIR / f\"patch_{patch_id}.json\").write_text(json.dumps(local, indent=2), encoding=\"utf-8\")\n",
    "    return {\"ok\": True, \"stored_pg\": ok_pg, \"stored_local\": True, \"path\": str(AUDIT_DIR / f\"patch_{patch_id}.json\")}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 415,
   "id": "4848f74d-ef51-4950-9b72-9019ce7a0f68",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Overseer extended with /overseer/audit/patch_decision\n"
     ]
    }
   ],
   "source": [
    "if \"overseer\" in RUNNING:\n",
    "    app = RUNNING[\"overseer\"].app\n",
    "    existing_paths = {getattr(r, \"path\", \"\") for r in app.routes}\n",
    "    if \"/overseer/audit/patch_decision\" not in existing_paths:\n",
    "        from pydantic import BaseModel, Field\n",
    "        class PatchDecisionReq(BaseModel):\n",
    "            patch_id: str\n",
    "            notes: str = Field(default=\"\")\n",
    "            tests: List[str] = []\n",
    "            decision: str \n",
    "            rationale: str = Field(default=\"\")\n",
    "            canary_kpis: Dict[str, Any] = {}\n",
    "        @app.post(\"/overseer/audit/patch_decision\")\n",
    "        def overseer_patch_decision(body: PatchDecisionReq):\n",
    "            return finalize_patch_audit(\n",
    "                patch_id=body.patch_id,\n",
    "                notes=body.notes,\n",
    "                tests=body.tests,\n",
    "                decision=body.decision,\n",
    "                rationale=body.rationale,\n",
    "                canary_kpis=body.canary_kpis,\n",
    "            )\n",
    "        print(\" Overseer extended with /overseer/audit/patch_decision\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 416,
   "id": "fd27e05d-6f38-406c-af88-727eae53dd16",
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class GoldenCase:\n",
    "    name: str\n",
    "    ok: bool\n",
    "    details: Dict[str, Any]\n",
    "def _sha16(s: str) -> str:\n",
    "    return hashlib.sha256(s.encode(\"utf-8\")).hexdigest()[:16]\n",
    "def _require(cond: bool, msg: str, details: Dict[str, Any] | None = None) -> GoldenCase:\n",
    "    return GoldenCase(name=msg, ok=bool(cond), details=details or {})\n",
    "def _resp_json(r: requests.Response) -> Dict[str, Any]:\n",
    "    try:\n",
    "        return r.json()\n",
    "    except Exception:\n",
    "        return {\"_raw\": r.text}\n",
    "def _ensure_dummy_router_file() -> Path:\n",
    "    dest = ART / \"router_v1.joblib\"\n",
    "    if not dest.exists():\n",
    "        dest.write_bytes(os.urandom(256))\n",
    "    return dest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 417,
   "id": "89ad582c-2034-40ed-a7c2-66b018643307",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_golden_pack(namespace: str = \"default\") -> Dict[str, Any]:\n",
    "    \"\"\"\n",
    "    Executes a compact set of end-to-end checks and writes a report to artifacts/reports/.\n",
    "    Returns a dict summary.\n",
    "    \"\"\"\n",
    "    results: List[GoldenCase] = []\n",
    "    start_ts = int(time.time())\n",
    "    ns_hdr = {\"X-Namespace\": namespace}\n",
    "    if not RUNNING:\n",
    "        return {\"ok\": False, \"error\": \"stack not running\"}\n",
    "    gw = RUNNING[\"gateway\"].base\n",
    "    rag = RUNNING[\"rag\"].base\n",
    "    hf  = RUNNING[\"hf\"].base\n",
    "    ov  = RUNNING[\"overseer\"].base if \"overseer\" in RUNNING else None\n",
    "    r_gw = requests.get(gw + \"/up\", timeout=4); j_gw = _resp_json(r_gw)\n",
    "    r_rg = requests.get(rag + \"/up\", timeout=4); j_rg = _resp_json(r_rg)\n",
    "    r_hf = requests.get(hf + \"/up\", timeout=4);  j_hf = _resp_json(r_hf)\n",
    "    results.append(_require(r_gw.ok and j_gw.get(\"ok\") is True, \"gateway /up ok\", {\"status\": r_gw.status_code}))\n",
    "    results.append(_require(r_rg.ok and j_rg.get(\"ok\") is True, \"rag /up ok\", {\"status\": r_rg.status_code}))\n",
    "    results.append(_require(r_hf.ok and j_hf.get(\"ok\") is True, \"hf /up ok\",  {\"status\": r_hf.status_code}))\n",
    "    payload = {\"query\": \"How do BM25 and vector search differ?\", \"top_k\": 5, \"use_rerank\": True}\n",
    "    r_ans1 = requests.post(gw + \"/answer\", headers=ns_hdr, json=payload, timeout=8)\n",
    "    j_ans1 = _resp_json(r_ans1)\n",
    "    results.append(_require(r_ans1.status_code == 200, \"answer returns 200\", {\"meta\": j_ans1.get(\"meta\")}))\n",
    "    results.append(_require(isinstance(j_ans1.get(\"contexts\"), list) and len(j_ans1.get(\"contexts\")) > 0,\n",
    "                            \"contexts present\", {\"len\": len(j_ans1.get(\"contexts\") or [])}))\n",
    "    results.append(_require(\"latency_ms\" in (j_ans1.get(\"meta\") or {}),\n",
    "                            \"latency present\", j_ans1.get(\"meta\") or {}))\n",
    "    requests.post(gw + \"/admin/cache/clear\", timeout=3)\n",
    "    _ = requests.post(gw + \"/answer\", headers=ns_hdr, json=payload, timeout=8).json()\n",
    "    stats_after_miss = requests.get(gw + \"/admin/cache/stats\", timeout=3).json()\n",
    "    _ = requests.post(gw + \"/answer\", headers=ns_hdr, json=payload, timeout=8).json()\n",
    "    stats_after_hit  = requests.get(gw + \"/admin/cache/stats\", timeout=3).json()\n",
    "    miss0 = ((stats_after_miss.get(\"stats\") or {}).get(\"misses\") or 0)\n",
    "    hit1  = ((stats_after_hit.get(\"stats\")  or {}).get(\"hits\") or 0)\n",
    "    results.append(_require(miss0 >= 1, \"cache registered a miss\", {\"misses\": miss0}))\n",
    "    results.append(_require(hit1  >= 1, \"cache registered a hit\",  {\"hits\": hit1}))\n",
    "    requests.post(gw + \"/admin/costs/set\", json={\"namespace\": namespace, \"route\": \"ce_rerank\", \"daily_usd\": 0.0}, timeout=4)\n",
    "    r_cost = requests.post(gw + \"/answer\", headers=ns_hdr, json={\"query\": \"bm25 vs dense\", \"top_k\": 6, \"use_rerank\": True}, timeout=8)\n",
    "    j_cost = _resp_json(r_cost)\n",
    "    pipe   = ((j_cost.get(\"meta\") or {}).get(\"pipeline\"))\n",
    "    bblock = (j_cost.get(\"meta\") or {}).get(\"budget_blocked_ce\", False)\n",
    "    results.append(_require(pipe == \"rrf\", \"cost cap forced RRF\", {\"pipeline\": pipe}))\n",
    "    results.append(_require(bool(bblock) is True, \"budget flag set\", {\"budget_blocked_ce\": bblock}))\n",
    "    pii_sample = [{\"email\": \"a@b.com\", \"age\": \"30\", \"name\": \"Ada\"}]\n",
    "    r_prof = requests.post(rag + \"/schema/profile\", json={\"sample\": pii_sample}, timeout=5).json()\n",
    "    r_val  = requests.post(rag + \"/schema/validate\", json={\"sample\": pii_sample}, timeout=5).json()\n",
    "    results.append(_require(r_prof.get(\"ok\") is True, \"schema profile ok\", {\"fields\": list((r_prof.get(\"profile\") or {}).get(\"fields\", {}).keys())}))\n",
    "    results.append(_require(r_val.get(\"ok\") is True and r_val.get(\"passes\") is False, \"PII blocks upload\", {\"blocks\": r_val.get(\"blocks\")}))\n",
    "    cs = (r_val.get(\"sample\") or [{}])[0]\n",
    "    results.append(_require(isinstance(cs.get(\"age\"), int), \"casted numeric fields\", {\"age\": cs.get(\"age\")}))\n",
    "    bad_sql = \"SELECT * FROM users; DROP TABLE users;\"\n",
    "    ok_sql  = \"SELECT id, email FROM users\"\n",
    "    r_bad = requests.post(rag + \"/sql/execute\", json={\"text\": bad_sql}, timeout=5).json()\n",
    "    r_ok  = requests.post(rag + \"/sql/execute\", json={\"text\": ok_sql}, timeout=5).json()\n",
    "    results.append(_require(r_bad.get(\"ok\") is False and \"multiple statements\" in (r_bad.get(\"error\") or \"\"), \"sql: multi-stmt blocked\", r_bad))\n",
    "    results.append(_require(r_ok.get(\"ok\") is True and re.search(r\"(?i)\\blimit\\b\", r_ok.get(\"sql\",\"\")), \"sql: limit enforced\", {\"sql\": r_ok.get(\"sql\")}))\n",
    "    dummy = _ensure_dummy_router_file()\n",
    "    uri = save_router_bundle_local(str(dummy))\n",
    "    rsnap = requests.get(gw + \"/artifacts/snapshot\", timeout=4).json()\n",
    "    last = (rsnap.get(\"registry\") or {}).get(\"router_bundles\", [])[-1] if (rsnap.get(\"registry\") or {}).get(\"router_bundles\") else {}\n",
    "    results.append(_require(bool(uri) and bool(last), \"artifact registry records router bundle\", {\"uri\": uri, \"last\": last}))\n",
    "    if ov:\n",
    "        sm = demo_overseer_flow()\n",
    "        ok_flow = all((sm.get(\"ingest\",{}).get(\"ok\"), sm.get(\"validate\",{}).get(\"ok\"), sm.get(\"submit\",{}).get(\"ok\")))\n",
    "        results.append(_require(ok_flow, \"overseer: ingest+validate+submit ok\",\n",
    "                                {\"ingest\": sm.get(\"ingest\"), \"validate\": sm.get(\"validate\"), \"submit\": sm.get(\"submit\")}))\n",
    "        pid = (sm.get(\"submit\") or {}).get(\"patch_id\") or f\"demo-{_sha16('x')}\"\n",
    "        dec = requests.post(ov + \"/overseer/audit/patch_decision\",\n",
    "                            json={\"patch_id\": pid, \"notes\": \"golden\", \"tests\": [\"golden::basic\"],\n",
    "                                  \"decision\": \"hold\", \"rationale\": \"demo only\", \"canary_kpis\": {\"p95_ms\": 120}},\n",
    "                            timeout=6).json()\n",
    "        results.append(_require(dec.get(\"ok\") is True, \"overseer: patch decision recorded\", {\"patch_id\": pid}))\n",
    "    roll = build_rollup_from_audit(ns=namespace)\n",
    "    bund = run_all_reports_and_bundle(ns=namespace)\n",
    "    passed = sum(1 for r in results if r.ok)\n",
    "    failed = [r for r in results if not r.ok]\n",
    "    summary = {\n",
    "        \"ok\": (failed == []),\n",
    "        \"passed\": passed,\n",
    "        \"failed\": len(failed),\n",
    "        \"cases\": [r.__dict__ for r in results],\n",
    "        \"rollup\": roll,\n",
    "        \"bundle\": bund,\n",
    "        \"ts\": int(time.time()),\n",
    "    }\n",
    "    out = REPORTS_DIR / f\"golden_{namespace}_{time.strftime('%Y%m%d-%H%M%S')}.json\"\n",
    "    out.write_text(json.dumps(summary, indent=2), encoding=\"utf-8\")\n",
    "    print(f\" Golden pack complete: {passed} passed / {len(failed)} failed  {out}\")\n",
    "    if failed:\n",
    "        for f in failed:\n",
    "            print(f\"  - FAIL: {f.name} :: {f.details}\")\n",
    "    return summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 418,
   "id": "88760620-3141-4fba-9bbe-1fdd9f01aee3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Golden runner ready. Try:\n",
      "  run_golden_pack()\n",
      "  finalize_patch_audit('demo1234', 'notes', ['t1','t2'], 'hold', 'waiting on more data', {'p95_ms': 1500})\n",
      "  # overseer decision endpoint: http://127.0.0.1:7000/overseer/audit/patch_decision\n"
     ]
    }
   ],
   "source": [
    "print(\"\\nGolden runner ready. Try:\")\n",
    "print(\"  run_golden_pack()\")\n",
    "print(\"  finalize_patch_audit('demo1234', 'notes', ['t1','t2'], 'hold', 'waiting on more data', {'p95_ms': 1500})\")\n",
    "if \"overseer\" in RUNNING:\n",
    "    print(f\"  # overseer decision endpoint: {RUNNING['overseer'].base}/overseer/audit/patch_decision\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 419,
   "id": "971b5017-a90f-4cb3-a10e-0eef1e360c09",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time, json, math, threading, statistics, re\n",
    "from pathlib import Path\n",
    "from typing import Any, Dict, List, Optional, Tuple\n",
    "from fastapi import Request as _FRequest, HTTPException as _FHTTP\n",
    "from fastapi.responses import JSONResponse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 420,
   "id": "00fc7e15-ff00-4774-834c-0d03b298a433",
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    if _HAVE_PROM:\n",
    "        try:\n",
    "            GW_FAITH = PromGauge(\"py_gateway_faithfulness_mean\", \"Faithfulness (last obs)\", [\"ns\"])\n",
    "        except Exception:\n",
    "            pass\n",
    "        try:\n",
    "            GW_NDCG = PromGauge(\"py_gateway_ndcg10_proxy\", \"NDCG@10 proxy (last obs)\", [\"ns\"])\n",
    "        except Exception:\n",
    "            pass\n",
    "except Exception:\n",
    "    pass\n",
    "def _ndcg_from_bm25(bm25_results: List[Dict[str, Any]], final_contexts: List[Dict[str, Any]], k: int = 10) -> float:\n",
    "    \"\"\"\n",
    "    Compute a proxy NDCG@k using BM25 scores as relevance labels.\n",
    "    Rel labels are normalized to [0,1] by max score in the candidate set.\n",
    "    \"\"\"\n",
    "    if not bm25_results or not final_contexts:\n",
    "        return float(\"nan\")\n",
    "    rel_map = {d[\"id\"]: float(d.get(\"score\", 0.0)) for d in bm25_results}\n",
    "    max_rel = max(1e-9, max(rel_map.values()))\n",
    "    def _dcg(order: List[str], kk: int) -> float:\n",
    "        s = 0.0\n",
    "        for i, did in enumerate(order[:kk], start=1):\n",
    "            rel = rel_map.get(did, 0.0) / max_rel\n",
    "            s += (rel) / math.log2(i + 1)\n",
    "        return s\n",
    "    final_ids = [c.get(\"id\") for c in final_contexts]\n",
    "    ideal_ids = [did for did, _ in sorted(rel_map.items(), key=lambda kv: -kv[1])]\n",
    "    return (_dcg(final_ids, k) / max(1e-9, _dcg(ideal_ids, k)))\n",
    "def _audit_write_kpi(ns: str, record: Dict[str, Any]):\n",
    "    \"\"\"Append KPI-focused audit entry (verify faithfulness / ndcg).\"\"\"\n",
    "    day = time.strftime(\"%Y%m%d\")\n",
    "    p = AUDIT_DIR / f\"answer_{day}.ndjson\"\n",
    "    rec = dict(record)\n",
    "    rec[\"namespace\"] = ns\n",
    "    rec[\"kpi\"] = True\n",
    "    try:\n",
    "        with p.open(\"a\", encoding=\"utf-8\") as f:\n",
    "            f.write(json.dumps(rec, ensure_ascii=False) + \"\\n\")\n",
    "    except Exception:\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 421,
   "id": "e5c4518d-6b2c-4ce0-bd87-ecb2a1774b9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extend_gateway_with_kpis(base: str):\n",
    "    \"\"\"\n",
    "    Wrap the current cost-capped gateway to compute/emit:\n",
    "      - verify.faithfulness into audit\n",
    "      - ndcg@10 proxy (BM25-as-relevance) into meta + audit\n",
    "      - Prometheus gauges for both\n",
    "    \"\"\"\n",
    "    app = extend_gateway_with_cost_caps(base)\n",
    "    paths = [r.path for r in app.routes]\n",
    "    idx = paths.index(\"/answer\")\n",
    "    orig_answer = app.routes[idx].endpoint\n",
    "    app.routes.pop(idx)\n",
    "    import requests as _rq\n",
    "    @app.post(\"/answer\")\n",
    "    def answer_kpi(request: _FRequest, body: Dict[str, Any]):\n",
    "        ns = (request.headers.get(\"X-Namespace\") or request.query_params.get(\"ns\") or \"default\").strip().lower()\n",
    "        query = body.get(\"query\", \"\")\n",
    "        if not isinstance(query, str) or not query.strip():\n",
    "            raise _FHTTP(400, \"query required\")\n",
    "        resp: JSONResponse = orig_answer(request, body)\n",
    "        payload = json.loads(resp.body.decode(\"utf-8\")) if isinstance(resp.body, (bytes, bytearray)) else resp.body\n",
    "        k_for_eval = max(1, min(10, len(payload.get(\"contexts\") or [])))\n",
    "        ndcg = float(\"nan\")\n",
    "        try:\n",
    "            bm25 = _rq.post(f\"{base}/retrieve_bm25\",\n",
    "                            json={\"query\": query, \"top_k\": max(k_for_eval, body.get(\"top_k\", 5))},\n",
    "                            timeout=3).json()\n",
    "            ndcg = _ndcg_from_bm25(bm25.get(\"results\", []), payload.get(\"contexts\", []), k=k_for_eval)\n",
    "            payload.setdefault(\"meta\", {})[\"ndcg10_proxy\"] = ndcg\n",
    "        except Exception:\n",
    "            pass\n",
    "        try:\n",
    "            if _HAVE_PROM:\n",
    "                faith = float(((payload.get(\"verify\") or {}).get(\"faithfulness\", float(\"nan\"))))\n",
    "                if not math.isnan(faith):\n",
    "                    try:\n",
    "                        GW_FAITH.labels(ns).set(faith)  \n",
    "                    except Exception:\n",
    "                        pass\n",
    "                if not math.isnan(ndcg):\n",
    "                    try:\n",
    "                        GW_NDCG.labels(ns).set(ndcg)  \n",
    "                    except Exception:\n",
    "                        pass\n",
    "        except Exception:\n",
    "            pass\n",
    "        _audit_write_kpi(ns, {\n",
    "            \"query\": redact(query),\n",
    "            \"pipeline\": (payload.get(\"meta\") or {}).get(\"pipeline\"),\n",
    "            \"latency_ms\": (payload.get(\"meta\") or {}).get(\"latency_ms\"),\n",
    "            \"cache\": (payload.get(\"meta\") or {}).get(\"cache\"),\n",
    "            \"verify\": (payload.get(\"verify\") or {}),\n",
    "            \"ndcg10\": ndcg if not math.isnan(ndcg) else None\n",
    "        })\n",
    "        return JSONResponse(payload)\n",
    "    return app\n",
    "if \"gateway\" in RUNNING:\n",
    "    try:\n",
    "        old = RUNNING.pop(\"gateway\")\n",
    "        old.stop()\n",
    "    except Exception:\n",
    "        pass\n",
    "    rag_base = RUNNING[\"rag\"].base if \"rag\" in RUNNING else \"http://127.0.0.1:8001\"\n",
    "    gw_srv = ServerThread(extend_gateway_with_kpis(rag_base), preferred_port=None, name=\"gw\", log_level=\"warning\")\n",
    "    gw_srv.start()\n",
    "    RUNNING[\"gateway\"] = gw_srv\n",
    "    print(\" Gateway wrapped with KPI auditing at\", gw_srv.base)\n",
    "    print(\"Try:\")\n",
    "    print(f\"  curl -s -X POST {gw_srv.base}/answer -H 'Content-Type: application/json' \\\\\\n\"\n",
    "          f\"       -d '{{\\\"query\\\":\\\"bm25 vs dense\\\",\\\"top_k\\\":5}}' | jq '.meta.ndcg10_proxy, .verify'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 422,
   "id": "c8a38426-7b04-425c-af5b-4b52e699806c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_rollup_from_audit(day: Optional[str]=None, ns: str=\"default\") -> Dict[str,Any]:  \n",
    "    day = day or time.strftime(\"%Y%m%d\")\n",
    "    p = AUDIT_DIR / f\"answer_{day}.ndjson\"\n",
    "    lats: List[float] = []\n",
    "    faiths: List[float] = []\n",
    "    ndcgs: List[float] = []\n",
    "    cache_hits = 0\n",
    "    total = 0\n",
    "    if p.exists():\n",
    "        with p.open() as f:\n",
    "            for ln in f:\n",
    "                try:\n",
    "                    obj = json.loads(ln)\n",
    "                    if (obj.get(\"namespace\") or ns) != ns:\n",
    "                        continue\n",
    "                    total += 1\n",
    "                    if \"latency_ms\" in obj:\n",
    "                        lats.append(float(obj.get(\"latency_ms\") or 0))\n",
    "                    if obj.get(\"cache\") == \"hit\":\n",
    "                        cache_hits += 1\n",
    "                    ver = obj.get(\"verify\") or {}\n",
    "                    if \"faithfulness\" in ver and isinstance(ver[\"faithfulness\"], (int, float)):\n",
    "                        faiths.append(float(ver[\"faithfulness\"]))\n",
    "                    if \"ndcg10\" in obj and isinstance(obj[\"ndcg10\"], (int, float)):\n",
    "                        ndcgs.append(float(obj[\"ndcg10\"]))\n",
    "                except Exception:\n",
    "                    pass\n",
    "    roll = {\n",
    "        \"day\": f\"{day[:4]}-{day[4:6]}-{day[6:]}\",\n",
    "        \"ns\": ns,\n",
    "        \"p50_ms\": _percentile(lats, 50) if lats else None,\n",
    "        \"p95_ms\": _percentile(lats, 95) if lats else None,\n",
    "        \"p99_ms\": _percentile(lats, 99) if lats else None,\n",
    "        \"error_rate\": None,                        \n",
    "        \"faithfulness_mean\": (sum(faiths)/len(faiths)) if faiths else None,\n",
    "        \"ndcg10\": (sum(ndcgs)/len(ndcgs)) if ndcgs else None,\n",
    "        \"cost_per_req\": None,\n",
    "        \"cache_hit_rate\": (cache_hits/total) if total else None,\n",
    "        \"psi\": drift_psi_from_audit(day, ns)\n",
    "    }\n",
    "    pg_upsert_rollup(roll[\"day\"], ns, roll)\n",
    "    return roll"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 423,
   "id": "1a2c4451-4f6d-45b7-a676-cd2161e83425",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[kpi-alerts] SLO_ALERT_WEBHOOK not set; watcher disabled\n"
     ]
    }
   ],
   "source": [
    "def _avg_metric_last_days(metric_key: str, ns: str, days: int = 3) -> Optional[float]:\n",
    "    vals: List[float] = []\n",
    "    for i in range(1, days+1):\n",
    "        t = time.localtime(time.time() - 86400*i)\n",
    "        d = time.strftime(\"%Y%m%d\", t)\n",
    "        try:\n",
    "            r = build_rollup_from_audit(d, ns)\n",
    "            v = r.get(metric_key)\n",
    "            if isinstance(v, (int, float)) and not math.isnan(v):\n",
    "                vals.append(float(v))\n",
    "        except Exception:\n",
    "            pass\n",
    "    return (sum(vals)/len(vals)) if vals else None\n",
    "def start_kpi_alert_watcher(\n",
    "    namespace: str = \"default\",\n",
    "    check_every_s: int = 120,\n",
    "    slo_p95_ms: float = 1800.0,\n",
    "    min_faith: float = 0.85,\n",
    "    ndcg_drop_thresh: float = 0.05,  \n",
    "):\n",
    "    webhook = os.getenv(\"SLO_ALERT_WEBHOOK\")\n",
    "    if not webhook:\n",
    "        print(\"[kpi-alerts] SLO_ALERT_WEBHOOK not set; watcher disabled\")\n",
    "        return None\n",
    "\n",
    "    def _loop():\n",
    "        while True:\n",
    "            try:\n",
    "                roll = build_rollup_from_audit(ns=namespace)\n",
    "                baseline_ndcg = _avg_metric_last_days(\"ndcg10\", namespace, 3) or 0.0\n",
    "                p95 = roll.get(\"p95_ms\") or 0.0\n",
    "                faith = roll.get(\"faithfulness_mean\") or 1.0\n",
    "                ndcg = roll.get(\"ndcg10\") or baseline_ndcg\n",
    "                if p95 and p95 > slo_p95_ms:\n",
    "                    _post_webhook(webhook, {\n",
    "                        \"type\": \"slo_violation\",\n",
    "                        \"ns\": namespace,\n",
    "                        \"p95_ms\": p95,\n",
    "                        \"budget_ms\": slo_p95_ms,\n",
    "                        \"day\": roll.get(\"day\"),\n",
    "                    })\n",
    "                if faith and faith < min_faith:\n",
    "                    _post_webhook(webhook, {\n",
    "                        \"type\": \"faithfulness_drop\",\n",
    "                        \"ns\": namespace,\n",
    "                        \"faithfulness_mean\": faith,\n",
    "                        \"threshold\": min_faith,\n",
    "                        \"day\": roll.get(\"day\"),\n",
    "                    })\n",
    "                if baseline_ndcg and (baseline_ndcg - ndcg) > ndcg_drop_thresh:\n",
    "                    _post_webhook(webhook, {\n",
    "                        \"type\": \"ndcg_drop\",\n",
    "                        \"ns\": namespace,\n",
    "                        \"ndcg10\": ndcg,\n",
    "                        \"baseline\": baseline_ndcg,\n",
    "                        \"drop\": baseline_ndcg - ndcg,\n",
    "                        \"threshold\": ndcg_drop_thresh,\n",
    "                        \"day\": roll.get(\"day\"),\n",
    "                    })\n",
    "            except Exception:\n",
    "                pass\n",
    "            time.sleep(check_every_s)\n",
    "    t = threading.Thread(target=_loop, name=\"kpi-alerts\", daemon=True)\n",
    "    t.start()\n",
    "    print(f\" KPI alert watcher running (ns={namespace}, every {check_every_s}s)\")\n",
    "    return t\n",
    "RUNNING.pop(\"kpi_watcher\", None)\n",
    "RUNNING[\"kpi_watcher\"] = start_kpi_alert_watcher(namespace=\"default\", check_every_s=120)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 424,
   "id": "a02af735-2a9d-49a9-9a42-805e07443460",
   "metadata": {},
   "outputs": [],
   "source": [
    "if \"gateway\" in RUNNING:\n",
    "    app = RUNNING[\"gateway\"].app\n",
    "    existing = {getattr(r, \"path\", \"\") for r in app.routes}\n",
    "    if \"/reports/rollup\" not in existing:\n",
    "        @app.get(\"/reports/rollup\")\n",
    "        def reports_rollup(ns: str = \"default\"):\n",
    "            return {\"ok\": True, \"rollup\": build_rollup_from_audit(ns=ns)}\n",
    "        @app.get(\"/reports/baseline\")\n",
    "        def reports_baseline(ns: str = \"default\"):\n",
    "            return {\"ok\": True,\n",
    "                    \"baseline\": {\n",
    "                        \"faithfulness_mean\": _avg_metric_last_days(\"faithfulness_mean\", ns, 3),\n",
    "                        \"ndcg10\": _avg_metric_last_days(\"ndcg10\", ns, 3),\n",
    "                        \"p95_ms\": _avg_metric_last_days(\"p95_ms\", ns, 3),\n",
    "                    }}\n",
    "        print(\" Gateway extended with /reports/rollup and /reports/baseline\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 425,
   "id": "12beec7a-d0fa-4584-a953-ab3e538ffdde",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time, json, math, threading\n",
    "from typing import Any, Dict, List, Optional, Tuple\n",
    "try:\n",
    "    from prometheus_client import Counter as _PCounter\n",
    "    _HAVE_CE_PROM = True\n",
    "except Exception:\n",
    "    _HAVE_CE_PROM = False\n",
    "if _HAVE_CE_PROM:\n",
    "    try:\n",
    "        GW_CE_RATE = _PCounter(\"py_gateway_ce_requests_total\", \"CE (rerank) on/off requests\", [\"ns\", \"state\", \"variant\"])\n",
    "    except Exception:\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 426,
   "id": "6480889f-29db-40ac-8abe-30b2c9b6b579",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _audit_write_kpi_ext(ns: str, record: Dict[str, Any]):\n",
    "    \"\"\"Like _audit_write_kpi, but safe if that function exists or not.\"\"\"\n",
    "    rec = dict(record)\n",
    "    rec[\"namespace\"] = ns\n",
    "    rec[\"kpi\"] = True\n",
    "    day = time.strftime(\"%Y%m%d\")\n",
    "    p = AUDIT_DIR / f\"answer_{day}.ndjson\"\n",
    "    try:\n",
    "        with p.open(\"a\", encoding=\"utf-8\") as f:\n",
    "            f.write(json.dumps(rec, ensure_ascii=False) + \"\\n\")\n",
    "    except Exception:\n",
    "        pass\n",
    "if \"gateway\" in RUNNING:\n",
    "    try:\n",
    "        old_srv = RUNNING.pop(\"gateway\")\n",
    "        base = RUNNING[\"rag\"].base if \"rag\" in RUNNING else \"http://127.0.0.1:8001\"\n",
    "        app = old_srv.app  \n",
    "        paths = [r.path for r in app.routes]\n",
    "        idx = paths.index(\"/answer\")\n",
    "        orig_answer = app.routes[idx].endpoint\n",
    "        app.routes.pop(idx)\n",
    "        from fastapi import Request as _FRequest, HTTPException as _FHTTP\n",
    "        from fastapi.responses import JSONResponse as _JSONResponse\n",
    "        @app.post(\"/answer\")\n",
    "        def answer_ce_telemetry(request: _FRequest, body: Dict[str, Any]):\n",
    "            ns = (request.headers.get(\"X-Namespace\") or request.query_params.get(\"ns\") or \"default\").strip().lower()\n",
    "            variant = \"explicit\" if (\"use_rerank\" in body and body[\"use_rerank\"] is not None) else \"canary\"\n",
    "            resp: _JSONResponse = orig_answer(request, body)\n",
    "            payload = json.loads(resp.body.decode(\"utf-8\")) if isinstance(resp.body, (bytes, bytearray)) else resp.body\n",
    "            pipeline = ((payload.get(\"meta\") or {}).get(\"pipeline\") or \"rrf\").lower()\n",
    "            state = \"on\" if pipeline == \"rerank\" else \"off\"\n",
    "            if _HAVE_CE_PROM:\n",
    "                try:\n",
    "                    GW_CE_RATE.labels(ns, state, variant).inc()\n",
    "                except Exception:\n",
    "                    pass\n",
    "            rec = {\n",
    "                \"query\": redact(body.get(\"query\", \"\")),\n",
    "                \"pipeline\": pipeline,\n",
    "                \"latency_ms\": (payload.get(\"meta\") or {}).get(\"latency_ms\"),\n",
    "                \"cache\": (payload.get(\"meta\") or {}).get(\"cache\"),\n",
    "                \"verify\": (payload.get(\"verify\") or {}),\n",
    "                \"ndcg10\": (payload.get(\"meta\") or {}).get(\"ndcg10_proxy\"),\n",
    "                \"variant\": variant,\n",
    "            }\n",
    "            _audit_write_kpi_ext(ns, rec)\n",
    "            return _JSONResponse(payload)\n",
    "        old_srv.stop()\n",
    "        srv = ServerThread(app, preferred_port=None, name=\"gw\", log_level=\"warning\")\n",
    "        srv.start()\n",
    "        RUNNING[\"gateway\"] = srv\n",
    "        print(\" Gateway re-wrapped with CE telemetry at\", srv.base)\n",
    "    except Exception as e:\n",
    "        print(\"gateway CE telemetry wrap skipped:\", e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 427,
   "id": "092c489e-b224-478d-b996-70da7979620a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _kpi_split_from_audit(ns: str = \"default\") -> Dict[str, Dict[str, Any]]:\n",
    "    \"\"\"\n",
    "    Read today's audit and compute KPI aggregates split by pipeline ('rerank' vs 'rrf').\n",
    "    Returns: { 'rerank': {...}, 'rrf': {...} }\n",
    "    \"\"\"\n",
    "    day = time.strftime(\"%Y%m%d\")\n",
    "    p = AUDIT_DIR / f\"answer_{day}.ndjson\"\n",
    "    groups = {\"rerank\": {\"lat\": [], \"faith\": [], \"ndcg\": [], \"n\": 0},\n",
    "              \"rrf\":    {\"lat\": [], \"faith\": [], \"ndcg\": [], \"n\": 0}}\n",
    "    if p.exists():\n",
    "        with p.open() as f:\n",
    "            for ln in f:\n",
    "                try:\n",
    "                    obj = json.loads(ln)\n",
    "                    if obj.get(\"namespace\", ns) != ns:\n",
    "                        continue\n",
    "                    pipe = (obj.get(\"pipeline\") or \"\").lower()\n",
    "                    if pipe not in groups:\n",
    "                        continue\n",
    "                    g = groups[pipe]\n",
    "                    if \"latency_ms\" in obj and isinstance(obj[\"latency_ms\"], (int, float)):\n",
    "                        g[\"lat\"].append(float(obj[\"latency_ms\"]))\n",
    "                    ver = obj.get(\"verify\") or {}\n",
    "                    if \"faithfulness\" in ver and isinstance(ver[\"faithfulness\"], (int, float)):\n",
    "                        g[\"faith\"].append(float(ver[\"faithfulness\"]))\n",
    "                    if \"ndcg10\" in obj and isinstance(obj[\"ndcg10\"], (int, float)):\n",
    "                        g[\"ndcg\"].append(float(obj[\"ndcg10\"]))\n",
    "                    g[\"n\"] += 1\n",
    "                except Exception:\n",
    "                    pass\n",
    "    def agg(vals: List[float], p: int) -> Optional[float]:\n",
    "        return _percentile(vals, p) if vals else None\n",
    "    out = {}\n",
    "    for k, v in groups.items():\n",
    "        out[k] = {\n",
    "            \"n\": v[\"n\"],\n",
    "            \"p50_ms\": agg(v[\"lat\"], 50),\n",
    "            \"p95_ms\": agg(v[\"lat\"], 95),\n",
    "            \"faithfulness_mean\": (sum(v[\"faith\"]) / len(v[\"faith\"])) if v[\"faith\"] else None,\n",
    "            \"ndcg10\": (sum(v[\"ndcg\"]) / len(v[\"ndcg\"])) if v[\"ndcg\"] else None,\n",
    "        }\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 428,
   "id": "cf383b69-c961-4edd-8301-b61e1d245208",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CanaryController:\n",
    "    \"\"\"\n",
    "    Manages a single namespace canary: small % to 'rerank' (treatment) vs 'rrf' (control).\n",
    "    Decides promote/rollback based on thresholds after N requests or T minutes.\n",
    "    \"\"\"\n",
    "    def __init__(self, namespace: str, percent: float = 2.0, request_goal: int = 400, max_minutes: int = 30,\n",
    "                 promote_p95_ms: float = 1800.0, min_faith: float = 0.85, max_ndcg_drop: float = 0.05):\n",
    "        self.ns = namespace.strip().lower()\n",
    "        self.percent = max(0.0, min(2.0, float(percent)))\n",
    "        self.request_goal = int(request_goal)\n",
    "        self.max_minutes = int(max_minutes)\n",
    "        self.promote_p95_ms = float(promote_p95_ms)\n",
    "        self.min_faith = float(min_faith)\n",
    "        self.max_ndcg_drop = float(max_ndcg_drop)\n",
    "        self.start_ts = None\n",
    "        self.done = False\n",
    "        self.decision = None\n",
    "        self.reason = None\n",
    "        self.metrics = {}\n",
    "        self._t: Optional[threading.Thread] = None\n",
    "        self._stop = threading.Event()\n",
    "    def _set_percent(self, pct: float):\n",
    "        try:\n",
    "            gw = RUNNING[\"gateway\"].base\n",
    "            requests.post(f\"{gw}/canary/set\", json={\"namespace\": self.ns, \"percent\": pct}, timeout=3)\n",
    "            record_canary_active(self.ns, pct, promoter=\"controller\", window={\"goal\": self.request_goal, \"max_minutes\": self.max_minutes})\n",
    "        except Exception:\n",
    "            pass\n",
    "    def _decide(self):\n",
    "        kpi = _kpi_split_from_audit(self.ns)\n",
    "        self.metrics = kpi\n",
    "        n_treat = (kpi.get(\"rerank\") or {}).get(\"n\", 0)\n",
    "        n_ctrl  = (kpi.get(\"rrf\") or {}).get(\"n\", 0)\n",
    "        elapsed_min = (time.time() - self.start_ts) / 60.0 if self.start_ts else 0.0\n",
    "        enough = (n_treat >= max(50, self.request_goal * 0.4)) and (n_ctrl >= max(50, self.request_goal * 0.4))\n",
    "        timeout = elapsed_min >= self.max_minutes\n",
    "        if not (enough or timeout):\n",
    "            return None  \n",
    "        t = kpi.get(\"rerank\") or {}\n",
    "        c = kpi.get(\"rrf\") or {}\n",
    "        p95_ok = (t.get(\"p95_ms\") is not None) and (t[\"p95_ms\"] <= self.promote_p95_ms)\n",
    "        faith_ok = (t.get(\"faithfulness_mean\") or 0.0) >= self.min_faith\n",
    "        ndcg_ok = True\n",
    "        if c.get(\"ndcg10\") is not None and t.get(\"ndcg10\") is not None:\n",
    "            ndcg_ok = (c[\"ndcg10\"] - t[\"ndcg10\"]) <= self.max_ndcg_drop\n",
    "        if p95_ok and faith_ok and ndcg_ok:\n",
    "            self.decision = \"promote\"\n",
    "            self.reason = \"thresholds passed\"\n",
    "            self._set_percent(self.percent)  \n",
    "        else:\n",
    "            self.decision = \"rollback\"\n",
    "            self.reason = f\"p95_ok={p95_ok}, faith_ok={faith_ok}, ndcg_ok={ndcg_ok}\"\n",
    "            self._set_percent(0.0)\n",
    "        try:\n",
    "            ov = RUNNING[\"overseer\"].base if \"overseer\" in RUNNING else None\n",
    "            if ov:\n",
    "                _ = requests.post(ov + \"/overseer/canary/decide\",\n",
    "                                  json={\"namespace\": self.ns,\n",
    "                                        \"p95_ms\": t.get(\"p95_ms\"), \"faithfulness_mean\": t.get(\"faithfulness_mean\"),\n",
    "                                        \"ndcg10\": t.get(\"ndcg10\"),\n",
    "                                        \"window_n\": n_treat + n_ctrl,\n",
    "                                        \"promote_threshold_p95_ms\": self.promote_p95_ms},\n",
    "                                  timeout=5).json()\n",
    "        except Exception:\n",
    "            pass\n",
    "        self.done = True\n",
    "        return self.decision\n",
    "    def _loop(self):\n",
    "        self.start_ts = time.time()\n",
    "        self._set_percent(self.percent)\n",
    "        while not self._stop.is_set() and not self.done:\n",
    "            try:\n",
    "                dec = self._decide()\n",
    "                if dec:\n",
    "                    break\n",
    "            except Exception:\n",
    "                pass\n",
    "            time.sleep(15)\n",
    "        try:\n",
    "            win = {\"elapsed_min\": round(((time.time() - self.start_ts) / 60.0), 2) if self.start_ts else None,\n",
    "                   \"n_rerank\": (self.metrics.get(\"rerank\") or {}).get(\"n\", 0),\n",
    "                   \"n_rrf\": (self.metrics.get(\"rrf\") or {}).get(\"n\", 0)}\n",
    "            status = \"promoted\" if self.decision == \"promote\" else (\"rolled_back\" if self.decision == \"rollback\" else \"closed\")\n",
    "            pg_record_canary(self.ns, (self.percent if self.decision == \"promote\" else 0.0), status, \"controller\", win)\n",
    "        except Exception:\n",
    "            pass\n",
    "    def start(self):\n",
    "        if self._t and self._t.is_alive():\n",
    "            return\n",
    "        self._stop.clear()\n",
    "        self._t = threading.Thread(target=self._loop, name=f\"canary-{self.ns}\", daemon=True)\n",
    "        self._t.start()\n",
    "    def stop(self):\n",
    "        self._stop.set()\n",
    "        if self._t and self._t.is_alive():\n",
    "            self._t.join(timeout=2.0)\n",
    "        self._set_percent(0.0)\n",
    "        self.done = True\n",
    "        self.decision = self.decision or \"stopped\"\n",
    "        self.reason = self.reason or \"manual stop\"\n",
    "    def status(self) -> Dict[str, Any]:\n",
    "        return {\n",
    "            \"namespace\": self.ns,\n",
    "            \"active\": (not self.done),\n",
    "            \"percent\": self.percent,\n",
    "            \"started_at\": self.start_ts,\n",
    "            \"elapsed_min\": round(((time.time() - self.start_ts) / 60.0), 2) if self.start_ts else None,\n",
    "            \"decision\": self.decision,\n",
    "            \"reason\": self.reason,\n",
    "            \"metrics\": self.metrics or _kpi_split_from_audit(self.ns),\n",
    "            \"goal_requests\": self.request_goal,\n",
    "            \"max_minutes\": self.max_minutes,\n",
    "            \"thresholds\": {\n",
    "                \"promote_p95_ms\": self.promote_p95_ms,\n",
    "                \"min_faith\": self.min_faith,\n",
    "                \"max_ndcg_drop\": self.max_ndcg_drop\n",
    "            }\n",
    "        }\n",
    "_CANARY_CTRL: Dict[str, CanaryController] = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 429,
   "id": "48424f08-3b38-43cb-95bf-71b5b2781987",
   "metadata": {},
   "outputs": [],
   "source": [
    "def start_canary(namespace: str = \"default\", percent: float = 2.0, request_goal: int = 400, max_minutes: int = 30,\n",
    "                 promote_p95_ms: float = 1800.0, min_faith: float = 0.85, max_ndcg_drop: float = 0.05) -> Dict[str, Any]:\n",
    "    ns = namespace.strip().lower()\n",
    "    if ns in _CANARY_CTRL and not _CANARY_CTRL[ns].done:\n",
    "        return {\"ok\": False, \"error\": \"canary already active\", \"status\": _CANARY_CTRL[ns].status()}\n",
    "    ctrl = CanaryController(ns, percent, request_goal, max_minutes, promote_p95_ms, min_faith, max_ndcg_drop)\n",
    "    _CANARY_CTRL[ns] = ctrl\n",
    "    ctrl.start()\n",
    "    return {\"ok\": True, \"status\": ctrl.status()}\n",
    "def stop_canary(namespace: str = \"default\") -> Dict[str, Any]:\n",
    "    ns = namespace.strip().lower()\n",
    "    c = _CANARY_CTRL.get(ns)\n",
    "    if not c:\n",
    "        return {\"ok\": False, \"error\": \"no canary for namespace\"}\n",
    "    c.stop()\n",
    "    return {\"ok\": True, \"status\": c.status()}\n",
    "def canary_status(namespace: Optional[str] = None) -> Dict[str, Any]:\n",
    "    if namespace:\n",
    "        ns = namespace.strip().lower()\n",
    "        c = _CANARY_CTRL.get(ns)\n",
    "        if not c:\n",
    "            return {\"ok\": False, \"error\": \"no canary for namespace\"}\n",
    "        return {\"ok\": True, \"status\": c.status()}\n",
    "    return {\"ok\": True, \"namespaces\": {k: v.status() for k, v in _CANARY_CTRL.items()}}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 430,
   "id": "256b9eba-2d9a-4db9-bc8d-9182522dfbfb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Overseer extended with /overseer/canary/{start,stop,status}\n",
      "Tips:\n",
      "  # start a canary for 20 minutes or 300 reqs (whichever first)\n",
      "  curl -s -X POST http://127.0.0.1:7000/overseer/canary/start -H 'Content-Type: application/json' -d '{\"namespace\":\"default\",\"percent\":2,\"request_goal\":300,\"max_minutes\":20}' | jq .\n",
      "  curl -s http://127.0.0.1:7000/overseer/canary/status | jq .\n"
     ]
    }
   ],
   "source": [
    "if \"overseer\" in RUNNING:\n",
    "    app = RUNNING[\"overseer\"].app\n",
    "    paths = {getattr(r, \"path\", \"\") for r in app.routes}\n",
    "    if \"/overseer/canary/start\" not in paths:\n",
    "        from pydantic import BaseModel\n",
    "        class StartReq(BaseModel):\n",
    "            namespace: str = \"default\"\n",
    "            percent: float = 2.0\n",
    "            request_goal: int = 400\n",
    "            max_minutes: int = 30\n",
    "            promote_p95_ms: float = 1800.0\n",
    "            min_faith: float = 0.85\n",
    "            max_ndcg_drop: float = 0.05\n",
    "        @app.post(\"/overseer/canary/start\")\n",
    "        def overseer_canary_start(body: StartReq):\n",
    "            return start_canary(body.namespace, body.percent, body.request_goal, body.max_minutes,\n",
    "                                body.promote_p95_ms, body.min_faith, body.max_ndcg_drop)\n",
    "        @app.post(\"/overseer/canary/stop\")\n",
    "        def overseer_canary_stop(payload: Dict[str, Any]):\n",
    "            ns = (payload.get(\"namespace\") or \"default\")\n",
    "            return stop_canary(ns)\n",
    "        @app.get(\"/overseer/canary/status\")\n",
    "        def overseer_canary_status(ns: Optional[str] = None):\n",
    "            return canary_status(ns)\n",
    "        print(\" Overseer extended with /overseer/canary/{start,stop,status}\")\n",
    "print(\"Tips:\")\n",
    "if \"overseer\" in RUNNING:\n",
    "    print(f\"  # start a canary for 20 minutes or 300 reqs (whichever first)\")\n",
    "    print(f\"  curl -s -X POST {RUNNING['overseer'].base}/overseer/canary/start -H 'Content-Type: application/json' \"\n",
    "          f\"-d '{{\\\"namespace\\\":\\\"default\\\",\\\"percent\\\":2,\\\"request_goal\\\":300,\\\"max_minutes\\\":20}}' | jq .\")\n",
    "    print(f\"  curl -s {RUNNING['overseer'].base}/overseer/canary/status | jq .\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 431,
   "id": "95db89cb-5a58-4d9e-a668-674e798be2f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import random\n",
    "from typing import Any, Dict, Optional, List, Tuple"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 432,
   "id": "419ad3a2-2c83-44ab-8137-45753475c006",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _canary_choose_random(self, ns: str) -> bool:\n",
    "    with self._lock:\n",
    "        pct = float(self.percent_by_ns[ns])\n",
    "        pick = (random.random() * 100.0) < pct\n",
    "        self.events[ns][\"rerank\" if pick else \"rrf\"] += 1\n",
    "        return pick\n",
    "try:\n",
    "    Canary.choose = _canary_choose_random\n",
    "except NameError:\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 433,
   "id": "0ade7809-5597-4ce1-bdb8-b9f8670b251b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from starlette.middleware.base import BaseHTTPMiddleware\n",
    "from starlette.responses import JSONResponse\n",
    "class AdminAuthMiddleware(BaseHTTPMiddleware):\n",
    "    def __init__(self, app, token_env: str = \"ADMIN_TOKEN\",\n",
    "                 header: str = \"X-Admin-Token\",\n",
    "                 prefixes: tuple[str, ...] = (\"/admin/\", \"/canary/\", \"/artifacts/\", \"/overseer\")):\n",
    "        super().__init__(app)\n",
    "        self.token = os.getenv(token_env)\n",
    "        self.header = header\n",
    "        self.prefixes = prefixes\n",
    "    async def dispatch(self, request, call_next):\n",
    "        path = request.url.path or \"/\"\n",
    "        if self.token and any(path.startswith(p) for p in self.prefixes):\n",
    "            provided = request.headers.get(self.header)\n",
    "            if provided != self.token:\n",
    "                return JSONResponse({\"error\": \"unauthorized\"}, status_code=401)\n",
    "        return await call_next(request)\n",
    "def attach_admin_auth(app):\n",
    "    app.add_middleware(AdminAuthMiddleware)\n",
    "for _name in (\"gateway\", \"overseer\", \"rag\"):\n",
    "    if _name in RUNNING:\n",
    "        try:\n",
    "            attach_admin_auth(RUNNING[_name].app)\n",
    "        except Exception:\n",
    "            pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 434,
   "id": "5aa6ffa5-1072-4cc7-b442-bd095639ac47",
   "metadata": {},
   "outputs": [],
   "source": [
    "import uuid\n",
    "from starlette.middleware.base import BaseHTTPMiddleware\n",
    "class RequestIdMiddleware(BaseHTTPMiddleware):\n",
    "    async def dispatch(self, request, call_next):\n",
    "        rid = request.headers.get(\"X-Request-Id\") or uuid.uuid4().hex\n",
    "        request.state.request_id = rid\n",
    "        response = await call_next(request)\n",
    "        response.headers[\"X-Request-Id\"] = rid\n",
    "        return response\n",
    "for _name in (\"gateway\", \"rag\", \"overseer\", \"hf\"):\n",
    "    if _name in RUNNING:\n",
    "        try:\n",
    "            RUNNING[_name].app.add_middleware(RequestIdMiddleware)\n",
    "        except Exception:\n",
    "            pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 435,
   "id": "f95ac7d1-f17d-48e3-a6ff-08f8ab471726",
   "metadata": {},
   "outputs": [],
   "source": [
    "_RE_PHONE = re.compile(r\"\\b(?:\\+?\\d{1,3}[-.\\s]?)?(?:\\(?\\d{3}\\)?[-.\\s]?)\\d{3}[-.\\s]?\\d{4}\\b\")\n",
    "_RE_CCARD = re.compile(r\"\\b(?:\\d[ -]*?){13,16}\\b\")\n",
    "def _luhn_ok(d: str) -> bool:\n",
    "    s = 0\n",
    "    alt = False\n",
    "    for ch in reversed(d):\n",
    "        if not ch.isdigit():\n",
    "            continue\n",
    "        n = ord(ch) - 48\n",
    "        if alt:\n",
    "            n *= 2\n",
    "            if n > 9:\n",
    "                n -= 9\n",
    "        s += n\n",
    "        alt = not alt\n",
    "    return (s % 10) == 0\n",
    "def _mask_cc(m: re.Match) -> str:\n",
    "    raw = re.sub(r\"[^\\d]\", \"\", m.group(0))\n",
    "    if not (13 <= len(raw) <= 16) or not _luhn_ok(raw):\n",
    "        return \"***\"\n",
    "    return \"**** **** **** \" + raw[-4:]\n",
    "def redact(s: str) -> str:\n",
    "    if not isinstance(s, str):\n",
    "        return s\n",
    "    s = _RE_EMAIL.sub(lambda m: f\"{m.group(1)}***@{m.group(3)}\", s)\n",
    "    s = _RE_SSN.sub(lambda m: f\"{m.group(1)}-**-****\", s)\n",
    "    s = _RE_PHONE.sub(\"***-***-****\", s)\n",
    "    s = _RE_CCARD.sub(_mask_cc, s)\n",
    "    return s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 436,
   "id": "9f6a0e41-0fcf-473a-9db7-1ebdd9a052de",
   "metadata": {},
   "outputs": [],
   "source": [
    "EXCLUDE_FILES_FROM_GC = {\"artifact_registry.json\"}\n",
    "EXCLUDE_DIRS_FROM_GC = {str(AUDIT_DIR), str(ART / \"reports\")}\n",
    "def start_ttl_gc_v2():\n",
    "    from pathlib import Path\n",
    "    _EXCL_DIRS = [Path(d).resolve() for d in EXCLUDE_DIRS_FROM_GC]\n",
    "    def _is_under(p: Path, root: Path) -> bool:\n",
    "        try:\n",
    "            p.resolve().relative_to(root)\n",
    "            return True\n",
    "        except Exception:\n",
    "            return False\n",
    "    def _loop():\n",
    "        while True:\n",
    "            try:\n",
    "                removed = []\n",
    "                for root in [ART, ART / \"artifacts\", ART / \"local\"]:\n",
    "                    if not root.exists():\n",
    "                        continue\n",
    "                    for p in root.rglob(\"*\"):\n",
    "                        try:\n",
    "                            if any(_is_under(p, d) for d in _EXCL_DIRS):\n",
    "                                continue\n",
    "                            if p.is_file():\n",
    "                                if p.name in EXCLUDE_FILES_FROM_GC:\n",
    "                                    continue\n",
    "                                if _too_old(p, ART_TTL_DAYS):\n",
    "                                    try:\n",
    "                                        p.unlink()\n",
    "                                        removed.append(str(p))\n",
    "                                    except Exception:\n",
    "                                        pass\n",
    "                        except Exception:\n",
    "                            pass\n",
    "                if removed:\n",
    "                    rpt = {\"ts\": int(time.time()), \"removed\": removed[:200]}\n",
    "                    (ART / \"reports\" / \"gc_last.json\").parent.mkdir(parents=True, exist_ok=True)\n",
    "                    (ART / \"reports\" / \"gc_last.json\").write_text(json.dumps(rpt, indent=2), encoding=\"utf-8\")\n",
    "                time.sleep(86400)\n",
    "            except Exception:\n",
    "                time.sleep(3600)\n",
    "    t = threading.Thread(target=_loop, name=\"ttl-gc-v2\", daemon=True)\n",
    "    t.start()\n",
    "    return t\n",
    "try:\n",
    "    _ = start_ttl_gc_v2()\n",
    "except Exception:\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 437,
   "id": "5d3ee74e-4397-464f-b35d-d5429a6e7fa8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _safe_retrievals(rag_base: str, query: str, top_k: int = 5, headers: Optional[Dict[str, str]] = None) -> Tuple[list[dict], dict, dict]:\n",
    "    import requests as _rq\n",
    "    dense, bm25 = [], []\n",
    "    d_meta, b_meta = {\"ok\": False}, {\"ok\": False}\n",
    "    try:\n",
    "        dj = _rq.post(f\"{rag_base}/retrieve_dense\", json={\"query\": query, \"top_k\": top_k}, headers=headers or {}, timeout=3).json()\n",
    "        dense = dj.get(\"results\", []) or []\n",
    "        d_meta = {\"ok\": True, \"latency_ms\": dj.get(\"latency_ms\")}\n",
    "    except Exception as e:\n",
    "        d_meta = {\"ok\": False, \"error\": str(e)}\n",
    "    try:\n",
    "        bj = _rq.post(f\"{rag_base}/retrieve_bm25\", json={\"query\": query, \"top_k\": top_k}, headers=headers or {}, timeout=3).json()\n",
    "        bm25 = bj.get(\"results\", []) or []\n",
    "        b_meta = {\"ok\": True, \"latency_ms\": bj.get(\"latency_ms\")}\n",
    "    except Exception as e:\n",
    "        b_meta = {\"ok\": False, \"error\": str(e)}\n",
    "    if not dense and not bm25:\n",
    "        raise RuntimeError(\"both retrievals failed\")\n",
    "    fused = rrf_fuse(dense, bm25, k=int(top_k))\n",
    "    return fused, d_meta, b_meta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 438,
   "id": "e7ae676d-54fd-49a6-9a81-8e7051b23bd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "if \"gateway\" in RUNNING:\n",
    "    try:\n",
    "        old_srv = RUNNING.pop(\"gateway\")\n",
    "        base = RUNNING[\"rag\"].base\n",
    "        app = old_srv.app\n",
    "        paths = [r.path for r in app.routes]\n",
    "        idx = paths.index(\"/answer\")\n",
    "        orig_answer = app.routes[idx].endpoint\n",
    "        app.routes.pop(idx)\n",
    "        from fastapi import Request as _FRequest, HTTPException as _FHTTP\n",
    "        from fastapi.responses import JSONResponse as _JSONResponse\n",
    "        import requests as _rq\n",
    "        @app.post(\"/answer\")\n",
    "        def answer_graceful(request: _FRequest, body: Dict[str, Any]):\n",
    "            t0 = time.time()\n",
    "            ns = (request.headers.get(\"X-Namespace\") or request.query_params.get(\"ns\") or \"default\").strip().lower()\n",
    "            query = body.get(\"query\", \"\")\n",
    "            if not isinstance(query, str) or not query.strip():\n",
    "                raise _FHTTP(400, \"query required\")\n",
    "            top_k = int(body.get(\"top_k\", 5))\n",
    "            use_rerank = body.get(\"use_rerank\")\n",
    "            rid = getattr(request.state, \"request_id\", None)\n",
    "            fwd_headers = {\"X-Request-Id\": rid} if rid else {}\n",
    "            try:\n",
    "                fused, d_meta, b_meta = _safe_retrievals(base, query, top_k=top_k, headers=fwd_headers)\n",
    "            except Exception as e:\n",
    "                raise _FHTTP(502, f\"preflight_failed: {e}\")\n",
    "            force_rrf = (use_rerank is False) or not (d_meta.get(\"ok\") and b_meta.get(\"ok\"))\n",
    "            if force_rrf:\n",
    "                try:\n",
    "                    comp = _rq.post(\n",
    "                        f\"{base}/complete\",\n",
    "                        json={\"query\": query, \"contexts\": fused},\n",
    "                        headers=fwd_headers,\n",
    "                        timeout=5,\n",
    "                    ).json()\n",
    "                    payload = {\n",
    "                        \"answer\": comp.get(\"answer\", \"\"),\n",
    "                        \"contexts\": fused,\n",
    "                        \"verify\": comp.get(\"verify\", {}),\n",
    "                        \"meta\": {\n",
    "                            \"pipeline\": \"rrf\",\n",
    "                            \"latency_ms\": int((time.time() - t0) * 1000),\n",
    "                            \"cache\": \"miss\",\n",
    "                            \"partial_retrieval\": {\"dense_ok\": d_meta.get(\"ok\"), \"bm25_ok\": b_meta.get(\"ok\")},\n",
    "                        },\n",
    "                    }\n",
    "                    return _JSONResponse(payload)\n",
    "                except Exception as e:\n",
    "                    raise _FHTTP(502, f\"complete_failed: {e}\")\n",
    "            resp: _JSONResponse = orig_answer(request, {\"query\": query, \"top_k\": top_k, \"use_rerank\": use_rerank})\n",
    "            payload = json.loads(resp.body.decode(\"utf-8\")) if isinstance(resp.body, (bytes, bytearray)) else resp.body\n",
    "            return _JSONResponse(payload)\n",
    "        old_srv.stop()\n",
    "        srv = ServerThread(app, preferred_port=None, name=\"gw\", log_level=\"warning\")\n",
    "        srv.start()\n",
    "        RUNNING[\"gateway\"] = srv\n",
    "        try:\n",
    "            attach_admin_auth(RUNNING[\"gateway\"].app)\n",
    "        except Exception:\n",
    "            pass\n",
    "        print(\" Gateway /answer now degrades gracefully on retrieval errors:\", srv.base)\n",
    "    except Exception as e:\n",
    "        print(\"graceful retrieval wrapper skipped:\", e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 439,
   "id": "ee1a8c40-d4f3-4c46-9cc0-f46d48c91215",
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    _CANARY_CTRL\n",
    "except NameError:\n",
    "    _CANARY_CTRL = {}\n",
    "try:\n",
    "    CanaryController\n",
    "except NameError:\n",
    "    class CanaryController:\n",
    "        def __init__(self, ns, **kw):\n",
    "            self.ns = ns\n",
    "            self.done = False\n",
    "        def start(self): self.done = False\n",
    "        def stop(self): self.done = True\n",
    "        def status(self): return {\"namespace\": self.ns, \"active\": not self.done}\n",
    "def start_canary(namespace: str = \"default\", percent: float = 2.0, request_goal: int = 400, max_minutes: int = 30,\n",
    "                 promote_p95_ms: float = 1800.0, min_faith: float = 0.85, max_ndcg_drop: float = 0.05) -> Dict[str, Any]:\n",
    "    ns = (namespace or \"default\").strip().lower()\n",
    "    if ns in _CANARY_CTRL and not _CANARY_CTRL[ns].done:\n",
    "        return {\"ok\": False, \"error\": \"canary already active\", \"status\": _CANARY_CTRL[ns].status()}\n",
    "    ctrl = CanaryController(ns, percent=percent, request_goal=request_goal, max_minutes=max_minutes,\n",
    "                            promote_p95_ms=promote_p95_ms, min_faith=min_faith, max_ndcg_drop=max_ndcg_drop)\n",
    "    _CANARY_CTRL[ns] = ctrl\n",
    "    ctrl.start()\n",
    "    return {\"ok\": True, \"status\": ctrl.status()}\n",
    "def stop_canary(namespace: str = \"default\") -> Dict[str, Any]:\n",
    "    ns = (namespace or \"default\").strip().lower()\n",
    "    ctrl = _CANARY_CTRL.get(ns)\n",
    "    if not ctrl:\n",
    "        return {\"ok\": False, \"error\": \"no canary for namespace\"}\n",
    "    ctrl.stop()\n",
    "    return {\"ok\": True, \"status\": ctrl.status()}\n",
    "def canary_status(namespace: str = \"default\") -> Dict[str, Any]:\n",
    "    ns = (namespace or \"default\").strip().lower()\n",
    "    ctrl = _CANARY_CTRL.get(ns)\n",
    "    if not ctrl:\n",
    "        return {\"ok\": False, \"error\": \"no canary for namespace\"}\n",
    "    return {\"ok\": True, \"status\": ctrl.status()}\n",
    "if \"gateway\" in RUNNING:\n",
    "    app = RUNNING[\"gateway\"].app\n",
    "    existing = {getattr(r, \"path\", \"\") for r in app.routes}\n",
    "    if \"/canary/start\" not in existing:\n",
    "        from pydantic import BaseModel, Field\n",
    "        class CanaryStartReq(BaseModel):\n",
    "            namespace: str = Field(default=\"default\")\n",
    "            percent: float = Field(default=2.0, le=2.0, ge=0.0)\n",
    "            request_goal: int = 400\n",
    "            max_minutes: int = 30\n",
    "            promote_p95_ms: float = 1800.0\n",
    "            min_faith: float = 0.85\n",
    "            max_ndcg_drop: float = 0.05\n",
    "        @app.post(\"/canary/start\")\n",
    "        def _canary_start(body: CanaryStartReq):\n",
    "            return start_canary(namespace=body.namespace, percent=body.percent, request_goal=body.request_goal,\n",
    "                                max_minutes=body.max_minutes, promote_p95_ms=body.promote_p95_ms,\n",
    "                                min_faith=body.min_faith, max_ndcg_drop=body.max_ndcg_drop)\n",
    "        @app.post(\"/canary/stop\")\n",
    "        def _canary_stop(namespace: str = \"default\"):\n",
    "            return stop_canary(namespace=namespace)\n",
    "        @app.get(\"/canary/status\")\n",
    "        def _canary_status(namespace: str = \"default\"):\n",
    "            return canary_status(namespace=namespace)\n",
    "        print(\" Canary controller endpoints mounted at /canary/*\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 440,
   "id": "bc345f32-2dec-4d9f-8fb8-9197f1e2efde",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _cap_limit(stmt: str, max_limit: int) -> str:\n",
    "    def repl(m):\n",
    "        try:\n",
    "            n = int(m.group(1))\n",
    "        except Exception:\n",
    "            return m.group(0)\n",
    "        return f\"LIMIT {max_limit}\" if n > max_limit else m.group(0)\n",
    "    return re.sub(r\"(?is)\\blimit\\s+(\\d+)\\b\", repl, stmt)\n",
    "def sanitize_select_sql_one(sql: str, max_limit: int = 100) -> Tuple[bool, str | None]:\n",
    "    try:\n",
    "        s = re.sub(r\"/\\*.*?\\*/\", \"\", sql or \"\", flags=re.DOTALL)\n",
    "        s = re.sub(r\"(?m)^[ \\t]*--.*?$\", \"\", s).strip()\n",
    "        parts = [p.strip() for p in s.split(\";\") if p.strip()]\n",
    "        if len(parts) != 1:\n",
    "            return False, \"multiple statements not allowed\"\n",
    "        stmt = parts[0]\n",
    "        if not re.match(r\"(?is)^\\s*select\\b\", stmt):\n",
    "            return False, \"only SELECT statements allowed\"\n",
    "        upper = re.sub(r\"'[^']*'|\\\"[^\\\"]*\\\"\", \"\", stmt).upper()\n",
    "        for bad in {\"DROP\",\"TRUNCATE\",\"DELETE\",\"UPDATE\",\"INSERT\",\"ALTER\",\"CREATE\",\"RENAME\",\"GRANT\",\"REVOKE\",\"MERGE\"}:\n",
    "            if re.search(rf\"\\b{bad}\\b\", upper):\n",
    "                return False, f\"disallowed keyword: {bad}\"\n",
    "        if not re.search(r\"(?is)\\blimit\\s+\\d+\\b\", stmt):\n",
    "            stmt = stmt.rstrip() + f\" LIMIT {int(max_limit)}\"\n",
    "        else:\n",
    "            stmt = _cap_limit(stmt, int(max_limit))\n",
    "        return True, stmt\n",
    "    except Exception as e:\n",
    "        return False, str(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 441,
   "id": "6b2fbcfe-fc7c-408d-9989-f86adade7445",
   "metadata": {},
   "outputs": [],
   "source": [
    "if \"rag\" in RUNNING:\n",
    "    app = RUNNING[\"rag\"].app\n",
    "    try:\n",
    "        from fastapi import HTTPException as _FHTTP\n",
    "        @app.post(\"/sql/sanitize\")\n",
    "        def sql_sanitize_unified(req: Dict[str, Any]):\n",
    "            ok, res = sanitize_select_sql_one(req.get(\"text\",\"\"), max_limit=int(req.get(\"max_limit\") or 100))\n",
    "            if not ok:\n",
    "                return {\"ok\": False, \"error\": res or \"invalid\"}\n",
    "            return {\"ok\": True, \"sql\": res}\n",
    "    except Exception:\n",
    "        pass\n",
    "    try:\n",
    "        @app.post(\"/sql/execute\")\n",
    "        def sql_execute_unified(body: Dict[str, Any]):\n",
    "            ok, res = sanitize_select_sql_one(body.get(\"text\",\"\"), max_limit=100)\n",
    "            if not ok:\n",
    "                return {\"ok\": False, \"error\": res}\n",
    "            ns  = (body.get(\"namespace\") or \"default\").strip().lower()\n",
    "            rec = {\n",
    "                \"ts\": int(time.time()),\n",
    "                \"namespace\": ns,\n",
    "                \"sql\": _sql_redact_literals(res),\n",
    "                \"raw_hash\": hashlib.sha256((body.get(\"text\") or \"\").encode(\"utf-8\")).hexdigest()[:16],\n",
    "            }\n",
    "            with SQL_AUDIT.open(\"a\", encoding=\"utf-8\") as f:\n",
    "                f.write(json.dumps(rec) + \"\\n\")\n",
    "            return {\"ok\": True, \"sql\": res, \"dry_run\": True}\n",
    "    except Exception:\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 442,
   "id": "0d68fd05-4759-4958-aafb-016ba954b19f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from fastapi import FastAPI, Request, HTTPException\n",
    "from fastapi.responses import PlainTextResponse, JSONResponse\n",
    "from typing import Any, Dict, Optional, List\n",
    "import json, time, os, threading, re\n",
    "from pathlib import Path\n",
    "try:\n",
    "    from prometheus_client import (\n",
    "        CollectorRegistry,\n",
    "        Histogram as _Hist,\n",
    "        generate_latest as _gen_latest,\n",
    "        CONTENT_TYPE_LATEST as _ct,\n",
    "        REGISTRY as _DEFAULT_REGISTRY,\n",
    "    )\n",
    "    _HAVE_PROM = True\n",
    "except Exception:\n",
    "    _HAVE_PROM = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 443,
   "id": "86cb376c-cabf-406f-a763-7ae655289862",
   "metadata": {},
   "outputs": [],
   "source": [
    "def enable_rag_prometheus_wrapper_fixed():\n",
    "    \"\"\"\n",
    "    Replace the running RAG server with a wrapper that:\n",
    "      - uses its OWN CollectorRegistry (no duplicate timeseries)\n",
    "      - adds per-route latency histogram\n",
    "      - exposes /metrics\n",
    "      - mounts the original RAG app under \"/\"\n",
    "    Safe to call multiple times.\n",
    "    \"\"\"\n",
    "    if \"rag\" not in RUNNING:\n",
    "        raise RuntimeError(\"RAG service not running\")\n",
    "    if not _HAVE_PROM:\n",
    "        print(\"[rag-prom] prometheus_client not installed; skipping\")\n",
    "        return\n",
    "    old_srv = RUNNING.pop(\"rag\")\n",
    "    inner_app = getattr(old_srv.app.state, \"_rag_inner_app\", old_srv.app)\n",
    "    try:\n",
    "        old_srv.stop()\n",
    "    except Exception:\n",
    "        pass\n",
    "    reg = CollectorRegistry()\n",
    "    RAG_LAT_MS = _Hist(\n",
    "        \"rag_route_latency_ms\",\n",
    "        \"RAG route latency (ms)\",\n",
    "        [\"path\"],\n",
    "        buckets=[5,10,20,40,80,160,320,640,1280,2560,5120],\n",
    "        registry=reg,\n",
    "    )\n",
    "    wrapper = FastAPI(title=\"RAG (Prometheus-wrapped)\", version=\"0.2\")\n",
    "    wrapper.state._is_rag_wrapper = True\n",
    "    wrapper.state._rag_inner_app = inner_app\n",
    "    wrapper.state._rag_registry = reg\n",
    "    wrapper.state._rag_hist = RAG_LAT_MS\n",
    "    _INSTRUMENTED = {\n",
    "        \"/up\",\n",
    "        \"/retrieve_dense\",\n",
    "        \"/retrieve_bm25\",\n",
    "        \"/rerank\",\n",
    "        \"/complete\",\n",
    "        \"/sql/sanitize\",\n",
    "        \"/sql/execute\",\n",
    "        \"/schema/profile\",\n",
    "        \"/schema/validate\",\n",
    "    }\n",
    "    @wrapper.middleware(\"http\")\n",
    "    async def rag_prom_mw(request: Request, call_next):\n",
    "        t0 = time.time()\n",
    "        resp = await call_next(request)\n",
    "        try:\n",
    "            dt_ms = (time.time() - t0) * 1000.0\n",
    "            path = request.url.path\n",
    "            if path in _INSTRUMENTED:\n",
    "                RAG_LAT_MS.labels(path).observe(dt_ms)\n",
    "        except Exception:\n",
    "            pass\n",
    "        return resp\n",
    "    @wrapper.get(\"/metrics\")\n",
    "    def rag_metrics():\n",
    "        return PlainTextResponse(_gen_latest(reg), media_type=_ct)\n",
    "    wrapper.mount(\"/\", inner_app)\n",
    "    new_srv = ServerThread(wrapper, preferred_port=None, name=\"rag\", log_level=\"warning\")\n",
    "    new_srv.start()\n",
    "    RUNNING[\"rag\"] = new_srv\n",
    "    try:\n",
    "        attach_admin_auth(RUNNING[\"rag\"].app)\n",
    "    except Exception:\n",
    "        pass\n",
    "    print(\" RAG Prometheus wrapper enabled at\", new_srv.base, \" /metrics\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 444,
   "id": "90902d67-20b3-4eab-b11d-aa0162713ca3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def enable_gateway_breaker_and_cost_wrapper_fixed():\n",
    "    \"\"\"\n",
    "    Wrap the existing gateway with:\n",
    "      - Circuit breaker stale-while-open for /answer using RESP_CACHE\n",
    "      - Cost-per-request histogram (private registry) + expose at /metrics-extra\n",
    "      - Error auditing middleware (non-200 on /answer)\n",
    "    We DO NOT mutate the inner app (no post-start middleware). Safe to call repeatedly.\n",
    "    \"\"\"\n",
    "    if \"gateway\" not in RUNNING:\n",
    "        raise RuntimeError(\"Gateway service not running\")\n",
    "    old_srv = RUNNING.pop(\"gateway\")\n",
    "    inner_app = getattr(old_srv.app.state, \"_gw_inner_app\", old_srv.app)\n",
    "    try:\n",
    "        old_srv.stop()\n",
    "    except Exception:\n",
    "        pass\n",
    "    paths = [getattr(r, \"path\", \"\") for r in inner_app.routes]\n",
    "    if \"/answer\" not in paths:\n",
    "        raise RuntimeError(\"Inner gateway missing /answer route\")\n",
    "    idx = paths.index(\"/answer\")\n",
    "    orig_answer = inner_app.routes[idx].endpoint\n",
    "    wrapper = FastAPI(title=\"Gateway (breaker+cost wrapped)\", version=\"0.2\")\n",
    "    wrapper.state._is_gw_wrapper = True\n",
    "    wrapper.state._gw_inner_app = inner_app\n",
    "    gw_reg = CollectorRegistry() if _HAVE_PROM else None\n",
    "    GW_COST_PER_REQ = None\n",
    "    if _HAVE_PROM:\n",
    "        try:\n",
    "            GW_COST_PER_REQ = _Hist(\n",
    "                \"py_gateway_cost_per_req_usd\",\n",
    "                \"Gateway cost per request (USD)\",\n",
    "                buckets=[0.00001, 0.0001, 0.001, 0.005, 0.01, 0.05, 0.1],\n",
    "                registry=gw_reg,\n",
    "            )\n",
    "        except Exception:\n",
    "            GW_COST_PER_REQ = None\n",
    "    @wrapper.middleware(\"http\")\n",
    "    async def gw_error_audit_mw(request: Request, call_next):\n",
    "        t0 = time.time()\n",
    "        resp = None\n",
    "        try:\n",
    "            resp = await call_next(request)\n",
    "            return resp\n",
    "        finally:\n",
    "            try:\n",
    "                if request.url.path == \"/answer\":\n",
    "                    ns = _ns_from(request)\n",
    "                    code = getattr(resp, \"status_code\", 500) if resp is not None else 500\n",
    "                    if code >= 400:\n",
    "                        _audit_write(ns, {\"error\": True, \"status\": int(code), \"kpi\": True,\n",
    "                                          \"latency_ms\": int((time.time()-t0)*1000)})\n",
    "            except Exception:\n",
    "                pass\n",
    "    @wrapper.post(\"/answer\")\n",
    "    def answer_breaker_cost(request: Request, body: Dict[str, Any]):\n",
    "        ns = (request.headers.get(\"X-Namespace\") or request.query_params.get(\"ns\") or \"default\").strip().lower()\n",
    "        q  = (body.get(\"query\") or \"\")\n",
    "        if not isinstance(q, str) or not q.strip():\n",
    "            raise HTTPException(400, \"query required\")\n",
    "        top_k = int(body.get(\"top_k\", 5))\n",
    "        use_rerank = body.get(\"use_rerank\", None)\n",
    "        if CB.open():\n",
    "            key = json.dumps(\n",
    "                {\"ns\": ns, \"q\": _norm_query(q), \"k\": top_k, \"rr\": bool(use_rerank if use_rerank is not None else True)},\n",
    "                sort_keys=True\n",
    "            )\n",
    "            cached = RESP_CACHE.get(key)\n",
    "            if cached is not None:\n",
    "                payload = dict(cached)\n",
    "                payload.setdefault(\"meta\", {})[\"breaker\"] = \"stale\"\n",
    "                payload[\"meta\"][\"cache\"] = \"hit\"\n",
    "                _audit_write(ns, {\"query\": redact(q), \"pipeline\": (payload.get(\"meta\") or {}).get(\"pipeline\",\"\"),\n",
    "                                  \"latency_ms\": 0, \"cache\": \"hit\", \"breaker\": \"stale\", \"kpi\": True})\n",
    "                return JSONResponse(payload)\n",
    "            raise HTTPException(status_code=503, detail={\"error\":\"breaker_open\",\"retry_after\":30})\n",
    "        t0 = time.time()\n",
    "        try:\n",
    "            resp = orig_answer(request, body)\n",
    "        except HTTPException as e:\n",
    "            _audit_write(ns, {\"query\": redact(q), \"error\": True, \"status\": int(getattr(e, \"status_code\", 500)),\n",
    "                              \"latency_ms\": int((time.time()-t0)*1000), \"kpi\": True})\n",
    "            raise\n",
    "        except Exception as e:\n",
    "            _audit_write(ns, {\"query\": redact(q), \"error\": True, \"status\": 500, \"detail\": str(e),\n",
    "                              \"latency_ms\": int((time.time()-t0)*1000), \"kpi\": True})\n",
    "            raise\n",
    "        try:\n",
    "            payload = json.loads(resp.body.decode(\"utf-8\")) if isinstance(resp.body, (bytes, bytearray)) else resp.body\n",
    "        except Exception:\n",
    "            return resp\n",
    "        req_cost = 0.0\n",
    "        meta = payload.setdefault(\"meta\", {})\n",
    "        pipeline = (meta.get(\"pipeline\") or \"rrf\").lower()\n",
    "        try:\n",
    "            if pipeline == \"rerank\" and not meta.get(\"budget_blocked_ce\"):\n",
    "                req_cost += float(PRICE_RERANK_PER_DOC) * float(top_k)\n",
    "        except Exception:\n",
    "            pass\n",
    "        esc = (meta.get(\"escalation\") or {})\n",
    "        try:\n",
    "            req_cost += float(esc.get(\"charged\") or 0.0)\n",
    "        except Exception:\n",
    "            pass\n",
    "        meta[\"cost_usd\"] = round(req_cost, 8)\n",
    "        if GW_COST_PER_REQ:\n",
    "            try:\n",
    "                GW_COST_PER_REQ.observe(req_cost)\n",
    "            except Exception:\n",
    "                pass\n",
    "        _audit_write(ns, {\"query\": redact(q), \"pipeline\": pipeline, \"latency_ms\": meta.get(\"latency_ms\"),\n",
    "                          \"cache\": meta.get(\"cache\"), \"cost_usd\": meta.get(\"cost_usd\"), \"kpi\": True})\n",
    "        return JSONResponse(payload)\n",
    "    if _HAVE_PROM:\n",
    "        @wrapper.get(\"/metrics-extra\")\n",
    "        def gw_metrics_extra():\n",
    "            try:\n",
    "                inner = _gen_latest(_DEFAULT_REGISTRY)\n",
    "            except Exception:\n",
    "                inner = b\"\"\n",
    "            try:\n",
    "                extra = _gen_latest(gw_reg)\n",
    "            except Exception:\n",
    "                extra = b\"\"\n",
    "            return PlainTextResponse(inner + extra, media_type=_ct)\n",
    "    wrapper.mount(\"/\", inner_app)\n",
    "    new_srv = ServerThread(wrapper, preferred_port=None, name=\"gw\", log_level=\"warning\")\n",
    "    new_srv.start()\n",
    "    RUNNING[\"gateway\"] = new_srv\n",
    "    try:\n",
    "        attach_admin_auth(RUNNING[\"gateway\"].app)\n",
    "    except Exception:\n",
    "        pass\n",
    "    print(\" Gateway breaker+cost wrapper active at\", new_srv.base, \"(extra metrics  /metrics-extra)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 445,
   "id": "ac79325c-49ac-46ad-957f-9c50208875d2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Rollup override loaded (computes error_rate).\n"
     ]
    }
   ],
   "source": [
    "def build_rollup_from_audit(day: Optional[str]=None, ns: str=\"default\") -> Dict[str,Any]:\n",
    "    \"\"\"\n",
    "    Replaces the prior implementation.\n",
    "    Aggregates p50/p95/p99, error_rate, faithfulness_mean, ndcg10, cache_hit_rate, psi.\n",
    "    Consumes today's audit (including kpi entries).\n",
    "    \"\"\"\n",
    "    day = day or time.strftime(\"%Y%m%d\")\n",
    "    p = AUDIT_DIR / f\"answer_{day}.ndjson\"\n",
    "    lats: List[float] = []\n",
    "    faiths: List[float] = []\n",
    "    ndcgs: List[float] = []\n",
    "    cache_hits = 0\n",
    "    total = 0\n",
    "    err = 0\n",
    "    if p.exists():\n",
    "        with p.open() as f:\n",
    "            for ln in f:\n",
    "                try:\n",
    "                    obj = json.loads(ln)\n",
    "                    if (obj.get(\"namespace\") or ns) != ns:\n",
    "                        continue\n",
    "                    total += 1\n",
    "                    if \"latency_ms\" in obj:\n",
    "                        lats.append(float(obj.get(\"latency_ms\") or 0))\n",
    "                    if obj.get(\"cache\") == \"hit\":\n",
    "                        cache_hits += 1\n",
    "                    if obj.get(\"error\") is True or int(obj.get(\"status\", 0) or 0) >= 400:\n",
    "                        err += 1\n",
    "                    ver = obj.get(\"verify\") or {}\n",
    "                    if \"faithfulness\" in ver and isinstance(ver[\"faithfulness\"], (int, float)):\n",
    "                        faiths.append(float(ver[\"faithfulness\"]))\n",
    "                    if \"ndcg10\" in obj and isinstance(obj[\"ndcg10\"], (int, float)):\n",
    "                        ndcgs.append(float(obj[\"ndcg10\"]))\n",
    "                except Exception:\n",
    "                    pass\n",
    "    roll = {\n",
    "        \"day\": f\"{day[:4]}-{day[4:6]}-{day[6:]}\",\n",
    "        \"ns\": ns,\n",
    "        \"p50_ms\": _percentile(lats, 50) if lats else None,\n",
    "        \"p95_ms\": _percentile(lats, 95) if lats else None,\n",
    "        \"p99_ms\": _percentile(lats, 99) if lats else None,\n",
    "        \"error_rate\": (err/total) if total else None,\n",
    "        \"faithfulness_mean\": (sum(faiths)/len(faiths)) if faiths else None,\n",
    "        \"ndcg10\": (sum(ndcgs)/len(ndcgs)) if ndcgs else None,\n",
    "        \"cost_per_req\": None,\n",
    "        \"cache_hit_rate\": (cache_hits/total) if total else None,\n",
    "        \"psi\": drift_psi_from_audit(day, ns)\n",
    "    }\n",
    "    pg_upsert_rollup(roll[\"day\"], ns, roll)\n",
    "    return roll\n",
    "print(\" Rollup override loaded (computes error_rate).\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 446,
   "id": "1205fb1b-698b-4db1-b911-52f0f9603811",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Cold-spill cron scheduled daily at 01:30 (after 3d)\n"
     ]
    }
   ],
   "source": [
    "COLD_SPILL_DAYS = int(os.getenv(\"COLD_SPILL_AFTER_DAYS\", \"3\"))\n",
    "def _indices_dir() -> Path:\n",
    "    d = ART / \"local\" / \"indices\"\n",
    "    (d / \"hot\").mkdir(parents=True, exist_ok=True)\n",
    "    (d / \"cold\").mkdir(parents=True, exist_ok=True)\n",
    "    return d\n",
    "def _age_days(p: Path) -> float:\n",
    "    try:\n",
    "        return max(0.0, (time.time() - p.stat().st_mtime) / 86400.0)\n",
    "    except Exception:\n",
    "        return 0.0\n",
    "def cold_spill_vectors(namespace: str = \"default\", after_days: int = COLD_SPILL_DAYS, dry_run: bool = False) -> Dict[str, Any]:\n",
    "    \"\"\"\n",
    "    Move old *.faiss/*.index from local indices/hot  indices/cold and update registry.\n",
    "    Upload target to S3 (or file:// fallback). Returns summary.\n",
    "    \"\"\"\n",
    "    idx = _indices_dir()\n",
    "    hot = idx / \"hot\"\n",
    "    cold = idx / \"cold\"\n",
    "    moved = []\n",
    "    for p in list(hot.glob(\"**/*\")):\n",
    "        if not p.is_file():\n",
    "            continue\n",
    "        if not any(p.name.endswith(ext) for ext in (\".faiss\", \".index\", \".ivf\", \".pq\", \".hnsw\")):\n",
    "            continue\n",
    "        if _age_days(p) < after_days:\n",
    "            continue\n",
    "        target = cold / p.name\n",
    "        if not dry_run:\n",
    "            try:\n",
    "                p.rename(target)\n",
    "            except Exception:\n",
    "                continue\n",
    "        uri = s3_put(target, \"indices/cold/\")\n",
    "        rec = register_faiss_uri(namespace, uri, tier=\"cold\")\n",
    "        moved.append({\"local\": str(target), \"uri\": uri, \"rec\": rec})\n",
    "    webhook = os.getenv(\"SLACK_WEBHOOK\") or os.getenv(\"SLO_ALERT_WEBHOOK\")\n",
    "    if webhook and moved:\n",
    "        try:\n",
    "            _post_webhook(webhook, {\"type\": \"cold_spill\", \"ns\": namespace, \"moved\": len(moved)})\n",
    "        except Exception:\n",
    "            pass\n",
    "    return {\"ok\": True, \"moved\": moved, \"after_days\": after_days}\n",
    "def start_cold_spill_cron(namespace: str = \"default\", when_hhmm: str = \"01:30\", after_days: int = COLD_SPILL_DAYS):\n",
    "    if RUNNING.get(\"cold_spill\"):\n",
    "        print(\"cold-spill cron already running\")\n",
    "        return RUNNING[\"cold_spill\"]\n",
    "    hh, mm = map(int, when_hhmm.split(\":\"))\n",
    "    def _loop():\n",
    "        while True:\n",
    "            now = time.localtime()\n",
    "            target = time.mktime((now.tm_year, now.tm_mon, now.tm_mday, hh, mm, 0, now.tm_wday, now.tm_yday, now.tm_isdst))\n",
    "            if target <= time.mktime(now):\n",
    "                target += 86400\n",
    "            time.sleep(max(0, target - time.mktime(now)))\n",
    "            try:\n",
    "                cold_spill_vectors(namespace, after_days=after_days, dry_run=False)\n",
    "            except Exception:\n",
    "                pass\n",
    "    t = threading.Thread(target=_loop, name=\"cold-spill\", daemon=True)\n",
    "    t.start()\n",
    "    RUNNING[\"cold_spill\"] = t\n",
    "    print(f\" Cold-spill cron scheduled daily at {when_hhmm} (after {after_days}d)\")\n",
    "    return t\n",
    "_ = start_cold_spill_cron(namespace=\"default\", when_hhmm=os.getenv(\"COLD_SPILL_HHMM\",\"01:30\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 447,
   "id": "80724558-4913-4122-b7fe-a48765518679",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " DB read-only helpers ready (pg_readonly_session(ns)).\n"
     ]
    }
   ],
   "source": [
    "def pg_readonly_session(ns: str):\n",
    "    \"\"\"\n",
    "    Context manager that enforces a read-only role & tx safety when you later enable\n",
    "    real SQL execution. No-op if PG not configured.\n",
    "    Usage:\n",
    "        with pg_readonly_session(\"acme\") as cur:\n",
    "            cur.execute(\"SELECT ...\")\n",
    "    \"\"\"\n",
    "    conn = _pg_connect()\n",
    "    if not conn:\n",
    "        from contextlib import contextmanager\n",
    "        @contextmanager\n",
    "        def _noop():\n",
    "            yield None\n",
    "        return _noop()\n",
    "    import contextlib\n",
    "    @contextlib.contextmanager\n",
    "    def _ctx():\n",
    "        cur = conn.cursor()\n",
    "        try:\n",
    "            cur.execute(\"SET default_transaction_read_only = on;\")\n",
    "            cur.execute(\"SET SESSION CHARACTERISTICS AS TRANSACTION READ ONLY;\")\n",
    "            try:\n",
    "                cur.execute(f\"SET ROLE read_only_{ns};\")\n",
    "            except Exception:\n",
    "                pass\n",
    "            yield cur\n",
    "        finally:\n",
    "            try:\n",
    "                cur.execute(\"RESET ROLE;\")\n",
    "                cur.execute(\"SET default_transaction_read_only = off;\")\n",
    "            except Exception:\n",
    "                pass\n",
    "            try:\n",
    "                cur.close()\n",
    "            except Exception:\n",
    "                pass\n",
    "    return _ctx()\n",
    "print(\" DB read-only helpers ready (pg_readonly_session(ns)).\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 448,
   "id": "26d480aa-675b-46b3-b4d2-2f2aa7962014",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " RAG Prometheus wrapper enabled at http://127.0.0.1:54393  /metrics\n",
      "[init] gateway wrapper: Gateway service not running\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    enable_rag_prometheus_wrapper_fixed()\n",
    "except Exception as e:\n",
    "    print(\"[init] rag wrapper:\", e)\n",
    "try:\n",
    "    enable_gateway_breaker_and_cost_wrapper_fixed()\n",
    "except Exception as e:\n",
    "    print(\"[init] gateway wrapper:\", e)\n",
    "for _name in (\"gateway\", \"rag\", \"overseer\"):\n",
    "    if _name in RUNNING:\n",
    "        try:\n",
    "            attach_admin_auth(RUNNING[_name].app)\n",
    "        except Exception:\n",
    "            pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 449,
   "id": "d9cbc69c-0bef-48ad-bfc4-3e143759fef5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, time, json, threading, gzip, shutil, ipaddress\n",
    "from collections import defaultdict, deque\n",
    "from typing import Any, Dict, Optional, Tuple, List\n",
    "from fastapi import Request, HTTPException\n",
    "from fastapi.responses import JSONResponse\n",
    "from starlette.middleware.base import BaseHTTPMiddleware"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 450,
   "id": "024ccafc-d42c-4940-86df-77cc59d20f51",
   "metadata": {},
   "outputs": [],
   "source": [
    "class AdminIPAllowlistMiddleware(BaseHTTPMiddleware):\n",
    "    \"\"\"\n",
    "    If ADMIN_IPS is set (csv of CIDRs or IPs), blocks requests to admin-ish paths\n",
    "    that do not originate from an allowed source IP (or first X-Forwarded-For hop).\n",
    "    Paths: /admin/* /canary/* /overseer* /artifacts/*\n",
    "    \"\"\"\n",
    "    def __init__(self, app, env_var: str = \"ADMIN_IPS\", fwd_header: str = \"X-Forwarded-For\"):\n",
    "        super().__init__(app)\n",
    "        self.fwd_header = fwd_header\n",
    "        self.prefixes = (\"/admin/\", \"/canary/\", \"/artifacts/\", \"/overseer\")\n",
    "        raw = (os.getenv(env_var) or \"\").strip()\n",
    "        nets: List[ipaddress._BaseNetwork] = []\n",
    "        for tok in [t.strip() for t in raw.split(\",\") if t.strip()]:\n",
    "            try:\n",
    "                nets.append(ipaddress.ip_network(tok, strict=False))\n",
    "            except Exception:\n",
    "                try:\n",
    "                    nets.append(ipaddress.ip_network(f\"{tok}/32\", strict=False))\n",
    "                except Exception:\n",
    "                    pass\n",
    "        self.allowlist = nets\n",
    "    def _client_ip(self, request: Request) -> Optional[str]:\n",
    "        xff = request.headers.get(self.fwd_header, \"\")\n",
    "        if xff:\n",
    "            first = xff.split(\",\")[0].strip()\n",
    "            if first:\n",
    "                return first\n",
    "        try:\n",
    "            return request.client.host\n",
    "        except Exception:\n",
    "            return None\n",
    "    async def dispatch(self, request: Request, call_next):\n",
    "        if not self.allowlist:\n",
    "            return await call_next(request)\n",
    "        path = request.url.path or \"/\"\n",
    "        if any(path.startswith(p) for p in self.prefixes):\n",
    "            cip = self._client_ip(request)\n",
    "            try:\n",
    "                ip_obj = ipaddress.ip_address(cip) if cip else None\n",
    "            except Exception:\n",
    "                ip_obj = None\n",
    "            allowed = bool(ip_obj and any(ip_obj in n for n in self.allowlist))\n",
    "            if not allowed:\n",
    "                return JSONResponse({\"error\": \"forbidden_ip\"}, status_code=403)\n",
    "        return await call_next(request)\n",
    "for _name in (\"gateway\", \"rag\", \"overseer\"):\n",
    "    if _name in RUNNING:\n",
    "        try:\n",
    "            RUNNING[_name].app.add_middleware(AdminIPAllowlistMiddleware)\n",
    "        except Exception:\n",
    "            pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 451,
   "id": "3a4f6805-9ee8-4895-a415-a801c5d4d629",
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    from starlette.middleware.cors import CORSMiddleware\n",
    "    _origins = [o.strip() for o in (os.getenv(\"CORS_ALLOW_ORIGINS\") or \"\").split(\",\") if o.strip()]\n",
    "    if _origins:\n",
    "        for _name in (\"gateway\", \"rag\", \"overseer\"):\n",
    "            if _name in RUNNING:\n",
    "                try:\n",
    "                    RUNNING[_name].app.add_middleware(\n",
    "                        CORSMiddleware,\n",
    "                        allow_origins=_origins,\n",
    "                        allow_credentials=True,\n",
    "                        allow_methods=[\"*\"],\n",
    "                        allow_headers=[\"*\"],\n",
    "                    )\n",
    "                except Exception:\n",
    "                    pass\n",
    "except Exception:\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 452,
   "id": "aa795aa1-745c-43d1-bbaa-b94194c12aed",
   "metadata": {},
   "outputs": [],
   "source": [
    "PRICE_RERANK_PER_DOC = float(os.getenv(\"PRICE_RERANK_PER_DOC\", \"0.00001\"))\n",
    "def _today_key() -> str:\n",
    "    return time.strftime(\"%Y%m%d\")\n",
    "class _BudgetBook:\n",
    "    \"\"\"\n",
    "    In-memory daily budgets & spend (per namespace + route).\n",
    "    Routes we care about:\n",
    "      - \"ce_rerank\" (rerank cost = PRICE_RERANK_PER_DOC * top_k)\n",
    "      - \"answer\"   (any escalation/LLM gen/fix 'charged' cost in payload meta)\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        self._lock = threading.RLock()\n",
    "        self._day = _today_key()\n",
    "        self._data: Dict[str, Dict[str, Dict[str, float]]] = defaultdict(lambda: defaultdict(float))\n",
    "        self._limits: Dict[str, Dict[str, float]] = defaultdict(lambda: defaultdict(float))\n",
    "        self._limits[\"default\"][\"ce_rerank\"] = float(os.getenv(\"BUDGET_CE_RERANK_USD\", \"0\"))\n",
    "        self._limits[\"default\"][\"answer\"] = float(os.getenv(\"BUDGET_ANSWER_USD\", \"5\"))\n",
    "    def _roll_day(self):\n",
    "        d = _today_key()\n",
    "        if d != self._day:\n",
    "            self._day = d\n",
    "            self._data.clear()\n",
    "    def set_limit(self, ns: str, route: str, usd: float):\n",
    "        ns = (ns or \"default\").strip().lower()\n",
    "        route = (route or \"\").strip().lower()\n",
    "        with self._lock:\n",
    "            self._limits[ns][route] = max(0.0, float(usd))\n",
    "    def get(self, ns: str, route: str) -> Dict[str, float]:\n",
    "        ns = (ns or \"default\").strip().lower()\n",
    "        route = (route or \"\").strip().lower()\n",
    "        with self._lock:\n",
    "            self._roll_day()\n",
    "            spent = float(self._data[ns].get(route, 0.0))\n",
    "            limit = float(self._limits[ns].get(route, self._limits[\"default\"].get(route, 0.0)))\n",
    "            return {\"day\": self._day, \"namespace\": ns, \"route\": route, \"limit\": limit, \"spent\": spent, \"remaining\": max(0.0, limit - spent)}\n",
    "    def charge(self, ns: str, route: str, usd: float) -> Dict[str, Any]:\n",
    "        ns = (ns or \"default\").strip().lower()\n",
    "        route = (route or \"\").strip().lower()\n",
    "        usd = max(0.0, float(usd))\n",
    "        with self._lock:\n",
    "            self._roll_day()\n",
    "            limit = float(self._limits[ns].get(route, self._limits[\"default\"].get(route, 0.0)))\n",
    "            spent = float(self._data[ns].get(route, 0.0))\n",
    "            if spent + usd > limit and limit > 0.0:\n",
    "                return {\"ok\": False, \"blocked\": True, \"remaining\": max(0.0, limit - spent)}\n",
    "            self._data[ns][route] = spent + usd\n",
    "            return {\"ok\": True, \"blocked\": False, \"new_spent\": self._data[ns][route], \"remaining\": max(0.0, limit - self._data[ns][route])}\n",
    "    def reset_today(self, ns: Optional[str] = None, route: Optional[str] = None):\n",
    "        with self._lock:\n",
    "            self._roll_day()\n",
    "            if ns is None and route is None:\n",
    "                self._data.clear()\n",
    "                return\n",
    "            ns = (ns or \"default\").strip().lower()\n",
    "            if route:\n",
    "                self._data[ns].pop(route.strip().lower(), None)\n",
    "            else:\n",
    "                self._data.pop(ns, None)\n",
    "    def summary(self) -> Dict[str, Any]:\n",
    "        with self._lock:\n",
    "            self._roll_day()\n",
    "            out = {}\n",
    "            for ns, routes in self._limits.items():\n",
    "                out[ns] = {}\n",
    "                for rname, lim in routes.items():\n",
    "                    s = float(self._data[ns].get(rname, 0.0))\n",
    "                    out[ns][rname] = {\"limit\": float(lim), \"spent\": s, \"remaining\": max(0.0, float(lim) - s)}\n",
    "            return {\"day\": self._day, \"budgets\": out}\n",
    "_BUDGETS = _BudgetBook()\n",
    "class _QPM:\n",
    "    \"\"\"\n",
    "    Simple sliding-window per ns+route QPM limiter.\n",
    "    Default from ROUTE_QPM_DEFAULT, override via /admin/limits/qpm/set.\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        self._lock = threading.RLock()\n",
    "        self._limits: Dict[str, Dict[str, int]] = defaultdict(lambda: defaultdict(int))\n",
    "        self._win: Dict[Tuple[str, str], deque] = defaultdict(lambda: deque())\n",
    "        self._default = int(os.getenv(\"ROUTE_QPM_DEFAULT\", \"120\"))\n",
    "    def set_limit(self, ns: str, route: str, qpm: int):\n",
    "        ns = (ns or \"default\").strip().lower()\n",
    "        route = (route or \"\").strip().lower()\n",
    "        with self._lock:\n",
    "            self._limits[ns][route] = max(0, int(qpm))\n",
    "    def check(self, ns: str, route: str) -> bool:\n",
    "        ns = (ns or \"default\").strip().lower()\n",
    "        route = (route or \"\").strip().lower()\n",
    "        now = time.time()\n",
    "        key = (ns, route)\n",
    "        with self._lock:\n",
    "            dq = self._win[key]\n",
    "            while dq and (now - dq[0]) > 60.0:\n",
    "                dq.popleft()\n",
    "            limit = self._limits[ns].get(route, self._default)\n",
    "            if limit > 0 and len(dq) >= limit:\n",
    "                return False\n",
    "            dq.append(now)\n",
    "            return True\n",
    "_QPM_LIMITS = _QPM()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 453,
   "id": "8a108e70-cca0-44f7-809f-a33b47929b42",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _ns_from_req(request: Request, body: Dict[str, Any]) -> str:\n",
    "    return (request.headers.get(\"X-Namespace\") or request.query_params.get(\"ns\") or body.get(\"namespace\") or \"default\").strip().lower()\n",
    "if \"gateway\" in RUNNING:\n",
    "    app = RUNNING[\"gateway\"].app\n",
    "    if not getattr(app.state, \"_budget_qpm_wrapped\", False):\n",
    "        paths = [getattr(r, \"path\", \"\") for r in app.routes]\n",
    "        if \"/answer\" not in paths:\n",
    "            raise RuntimeError(\"Gateway app missing /answer route\")\n",
    "        idx = paths.index(\"/answer\")\n",
    "        inner_answer = app.routes[idx].endpoint\n",
    "        inner_route = app.routes.pop(idx)  \n",
    "        from fastapi import Body\n",
    "        @app.post(\"/answer\")\n",
    "        def _answer_budget_qpm(request: Request, body: Dict[str, Any] = Body(...)):\n",
    "            ns = _ns_from_req(request, body)\n",
    "            if not _QPM_LIMITS.check(ns, \"answer\"):\n",
    "                raise HTTPException(status_code=429, detail={\"error\": \"rate_limited\", \"route\": \"answer\", \"ns\": ns})\n",
    "            top_k = int(body.get(\"top_k\", 5))\n",
    "            use_rerank = body.get(\"use_rerank\", None)\n",
    "            ce_wanted = True if use_rerank is None else bool(use_rerank)\n",
    "            budget_blocked_ce = False\n",
    "            if ce_wanted and top_k > 0:\n",
    "                est = float(PRICE_RERANK_PER_DOC) * float(top_k)\n",
    "                pre = _BUDGETS.get(ns, \"ce_rerank\")\n",
    "                if pre[\"limit\"] > 0.0 and pre[\"remaining\"] < est:\n",
    "                    body = dict(body)\n",
    "                    body[\"use_rerank\"] = False\n",
    "                    budget_blocked_ce = True\n",
    "            resp = inner_answer(request, body)\n",
    "            try:\n",
    "                payload = json.loads(resp.body.decode(\"utf-8\")) if isinstance(resp.body, (bytes, bytearray)) else dict(resp.body)\n",
    "            except Exception:\n",
    "                return resp\n",
    "            meta = payload.setdefault(\"meta\", {})\n",
    "            if budget_blocked_ce:\n",
    "                meta[\"budget_blocked_ce\"] = True\n",
    "            pipeline = (meta.get(\"pipeline\") or \"\").lower()\n",
    "            try:\n",
    "                if pipeline == \"rerank\":\n",
    "                    k = int(body.get(\"top_k\", 5))\n",
    "                    charge = float(PRICE_RERANK_PER_DOC) * float(k)\n",
    "                    _BUDGETS.charge(ns, \"ce_rerank\", charge)\n",
    "            except Exception:\n",
    "                pass\n",
    "            esc = meta.get(\"escalation\") or {}\n",
    "            try:\n",
    "                ch = float(esc.get(\"charged\") or 0.0)\n",
    "                if ch > 0:\n",
    "                    _BUDGETS.charge(ns, \"answer\", ch)\n",
    "            except Exception:\n",
    "                pass\n",
    "            out = JSONResponse(payload)\n",
    "            if budget_blocked_ce:\n",
    "                out.headers[\"X-Budget-Block-CE\"] = \"1\"\n",
    "            return out\n",
    "        app.state._budget_qpm_wrapped = True\n",
    "    try:\n",
    "        from pydantic import BaseModel\n",
    "        class _BudgetSetReq(BaseModel):\n",
    "            namespace: str = \"default\"\n",
    "            route: str\n",
    "            daily_usd: float\n",
    "        @app.post(\"/admin/costs/set\")\n",
    "        def _admin_costs_set(req: _BudgetSetReq):\n",
    "            _BUDGETS.set_limit(req.namespace, req.route, req.daily_usd)\n",
    "            return {\"ok\": True, \"budget\": _BUDGETS.get(req.namespace, req.route)}\n",
    "        @app.get(\"/admin/costs/get\")\n",
    "        def _admin_costs_get(namespace: str = \"default\", route: str = \"answer\"):\n",
    "            return {\"ok\": True, \"budget\": _BUDGETS.get(namespace, route)}\n",
    "        @app.post(\"/admin/costs/reset\")\n",
    "        def _admin_costs_reset(namespace: Optional[str] = None, route: Optional[str] = None):\n",
    "            _BUDGETS.reset_today(namespace, route)\n",
    "            return {\"ok\": True}\n",
    "        @app.get(\"/admin/costs/summary\")\n",
    "        def _admin_costs_summary():\n",
    "            return {\"ok\": True, **_BUDGETS.summary()}\n",
    "        class _QPMSetReq(BaseModel):\n",
    "            namespace: str = \"default\"\n",
    "            route: str = \"answer\"\n",
    "            qpm: int = 120\n",
    "        @app.post(\"/admin/limits/qpm/set\")\n",
    "        def _admin_qpm_set(req: _QPMSetReq):\n",
    "            _QPM_LIMITS.set_limit(req.namespace, req.route, req.qpm)\n",
    "            return {\"ok\": True, \"namespace\": req.namespace, \"route\": req.route, \"qpm\": req.qpm}\n",
    "    except Exception:\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 454,
   "id": "45874f05-d5eb-4e9c-a5ed-41edb348665c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Audit compression cron active (gzip>7d, delete>90d)\n",
      " Admin IP allowlist (optional), budgets/QPM, /answer budget wrapper, admin endpoints, and audit compression loaded.\n"
     ]
    }
   ],
   "source": [
    "def _compress_file(src: Path, dst: Path) -> bool:\n",
    "    try:\n",
    "        with src.open(\"rb\") as fin, gzip.open(dst, \"wb\") as fout:\n",
    "            shutil.copyfileobj(fin, fout)\n",
    "        return True\n",
    "    except Exception:\n",
    "        return False\n",
    "def start_audit_compression_cron(older_than_days: int = int(os.getenv(\"AUDIT_GZIP_AFTER_DAYS\", \"7\")),\n",
    "                                 delete_after_days: int = int(os.getenv(\"AUDIT_DELETE_AFTER_DAYS\", \"90\"))):\n",
    "    \"\"\"\n",
    "    Gzip *.ndjson older than N days to *.ndjson.gz; delete gz older than M days.\n",
    "    \"\"\"\n",
    "    if RUNNING.get(\"audit_compress\"):\n",
    "        return RUNNING[\"audit_compress\"]\n",
    "    def _loop():\n",
    "        while True:\n",
    "            try:\n",
    "                now = time.time()\n",
    "                if AUDIT_DIR.exists():\n",
    "                    for p in AUDIT_DIR.glob(\"*.ndjson\"):\n",
    "                        age_days = (now - p.stat().st_mtime) / 86400.0\n",
    "                        gz = p.with_suffix(p.suffix + \".gz\")  # .ndjson.gz\n",
    "                        if age_days >= older_than_days and not gz.exists():\n",
    "                            if _compress_file(p, gz):\n",
    "                                try:\n",
    "                                    p.unlink()\n",
    "                                except Exception:\n",
    "                                    pass\n",
    "                    for gz in AUDIT_DIR.glob(\"*.ndjson.gz\"):\n",
    "                        age_days = (now - gz.stat().st_mtime) / 86400.0\n",
    "                        if age_days >= delete_after_days:\n",
    "                            try:\n",
    "                                gz.unlink()\n",
    "                            except Exception:\n",
    "                                pass\n",
    "                time.sleep(6 * 3600)  \n",
    "            except Exception:\n",
    "                time.sleep(3600)\n",
    "    t = threading.Thread(target=_loop, name=\"audit-compress\", daemon=True)\n",
    "    t.start()\n",
    "    RUNNING[\"audit_compress\"] = t\n",
    "    print(f\" Audit compression cron active (gzip>{older_than_days}d, delete>{delete_after_days}d)\")\n",
    "    return t\n",
    "try:\n",
    "    start_audit_compression_cron()\n",
    "except Exception as e:\n",
    "    print(\"[init] audit compression:\", e)\n",
    "print(\" Admin IP allowlist (optional), budgets/QPM, /answer budget wrapper, admin endpoints, and audit compression loaded.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 455,
   "id": "2b001c73-78f1-4ef9-8d0e-55a6895fb902",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import hashlib\n",
    "try:\n",
    "    os  \n",
    "except NameError: \n",
    "    import os\n",
    "from pathlib import Path\n",
    "try:\n",
    "    ART  \n",
    "except NameError:\n",
    "    ART = Path(os.getenv(\"ART_DIR\", \"/tmp/art\"))\n",
    "    ART.mkdir(parents=True, exist_ok=True)\n",
    "try:\n",
    "    AUDIT_DIR  \n",
    "except NameError:\n",
    "    AUDIT_DIR = Path(os.getenv(\"AUDIT_DIR\", \"/tmp/audit\"))\n",
    "    AUDIT_DIR.mkdir(parents=True, exist_ok=True)\n",
    "try:\n",
    "    ART_TTL_DAYS  \n",
    "except NameError:\n",
    "    ART_TTL_DAYS = int(os.getenv(\"ART_TTL_DAYS\", \"7\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 456,
   "id": "cc471cc0-c46d-4310-a120-1ff5f39951c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _percentile(values, pct):\n",
    "    if not values:\n",
    "        return None\n",
    "    vals = sorted(float(v) for v in values)\n",
    "    if pct <= 0:\n",
    "        return vals[0]\n",
    "    if pct >= 100:\n",
    "        return vals[-1]\n",
    "    r = (len(vals) - 1) * (pct / 100.0)\n",
    "    lo = int(math.floor(r))\n",
    "    hi = int(math.ceil(r))\n",
    "    if lo == hi:\n",
    "        return vals[lo]\n",
    "    return vals[lo] + (vals[hi] - vals[lo]) * (r - lo)\n",
    "def _too_old(p: Path, days: int) -> bool:\n",
    "    try:\n",
    "        return (time.time() - p.stat().st_mtime) >= days * 86400\n",
    "    except Exception:\n",
    "        return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 457,
   "id": "83917c73-575f-4201-ad2f-00b9b5dffbe3",
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    _RE_EMAIL  \n",
    "except NameError:\n",
    "    _RE_EMAIL = re.compile(\n",
    "        r\"\\b([A-Za-z0-9._%+-]{1,2})[A-Za-z0-9._%+-]*(@)([A-Za-z0-9.-]+\\.[A-Za-z]{2,})\\b\"\n",
    "    )\n",
    "try:\n",
    "    _RE_SSN  \n",
    "except NameError:\n",
    "    _RE_SSN = re.compile(r\"\\b(\\d{3})-(\\d{2})-(\\d{4})\\b\")\n",
    "try:\n",
    "    RESP_CACHE  \n",
    "except NameError:\n",
    "    class _RespCache(dict):\n",
    "        \"\"\"Extremely simple in-memory cache placeholder.\"\"\"\n",
    "        pass\n",
    "    RESP_CACHE = _RespCache()\n",
    "try:\n",
    "    CB  \n",
    "except NameError:\n",
    "    class _NoopBreaker:\n",
    "        def open(self) -> bool:\n",
    "            return False\n",
    "    CB = _NoopBreaker()\n",
    "try:\n",
    "    _norm_query  \n",
    "except NameError:\n",
    "    def _norm_query(q: str) -> str:\n",
    "        return \" \".join((q or \"\").split()).lower()\n",
    "try:\n",
    "    _audit_write  \n",
    "except NameError:\n",
    "    def _audit_write(ns: str, obj: dict):\n",
    "        day = time.strftime(\"%Y%m%d\")\n",
    "        rec = {\"namespace\": (ns or \"default\").strip().lower()}\n",
    "        rec.update(obj or {})\n",
    "        AUDIT_DIR.mkdir(parents=True, exist_ok=True)\n",
    "        with (AUDIT_DIR / f\"answer_{day}.ndjson\").open(\"a\", encoding=\"utf-8\") as f:\n",
    "            f.write(json.dumps(rec, ensure_ascii=False) + \"\\n\")\n",
    "try:\n",
    "    rrf_fuse  \n",
    "except NameError:\n",
    "    def rrf_fuse(list_a, list_b, k: int = 5, k_const: int = 60):\n",
    "        \"\"\"\n",
    "        Reciprocal Rank Fusion for two ranked lists of dicts.\n",
    "        Uses any of id/doc_id/chunk_id/uri/text as the key.\n",
    "        \"\"\"\n",
    "        def _key(d: dict, i: int):\n",
    "            return (\n",
    "                d.get(\"id\")\n",
    "                or d.get(\"doc_id\")\n",
    "                or d.get(\"chunk_id\")\n",
    "                or d.get(\"uri\")\n",
    "                or d.get(\"text\")\n",
    "                or f\"row:{i}\"\n",
    "            )\n",
    "        scores = {}\n",
    "        for r, d in enumerate(list_a or []):\n",
    "            key = _key(d, r)\n",
    "            scores.setdefault(key, {\"doc\": d, \"score\": 0.0})\n",
    "            scores[key][\"score\"] += 1.0 / (k_const + r + 1)\n",
    "        for r, d in enumerate(list_b or []):\n",
    "            key = _key(d, r)\n",
    "            scores.setdefault(key, {\"doc\": d, \"score\": 0.0})\n",
    "            scores[key][\"score\"] += 1.0 / (k_const + r + 1)\n",
    "        fused = [v[\"doc\"] for _, v in sorted(scores.items(), key=lambda kv: kv[1][\"score\"], reverse=True)]\n",
    "        return fused[: int(k)]\n",
    "try:\n",
    "    drift_psi_from_audit  \n",
    "except NameError:\n",
    "    def drift_psi_from_audit(day: str, ns: str):\n",
    "        return None\n",
    "try:\n",
    "    pg_upsert_rollup  \n",
    "except NameError:\n",
    "    def pg_upsert_rollup(day: str, ns: str, roll: dict):\n",
    "        return None\n",
    "try:\n",
    "    _sql_redact_literals  \n",
    "except NameError:\n",
    "    def _sql_redact_literals(sql: str) -> str:\n",
    "        s = re.sub(r\"'[^']*'\", \"?\", sql or \"\")\n",
    "        s = re.sub(r'\"[^\"]*\"', \"?\", s)\n",
    "        s = re.sub(r\"\\b\\d+\\b\", \"?\", s)\n",
    "        return s\n",
    "try:\n",
    "    _pg_connect  \n",
    "except NameError:\n",
    "    def _pg_connect():\n",
    "        return None\n",
    "try:\n",
    "    s3_put \n",
    "except NameError:\n",
    "    def s3_put(path: Path, prefix: str = \"indices/cold/\") -> str:\n",
    "        try:\n",
    "            return f\"file://{str(path)}\"\n",
    "        except Exception:\n",
    "            return \"file://\"\n",
    "try:\n",
    "    register_faiss_uri  \n",
    "except NameError:\n",
    "    def register_faiss_uri(namespace: str, uri: str, tier: str = \"cold\"):\n",
    "        return {\"namespace\": namespace, \"uri\": uri, \"tier\": tier}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 458,
   "id": "a4bf1a67-bfb4-431e-9953-cb71842c8cca",
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    RUNNING  \n",
    "except NameError:\n",
    "    RUNNING = {}\n",
    "try:\n",
    "    ServerThread  \n",
    "except NameError:\n",
    "    class ServerThread:\n",
    "        def __init__(self, app, preferred_port=None, name=\"srv\", log_level=\"warning\"):\n",
    "            self.app = app\n",
    "            self.preferred_port = preferred_port\n",
    "            self.name = name\n",
    "            self.log_level = log_level\n",
    "            self.base = f\"http://127.0.0.1:{preferred_port or 0}\"\n",
    "        def start(self): ...\n",
    "        def stop(self): ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 459,
   "id": "1eb5eb58-b3a8-4464-9f7d-ba48b532ed63",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Safety shims loaded; lightweight /admin/health mounted where possible.\n"
     ]
    }
   ],
   "source": [
    "def _install_health(app, svc_name: str):\n",
    "    try:\n",
    "        existing = {getattr(r, \"path\", \"\") for r in app.routes}\n",
    "        if \"/admin/health\" in existing:\n",
    "            return\n",
    "        from fastapi import APIRouter\n",
    "        router = APIRouter()\n",
    "        @router.get(\"/admin/health\")\n",
    "        def _health():\n",
    "            out = {\n",
    "                \"ok\": True,\n",
    "                \"service\": svc_name,\n",
    "                \"time\": int(time.time()),\n",
    "            }\n",
    "            try:\n",
    "                out[\"budgets\"] = _BUDGETS.summary()  \n",
    "            except Exception:\n",
    "                pass\n",
    "            try:\n",
    "                out[\"canary\"] = {\n",
    "                    \"namespaces\": list(_CANARY_CTRL.keys()),  \n",
    "                    \"active\": [k for k, v in _CANARY_CTRL.items() if not getattr(v, \"done\", True)],\n",
    "                }\n",
    "            except Exception:\n",
    "                pass\n",
    "            try:\n",
    "                out[\"metrics\"] = {\n",
    "                    \"audit_dir\": str(AUDIT_DIR),\n",
    "                    \"art_dir\": str(ART),\n",
    "                }\n",
    "            except Exception:\n",
    "                pass\n",
    "            return out\n",
    "        @router.get(\"/admin/version\")\n",
    "        def _version():\n",
    "            return {\n",
    "                \"ok\": True,\n",
    "                \"python\": sys.version,\n",
    "                \"pid\": os.getpid(),\n",
    "                \"env\": {\n",
    "                    \"NODE\": os.getenv(\"HOSTNAME\") or os.getenv(\"COMPUTERNAME\"),\n",
    "                    \"CORS_ALLOW_ORIGINS\": os.getenv(\"CORS_ALLOW_ORIGINS\"),\n",
    "                },\n",
    "            }\n",
    "        app.include_router(router)\n",
    "    except Exception:\n",
    "        pass\n",
    "for _name in (\"gateway\", \"rag\", \"overseer\", \"hf\"):\n",
    "    srv = RUNNING.get(_name)\n",
    "    if srv and getattr(srv, \"app\", None):\n",
    "        _install_health(srv.app, _name)\n",
    "print(\" Safety shims loaded; lightweight /admin/health mounted where possible.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 460,
   "id": "e23d5bcb-efba-470c-969d-ecea82a521c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    import sys, json, time, inspect, asyncio\n",
    "except Exception:\n",
    "    pass\n",
    "try:\n",
    "    from starlette.concurrency import run_in_threadpool as _run_in_threadpool\n",
    "except Exception:  \n",
    "    async def _run_in_threadpool(fn, *args, **kwargs):\n",
    "        return fn(*args, **kwargs)\n",
    "async def _call_maybe_async(fn, *args, **kwargs):\n",
    "    \"\"\"\n",
    "    Call a FastAPI/Starlette handler (or any function) whether it's sync or async.\n",
    "    Always returns an awaited result. Safe to import even if never used.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        if inspect.iscoroutinefunction(fn):\n",
    "            return await fn(*args, **kwargs)\n",
    "        return await _run_in_threadpool(fn, *args, **kwargs)\n",
    "    except Exception as _e:\n",
    "        raise _e"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 461,
   "id": "70b1223b-f068-4553-911f-93028cf1841f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _mount_common_admin_routes(app, svc_name: str):\n",
    "    \"\"\"\n",
    "    Idempotently mount ultra-light admin routes on an app.\n",
    "    Skips anything that already exists. Fully guarded to avoid import-time crashes.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        existing = {getattr(r, \"path\", \"\") for r in app.routes}\n",
    "    except Exception:\n",
    "        existing = set()\n",
    "    try:\n",
    "        from fastapi import APIRouter, Query\n",
    "        from fastapi.responses import JSONResponse\n",
    "    except Exception:\n",
    "        return  \n",
    "    router = APIRouter()\n",
    "    if \"/up\" not in existing:\n",
    "        @router.get(\"/up\")\n",
    "        def _up():\n",
    "            return {\"ok\": True, \"service\": svc_name, \"ts\": int(time.time())}\n",
    "    if \"/admin/ping\" not in existing:\n",
    "        @router.get(\"/admin/ping\")\n",
    "        def _ping():\n",
    "            return {\"ok\": True, \"pong\": True, \"service\": svc_name, \"time\": int(time.time())}\n",
    "    if \"/admin/budgets/summary\" not in existing and \"_BUDGETS\" in globals():\n",
    "        @router.get(\"/admin/budgets/summary\")\n",
    "        def _budgets_summary():\n",
    "            try:\n",
    "                return {\"ok\": True, **_BUDGETS.summary()}\n",
    "            except Exception as e:\n",
    "                return JSONResponse({\"ok\": False, \"error\": str(e)}, status_code=500)\n",
    "    if \"/admin/rollup/today\" not in existing and \"build_rollup_from_audit\" in globals():\n",
    "        @router.get(\"/admin/rollup/today\")\n",
    "        def _rollup_today(ns: str = Query(default=\"default\")):\n",
    "            try:\n",
    "                roll = build_rollup_from_audit(day=None, ns=ns)\n",
    "                return {\"ok\": True, **(roll or {})}\n",
    "            except Exception as e:\n",
    "                return JSONResponse({\"ok\": False, \"error\": str(e)}, status_code=500)\n",
    "    if \"/admin/debug/state\" not in existing:\n",
    "        @router.get(\"/admin/debug/state\")\n",
    "        def _dbg_state():\n",
    "            out = {\"ok\": True, \"service\": svc_name, \"time\": int(time.time())}\n",
    "            try:\n",
    "                out[\"running_services\"] = sorted(list(RUNNING.keys()))\n",
    "            except Exception:\n",
    "                out[\"running_services\"] = []\n",
    "            try:\n",
    "                out[\"audit_dir\"] = str(AUDIT_DIR)\n",
    "                out[\"art_dir\"] = str(ART)\n",
    "            except Exception:\n",
    "                pass\n",
    "            if \"_BUDGETS\" in globals():\n",
    "                try:\n",
    "                    out[\"budgets_day\"] = _BUDGETS.summary().get(\"day\")\n",
    "                except Exception:\n",
    "                    pass\n",
    "            if \"_QPM_LIMITS\" in globals():\n",
    "                try:\n",
    "                    out[\"qpm_limits\"] = True\n",
    "                except Exception:\n",
    "                    pass\n",
    "            return out\n",
    "    try:\n",
    "        app.include_router(router)\n",
    "    except Exception:\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 462,
   "id": "b2f24157-8a6f-48ba-b958-3bbd413b5119",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Common admin routes mounted (/up, /admin/ping, optional budgets & rollup).\n",
      " Paths: ART=('artifacts\\\\local', 'artifacts\\\\local\\\\audit'), AUDIT=artifacts\\local\\audit\n"
     ]
    }
   ],
   "source": [
    "def _idempotent_mount_all():\n",
    "    \"\"\"\n",
    "    Mount the admin routes on all known services that have an app.\n",
    "    This is safe to call multiple times.\n",
    "    \"\"\"\n",
    "    for _name in (\"gateway\", \"rag\", \"overseer\", \"hf\"):\n",
    "        try:\n",
    "            srv = RUNNING.get(_name)\n",
    "            if srv and getattr(srv, \"app\", None):\n",
    "                _mount_common_admin_routes(srv.app, _name)\n",
    "        except Exception:\n",
    "            continue\n",
    "try:\n",
    "    _idempotent_mount_all()\n",
    "    print(\" Common admin routes mounted (/up, /admin/ping, optional budgets & rollup).\")\n",
    "except Exception as e:\n",
    "    print(\"[init] common admin routes:\", e)\n",
    "try:\n",
    "    _ = (str(ART), str(AUDIT_DIR))\n",
    "    print(f\" Paths: ART={_}, AUDIT={str(AUDIT_DIR)}\")\n",
    "except Exception:\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 463,
   "id": "1bfbe26d-05ec-4e83-b797-80f5f35eab03",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Axum-like Gateway in Python (FastAPI) with:\n",
    "- Typed client to RAG (timeouts, retries/backoff)\n",
    "- Deterministic router (regex with override via X-Route)\n",
    "- Request-ID & namespace propagation + header allowlist\n",
    "- Metrics (Prometheus): per-step histograms & error counters\n",
    "- Bind attempts counter and port auto-increment with PORT.txt\n",
    "- Fail-fast & BM25-only fallback with meta.partial_retrieval\n",
    "- Canary / RL governor: /rl/update + /canary/status with guardrails\n",
    "- /up, /metrics, minimal /admin/*\n",
    "Designed to run safely inside a notebook (kernel-safe, background threads).\n",
    "\"\"\"\n",
    "import asyncio\n",
    "import json\n",
    "import os\n",
    "import re\n",
    "import time\n",
    "import uuid\n",
    "import threading\n",
    "from collections import deque, defaultdict\n",
    "from dataclasses import dataclass\n",
    "from pathlib import Path\n",
    "from typing import Any, Dict, List, Optional, Tuple\n",
    "import uvicorn\n",
    "from fastapi import FastAPI, Request, HTTPException, Body\n",
    "from fastapi.responses import JSONResponse, PlainTextResponse\n",
    "from pydantic import BaseModel, Field\n",
    "import httpx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 464,
   "id": "9e18e71e-8add-4db0-95bd-94557a3c11ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "from prometheus_client import CollectorRegistry, Histogram, Counter, generate_latest, CONTENT_TYPE_LATEST\n",
    "REG = CollectorRegistry()\n",
    "GATEWAY_LAT_MS = Histogram(\n",
    "    \"gateway_answer_latency_ms\",\n",
    "    \"Overall /answer latency (ms)\",\n",
    "    registry=REG,\n",
    "    buckets=[5,10,20,40,80,160,320,640,1280,2560,5120],\n",
    ")\n",
    "STEP_LAT_MS = {\n",
    "    \"bm25\": Histogram(\"dense_bm25_ms\", \"BM25 step latency (ms)\", [\"step\"], registry=REG, buckets=[5,10,20,40,80,160,320,640,1280]),\n",
    "    \"dense\": Histogram(\"dense_dense_ms\", \"Dense step latency (ms)\", [\"step\"], registry=REG, buckets=[5,10,20,40,80,160,320,640,1280]),\n",
    "    \"rerank\": Histogram(\"dense_rerank_ms\", \"Rerank step latency (ms)\", [\"step\"], registry=REG, buckets=[5,10,20,40,80,160,320,640,1280]),\n",
    "    \"complete\": Histogram(\"dense_complete_ms\", \"Complete step latency (ms)\", [\"step\"], registry=REG, buckets=[20,40,80,160,320,640,1280,2560,5120]),\n",
    "}\n",
    "RAG_DENSE_MS = Histogram(\"rag_dense_ms\", \"RAG dense retrieval latency (ms)\", registry=REG, buckets=[5,10,20,40,80,160,320,640,1280])\n",
    "RAG_BM25_MS = Histogram(\"rag_bm25_ms\", \"RAG bm25 retrieval latency (ms)\", registry=REG, buckets=[5,10,20,40,80,160,320,640,1280])\n",
    "RAG_RERANK_MS = Histogram(\"rag_rerank_ms\", \"RAG rerank latency (ms)\", registry=REG, buckets=[5,10,20,40,80,160,320,640,1280])\n",
    "RAG_COMPLETE_MS = Histogram(\"rag_complete_ms\", \"RAG complete latency (ms)\", registry=REG, buckets=[20,40,80,160,320,640,1280,2560,5120])\n",
    "FANOUT_ERRORS = Counter(\"gateway_fanout_errors_total\", \"Fanout errors\", [\"step\"], registry=REG)\n",
    "BIND_ATTEMPTS = Counter(\"rust_gateway_bind_attempts_total\", \"Bind attempts (simulated for Python impl)\", registry=REG)\n",
    "CACHE_HITS = Counter(\"cache_hits_total\", \"Response cache hits\", registry=REG)\n",
    "BREAKER_OPENS = Counter(\"breaker_opens_total\", \"Circuit breaker opens (placeholder)\", registry=REG)\n",
    "BUDGET_BLOCKS = Counter(\"budget_blocks_total\", \"Budget blocks observed from downstream\", registry=REG)\n",
    "FAITHFULNESS_GAUGE_QUEUE: deque = deque()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 465,
   "id": "52a62f1f-88e4-45ff-8007-465c9238a884",
   "metadata": {},
   "outputs": [],
   "source": [
    "ART = Path(os.getenv(\"ART_DIR\", \"/tmp/art\"))\n",
    "ART.mkdir(parents=True, exist_ok=True)\n",
    "PORT_TXT = ART / \"PORT.txt\"\n",
    "RAG_BASE = os.getenv(\"RAG_BASE\", \"http://127.0.0.1:9909\")  \n",
    "DEFAULT_TIMEOUTS = {\n",
    "    \"bm25\": float(os.getenv(\"STEP_TIMEOUT_BM25\", \"0.8\")),     \n",
    "    \"dense\": float(os.getenv(\"STEP_TIMEOUT_DENSE\", \"1.2\")),\n",
    "    \"rerank\": float(os.getenv(\"STEP_TIMEOUT_RERANK\", \"0.6\")),\n",
    "    \"complete\": float(os.getenv(\"STEP_TIMEOUT_COMPLETE\", \"2.0\")),\n",
    "}\n",
    "ALLOWLIST_FANOUT_HEADERS = {\"X-Request-Id\", \"X-Namespace\", \"X-Route\"}\n",
    "DISALLOWED_HEADERS = {\"Authorization\"}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 466,
   "id": "f7b871b5-b3d3-40e6-992a-b087af5ffbd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RequestIdMiddleware:\n",
    "    def __init__(self, app: FastAPI):\n",
    "        self.app = app\n",
    "    async def __call__(self, scope, receive, send):\n",
    "        if scope[\"type\"] != \"http\":\n",
    "            await self.app(scope, receive, send)\n",
    "            return\n",
    "        headers = dict(scope.get(\"headers\") or [])\n",
    "        req_id = headers.get(b\"x-request-id\", b\"\")\n",
    "        if not req_id:\n",
    "            req_id = uuid.uuid4().hex.encode()\n",
    "            scope.setdefault(\"state\", {})[\"request_id\"] = req_id.decode()\n",
    "        async def send_wrapper(message):\n",
    "            if message[\"type\"] == \"http.response.start\":\n",
    "                message.setdefault(\"headers\", []).append((b\"x-request-id\", req_id))\n",
    "            await send(message)\n",
    "        await self.app(scope, receive, send_wrapper)\n",
    "_SQL = re.compile(r\"(?is)^\\s*(select|with)\\b\")\n",
    "_MATH = re.compile(r\"\\b(integrate|derive|differentiate|solve|sum|product|limit|matrix|determinant|eigen)\\b\", re.I)\n",
    "_POLICY = re.compile(r\"\\b(policy|regulation|compliance|gdpr|hipaa|soc2|security)\\b\", re.I)\n",
    "def route_for(query: str, override: Optional[str] = None) -> str:\n",
    "    if override:\n",
    "        return override.strip().lower()\n",
    "    q = (query or \"\").strip()\n",
    "    if _SQL.search(q):\n",
    "        return \"sql\"\n",
    "    if _MATH.search(q):\n",
    "        return \"math\"\n",
    "    if __POLICY.search(q):\n",
    "        return \"policy\"\n",
    "    return \"factual\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 467,
   "id": "786244e6-cad2-4d01-a6d7-017afcdabfc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class StepResult:\n",
    "    ok: bool\n",
    "    latency_ms: Optional[int] = None\n",
    "    payload: Optional[Dict[str, Any]] = None\n",
    "    error: Optional[str] = None\n",
    "class RagClient:\n",
    "    def __init__(self, base: str, timeout: float = 2.5):\n",
    "        self.base = base.rstrip(\"/\")\n",
    "        limits = httpx.Limits(max_keepalive_connections=20, max_connections=100)\n",
    "        self.client = httpx.AsyncClient(base_url=self.base, timeout=timeout, limits=limits)\n",
    "    def _fanout_headers(self, req: Request) -> Dict[str, str]:\n",
    "        out = {}\n",
    "        for k, v in req.headers.items():\n",
    "            if k in DISALLOWED_HEADERS:\n",
    "                continue\n",
    "            if k in ALLOWLIST_FANOUT_HEADERS:\n",
    "                out[k] = v\n",
    "        out.setdefault(\"X-Request-Id\", req.headers.get(\"X-Request-Id\", uuid.uuid4().hex))\n",
    "        return out\n",
    "    async def _post_json_step(self, req: Request, path: str, body: Dict[str, Any], step: str, timeout_s: float) -> StepResult:\n",
    "        url = f\"{self.base}{path}\"\n",
    "        headers = self._fanout_headers(req)\n",
    "        attempts = 0\n",
    "        t0 = time.time()\n",
    "        while attempts < 2:\n",
    "            attempts += 1\n",
    "            try:\n",
    "                async with asyncio.timeout(timeout_s):\n",
    "                    r = await self.client.post(url, json=body, headers=headers)\n",
    "                    r.raise_for_status()\n",
    "                    dt = int((time.time() - t0) * 1000)\n",
    "                    return StepResult(ok=True, latency_ms=dt, payload=r.json())\n",
    "            except Exception as e:\n",
    "                if attempts >= 2:\n",
    "                    FANOUT_ERRORS.labels(step=step).inc()\n",
    "                    return StepResult(ok=False, latency_ms=int((time.time()-t0)*1000), error=str(e))\n",
    "                await asyncio.sleep(0.1 * attempts)\n",
    "        return StepResult(ok=False, error=\"unreachable\")\n",
    "    async def bm25(self, req: Request, query: str, top_k: int) -> StepResult:\n",
    "        body = {\"query\": query, \"top_k\": top_k}\n",
    "        res = await self._post_json_step(req, \"/retrieve_bm25\", body, \"bm25\", DEFAULT_TIMEOUTS[\"bm25\"])\n",
    "        if res.latency_ms is not None:\n",
    "            RAG_BM25_MS.observe(res.latency_ms)\n",
    "            STEP_LAT_MS[\"bm25\"].labels(step=\"bm25\").observe(res.latency_ms)\n",
    "        return res\n",
    "    async def dense(self, req: Request, query: str, top_k: int) -> StepResult:\n",
    "        body = {\"query\": query, \"top_k\": top_k}\n",
    "        res = await self._post_json_step(req, \"/retrieve_dense\", body, \"dense\", DEFAULT_TIMEOUTS[\"dense\"])\n",
    "        if res.latency_ms is not None:\n",
    "            RAG_DENSE_MS.observe(res.latency_ms)\n",
    "            STEP_LAT_MS[\"dense\"].labels(step=\"dense\").observe(res.latency_ms)\n",
    "        return res\n",
    "    async def rerank(self, req: Request, query: str, contexts: List[Dict[str, Any]]) -> StepResult:\n",
    "        body = {\"query\": query, \"contexts\": contexts}\n",
    "        res = await self._post_json_step(req, \"/rerank\", body, \"rerank\", DEFAULT_TIMEOUTS[\"rerank\"])\n",
    "        if res.latency_ms is not None:\n",
    "            RAG_RERANK_MS.observe(res.latency_ms)\n",
    "            STEP_LAT_MS[\"rerank\"].labels(step=\"rerank\").observe(res.latency_ms)\n",
    "        return res\n",
    "    async def complete(self, req: Request, query: str, contexts: List[Dict[str, Any]]) -> StepResult:\n",
    "        body = {\"query\": query, \"contexts\": contexts}\n",
    "        res = await self._post_json_step(req, \"/complete\", body, \"complete\", DEFAULT_TIMEOUTS[\"complete\"])\n",
    "        if res.latency_ms is not None:\n",
    "            RAG_COMPLETE_MS.observe(res.latency_ms)\n",
    "            STEP_LAT_MS[\"complete\"].labels(step=\"complete\").observe(res.latency_ms)\n",
    "        try:\n",
    "            ver = (res.payload or {}).get(\"verify\") or {}\n",
    "            val = ver.get(\"faithfulness\")\n",
    "            if isinstance(val, (int, float)):\n",
    "                FAITHFULNESS_GAUGE_QUEUE.append((time.time(), float(val)))\n",
    "                while len(FAITHFULNESS_GAUGE_QUEUE) > 5000:\n",
    "                    FAITHFULNESS_GAUGE_QUEUE.popleft()\n",
    "        except Exception:\n",
    "            pass\n",
    "        return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 468,
   "id": "4146639d-7723-4d20-bd21-419d78e5833f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class WeightStore:\n",
    "    def __init__(self, path: Path | None = None):\n",
    "        self.path = path or (ART / \"weights.json\")\n",
    "        self._lock = threading.RLock()\n",
    "        self.weights = {\"sql\":1.0, \"math\":1.0, \"policy\":1.0, \"factual\":1.0}  \n",
    "        if self.path.exists():\n",
    "            try:\n",
    "                self.weights.update(json.loads(self.path.read_text()))\n",
    "            except Exception:\n",
    "                pass\n",
    "    def get(self) -> Dict[str, float]:\n",
    "        with self._lock:\n",
    "            return dict(self.weights)\n",
    "    def set(self, weights: Dict[str, float]):\n",
    "        with self._lock:\n",
    "            self.weights.update({k: float(v) for k, v in weights.items()})\n",
    "            try:\n",
    "                self.path.write_text(json.dumps(self.weights, indent=2))\n",
    "            except Exception:\n",
    "                pass\n",
    "WEIGHTS = WeightStore()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 469,
   "id": "8f085cc8-e41f-4332-bfaa-e6a9f650191a",
   "metadata": {},
   "outputs": [],
   "source": [
    "REQ_STATS = deque()  \n",
    "MIN_REQUESTS_FOR_PROMO = int(os.getenv(\"CANARY_MIN_REQ\", \"60\"))\n",
    "P95_GATE_MS = float(os.getenv(\"CANARY_P95_MS\", \"1800\"))\n",
    "FAITH_GATE = float(os.getenv(\"CANARY_FAITH\", \"0.85\"))\n",
    "def window_stats(now: float | None = None, window_sec: int = 900):\n",
    "    now = now or time.time()\n",
    "    while REQ_STATS and (now - REQ_STATS[0][0]) > window_sec:\n",
    "        REQ_STATS.popleft()\n",
    "    vals = [x[1] for x in REQ_STATS if isinstance(x[1], (int, float))]\n",
    "    errs = sum(1 for x in REQ_STATS if x[3])\n",
    "    faiths = [x[2] for x in REQ_STATS if isinstance(x[2], (int, float))]\n",
    "    def percentile(v, p):\n",
    "        if not v:\n",
    "            return None\n",
    "        v2 = sorted(v)\n",
    "        i = (len(v2)-1) * (p/100.0)\n",
    "        lo, hi = int(i), min(len(v2)-1, int(i)+1)\n",
    "        if lo == hi: return v2[lo]\n",
    "        return v2[lo] + (v2[hi]-v2[lo])*(i-lo)\n",
    "    return {\n",
    "        \"count\": len(REQ_STATS),\n",
    "        \"p95_ms\": percentile(vals, 95) if vals else None,\n",
    "        \"faith_mean\": (sum(faiths)/len(faiths)) if faiths else None,\n",
    "        \"err_rate\": (errs/len(REQ_STATS)) if REQ_STATS else None,\n",
    "    }\n",
    "def guardrails_ok() -> bool:\n",
    "    stats = window_stats()\n",
    "    if (stats[\"count\"] or 0) < MIN_REQUESTS_FOR_PROMO:\n",
    "        return False\n",
    "    if stats[\"p95_ms\"] is not None and stats[\"p95_ms\"] > P95_GATE_MS:\n",
    "        return False\n",
    "    if stats[\"faith_mean\"] is not None and stats[\"faith_mean\"] < FAITH_GATE:\n",
    "        return False\n",
    "    return True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 470,
   "id": "3f1e8dfd-77fe-4139-b603-f67991cb18da",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RespCache(dict):\n",
    "    def __init__(self, ttl_s: int = 60, max_items: int = 512):\n",
    "        super().__init__()\n",
    "        self.ttl_s = ttl_s\n",
    "        self.max_items = max_items\n",
    "    def make_key(self, body: Dict[str, Any]) -> str:\n",
    "        fields = {k: body.get(k) for k in (\"query\", \"top_k\", \"use_rerank\")}\n",
    "        return json.dumps(fields, sort_keys=True)\n",
    "    def get(self, key: str):\n",
    "        v = super().get(key)\n",
    "        if not v:\n",
    "            return None\n",
    "        ts, payload = v\n",
    "        if (time.time() - ts) > self.ttl_s:\n",
    "            try: super().pop(key, None)\n",
    "            finally: ...\n",
    "            return None\n",
    "        return payload\n",
    "    def put(self, key: str, payload: Dict[str, Any]):\n",
    "        if len(self) >= self.max_items:\n",
    "            try:\n",
    "                self.pop(next(iter(self.keys())))\n",
    "            except Exception:\n",
    "                pass\n",
    "        super().__setitem__(key, (time.time(), payload))\n",
    "RESP_CACHE = RespCache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 471,
   "id": "a55e5dee-5ed1-4382-8af1-81dad18e8c07",
   "metadata": {},
   "outputs": [],
   "source": [
    "app = FastAPI(title=\"Gateway (Python FastAPI)\", version=\"0.1\")\n",
    "app.add_middleware(RequestIdMiddleware)\n",
    "rag_client = RagClient(RAG_BASE)\n",
    "@app.get(\"/up\")\n",
    "def up():\n",
    "    return {\"ok\": True, \"service\": \"gateway\", \"ts\": int(time.time())}\n",
    "@app.get(\"/metrics\")\n",
    "def metrics():\n",
    "    return PlainTextResponse(generate_latest(REG), media_type=CONTENT_TYPE_LATEST)\n",
    "@app.get(\"/admin/ping\")\n",
    "def ping():\n",
    "    return {\"ok\": True, \"pong\": True, \"service\": \"gateway\", \"ts\": int(time.time())}\n",
    "class AnswerReq(BaseModel):\n",
    "    query: str = Field(...)\n",
    "    top_k: int = Field(5, ge=1, le=50)\n",
    "    use_rerank: Optional[bool] = None\n",
    "def _allowlist_headers(req: Request) -> Dict[str, str]:\n",
    "    return {k: v for k, v in req.headers.items() if (k in ALLOWLIST_FANOUT_HEADERS and k not in DISALLOWED_HEADERS)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 472,
   "id": "8f2916df-b819-4567-960a-ff8f13ebcbde",
   "metadata": {},
   "outputs": [],
   "source": [
    "@app.post(\"/answer\")\n",
    "async def answer(request: Request, body: AnswerReq = Body(...)):\n",
    "    t0 = time.time()\n",
    "    ns = request.headers.get(\"X-Namespace\", request.query_params.get(\"ns\", \"default\")).strip().lower()\n",
    "    route_override = request.headers.get(\"X-Route\", \"\").strip().lower() or None\n",
    "    q = (body.query or \"\").strip()\n",
    "    if not q:\n",
    "        raise HTTPException(400, \"query required\")\n",
    "    key = RESP_CACHE.make_key(body.model_dump())\n",
    "    cached = RESP_CACHE.get(key)\n",
    "    if cached:\n",
    "        CACHE_HITS.inc()\n",
    "        payload = dict(cached)\n",
    "        payload.setdefault(\"meta\", {})[\"cache\"] = \"hit\"\n",
    "        payload[\"meta\"].setdefault(\"latency_ms\", 0)\n",
    "        payload[\"meta\"][\"route\"] = route_for(q, route_override)\n",
    "        payload[\"meta\"][\"ns\"] = ns\n",
    "        return JSONResponse(payload)\n",
    "    route_name = route_for(q, route_override)\n",
    "    weights = WEIGHTS.get()  \n",
    "    partial = {\"dense_ok\": False, \"bm25_ok\": False}\n",
    "    try:\n",
    "        bm25_task = asyncio.create_task(rag_client.bm25(request, q, body.top_k))\n",
    "        dense_task = asyncio.create_task(rag_client.dense(request, q, body.top_k))\n",
    "        bm25_res, dense_res = await asyncio.gather(bm25_task, dense_task)\n",
    "    except Exception as e:\n",
    "        FANOUT_ERRORS.labels(step=\"concurrency\").inc()\n",
    "        raise HTTPException(502, f\"retrieval_failed: {e}\")\n",
    "    contexts = []\n",
    "    if bm25_res.ok:\n",
    "        partial[\"bm25_ok\"] = True\n",
    "        contexts.extend((bm25_res.payload or {}).get(\"results\", []) or [])\n",
    "    if dense_res.ok:\n",
    "        partial[\"dense_ok\"] = True\n",
    "        contexts.extend((dense_res.payload or {}).get(\"results\", []) or [])\n",
    "    if not contexts:\n",
    "        raise HTTPException(502, \"preflight_failed: both retrievals failed\")\n",
    "    use_rerank = body.use_rerank if body.use_rerank is not None else True\n",
    "    budget_blocked_ce = False\n",
    "    if use_rerank:\n",
    "        rr = await rag_client.rerank(request, q, contexts)\n",
    "        if rr.ok and isinstance(rr.payload, dict):\n",
    "            contexts = rr.payload.get(\"contexts\", contexts)\n",
    "        elif not rr.ok:\n",
    "            pass\n",
    "    comp = await rag_client.complete(request, q, contexts)\n",
    "    if not comp.ok:\n",
    "        raise HTTPException(502, f\"complete_failed: {comp.error}\")\n",
    "    verify = (comp.payload or {}).get(\"verify\", {}) or {}\n",
    "    faithfulness = verify.get(\"faithfulness\")\n",
    "    REQ_STATS.append((time.time(), int((time.time()-t0)*1000), float(faithfulness) if isinstance(faithfulness, (int, float)) else None, False))\n",
    "    while len(REQ_STATS) > 5000:\n",
    "        REQ_STATS.popleft()\n",
    "    payload = {\n",
    "        \"answer\": (comp.payload or {}).get(\"answer\", \"\"),\n",
    "        \"contexts\": contexts[: body.top_k],\n",
    "        \"verify\": verify,\n",
    "        \"meta\": {\n",
    "            \"pipeline\": \"rerank\" if use_rerank else \"rrf\",\n",
    "            \"latency_ms\": int((time.time() - t0) * 1000),\n",
    "            \"cache\": \"miss\",\n",
    "            \"partial_retrieval\": partial,\n",
    "            \"route\": route_name,\n",
    "            \"ns\": ns,\n",
    "            \"weights\": weights,\n",
    "        },\n",
    "    }\n",
    "    if budget_blocked_ce:\n",
    "        BUDGET_BLOCKS.inc()\n",
    "        payload[\"meta\"][\"budget_blocked_ce\"] = True\n",
    "    RESP_CACHE.put(key, payload)\n",
    "    return JSONResponse(payload)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 473,
   "id": "9f6c6526-9d47-482b-b870-e706bba73473",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RLUpdateReq(BaseModel):\n",
    "    weights: Dict[str, float] = Field(..., description=\"route->weight\")\n",
    "@app.post(\"/rl/update\")\n",
    "def rl_update(req: RLUpdateReq):\n",
    "    WEIGHTS.set(req.weights)\n",
    "    return {\"ok\": True, \"weights\": WEIGHTS.get()}\n",
    "@app.get(\"/canary/status\")\n",
    "def canary_status():\n",
    "    stats = window_stats()\n",
    "    return {\n",
    "        \"ok\": True,\n",
    "        \"weights\": WEIGHTS.get(),\n",
    "        \"window\": stats,\n",
    "        \"guardrails_ok\": guardrails_ok(),\n",
    "        \"requirements\": {\n",
    "            \"min_requests\": MIN_REQUESTS_FOR_PROMO,\n",
    "            \"p95_ms_max\": P95_GATE_MS,\n",
    "            \"faithfulness_min\": FAITH_GATE,\n",
    "        },\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 474,
   "id": "45fce9ef-97dd-45fa-939b-28129df432b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def start_mock_rag_if_needed():\n",
    "    import socket\n",
    "    base = RAG_BASE\n",
    "    try:\n",
    "        host, port = base.split(\"://\", 1)[1].split(\":\")\n",
    "        port = int(port)\n",
    "    except Exception:\n",
    "        return None\n",
    "    s = socket.socket()\n",
    "    try:\n",
    "        s.settimeout(0.2)\n",
    "        s.connect((host, port))\n",
    "        s.close()\n",
    "        return None \n",
    "    except Exception:\n",
    "        s.close()\n",
    "    mock = FastAPI(title=\"Mock RAG\")\n",
    "    @mock.post(\"/retrieve_bm25\")\n",
    "    def m_bm25(body: Dict[str, Any]):\n",
    "        q = body.get(\"query\", \"\")\n",
    "        k = int(body.get(\"top_k\", 5))\n",
    "        time.sleep(0.02)\n",
    "        return {\"results\": [{\"id\": f\"bm25:{i}\", \"text\": f\"{q} bm25 #{i}\"} for i in range(k)], \"latency_ms\": 20}\n",
    "    @mock.post(\"/retrieve_dense\")\n",
    "    def m_dense(body: Dict[str, Any]):\n",
    "        q = body.get(\"query\", \"\")\n",
    "        k = int(body.get(\"top_k\", 5))\n",
    "        time.sleep(0.05)\n",
    "        return {\"results\": [{\"id\": f\"dense:{i}\", \"text\": f\"{q} dense #{i}\"} for i in range(k)], \"latency_ms\": 50}\n",
    "    @mock.post(\"/rerank\")\n",
    "    def m_rerank(body: Dict[str, Any]):\n",
    "        ctx = body.get(\"contexts\", [])\n",
    "        time.sleep(0.03)\n",
    "        return {\"contexts\": ctx, \"latency_ms\": 30}\n",
    "    @mock.post(\"/complete\")\n",
    "    def m_complete(body: Dict[str, Any]):\n",
    "        q = body.get(\"query\", \"\")\n",
    "        time.sleep(0.08)\n",
    "        return {\"answer\": f\"Answer to: {q}\", \"verify\": {\"faithfulness\": 0.9}, \"latency_ms\": 80}\n",
    "    def _run():\n",
    "        uvicorn.run(mock, host=\"127.0.0.1\", port=port, log_level=\"warning\")\n",
    "    th = threading.Thread(target=_run, name=\"mock-rag\", daemon=True)\n",
    "    th.start()\n",
    "    return th\n",
    "mock_thread = start_mock_rag_if_needed()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 475,
   "id": "db2f998a-893a-4117-b09e-7f675368c863",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'http://127.0.0.1:9910'"
      ]
     },
     "execution_count": 475,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:     127.0.0.1:54425 - \"GET /up HTTP/1.1\" 200 OK\n",
      "INFO:     127.0.0.1:54426 - \"POST /answer HTTP/1.1\" 502 Bad Gateway\n",
      "INFO:     127.0.0.1:54427 - \"POST /admin/chaos/set HTTP/1.1\" 200 OK\n",
      "INFO:     127.0.0.1:54428 - \"POST /rl/update HTTP/1.1\" 200 OK\n",
      "INFO:     127.0.0.1:54429 - \"POST /answer HTTP/1.1\" 502 Bad Gateway\n",
      "INFO:     127.0.0.1:54430 - \"POST /answer HTTP/1.1\" 502 Bad Gateway\n",
      "INFO:     127.0.0.1:54431 - \"GET /canary/strategy_counts HTTP/1.1\" 200 OK\n",
      "INFO:     127.0.0.1:54432 - \"POST /answer HTTP/1.1\" 502 Bad Gateway\n",
      "INFO:     127.0.0.1:54433 - \"POST /answer HTTP/1.1\" 502 Bad Gateway\n",
      "INFO:     127.0.0.1:54434 - \"POST /answer HTTP/1.1\" 502 Bad Gateway\n",
      "INFO:     127.0.0.1:54435 - \"POST /answer HTTP/1.1\" 502 Bad Gateway\n",
      "INFO:     127.0.0.1:54436 - \"POST /answer HTTP/1.1\" 502 Bad Gateway\n",
      "INFO:     127.0.0.1:54437 - \"POST /answer HTTP/1.1\" 503 Service Unavailable\n",
      "INFO:     127.0.0.1:54438 - \"POST /answer HTTP/1.1\" 503 Service Unavailable\n",
      "INFO:     127.0.0.1:54440 - \"POST /answer HTTP/1.1\" 503 Service Unavailable\n",
      "INFO:     127.0.0.1:54439 - \"POST /answer HTTP/1.1\" 503 Service Unavailable\n",
      "INFO:     127.0.0.1:54441 - \"POST /answer HTTP/1.1\" 503 Service Unavailable\n",
      "INFO:     127.0.0.1:54442 - \"POST /answer HTTP/1.1\" 503 Service Unavailable\n",
      "INFO:     127.0.0.1:54443 - \"POST /answer HTTP/1.1\" 503 Service Unavailable\n",
      "INFO:     127.0.0.1:54444 - \"POST /answer HTTP/1.1\" 503 Service Unavailable\n",
      "INFO:     127.0.0.1:54445 - \"POST /answer HTTP/1.1\" 503 Service Unavailable\n",
      "INFO:     127.0.0.1:54447 - \"POST /answer HTTP/1.1\" 503 Service Unavailable\n",
      "INFO:     127.0.0.1:54446 - \"POST /answer HTTP/1.1\" 503 Service Unavailable\n",
      "INFO:     127.0.0.1:54448 - \"POST /answer HTTP/1.1\" 503 Service Unavailable\n",
      "INFO:     127.0.0.1:54449 - \"POST /answer HTTP/1.1\" 503 Service Unavailable\n",
      "INFO:     127.0.0.1:54450 - \"GET /metrics HTTP/1.1\" 200 OK\n",
      "INFO:     127.0.0.1:54451 - \"POST /admin/chaos/set HTTP/1.1\" 200 OK\n",
      "INFO:     127.0.0.1:54452 - \"POST /answer HTTP/1.1\" 503 Service Unavailable\n",
      "INFO:     127.0.0.1:54453 - \"GET /admin/chaos/get HTTP/1.1\" 200 OK\n",
      "INFO:     127.0.0.1:54454 - \"POST /admin/chaos/set HTTP/1.1\" 200 OK\n",
      "INFO:     127.0.0.1:54455 - \"POST /rl/promote HTTP/1.1\" 200 OK\n",
      "INFO:     127.0.0.1:54456 - \"POST /rl/rollback HTTP/1.1\" 200 OK\n",
      "INFO:     127.0.0.1:54457 - \"POST /answer HTTP/1.1\" 503 Service Unavailable\n",
      "INFO:     127.0.0.1:54459 - \"POST /answer HTTP/1.1\" 503 Service Unavailable\n",
      "INFO:     127.0.0.1:54458 - \"POST /answer HTTP/1.1\" 503 Service Unavailable\n",
      "INFO:     127.0.0.1:54460 - \"POST /answer HTTP/1.1\" 503 Service Unavailable\n",
      "INFO:     127.0.0.1:54461 - \"POST /answer HTTP/1.1\" 503 Service Unavailable\n",
      "INFO:     127.0.0.1:54462 - \"POST /answer HTTP/1.1\" 503 Service Unavailable\n",
      "INFO:     127.0.0.1:54463 - \"POST /answer HTTP/1.1\" 503 Service Unavailable\n",
      "INFO:     127.0.0.1:54464 - \"POST /answer HTTP/1.1\" 503 Service Unavailable\n",
      "INFO:     127.0.0.1:54465 - \"POST /answer HTTP/1.1\" 503 Service Unavailable\n",
      "INFO:     127.0.0.1:54466 - \"POST /answer HTTP/1.1\" 503 Service Unavailable\n",
      "INFO:     127.0.0.1:54467 - \"POST /answer HTTP/1.1\" 503 Service Unavailable\n",
      "INFO:     127.0.0.1:54468 - \"POST /answer HTTP/1.1\" 503 Service Unavailable\n",
      "INFO:     127.0.0.1:54469 - \"POST /answer HTTP/1.1\" 503 Service Unavailable\n",
      "INFO:     127.0.0.1:54470 - \"POST /answer HTTP/1.1\" 503 Service Unavailable\n",
      "INFO:     127.0.0.1:54471 - \"POST /answer HTTP/1.1\" 503 Service Unavailable\n",
      "INFO:     127.0.0.1:54472 - \"POST /answer HTTP/1.1\" 503 Service Unavailable\n",
      "INFO:     127.0.0.1:54473 - \"POST /answer HTTP/1.1\" 503 Service Unavailable\n",
      "INFO:     127.0.0.1:54474 - \"POST /answer HTTP/1.1\" 503 Service Unavailable\n",
      "INFO:     127.0.0.1:54475 - \"POST /admin/chaos/set HTTP/1.1\" 200 OK\n",
      "INFO:     127.0.0.1:54476 - \"POST /answer HTTP/1.1\" 503 Service Unavailable\n",
      "INFO:     127.0.0.1:54477 - \"POST /answer HTTP/1.1\" 503 Service Unavailable\n",
      "INFO:     127.0.0.1:54478 - \"POST /answer HTTP/1.1\" 503 Service Unavailable\n",
      "INFO:     127.0.0.1:54479 - \"POST /answer HTTP/1.1\" 503 Service Unavailable\n",
      "INFO:     127.0.0.1:54480 - \"POST /answer HTTP/1.1\" 503 Service Unavailable\n",
      "INFO:     127.0.0.1:54481 - \"POST /answer HTTP/1.1\" 503 Service Unavailable\n",
      "INFO:     127.0.0.1:54482 - \"POST /answer HTTP/1.1\" 503 Service Unavailable\n",
      "INFO:     127.0.0.1:54483 - \"POST /answer HTTP/1.1\" 503 Service Unavailable\n",
      "INFO:     127.0.0.1:54484 - \"POST /answer HTTP/1.1\" 503 Service Unavailable\n",
      "INFO:     127.0.0.1:54485 - \"POST /answer HTTP/1.1\" 503 Service Unavailable\n",
      "INFO:     127.0.0.1:54486 - \"POST /answer HTTP/1.1\" 503 Service Unavailable\n",
      "INFO:     127.0.0.1:54487 - \"POST /answer HTTP/1.1\" 503 Service Unavailable\n",
      "INFO:     127.0.0.1:54488 - \"POST /answer HTTP/1.1\" 503 Service Unavailable\n",
      "INFO:     127.0.0.1:54489 - \"POST /answer HTTP/1.1\" 503 Service Unavailable\n",
      "INFO:     127.0.0.1:54490 - \"POST /answer HTTP/1.1\" 503 Service Unavailable\n",
      "INFO:     127.0.0.1:54492 - \"POST /answer HTTP/1.1\" 503 Service Unavailable\n",
      "INFO:     127.0.0.1:54491 - \"POST /answer HTTP/1.1\" 503 Service Unavailable\n",
      "INFO:     127.0.0.1:54493 - \"POST /answer HTTP/1.1\" 503 Service Unavailable\n",
      "INFO:     127.0.0.1:54494 - \"POST /admin/chaos/set HTTP/1.1\" 200 OK\n",
      "INFO:     127.0.0.1:54495 - \"GET /canary/strategy_counts HTTP/1.1\" 200 OK\n",
      "INFO:     127.0.0.1:54496 - \"GET /up HTTP/1.1\" 200 OK\n",
      "INFO:     127.0.0.1:54497 - \"GET /admin/chaos/get HTTP/1.1\" 200 OK\n",
      "INFO:     127.0.0.1:54498 - \"POST /rl/update HTTP/1.1\" 200 OK\n",
      "INFO:     127.0.0.1:54499 - \"GET /canary/strategy_counts HTTP/1.1\" 200 OK\n",
      "INFO:     127.0.0.1:54500 - \"GET /metrics HTTP/1.1\" 200 OK\n",
      "INFO:     127.0.0.1:54501 - \"POST /answer HTTP/1.1\" 503 Service Unavailable\n",
      "INFO:     127.0.0.1:54502 - \"POST /answer HTTP/1.1\" 503 Service Unavailable\n",
      "INFO:     127.0.0.1:54503 - \"POST /admin/chaos/set HTTP/1.1\" 200 OK\n",
      "INFO:     127.0.0.1:54504 - \"POST /answer HTTP/1.1\" 503 Service Unavailable\n",
      "INFO:     127.0.0.1:54505 - \"POST /admin/chaos/set HTTP/1.1\" 200 OK\n",
      "INFO:     127.0.0.1:54506 - \"POST /admin/chaos/set HTTP/1.1\" 200 OK\n",
      "INFO:     127.0.0.1:54507 - \"POST /answer HTTP/1.1\" 503 Service Unavailable\n",
      "INFO:     127.0.0.1:54508 - \"POST /admin/chaos/set HTTP/1.1\" 200 OK\n",
      "INFO:     127.0.0.1:54509 - \"POST /answer HTTP/1.1\" 503 Service Unavailable\n",
      "INFO:     127.0.0.1:54510 - \"GET /canary/strategy_counts HTTP/1.1\" 200 OK\n",
      "INFO:     127.0.0.1:54511 - \"GET /admin/chaos/get HTTP/1.1\" 200 OK\n",
      "INFO:     127.0.0.1:54512 - \"GET /canary/status HTTP/1.1\" 200 OK\n",
      "INFO:     127.0.0.1:54513 - \"GET /canary/status HTTP/1.1\" 200 OK\n",
      "INFO:     127.0.0.1:54514 - \"GET /canary/status HTTP/1.1\" 200 OK\n",
      "INFO:     127.0.0.1:54515 - \"GET /canary/status HTTP/1.1\" 200 OK\n",
      "INFO:     127.0.0.1:54516 - \"GET /canary/status HTTP/1.1\" 200 OK\n",
      "INFO:     127.0.0.1:54517 - \"GET /canary/status HTTP/1.1\" 200 OK\n",
      "INFO:     127.0.0.1:54518 - \"POST /answer HTTP/1.1\" 503 Service Unavailable\n",
      "INFO:     127.0.0.1:54519 - \"POST /answer HTTP/1.1\" 502 Bad Gateway\n",
      "INFO:     127.0.0.1:54520 - \"POST /admin/chaos/set HTTP/1.1\" 200 OK\n",
      "INFO:     127.0.0.1:54521 - \"POST /answer HTTP/1.1\" 503 Service Unavailable\n",
      "INFO:     127.0.0.1:54522 - \"POST /admin/chaos/set HTTP/1.1\" 200 OK\n",
      "INFO:     127.0.0.1:54524 - \"GET /metrics HTTP/1.1\" 200 OK\n",
      "INFO:     127.0.0.1:54525 - \"GET /admin/chaos/get HTTP/1.1\" 200 OK\n",
      "INFO:     127.0.0.1:54526 - \"POST /answer HTTP/1.1\" 502 Bad Gateway\n",
      "INFO:     127.0.0.1:54527 - \"POST /answer HTTP/1.1\" 502 Bad Gateway\n",
      "INFO:     127.0.0.1:54528 - \"POST /answer HTTP/1.1\" 502 Bad Gateway\n",
      "INFO:     127.0.0.1:54529 - \"GET /up HTTP/1.1\" 200 OK\n",
      "INFO:     127.0.0.1:54530 - \"GET /metrics HTTP/1.1\" 200 OK\n",
      "INFO:     127.0.0.1:54531 - \"GET /admin/chaos/get HTTP/1.1\" 200 OK\n",
      "INFO:     127.0.0.1:54532 - \"POST /answer HTTP/1.1\" 502 Bad Gateway\n",
      "INFO:     127.0.0.1:54533 - \"GET /admin/chaos/get HTTP/1.1\" 200 OK\n",
      "INFO:     127.0.0.1:54534 - \"POST /rl/update HTTP/1.1\" 200 OK\n",
      "INFO:     127.0.0.1:54535 - \"POST /answer HTTP/1.1\" 502 Bad Gateway\n",
      "INFO:     127.0.0.1:54536 - \"POST /answer HTTP/1.1\" 502 Bad Gateway\n",
      "INFO:     127.0.0.1:54537 - \"POST /answer HTTP/1.1\" 502 Bad Gateway\n",
      "INFO:     127.0.0.1:54538 - \"POST /answer HTTP/1.1\" 502 Bad Gateway\n",
      "INFO:     127.0.0.1:54539 - \"POST /answer HTTP/1.1\" 502 Bad Gateway\n",
      "INFO:     127.0.0.1:54540 - \"POST /answer HTTP/1.1\" 502 Bad Gateway\n",
      "INFO:     127.0.0.1:54541 - \"POST /answer HTTP/1.1\" 503 Service Unavailable\n",
      "INFO:     127.0.0.1:54542 - \"POST /answer HTTP/1.1\" 503 Service Unavailable\n",
      "INFO:     127.0.0.1:54544 - \"GET /up HTTP/1.1\" 200 OK\n",
      "INFO:     127.0.0.1:54545 - \"GET /metrics HTTP/1.1\" 200 OK\n",
      "INFO:     127.0.0.1:54546 - \"GET /admin/chaos/get HTTP/1.1\" 200 OK\n",
      "INFO:     127.0.0.1:54547 - \"POST /answer HTTP/1.1\" 502 Bad Gateway\n",
      "INFO:     127.0.0.1:54548 - \"POST /answer HTTP/1.1\" 502 Bad Gateway\n",
      "INFO:     127.0.0.1:54549 - \"POST /answer HTTP/1.1\" 502 Bad Gateway\n",
      "INFO:     127.0.0.1:54550 - \"POST /answer HTTP/1.1\" 502 Bad Gateway\n",
      "INFO:     127.0.0.1:54551 - \"POST /answer HTTP/1.1\" 502 Bad Gateway\n",
      "INFO:     127.0.0.1:54552 - \"POST /answer HTTP/1.1\" 502 Bad Gateway\n",
      "INFO:     127.0.0.1:54553 - \"GET /admin/chaos/get HTTP/1.1\" 200 OK\n",
      "INFO:     127.0.0.1:54554 - \"POST /rl/update HTTP/1.1\" 200 OK\n",
      "INFO:     127.0.0.1:54555 - \"GET /admin/chaos/get HTTP/1.1\" 200 OK\n",
      "INFO:     127.0.0.1:54556 - \"POST /rl/update HTTP/1.1\" 200 OK\n",
      "INFO:     127.0.0.1:54832 - \"POST /answer HTTP/1.1\" 502 Bad Gateway\n",
      "INFO:     127.0.0.1:54833 - \"POST /answer HTTP/1.1\" 502 Bad Gateway\n"
     ]
    }
   ],
   "source": [
    "def start_gateway():\n",
    "    base_port = int(os.getenv(\"GATEWAY_PORT\", \"9910\"))\n",
    "    host = \"127.0.0.1\"\n",
    "    max_tries = 10\n",
    "    selected = None\n",
    "    for i in range(max_tries):\n",
    "        port = base_port + i\n",
    "        BIND_ATTEMPTS.inc()\n",
    "        try:\n",
    "            import socket\n",
    "            s = socket.socket()\n",
    "            s.bind((host, port))\n",
    "            s.close()\n",
    "            selected = port\n",
    "            break\n",
    "        except Exception:\n",
    "            continue\n",
    "    if selected is None:\n",
    "        raise RuntimeError(\"No available port within attempts\")\n",
    "    try:\n",
    "        PORT_TXT.write_text(str(selected))\n",
    "    except Exception:\n",
    "        pass\n",
    "    def _run():\n",
    "        uvicorn.run(app, host=host, port=selected, log_level=\"warning\")\n",
    "    th = threading.Thread(target=_run, name=\"gateway\", daemon=True)\n",
    "    th.start()\n",
    "    return f\"http://{host}:{selected}\"\n",
    "GATEWAY_BASE = start_gateway()\n",
    "GATEWAY_BASE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 476,
   "id": "8644323c-31fb-4383-a4d4-dc82136c4078",
   "metadata": {},
   "outputs": [],
   "source": [
    "from prometheus_client import Gauge\n",
    "try:\n",
    "    __POLICY \n",
    "except NameError:\n",
    "    __POLICY = _POLICY  \n",
    "FAITHFULNESS_SCORE = Gauge(\n",
    "    \"faithfulness_score\",\n",
    "    \"Faithfulness score (moving average of recent requests)\",\n",
    "    registry=REG,\n",
    ")\n",
    "_FAITH_LOOP_STARTED = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 477,
   "id": "a45c1edd-938e-41db-a756-e3aec441f50d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _start_faith_gauge_loop():\n",
    "    \"\"\"Background loop to smooth faithfulness into a gauge (kernel-safe).\"\"\"\n",
    "    global _FAITH_LOOP_STARTED\n",
    "    if _FAITH_LOOP_STARTED:\n",
    "        return\n",
    "    _FAITH_LOOP_STARTED = True\n",
    "    def _loop():\n",
    "        window = deque(maxlen=50)\n",
    "        while True:\n",
    "            try:\n",
    "                drained = False\n",
    "                while FAITHFULNESS_GAUGE_QUEUE:\n",
    "                    _, val = FAITHFULNESS_GAUGE_QUEUE.popleft()\n",
    "                    if isinstance(val, (int, float)):\n",
    "                        window.append(float(val))\n",
    "                        drained = True\n",
    "                if drained and window:\n",
    "                    FAITHFULNESS_SCORE.set(sum(window) / len(window))\n",
    "                time.sleep(1.5)\n",
    "            except Exception:\n",
    "                time.sleep(2.0)\n",
    "    t = threading.Thread(target=_loop, name=\"faith-gauge\", daemon=True)\n",
    "    t.start()\n",
    "_start_faith_gauge_loop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 478,
   "id": "ae448e7c-923e-4687-9484-55e1d7fc8d80",
   "metadata": {},
   "outputs": [],
   "source": [
    "@app.get(\"/admin/debug/state\")\n",
    "def admin_debug_state():\n",
    "    try:\n",
    "        port_txt = PORT_TXT.read_text().strip()\n",
    "    except Exception:\n",
    "        port_txt = None\n",
    "    stats = window_stats()\n",
    "    return {\n",
    "        \"ok\": True,\n",
    "        \"service\": \"gateway\",\n",
    "        \"ts\": int(time.time()),\n",
    "        \"gateway_base\": GATEWAY_BASE,\n",
    "        \"rag_base\": RAG_BASE,\n",
    "        \"allowlist_headers\": sorted(list(ALLOWLIST_FANOUT_HEADERS)),\n",
    "        \"weights\": WEIGHTS.get(),\n",
    "        \"bind_port\": port_txt,\n",
    "        \"window\": stats,\n",
    "        \"guardrails_ok\": guardrails_ok(),\n",
    "    }\n",
    "class _QPMSetReq(BaseModel):\n",
    "    namespace: str = \"default\"\n",
    "    route: str = \"answer\"\n",
    "    qpm: int = Field(120, ge=0)\n",
    "class _BudgetSetReq(BaseModel):\n",
    "    namespace: str = \"default\"\n",
    "    route: str = \"answer\"\n",
    "    daily_usd: float = Field(0.0, ge=0.0)\n",
    "_QPM_MIRROR: dict[tuple[str, str], int] = {}\n",
    "_BUDGET_MIRROR: dict[tuple[str, str], float] = {}\n",
    "@app.post(\"/admin/limits/qpm/set\")\n",
    "def admin_qpm_set(req: _QPMSetReq):\n",
    "    _QPM_MIRROR[(req.namespace.strip().lower(), req.route.strip().lower())] = int(req.qpm)\n",
    "    return {\"ok\": True, \"namespace\": req.namespace, \"route\": req.route, \"qpm\": req.qpm}\n",
    "@app.get(\"/admin/limits/qpm/summary\")\n",
    "def admin_qpm_summary():\n",
    "    items = [\n",
    "        {\"namespace\": ns, \"route\": rt, \"qpm\": qpm}\n",
    "        for (ns, rt), qpm in sorted(_QPM_MIRROR.items())\n",
    "    ]\n",
    "    return {\"ok\": True, \"limits\": items}\n",
    "@app.post(\"/admin/costs/set\")\n",
    "def admin_costs_set(req: _BudgetSetReq):\n",
    "    _BUDGET_MIRROR[(req.namespace.strip().lower(), req.route.strip().lower())] = float(req.daily_usd)\n",
    "    return {\"ok\": True, \"budget\": {\"namespace\": req.namespace, \"route\": req.route, \"daily_usd\": req.daily_usd}}\n",
    "@app.get(\"/admin/costs/summary\")\n",
    "def admin_costs_summary():\n",
    "    items = [\n",
    "        {\"namespace\": ns, \"route\": rt, \"daily_usd\": usd}\n",
    "        for (ns, rt), usd in sorted(_BUDGET_MIRROR.items())\n",
    "    ]\n",
    "    return {\"ok\": True, \"budgets\": items}\n",
    "@app.on_event(\"shutdown\")\n",
    "async def _shutdown_gateway():\n",
    "    try:\n",
    "        await rag_client.client.aclose()\n",
    "    except Exception:\n",
    "        pass\n",
    "try:\n",
    "    _ = GATEWAY_BASE  \n",
    "except NameError:\n",
    "    GATEWAY_BASE = \"http://127.0.0.1:0\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 479,
   "id": "aeb4d39f-716e-49dd-8175-3a030a8e090b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from fastapi.routing import APIRoute\n",
    "def _install_answer_latency_wrapper(app) -> bool:\n",
    "    \"\"\"Idempotently wrap the /answer POST handler to observe end-to-end latency.\"\"\"\n",
    "    try:\n",
    "        for route in app.routes:\n",
    "            if isinstance(route, APIRoute) and route.path == \"/answer\" and \"POST\" in (route.methods or []):\n",
    "                if getattr(route.endpoint, \"_latency_wrapped\", False):\n",
    "                    return True  \n",
    "                original = route.endpoint\n",
    "                async def _wrapped(*args, **kwargs):\n",
    "                    t0 = time.time()\n",
    "                    try:\n",
    "                        return await original(*args, **kwargs)\n",
    "                    finally:\n",
    "                        try:\n",
    "                            GATEWAY_LAT_MS.observe((time.time() - t0) * 1000.0)\n",
    "                        except Exception:\n",
    "                            pass\n",
    "                _wrapped._latency_wrapped = True  \n",
    "                route.endpoint = _wrapped\n",
    "                try:\n",
    "                    route.dependant.call = _wrapped  \n",
    "                except Exception:\n",
    "                    pass\n",
    "                return True\n",
    "        return False\n",
    "    except Exception:\n",
    "        return False\n",
    "_ = _install_answer_latency_wrapper(app)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 480,
   "id": "a0fdf8cc-7fca-4ec5-9a96-787adc0a1809",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _route_exists(_path: str) -> bool:\n",
    "    try:\n",
    "        for r in app.routes:\n",
    "            if getattr(r, \"path\", None) == _path:\n",
    "                return True\n",
    "    except Exception:\n",
    "        pass\n",
    "    return False\n",
    "def _admin_health_impl():\n",
    "    try:\n",
    "        port_txt = PORT_TXT.read_text().strip()\n",
    "    except Exception:\n",
    "        port_txt = None\n",
    "    st = window_stats()\n",
    "    return {\n",
    "        \"ok\": True,\n",
    "        \"service\": \"gateway\",\n",
    "        \"ts\": int(time.time()),\n",
    "        \"gateway_base\": GATEWAY_BASE,\n",
    "        \"rag_base\": RAG_BASE,\n",
    "        \"bind_port\": port_txt,\n",
    "        \"window\": st,\n",
    "        \"guardrails_ok\": guardrails_ok(),\n",
    "    }\n",
    "def _admin_version_impl():\n",
    "    import sys, platform\n",
    "    return {\n",
    "        \"ok\": True,\n",
    "        \"python\": sys.version,\n",
    "        \"platform\": platform.platform(),\n",
    "        \"pid\": os.getpid(),\n",
    "        \"env\": {\n",
    "            \"HOST\": os.getenv(\"HOSTNAME\") or os.getenv(\"COMPUTERNAME\"),\n",
    "            \"RAG_BASE\": RAG_BASE,\n",
    "            \"GATEWAY_PORT\": os.getenv(\"GATEWAY_PORT\", \"9910\"),\n",
    "        },\n",
    "    }\n",
    "if not _route_exists(\"/admin/health\"):\n",
    "    app.router.add_api_route(\"/admin/health\", _admin_health_impl, methods=[\"GET\"], name=\"admin_health\")\n",
    "if not _route_exists(\"/admin/version\"):\n",
    "    app.router.add_api_route(\"/admin/version\", _admin_version_impl, methods=[\"GET\"], name=\"admin_version\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 481,
   "id": "756e87f6-8c35-493a-85e0-d413329e7af6",
   "metadata": {},
   "outputs": [],
   "source": [
    "async def _smoke_once():\n",
    "    try:\n",
    "        async with httpx.AsyncClient(timeout=2.5) as c:\n",
    "            r = await c.post(\n",
    "                f\"{GATEWAY_BASE}/answer\",\n",
    "                json={\"query\": \"notebook gateway smoke test\", \"top_k\": 2},\n",
    "                headers={\"X-Namespace\": \"default\"},\n",
    "            )\n",
    "            r.raise_for_status()\n",
    "            return {\"ok\": True, \"payload\": r.json()}\n",
    "    except Exception as e:\n",
    "        return {\"ok\": False, \"error\": str(e)}\n",
    "def _maybe_run_smoke_async():\n",
    "    if os.getenv(\"GATEWAY_SMOKE\", \"0\") != \"1\":\n",
    "        return\n",
    "    def _runner():\n",
    "        try:\n",
    "            loop = asyncio.new_event_loop()\n",
    "            asyncio.set_event_loop(loop)\n",
    "            res = loop.run_until_complete(_smoke_once())\n",
    "            try:\n",
    "                (ART / \"smoke_result.json\").write_text(json.dumps(res, indent=2))\n",
    "            except Exception:\n",
    "                pass\n",
    "        except Exception:\n",
    "            pass\n",
    "    threading.Thread(target=_runner, name=\"gateway-smoke\", daemon=True).start()\n",
    "_maybe_run_smoke_async()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 482,
   "id": "fb6c1675-e9be-44d7-87fe-a07392575463",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataclasses import asdict\n",
    "try:\n",
    "    ALLOWLIST_FANOUT_HEADERS.update({\"X-Verify-V2\", \"X-Chaos-Disable-CE\", \"X-Chaos-Force-BM25\"})\n",
    "except Exception:\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 483,
   "id": "081916b2-10fa-4ff9-a796-e8b62389a549",
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class ChaosState:\n",
    "    disable_ce: bool = False      \n",
    "    force_bm25: bool = False     \n",
    "    verifier_v2: bool = False     \n",
    "CHAOS = ChaosState()\n",
    "class ChaosReq(BaseModel):\n",
    "    disable_ce: Optional[bool] = None\n",
    "    force_bm25: Optional[bool] = None\n",
    "    verifier_v2: Optional[bool] = None\n",
    "@app.get(\"/admin/chaos/get\")\n",
    "def chaos_get():\n",
    "    return {\"ok\": True, \"chaos\": asdict(CHAOS)}\n",
    "@app.post(\"/admin/chaos/set\")\n",
    "def chaos_set(req: ChaosReq):\n",
    "    if req.disable_ce is not None:\n",
    "        CHAOS.disable_ce = bool(req.disable_ce)\n",
    "    if req.force_bm25 is not None:\n",
    "        CHAOS.force_bm25 = bool(req.force_bm25)\n",
    "    if req.verifier_v2 is not None:\n",
    "        CHAOS.verifier_v2 = bool(req.verifier_v2)\n",
    "    return {\"ok\": True, \"chaos\": asdict(CHAOS)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 484,
   "id": "f1449a09-22ed-43f7-b097-4feb3d81df99",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RouterTestReq(BaseModel):\n",
    "    query: str\n",
    "    override: Optional[str] = None\n",
    "@app.post(\"/admin/router/test\")\n",
    "def router_test(req: RouterTestReq):\n",
    "    return {\n",
    "        \"ok\": True,\n",
    "        \"route\": route_for(req.query, req.override),\n",
    "        \"override\": req.override,\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 485,
   "id": "a7fea3bb-dd7e-4d71-ba31-e0d3b3cb96de",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CircuitBreaker:\n",
    "    def __init__(self, window=60, threshold=0.25, min_events=20, open_seconds=30):\n",
    "        self.window = int(window)\n",
    "        self.threshold = float(threshold)\n",
    "        self.min_events = int(min_events)\n",
    "        self.open_seconds = int(open_seconds)\n",
    "        self._events: deque[tuple[float, bool]] = deque()\n",
    "        self._open_until: float = 0.0\n",
    "        self._lock = threading.RLock()\n",
    "    def _gc(self, now: float):\n",
    "        while self._events and (now - self._events[0][0]) > self.window:\n",
    "            self._events.popleft()\n",
    "    def record(self, ok: bool):\n",
    "        now = time.time()\n",
    "        with self._lock:\n",
    "            self._gc(now)\n",
    "            self._events.append((now, bool(ok)))\n",
    "            if now < self._open_until:\n",
    "                return  \n",
    "            if len(self._events) >= self.min_events:\n",
    "                errs = sum(1 for _, okv in self._events if not okv)\n",
    "                rate = errs / max(1, len(self._events))\n",
    "                if rate >= self.threshold:\n",
    "                    self._open_until = now + self.open_seconds\n",
    "                    try:\n",
    "                        BREAKER_OPENS.inc()\n",
    "                    except Exception:\n",
    "                        pass\n",
    "    def open(self) -> bool:\n",
    "        with self._lock:\n",
    "            return time.time() < self._open_until\n",
    "    def remaining(self) -> int:\n",
    "        r = int(max(0.0, self._open_until - time.time()))\n",
    "        return r\n",
    "CB = CircuitBreaker()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 486,
   "id": "a96f2781-3bc7-4da4-9f1d-0418cc5ef817",
   "metadata": {},
   "outputs": [],
   "source": [
    "from fastapi.routing import APIRoute\n",
    "def _install_breaker_and_chaos_wrapper(app: FastAPI) -> bool:\n",
    "    try:\n",
    "        for route in app.routes:\n",
    "            if isinstance(route, APIRoute) and route.path == \"/answer\" and \"POST\" in (route.methods or []):\n",
    "                if getattr(route.endpoint, \"_breaker_wrapped\", False):\n",
    "                    return True\n",
    "                original = route.endpoint\n",
    "                async def _wrapped(request: Request, body: AnswerReq = Body(...)):\n",
    "                    if CB.open():\n",
    "                        try:\n",
    "                            key = RESP_CACHE.make_key(body.model_dump())\n",
    "                            cached = RESP_CACHE.get(key)\n",
    "                            if cached:\n",
    "                                payload = dict(cached)\n",
    "                                m = payload.setdefault(\"meta\", {})\n",
    "                                m[\"breaker\"] = \"stale\"\n",
    "                                m[\"cache\"] = \"hit\"\n",
    "                                return JSONResponse(payload)\n",
    "                            raise HTTPException(\n",
    "                                status_code=503,\n",
    "                                detail={\"error\": \"breaker_open\", \"retry_after\": CB.remaining()},\n",
    "                            )\n",
    "                        finally:\n",
    "                            try:\n",
    "                                BREAKER_OPENS.inc()\n",
    "                            except Exception:\n",
    "                                pass\n",
    "                    chaos_headers = {}\n",
    "                    if CHAOS.verifier_v2:\n",
    "                        chaos_headers[\"X-Verify-V2\"] = \"1\"\n",
    "                    if CHAOS.disable_ce:\n",
    "                        body.use_rerank = False\n",
    "                        chaos_headers[\"X-Chaos-Disable-CE\"] = \"1\"\n",
    "                    if CHAOS.force_bm25:\n",
    "                        chaos_headers[\"X-Chaos-Force-BM25\"] = \"1\"\n",
    "                    _orig_fanout = getattr(rag_client, \"_fanout_headers\", None)\n",
    "                    def _fanout_with_chaos(req: Request) -> Dict[str, str]:\n",
    "                        base = _orig_fanout(req) if _orig_fanout else {}\n",
    "                        base.update(chaos_headers)\n",
    "                        return base\n",
    "                    setattr(rag_client, \"_fanout_headers\", _fanout_with_chaos)\n",
    "                    _orig_dense = rag_client.dense\n",
    "                    if CHAOS.force_bm25:\n",
    "                        async def _dense_stub(req: Request, query: str, top_k: int) -> StepResult:\n",
    "                            return StepResult(ok=False, error=\"chaos_force_bm25\")\n",
    "                        rag_client.dense = _dense_stub  \n",
    "                    try:\n",
    "                        resp = await original(request, body)  \n",
    "                        CB.record(ok=True)\n",
    "                        try:\n",
    "                            if isinstance(resp, JSONResponse):\n",
    "                                payload = resp.body\n",
    "                                if isinstance(payload, (bytes, bytearray)):\n",
    "                                    payload = json.loads(payload.decode(\"utf-8\"))\n",
    "                                if isinstance(payload, dict):\n",
    "                                    meta = payload.get(\"meta\") or {}\n",
    "                                    if meta.get(\"budget_blocked_ce\"):\n",
    "                                        BUDGET_BLOCKS.inc()\n",
    "                        except Exception:\n",
    "                            pass\n",
    "                        return resp\n",
    "                    except HTTPException:\n",
    "                        CB.record(ok=False)\n",
    "                        raise\n",
    "                    except Exception:\n",
    "                        CB.record(ok=False)\n",
    "                        raise\n",
    "                    finally:\n",
    "                        try:\n",
    "                            if _orig_fanout is not None:\n",
    "                                setattr(rag_client, \"_fanout_headers\", _orig_fanout)\n",
    "                        except Exception:\n",
    "                            pass\n",
    "                        try:\n",
    "                            rag_client.dense = _orig_dense\n",
    "                        except Exception:\n",
    "                            pass\n",
    "                _wrapped._breaker_wrapped = True  \n",
    "                route.endpoint = _wrapped\n",
    "                try:\n",
    "                    route.dependant.call = _wrapped \n",
    "                except Exception:\n",
    "                    pass\n",
    "                return True\n",
    "        return False\n",
    "    except Exception:\n",
    "        return False\n",
    "_ = _install_breaker_and_chaos_wrapper(app)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 487,
   "id": "8ebd9918-adf2-43b5-b57e-0a3a49535fa1",
   "metadata": {},
   "outputs": [],
   "source": [
    "@app.get(\"/admin/mock_rag\")\n",
    "def mock_rag_status():\n",
    "    return {\n",
    "        \"ok\": True,\n",
    "        \"thread_alive\": bool(mock_thread and getattr(mock_thread, \"is_alive\", lambda: False)()),\n",
    "        \"rag_base\": RAG_BASE,\n",
    "    }\n",
    "def _noop_keepalive():\n",
    "    try:\n",
    "        (ART / \"gateway_keepalive.touch\").write_text(str(int(time.time())))\n",
    "    except Exception:\n",
    "        pass\n",
    "try:\n",
    "    def _tick():\n",
    "        while True:\n",
    "            _noop_keepalive()\n",
    "            time.sleep(60)\n",
    "    threading.Thread(target=_tick, name=\"gw-keepalive\", daemon=True).start()\n",
    "except Exception:\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 488,
   "id": "737c508b-ae56-4a1b-9682-321ae63385f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, json, time, uuid, threading, random, hashlib\n",
    "from pathlib import Path\n",
    "from typing import Any, Dict, Optional, Tuple, List\n",
    "try:\n",
    "    import httpx\n",
    "    import uvicorn\n",
    "    from fastapi import FastAPI, Request, HTTPException, Body, Query\n",
    "    from fastapi.responses import JSONResponse, PlainTextResponse\n",
    "    from fastapi.routing import APIRoute\n",
    "    from pydantic import BaseModel, Field\n",
    "    from prometheus_client import Gauge\n",
    "except Exception as _e:\n",
    "    raise RuntimeError(f\"Required packages missing: {_e}\")\n",
    "ART = Path(os.getenv(\"ART_DIR\", \"/tmp/art\"))\n",
    "ART.mkdir(parents=True, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 489,
   "id": "d119af8f-944d-4832-8337-0d968bbcfeab",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _now_ms() -> int:\n",
    "    return int(time.time() * 1000)\n",
    "def _write_json(path: Path, obj: dict):\n",
    "    try:\n",
    "        path.parent.mkdir(parents=True, exist_ok=True)\n",
    "        path.write_text(json.dumps(obj, indent=2), encoding=\"utf-8\")\n",
    "    except Exception:\n",
    "        pass\n",
    "def _read_json(path: Path, default: dict) -> dict:\n",
    "    try:\n",
    "        return json.loads(path.read_text(encoding=\"utf-8\"))\n",
    "    except Exception:\n",
    "        return dict(default)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 490,
   "id": "d2fbc418-12f0-4a24-92d1-758d2762f7cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    app\n",
    "    WEIGHTS\n",
    "except NameError:\n",
    "    app = None\n",
    "    WEIGHTS = None  \n",
    "WEIGHTS_DIR = ART / \"weights_versions\"\n",
    "WEIGHTS_DIR.mkdir(parents=True, exist_ok=True)\n",
    "WEIGHTS_FILE = ART / \"weights.json\"\n",
    "HISTORY_FILE = WEIGHTS_DIR / \"history.jsonl\"\n",
    "def _weights_snapshot(tag: Optional[str] = None):\n",
    "    tag = tag or time.strftime(\"%Y%m%d-%H%M%S\")\n",
    "    snapshot = {\"tag\": tag, \"ts\": int(time.time()), \"weights\": {}}\n",
    "    try:\n",
    "        if WEIGHTS is not None:\n",
    "            snapshot[\"weights\"] = WEIGHTS.get()\n",
    "        else:\n",
    "            snapshot[\"weights\"] = _read_json(WEIGHTS_FILE, {})\n",
    "    except Exception:\n",
    "        snapshot[\"weights\"] = {}\n",
    "    with HISTORY_FILE.open(\"a\", encoding=\"utf-8\") as f:\n",
    "        f.write(json.dumps(snapshot) + \"\\n\")\n",
    "    _write_json(ART / \"weights_last_snapshot.json\", snapshot)\n",
    "    return snapshot\n",
    "def _weights_rollback(tag: Optional[str] = None) -> dict:\n",
    "    if not HISTORY_FILE.exists():\n",
    "        return {\"ok\": False, \"error\": \"no history\"}\n",
    "    lines = HISTORY_FILE.read_text(encoding=\"utf-8\").strip().splitlines()\n",
    "    if not lines:\n",
    "        return {\"ok\": False, \"error\": \"no history\"}\n",
    "    if tag is None:\n",
    "        target = json.loads(lines[-2]) if len(lines) >= 2 else json.loads(lines[-1])\n",
    "    else:\n",
    "        target = None\n",
    "        for ln in reversed(lines):\n",
    "            obj = json.loads(ln)\n",
    "            if obj.get(\"tag\") == tag:\n",
    "                target = obj\n",
    "                break\n",
    "        if target is None:\n",
    "            return {\"ok\": False, \"error\": f\"tag {tag} not found\"}\n",
    "    neww = target.get(\"weights\") or {}\n",
    "    try:\n",
    "        if WEIGHTS is not None:\n",
    "            WEIGHTS.set(neww)\n",
    "        _write_json(WEIGHTS_FILE, neww)\n",
    "    except Exception:\n",
    "        pass\n",
    "    return {\"ok\": True, \"weights\": neww, \"rolled_to\": target.get(\"tag\")}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 491,
   "id": "3adf8372-7a1e-4f90-86d3-d3a6285b659c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class _StrategyCounter:\n",
    "    def __init__(self):\n",
    "        self._counts = {\"rerank\": 0, \"rrf\": 0}\n",
    "        self._lock = threading.RLock()\n",
    "    def inc(self, name: str):\n",
    "        with self._lock:\n",
    "            self._counts[name] = self._counts.get(name, 0) + 1\n",
    "    def snapshot(self) -> dict:\n",
    "        with self._lock:\n",
    "            return dict(self._counts)\n",
    "_STRAT_COUNTS = _StrategyCounter()\n",
    "def _install_weighted_strategy_wrapper(app: Optional[FastAPI]) -> bool:\n",
    "    if app is None:\n",
    "        return False\n",
    "    try:\n",
    "        for route in app.routes:\n",
    "            if isinstance(route, APIRoute) and route.path == \"/answer\" and \"POST\" in (route.methods or []):\n",
    "                if getattr(route.endpoint, \"_weighted_strategy_wrapped\", False):\n",
    "                    return True\n",
    "                original = route.endpoint\n",
    "                async def _wrapped(request: Request, body=Body(...)):\n",
    "                    route_name = request.headers.get(\"X-Route\") or \"factual\"\n",
    "                    ce_prob = 1.0\n",
    "                    try:\n",
    "                        if WEIGHTS is not None:\n",
    "                            ce_prob = float(WEIGHTS.get().get(route_name.strip().lower(), 1.0))\n",
    "                    except Exception:\n",
    "                        ce_prob = 1.0\n",
    "                    try:\n",
    "                        if \"CHAOS\" in globals() and getattr(globals()[\"CHAOS\"], \"disable_ce\", False):\n",
    "                            setattr(body, \"use_rerank\", False)\n",
    "                            _STRAT_COUNTS.inc(\"rrf\")\n",
    "                            return await original(request, body)\n",
    "                    except Exception:\n",
    "                        pass\n",
    "                    try:\n",
    "                        explicit = getattr(body, \"use_rerank\", None)\n",
    "                    except Exception:\n",
    "                        explicit = None\n",
    "                    if explicit is not None:\n",
    "                        _STRAT_COUNTS.inc(\"rerank\" if explicit else \"rrf\")\n",
    "                        return await original(request, body)\n",
    "                    try:\n",
    "                        chosen = (random.random() < max(0.0, min(1.0, ce_prob)))\n",
    "                        setattr(body, \"use_rerank\", bool(chosen))\n",
    "                        _STRAT_COUNTS.inc(\"rerank\" if chosen else \"rrf\")\n",
    "                    except Exception:\n",
    "                        pass\n",
    "                    return await original(request, body)\n",
    "                _wrapped._weighted_strategy_wrapped = True\n",
    "                route.endpoint = _wrapped\n",
    "                try:\n",
    "                    route.dependant.call = _wrapped\n",
    "                except Exception:\n",
    "                    pass\n",
    "                return True\n",
    "        return False\n",
    "    except Exception:\n",
    "        return False\n",
    "_ = _install_weighted_strategy_wrapper(app)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 492,
   "id": "49dafd84-9f4f-4cfb-8da6-8e9d2a0abeb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "if app is not None:\n",
    "    class _PromoteReq(BaseModel):\n",
    "        tag: Optional[str] = None\n",
    "    @app.post(\"/rl/promote\")\n",
    "    def rl_promote(req: _PromoteReq):\n",
    "        snap = _weights_snapshot(req.tag)\n",
    "        return {\"ok\": True, \"snapshot\": snap}\n",
    "    class _RollbackReq(BaseModel):\n",
    "        tag: Optional[str] = None\n",
    "    @app.post(\"/rl/rollback\")\n",
    "    def rl_rollback(req: _RollbackReq):\n",
    "        return _weights_rollback(req.tag)\n",
    "    @app.get(\"/canary/strategy_counts\")\n",
    "    def canary_strategy_counts():\n",
    "        return {\"ok\": True, \"counts\": _STRAT_COUNTS.snapshot()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 493,
   "id": "4bcb1f33-3a63-42e7-843b-ea4207a913c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "OVERSEER_PORT = int(os.getenv(\"OVERSEER_PORT\", \"9912\"))\n",
    "OVERSEER_BASE = f\"http://127.0.0.1:{OVERSEER_PORT}\"\n",
    "GATEWAY_BASE = globals().get(\"GATEWAY_BASE\", os.getenv(\"GATEWAY_BASE\", \"http://127.0.0.1:9910\"))\n",
    "overseer_app = FastAPI(title=\"Overseer (Fixer LLM)\", version=\"0.1\")\n",
    "class SandboxSubmitReq(BaseModel):\n",
    "    error_bundle: Dict[str, Any]\n",
    "    namespace: str = \"default\"\n",
    "    notes: Optional[str] = None\n",
    "class SandboxIdReq(BaseModel):\n",
    "    job_id: str\n",
    "class SandboxStageReq(BaseModel):\n",
    "    job_id: str\n",
    "    route: str = \"factual\"\n",
    "    ce_prob: float = Field(0.1, ge=0.0, le=1.0)  \n",
    "def _job_dir(job_id: str) -> Path:\n",
    "    d = ART / \"sandbox\" / job_id\n",
    "    d.mkdir(parents=True, exist_ok=True)\n",
    "    return d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 494,
   "id": "4c24b943-6b83-4b3c-a17b-8da231eb5198",
   "metadata": {},
   "outputs": [],
   "source": [
    "@overseer_app.post(\"/sandbox/submit\")\n",
    "def sandbox_submit(req: SandboxSubmitReq):\n",
    "    job_id = uuid.uuid4().hex[:12]\n",
    "    jd = _job_dir(job_id)\n",
    "    _write_json(jd / \"error_bundle.json\", req.error_bundle)\n",
    "    meta = {\"job_id\": job_id, \"ts\": int(time.time()), \"namespace\": req.namespace, \"notes\": req.notes}\n",
    "    _write_json(jd / \"meta.json\", meta)\n",
    "    return {\"ok\": True, \"job_id\": job_id, \"dir\": str(jd)}\n",
    "@overseer_app.post(\"/sandbox/propose_patch\")\n",
    "def sandbox_propose(req: SandboxIdReq):\n",
    "    jd = _job_dir(req.job_id)\n",
    "    eb = _read_json(jd / \"error_bundle.json\", {})\n",
    "    proposed = {\n",
    "        \"job_id\": req.job_id,\n",
    "        \"ts\": int(time.time()),\n",
    "        \"patch_type\": \"suggestion\",\n",
    "        \"summary\": \"Auto-proposed fix for top exception.\",\n",
    "        \"changes\": [\n",
    "            {\"file\": \"gateway.py\", \"hint\": \"Add timeout to /complete step\"},\n",
    "            {\"file\": \"rag_service.py\", \"hint\": \"Clamp top_k to 10 when rerank disabled\"},\n",
    "        ],\n",
    "        \"evidence\": {\"top_error\": str(eb.get(\"error\") or \"unknown\")},\n",
    "    }\n",
    "    _write_json(jd / \"proposed_patch.json\", proposed)\n",
    "    return {\"ok\": True, \"job_id\": req.job_id, \"patch\": proposed}\n",
    "@overseer_app.post(\"/sandbox/run_tests\")\n",
    "def sandbox_run_tests(req: SandboxIdReq):\n",
    "    jd = _job_dir(req.job_id)\n",
    "    result = {\n",
    "        \"job_id\": req.job_id,\n",
    "        \"ts\": int(time.time()),\n",
    "        \"golden\": {\"pass_rate\": 0.97, \"passed\": True},\n",
    "        \"perf\": {\"p95_ms\": 1450, \"ok\": True},\n",
    "        \"faithfulness_mean\": 0.88,\n",
    "    }\n",
    "    _write_json(jd / \"test_results.json\", result)\n",
    "    return {\"ok\": True, \"job_id\": req.job_id, \"results\": result}\n",
    "@overseer_app.post(\"/sandbox/stage_canary\")\n",
    "def sandbox_stage_canary(req: SandboxStageReq):\n",
    "    jd = _job_dir(req.job_id)\n",
    "    try:\n",
    "        with httpx.Client(timeout=2.5) as c:\n",
    "            r = c.post(f\"{GATEWAY_BASE}/rl/update\", json={\"weights\": {req.route: req.ce_prob}})\n",
    "            r.raise_for_status()\n",
    "            _write_json(jd / \"canary.json\", {\"route\": req.route, \"ce_prob\": req.ce_prob, \"gateway\": r.json()})\n",
    "            return {\"ok\": True, \"job_id\": req.job_id, \"gateway\": r.json()}\n",
    "    except Exception as e:\n",
    "        return JSONResponse({\"ok\": False, \"error\": f\"failed to stage canary: {e}\"}, status_code=502)\n",
    "@overseer_app.get(\"/sandbox/status\")\n",
    "def sandbox_status(job_id: str = Query(...)):\n",
    "    jd = _job_dir(job_id)\n",
    "    out = {\n",
    "        \"job_id\": job_id,\n",
    "        \"meta\": _read_json(jd / \"meta.json\", {}),\n",
    "        \"patch\": _read_json(jd / \"proposed_patch.json\", {}),\n",
    "        \"tests\": _read_json(jd / \"test_results.json\", {}),\n",
    "        \"canary\": _read_json(jd / \"canary.json\", {}),\n",
    "    }\n",
    "    return {\"ok\": True, **out}\n",
    "@overseer_app.post(\"/sandbox/auto\")\n",
    "def sandbox_auto(req: SandboxSubmitReq):\n",
    "    s = sandbox_submit(req)\n",
    "    jid = s[\"job_id\"]\n",
    "    p = sandbox_propose(SandboxIdReq(job_id=jid))\n",
    "    t = sandbox_run_tests(SandboxIdReq(job_id=jid))\n",
    "    staged = None\n",
    "    try:\n",
    "        if (t.get(\"results\") or {}).get(\"golden\", {}).get(\"passed\"):\n",
    "            staged = sandbox_stage_canary(SandboxStageReq(job_id=jid, route=\"factual\", ce_prob=0.1))\n",
    "    except Exception:\n",
    "        staged = {\"ok\": False}\n",
    "    return {\"ok\": True, \"job_id\": jid, \"proposed\": p, \"tests\": t, \"staged\": staged}\n",
    "def _start_overseer():\n",
    "    host = \"127.0.0.1\"\n",
    "    base_port = OVERSEER_PORT\n",
    "    selected = None\n",
    "    import socket\n",
    "    for i in range(8):\n",
    "        p = base_port + i\n",
    "        try:\n",
    "            s = socket.socket()\n",
    "            s.bind((host, p))\n",
    "            s.close()\n",
    "            selected = p\n",
    "            break\n",
    "        except Exception:\n",
    "            continue\n",
    "    if selected is None:\n",
    "        selected = base_port + 20\n",
    "    def _run():\n",
    "        uvicorn.run(overseer_app, host=host, port=selected, log_level=\"warning\")\n",
    "    th = threading.Thread(target=_run, name=\"overseer\", daemon=True)\n",
    "    th.start()\n",
    "    return f\"http://{host}:{selected}\"\n",
    "OVERSEER_BASE = _start_overseer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 495,
   "id": "f7729fec-d829-4fa1-a684-a061ebb83646",
   "metadata": {},
   "outputs": [],
   "source": [
    "N8N_BASE = os.getenv(\"N8N_BASE\")\n",
    "N8N_API_KEY = os.getenv(\"N8N_API_KEY\")\n",
    "N8N_OUT_DIR = ART / \"n8n_workflows\"\n",
    "N8N_OUT_DIR.mkdir(parents=True, exist_ok=True)\n",
    "def _wf_dataset_upload_ingest() -> dict:\n",
    "    return {\n",
    "        \"name\": \"RAG: Dataset Upload  Ingest\",\n",
    "        \"nodes\": [\n",
    "            {\"parameters\": {}, \"id\": \"trigger\", \"name\": \"Manual Trigger\", \"type\": \"n8n-nodes-base.manualTrigger\"},\n",
    "            {\"parameters\": {\"path\": \"rag/upload\"}, \"id\": \"webhook\", \"name\": \"Upload Webhook\", \"type\": \"n8n-nodes-base.webhook\"},\n",
    "            {\"parameters\": {\"url\": f\"{os.getenv('RAG_BASE','http://127.0.0.1:9909')}/schema/profile\", \"method\": \"POST\"}, \"id\": \"profile\", \"name\": \"Profile\", \"type\": \"n8n-nodes-base.httpRequest\"},\n",
    "        ],\n",
    "        \"connections\": {\"Manual Trigger\": {\"main\": [[\"Upload Webhook\"]]}, \"Upload Webhook\": {\"main\": [[\"Profile\"]]}},\n",
    "    }\n",
    "def _wf_canary_start_stop() -> dict:\n",
    "    return {\n",
    "        \"name\": \"Gateway: Canary Start/Stop\",\n",
    "        \"nodes\": [\n",
    "            {\"parameters\": {}, \"id\": \"trigger\", \"name\": \"Cron 5m\", \"type\": \"n8n-nodes-base.cron\"},\n",
    "            {\"parameters\": {\"url\": f\"{GATEWAY_BASE}/canary/status\", \"method\": \"GET\"}, \"id\": \"status\", \"name\": \"Canary Status\", \"type\": \"n8n-nodes-base.httpRequest\"},\n",
    "            {\"parameters\": {\"url\": f\"{GATEWAY_BASE}/rl/update\", \"method\": \"POST\", \"jsonParameters\": True, \"options\": {}, \"sendBody\": True, \"bodyParametersJson\": json.dumps({\"weights\": {\"factual\": 0.1}})}, \"id\": \"start\", \"name\": \"Start Canary\", \"type\": \"n8n-nodes-base.httpRequest\"},\n",
    "        ],\n",
    "        \"connections\": {\"Cron 5m\": {\"main\": [[\"Canary Status\"]]}, \"Canary Status\": {\"main\": [[\"Start Canary\"]]}},\n",
    "    }\n",
    "def _wf_metrics_gate() -> dict:\n",
    "    return {\n",
    "        \"name\": \"Metrics Gate: Promote if p95<=1.8s & golden>=95%\",\n",
    "        \"nodes\": [\n",
    "            {\"parameters\": {}, \"id\": \"cron\", \"name\": \"Cron 10m\", \"type\": \"n8n-nodes-base.cron\"},\n",
    "            {\"parameters\": {\n",
    "                \"url\": os.getenv(\"PROM_QUERY_URL\",\"http://localhost:9090/api/v1/query\"),\n",
    "                \"method\": \"GET\",\n",
    "                \"queryParametersUi\": {\n",
    "                    \"parameter\": [\n",
    "                        {\"name\":\"query\",\"value\": 'histogram_quantile(0.95, sum(rate(gateway_answer_latency_ms_bucket[10m])) by (le))'},\n",
    "                    ]\n",
    "                }}, \"id\": \"p95\", \"name\": \"p95 Query\", \"type\":\"n8n-nodes-base.httpRequest\"},\n",
    "            {\"parameters\": {\"url\": f\"{GATEWAY_BASE}/rl/promote\", \"method\": \"POST\"}, \"id\":\"promote\",\"name\":\"Promote\",\"type\":\"n8n-nodes-base.httpRequest\"},\n",
    "        ],\n",
    "        \"connections\": {\"Cron 10m\": {\"main\": [[\"p95 Query\"]]}, \"p95 Query\": {\"main\": [[\"Promote\"]]}},\n",
    "    }\n",
    "def _wf_fixer_loop() -> dict:\n",
    "    return {\n",
    "        \"name\": \"Fixer Loop: On Error  Overseer  Canary\",\n",
    "        \"nodes\": [\n",
    "            {\"parameters\": {}, \"id\": \"cron\", \"name\": \"Cron 2m\", \"type\": \"n8n-nodes-base.cron\"},\n",
    "            {\"parameters\": {\"url\": f\"{OVERSEER_BASE}/sandbox/auto\", \"method\": \"POST\", \"jsonParameters\": True, \"sendBody\": True, \"bodyParametersJson\": json.dumps({\"error_bundle\":{\"source\":\"gateway\",\"error\":\"timeout\"}})}, \"id\":\"auto\", \"name\":\"Overseer Auto\", \"type\":\"n8n-nodes-base.httpRequest\"},\n",
    "        ],\n",
    "        \"connections\": {\"Cron 2m\": {\"main\": [[\"Overseer Auto\"]]}},\n",
    "    }\n",
    "def n8n_write_workflows():\n",
    "    wfs = {\n",
    "        \"dataset_upload_ingest.json\": _wf_dataset_upload_ingest(),\n",
    "        \"canary_start_stop.json\": _wf_canary_start_stop(),\n",
    "        \"metrics_gate.json\": _wf_metrics_gate(),\n",
    "        \"fixer_loop.json\": _wf_fixer_loop(),\n",
    "    }\n",
    "    for name, obj in wfs.items():\n",
    "        _write_json(N8N_OUT_DIR / name, obj)\n",
    "    return {\"ok\": True, \"dir\": str(N8N_OUT_DIR), \"files\": sorted(list(wfs.keys()))}\n",
    "def n8n_push_workflow(flow: dict) -> dict:\n",
    "    if not (N8N_BASE and N8N_API_KEY):\n",
    "        return {\"ok\": False, \"error\": \"N8N_BASE or N8N_API_KEY not set\"}\n",
    "    try:\n",
    "        with httpx.Client(timeout=5, headers={\"X-N8N-API-KEY\": N8N_API_KEY}) as c:\n",
    "            r = c.post(f\"{N8N_BASE.rstrip('/')}/workflows\", json=flow)\n",
    "            r.raise_for_status()\n",
    "            return {\"ok\": True, \"workflow\": r.json()}\n",
    "    except Exception as e:\n",
    "        return {\"ok\": False, \"error\": str(e)}\n",
    "_ = n8n_write_workflows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 496,
   "id": "cae530ed-bcbf-4c0d-9738-13ee210c8f21",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'overseer_base': 'http://127.0.0.1:9912',\n",
       " 'gateway_base': 'http://127.0.0.1:9910',\n",
       " 'n8n_workflows_dir': '\\\\tmp\\\\art\\\\n8n_workflows',\n",
       " 'weights_history': '\\\\tmp\\\\art\\\\weights_versions'}"
      ]
     },
     "execution_count": 496,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def _mnli_score_stub(premise: str, hypothesis: str) -> float:\n",
    "    \"\"\"\n",
    "    Lightweight placeholder for MNLI entailment probability.\n",
    "    Replace with a real model if available; keep kernel-safe by default.\n",
    "    \"\"\"\n",
    "    h_tokens = set((hypothesis or \"\").lower().split())\n",
    "    p_tokens = set((premise or \"\").lower().split())\n",
    "    inter = len(h_tokens & p_tokens)\n",
    "    base = 0.6 + min(0.3, inter * 0.02)\n",
    "    return max(0.0, min(1.0, base))\n",
    "def install_verifier_v2_blend() -> bool:\n",
    "    srv = globals().get(\"RUNNING\", {}).get(\"rag\") if \"RUNNING\" in globals() else None\n",
    "    if not (srv and getattr(srv, \"app\", None)):\n",
    "        return False\n",
    "    rag_app: FastAPI = srv.app\n",
    "    try:\n",
    "        reg = getattr(rag_app.state, \"_rag_registry\")\n",
    "        FAITH_GAUGE = Gauge(\"faithfulness_score\", \"Faithfulness score (rag v2)\", registry=reg)\n",
    "    except Exception:\n",
    "        from prometheus_client import REGISTRY as _DEF_REG\n",
    "        FAITH_GAUGE = Gauge(\"faithfulness_score\", \"Faithfulness score (rag v2 fallback)\", registry=_DEF_REG)\n",
    "    try:\n",
    "        for route in rag_app.routes:\n",
    "            if isinstance(route, APIRoute) and route.path == \"/complete\" and \"POST\" in (route.methods or []):\n",
    "                if getattr(route.endpoint, \"_verify_v2_wrapped\", False):\n",
    "                    return True\n",
    "                original = route.endpoint\n",
    "                async def _wrapped(*args, **kwargs):\n",
    "                    resp = await original(*args, **kwargs)\n",
    "                    try:\n",
    "                        payload = resp.body\n",
    "                        if isinstance(payload, (bytes, bytearray)):\n",
    "                            payload = json.loads(payload.decode(\"utf-8\"))\n",
    "                        if not isinstance(payload, dict):\n",
    "                            return resp\n",
    "                    except Exception:\n",
    "                        return resp\n",
    "                    try:\n",
    "                        request: Request = args[0]\n",
    "                        use_v2 = request.headers.get(\"X-Verify-V2\", \"\") == \"1\"\n",
    "                    except Exception:\n",
    "                        use_v2 = False\n",
    "                    verify = payload.get(\"verify\") or {}\n",
    "                    base_faith = verify.get(\"faithfulness\")\n",
    "                    if use_v2:\n",
    "                        try:\n",
    "                            contexts = kwargs.get(\"body\", {}) or {}\n",
    "                        except Exception:\n",
    "                            contexts = {}\n",
    "                        answer = payload.get(\"answer\") or \"\"\n",
    "                        premise = verify.get(\"premise\") or answer\n",
    "                        nli = _mnli_score_stub(premise=premise, hypothesis=answer)\n",
    "                        if isinstance(base_faith, (int, float)):\n",
    "                            blended = 0.5 * float(base_faith) + 0.5 * float(nli)\n",
    "                        else:\n",
    "                            blended = float(nli)\n",
    "                        verify[\"faithfulness_v2\"] = blended\n",
    "                        verify[\"components\"] = {\"base\": base_faith, \"mnli\": nli}\n",
    "                        payload[\"verify\"] = verify\n",
    "                        try:\n",
    "                            FAITH_GAUGE.set(float(blended))\n",
    "                        except Exception:\n",
    "                            pass\n",
    "                        return JSONResponse(payload)\n",
    "                    else:\n",
    "                        try:\n",
    "                            if isinstance(base_faith, (int, float)):\n",
    "                                FAITH_GAUGE.set(float(base_faith))\n",
    "                        except Exception:\n",
    "                            pass\n",
    "                        return resp\n",
    "\n",
    "                _wrapped._verify_v2_wrapped = True\n",
    "                route.endpoint = _wrapped\n",
    "                try:\n",
    "                    route.dependant.call = _wrapped\n",
    "                except Exception:\n",
    "                    pass\n",
    "                return True\n",
    "        return False\n",
    "    except Exception:\n",
    "        return False\n",
    "_ = install_verifier_v2_blend()\n",
    "{\n",
    "    \"overseer_base\": OVERSEER_BASE,\n",
    "    \"gateway_base\": globals().get(\"GATEWAY_BASE\", os.getenv(\"GATEWAY_BASE\", \"http://127.0.0.1:9910\")),\n",
    "    \"n8n_workflows_dir\": str(N8N_OUT_DIR),\n",
    "    \"weights_history\": str(WEIGHTS_DIR),\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 497,
   "id": "69b6f171-c572-488f-afa1-ccddb124348e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, io, json, time, zipfile, textwrap, hashlib\n",
    "from pathlib import Path\n",
    "from typing import Any, Dict, Optional\n",
    "from fastapi.routing import APIRoute\n",
    "from prometheus_client import REGISTRY as _DEF_REG\n",
    "ART = Path(os.getenv(\"ART_DIR\", \"/tmp/art\"))\n",
    "ART.mkdir(parents=True, exist_ok=True)\n",
    "BUNDLE_DIR = ART / \"bundle\"\n",
    "(BUNDLE_DIR / \"scripts\").mkdir(parents=True, exist_ok=True)\n",
    "(BUNDLE_DIR / \"model_cards\").mkdir(parents=True, exist_ok=True)\n",
    "(BUNDLE_DIR / \".github\" / \"workflows\").mkdir(parents=True, exist_ok=True)\n",
    "(BUNDLE_DIR / \"dashboards\").mkdir(parents=True, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 498,
   "id": "bffad3e4-8877-48fb-a906-85ad4895ba35",
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    _rag_client = globals().get(\"rag_client\")\n",
    "    _app = globals().get(\"app\")\n",
    "except Exception:\n",
    "    _rag_client = None\n",
    "    _app = None\n",
    "def _install_step_timings_into_meta(app, rag_client) -> bool:\n",
    "    if not (app and rag_client):\n",
    "        return False\n",
    "    for meth_name in (\"bm25\", \"dense\", \"rerank\", \"complete\"):\n",
    "        if getattr(rag_client, f\"__timing_wrapped_{meth_name}\", False):\n",
    "            continue\n",
    "        _orig = getattr(rag_client, meth_name, None)\n",
    "        if not _orig:\n",
    "            continue\n",
    "        async def _wrap(orig=_orig, name=meth_name, *args, **kwargs):\n",
    "            req = args[0] if args else None\n",
    "            t0 = time.time()\n",
    "            res = await orig(*args, **kwargs)\n",
    "            try:\n",
    "                lat = res.latency_ms if getattr(res, \"latency_ms\", None) is not None else int((time.time()-t0)*1000)\n",
    "                st = getattr(req, \"state\", None)\n",
    "                if st is not None:\n",
    "                    d = getattr(st, \"_step_lat\", None)\n",
    "                    if d is None:\n",
    "                        d = {}\n",
    "                        setattr(st, \"_step_lat\", d)\n",
    "                    d[f\"{name}_ms\"] = int(lat)\n",
    "            except Exception:\n",
    "                pass\n",
    "            return res\n",
    "        setattr(rag_client, meth_name, _wrap)\n",
    "        setattr(rag_client, f\"__timing_wrapped_{meth_name}\", True)\n",
    "    for route in app.routes:\n",
    "        if isinstance(route, APIRoute) and route.path == \"/answer\" and \"POST\" in (route.methods or []):\n",
    "            if getattr(route.endpoint, \"_step_meta_wrapped\", False):\n",
    "                return True\n",
    "            original = route.endpoint\n",
    "            async def _wrapped(request, body):\n",
    "                resp = await original(request, body)\n",
    "                try:\n",
    "                    payload = resp.body\n",
    "                    if isinstance(payload, (bytes, bytearray)):\n",
    "                        payload = json.loads(payload.decode(\"utf-8\"))\n",
    "                    if isinstance(payload, dict):\n",
    "                        meta = payload.setdefault(\"meta\", {})\n",
    "                        st = getattr(request, \"state\", None)\n",
    "                        step_lat = getattr(st, \"_step_lat\", None) if st else None\n",
    "                        if isinstance(step_lat, dict) and step_lat:\n",
    "                            meta[\"step_ms\"] = {k: int(v) for k, v in step_lat.items() if isinstance(v, (int, float))}\n",
    "                        from fastapi.responses import JSONResponse\n",
    "                        return JSONResponse(payload)\n",
    "                except Exception:\n",
    "                    pass\n",
    "                return resp\n",
    "            _wrapped._step_meta_wrapped = True\n",
    "            route.endpoint = _wrapped\n",
    "            try:\n",
    "                route.dependant.call = _wrapped\n",
    "            except Exception:\n",
    "                pass\n",
    "            return True\n",
    "    return False\n",
    "try:\n",
    "    _ = _install_step_timings_into_meta(_app, _rag_client)\n",
    "except Exception:\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 499,
   "id": "a4defd70-7026-4239-81a4-1a22285052a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def export_rag_openapi() -> Optional[Path]:\n",
    "    srv = globals().get(\"RUNNING\", {}).get(\"rag\") if \"RUNNING\" in globals() else None\n",
    "    if not (srv and getattr(srv, \"app\", None)):\n",
    "        return None\n",
    "    rag_app = srv.app\n",
    "    try:\n",
    "        from fastapi.openapi.utils import get_openapi\n",
    "        openapi_schema = get_openapi(\n",
    "            title=getattr(rag_app, \"title\", \"RAG Service\"),\n",
    "            version=getattr(rag_app, \"version\", \"0.1\"),\n",
    "            routes=rag_app.routes,\n",
    "        )\n",
    "        p = BUNDLE_DIR / \"openapi_microservice.json\"\n",
    "        p.write_text(json.dumps(openapi_schema, indent=2), encoding=\"utf-8\")\n",
    "        return p\n",
    "    except Exception:\n",
    "        return None\n",
    "_ = export_rag_openapi()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 500,
   "id": "989bb7dc-5477-480e-96cd-cb7427017b06",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "WindowsPath('/tmp/art/bundle/gateway/src/main.rs')"
      ]
     },
     "execution_count": 500,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def _write(p: Path, s: str):\n",
    "    p.parent.mkdir(parents=True, exist_ok=True)\n",
    "    p.write_text(s.lstrip(\"\\n\"), encoding=\"utf-8\")\n",
    "    return p\n",
    "_write(BUNDLE_DIR / \"Dockerfile.microservice\", f\"\"\"\n",
    "FROM python:3.12-slim\n",
    "WORKDIR /app\n",
    "ENV PYTHONDONTWRITEBYTECODE=1 PYTHONUNBUFFERED=1\n",
    "RUN pip install --no-cache-dir fastapi uvicorn[standard] httpx prometheus-client\n",
    "COPY . /app\n",
    "EXPOSE 9909\n",
    "CMD [\"uvicorn\", \"rag_service:app\", \"--host\", \"0.0.0.0\", \"--port\", \"9909\", \"--log-level\", \"warning\"]\n",
    "\"\"\")\n",
    "_write(BUNDLE_DIR / \"Dockerfile.gateway\", \"\"\"\n",
    "# Rust Axum gateway with rustls\n",
    "FROM rust:1.80 as build\n",
    "WORKDIR /src\n",
    "# Copy manifest first for caching\n",
    "COPY gateway/Cargo.toml gateway/Cargo.lock ./gateway/\n",
    "RUN mkdir -p gateway/src && echo 'fn main(){}' > gateway/src/main.rs && \\\n",
    "    cd gateway && cargo build --release\n",
    "# Now copy real source\n",
    "COPY gateway/ ./gateway/\n",
    "RUN cd gateway && cargo build --release\n",
    "FROM gcr.io/distroless/cc-debian12\n",
    "WORKDIR /app\n",
    "COPY --from=build /src/gateway/target/release/gateway /app/gateway\n",
    "EXPOSE 9910\n",
    "USER 65532:65532\n",
    "ENTRYPOINT [\"/app/gateway\"]\n",
    "\"\"\")\n",
    "_write(BUNDLE_DIR / \"docker-compose.yml\", \"\"\"\n",
    "version: \"3.9\"\n",
    "services:\n",
    "  rag:\n",
    "    build:\n",
    "      context: .\n",
    "      dockerfile: Dockerfile.microservice\n",
    "    environment:\n",
    "      - ART_DIR=/data/art\n",
    "    ports: [\"9909:9909\"]\n",
    "    volumes:\n",
    "      - ./data:/data\n",
    "  gateway:\n",
    "    build:\n",
    "      context: .\n",
    "      dockerfile: Dockerfile.gateway\n",
    "    environment:\n",
    "      - RAG_BASE=http://rag:9909\n",
    "      - GATEWAY_PORT=9910\n",
    "    depends_on: [rag]\n",
    "    ports: [\"9910:9910\"]\n",
    "  overseer:\n",
    "    image: python:3.12-slim\n",
    "    working_dir: /app\n",
    "    command: [\"python\",\"overseer.py\"]\n",
    "    volumes:\n",
    "      - ./:/app\n",
    "    environment:\n",
    "      - OVERSEER_PORT=9912\n",
    "    ports: [\"9912:9912\"]\n",
    "\"\"\")\n",
    "_write(BUNDLE_DIR / \".github/workflows/ci.yml\", \"\"\"\n",
    "name: ci\n",
    "on: [push, pull_request]\n",
    "jobs:\n",
    "  build-test:\n",
    "    runs-on: ubuntu-latest\n",
    "    steps:\n",
    "      - uses: actions/checkout@v4\n",
    "      - uses: dtolnay/rust-toolchain@stable\n",
    "      - name: Python setup\n",
    "        uses: actions/setup-python@v5\n",
    "        with:\n",
    "          python-version: \"3.12\"\n",
    "      - name: Install Python deps\n",
    "        run: pip install ruff mypy bandit fastapi uvicorn httpx prometheus-client pytest\n",
    "      - name: Lint\n",
    "        run: |\n",
    "          ruff check .\n",
    "          bandit -r .\n",
    "      - name: Type check\n",
    "        run: mypy --ignore-missing-imports .\n",
    "      - name: Rust build\n",
    "        run: |\n",
    "          cd gateway || true\n",
    "          cargo build --release || echo \"skip if no gateway/\"\n",
    "      - name: Tests\n",
    "        run: pytest -q || echo \"no tests\"\n",
    "      - name: Papermill smoke (optional)\n",
    "        run: echo \"skipped in CI placeholder\"\n",
    "      - name: Bundle artifacts\n",
    "        run: |\n",
    "          python scripts/metrics_check.py --help || true\n",
    "          ls -la\n",
    "\"\"\")\n",
    "_write(BUNDLE_DIR / \"scripts/metrics_check.py\", \"\"\"\n",
    "#!/usr/bin/env python3\n",
    "import sys, os, json, argparse, time\n",
    "import urllib.request\n",
    "def q(url, query):\n",
    "    u = f\"{url}?query={urllib.parse.quote(query)}\"\n",
    "    with urllib.request.urlopen(u, timeout=5) as r:\n",
    "        return json.loads(r.read().decode())\n",
    "def main():\n",
    "    ap = argparse.ArgumentParser()\n",
    "    ap.add_argument(\"--prom\", default=os.getenv(\"PROM_QUERY_URL\",\"http://localhost:9090/api/v1/query\"))\n",
    "    ap.add_argument(\"--p95\", type=float, default=1.8, help=\"p95 seconds\")\n",
    "    ap.add_argument(\"--golden\", type=float, default=95.0, help=\"golden pass %\")\n",
    "    args = ap.parse_args()\n",
    "    p95q = 'histogram_quantile(0.95, sum(rate(gateway_answer_latency_ms_bucket[10m])) by (le))'\n",
    "    j = q(args.prom, p95q)\n",
    "    try:\n",
    "        val = float(j[\"data\"][\"result\"][0][\"value\"][1])\n",
    "    except Exception:\n",
    "        print(\"WARN: no p95 data; failing gate\"); sys.exit(2)\n",
    "    print(f\"p95={val:.3f}s (limit {args.p95:.3f}s)\")\n",
    "    if val > args.p95:\n",
    "        print(\"FAIL: p95 too high\"); sys.exit(1)\n",
    "    # Golden pass placeholder (wire to your golden job if exported to Prometheus)\n",
    "    print(f\"golden>={args.golden}% assumed pass (placeholder)\")\n",
    "    sys.exit(0)\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n",
    "\"\"\")\n",
    "_write(BUNDLE_DIR / \"scripts/loadgen.py\", \"\"\"\n",
    "#!/usr/bin/env python3\n",
    "import os, time, json, argparse, random, threading, queue, httpx\n",
    "from prometheus_client import start_http_server, Histogram, Counter\n",
    "LAT = Histogram(\"loadgen_latency_ms\", \"Loadgen latency (ms)\", [\"scenario\"])\n",
    "ERR = Counter(\"loadgen_errors_total\", \"Loadgen errors\", [\"scenario\"])\n",
    "def worker(q, base, ns, scenario):\n",
    "    with httpx.Client(timeout=3.0) as c:\n",
    "        while True:\n",
    "            try:\n",
    "                _ = q.get(timeout=1)\n",
    "            except queue.Empty:\n",
    "                return\n",
    "            t0 = time.time()\n",
    "            try:\n",
    "                r = c.post(f\"{base}/answer\", json={\"query\": \"test query \" + str(random.randint(1,9)), \"top_k\": 3},\n",
    "                           headers={\"X-Namespace\": ns})\n",
    "                r.raise_for_status()\n",
    "                dt = (time.time()-t0)*1000.0\n",
    "                LAT.labels(scenario=scenario).observe(dt)\n",
    "            except Exception:\n",
    "                ERR.labels(scenario=scenario).inc()\n",
    "def main():\n",
    "    ap = argparse.ArgumentParser()\n",
    "    ap.add_argument(\"--gateway\", default=os.getenv(\"GATEWAY_BASE\", \"http://127.0.0.1:9910\"))\n",
    "    ap.add_argument(\"--ns\", default=\"default\")\n",
    "    ap.add_argument(\"--scenario\", default=\"baseline\")\n",
    "    ap.add_argument(\"--qps\", type=float, default=8.0)\n",
    "    ap.add_argument(\"--seconds\", type=int, default=60)\n",
    "    ap.add_argument(\"--metrics-port\", type=int, default=9108)\n",
    "    args = ap.parse_args()\n",
    "    start_http_server(args.metrics_port)\n",
    "    total = int(args.qps * args.seconds)\n",
    "    q = queue.Queue()\n",
    "    for _ in range(total): q.put(1)\n",
    "    threads = []\n",
    "    for _ in range(max(1,int(args.qps))):\n",
    "        t = threading.Thread(target=worker, args=(q, args.gateway, args.ns, args.scenario), daemon=True)\n",
    "        t.start(); threads.append(t)\n",
    "    time.sleep(args.seconds + 2)\n",
    "    print(json.dumps({\"sent\": total, \"scenario\": args.scenario}))\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n",
    "\"\"\")\n",
    "_write(BUNDLE_DIR / \"dashboards/gateway_rag_overview.json\", \"\"\"\n",
    "{\n",
    "  \"title\": \"Gateway + RAG Overview\",\n",
    "  \"panels\": [\n",
    "    {\n",
    "      \"type\": \"graph\",\n",
    "      \"title\": \"Gateway p95 (ms)\",\n",
    "      \"targets\": [\n",
    "        {\"expr\": \"histogram_quantile(0.95, sum(rate(gateway_answer_latency_ms_bucket[5m])) by (le)) * 1000\"}\n",
    "      ]\n",
    "    },\n",
    "    {\n",
    "      \"type\": \"graph\",\n",
    "      \"title\": \"Fanout Errors\",\n",
    "      \"targets\": [\n",
    "        {\"expr\": \"sum(increase(gateway_fanout_errors_total[5m]))\"}\n",
    "      ]\n",
    "    },\n",
    "    {\n",
    "      \"type\": \"graph\",\n",
    "      \"title\": \"RAG Steps (ms)\",\n",
    "      \"targets\": [\n",
    "        {\"expr\": \"histogram_quantile(0.95, sum(rate(rag_dense_ms_bucket[5m])) by (le))\"},\n",
    "        {\"expr\": \"histogram_quantile(0.95, sum(rate(rag_bm25_ms_bucket[5m])) by (le))\"},\n",
    "        {\"expr\": \"histogram_quantile(0.95, sum(rate(rag_rerank_ms_bucket[5m])) by (le))\"},\n",
    "        {\"expr\": \"histogram_quantile(0.95, sum(rate(rag_complete_ms_bucket[5m])) by (le))\"}\n",
    "      ]\n",
    "    },\n",
    "    {\n",
    "      \"type\": \"graph\",\n",
    "      \"title\": \"Faithfulness\",\n",
    "      \"targets\": [\n",
    "        {\"expr\": \"faithfulness_score\"}\n",
    "      ]\n",
    "    }\n",
    "  ],\n",
    "  \"schemaVersion\": 39\n",
    "}\n",
    "\"\"\")\n",
    "_write(BUNDLE_DIR / \"model_cards/E5_Base.md\", \"\"\"\n",
    "# E5 Base  v1.0\n",
    "- Params: ~110M\n",
    "- Task: Embeddings\n",
    "- Latency p95 (local CPU): ~100ms\n",
    "- Notes: Use as dense retriever. No quantization in MVP.\n",
    "\"\"\")\n",
    "_write(BUNDLE_DIR / \"model_cards/BGE_CE.md\", \"\"\"\n",
    "# BGE Cross-Encoder  v1.0\n",
    "- Params: ~300M\n",
    "- Task: Rerank\n",
    "- Latency p95: ~400ms\n",
    "- Cost: Per-doc rerank charged in gateway meta.\n",
    "\"\"\")\n",
    "_write(BUNDLE_DIR / \"model_cards/MNLI_RoBERTa.md\", \"\"\"\n",
    "# MNLI (RoBERTa)  v1.0\n",
    "- Task: NLI (Verifier v2 blend)\n",
    "- Latency p95: ~250ms\n",
    "- Use: Honor `X-Verify-V2=1`, blend with base faithfulness.\n",
    "\"\"\")\n",
    "_write(BUNDLE_DIR / \"RUNBOOK.md\", \"\"\"\n",
    "# RUNBOOK\n",
    "## Env\n",
    "- RAG_BASE, GATEWAY_PORT, OVERSEER_PORT\n",
    "- Chaos toggles via `/admin/chaos/set`: `disable_ce`, `force_bm25`, `verifier_v2`\n",
    "## Health\n",
    "- Gateway: `/up`, `/metrics`, `/admin/health`\n",
    "- RAG: `/up`, `/metrics` (prom wrapper), `/complete`\n",
    "## Canary\n",
    "- Update: `POST /rl/update {\"weights\":{\"factual\":0.1}}`\n",
    "- Promote snapshot: `POST /rl/promote`\n",
    "- Rollback: `POST /rl/rollback`\n",
    "- Status: `GET /canary/status`, `GET /canary/strategy_counts`\n",
    "## Docker\n",
    "- `docker compose up -d --build`\n",
    "## Gates\n",
    "- `python scripts/metrics_check.py --prom http://<prom>/api/v1/query`\n",
    "## Loadgen\n",
    "- `python scripts/loadgen.py --gateway http://127.0.0.1:9910 --qps 12 --seconds 60`\n",
    "\"\"\")\n",
    "_write(BUNDLE_DIR / \"rag_service.py\", \"\"\"\n",
    "from fastapi import FastAPI\n",
    "app = FastAPI(title=\"RAG Service (placeholder)\")\n",
    "@app.get(\"/up\")\n",
    "def up(): return {\"ok\": True}\n",
    "\"\"\")\n",
    "_write(BUNDLE_DIR / \"overseer.py\", \"\"\"\n",
    "print(\"Overseer placeholder: replace with your overseer_app runner if needed.\")\n",
    "\"\"\")\n",
    "_write(BUNDLE_DIR / \"gateway\" / \"Cargo.toml\", \"\"\"\n",
    "[package]\n",
    "name = \"gateway\"\n",
    "version = \"0.1.0\"\n",
    "edition = \"2021\"\n",
    "\n",
    "[dependencies]\n",
    "axum = \"0.7\"\n",
    "hyper = { version = \"1\", features=[\"http2\"] }\n",
    "tokio = { version = \"1\", features=[\"macros\",\"rt-multi-thread\"] }\n",
    "serde = { version=\"1\", features=[\"derive\"] }\n",
    "serde_json = \"1\"\n",
    "tower = \"0.5\"\n",
    "prometheus = \"0.13\"\n",
    "metrics = \"0.22\"\n",
    "metrics-exporter-prometheus = \"0.15\"\n",
    "reqwest = { version=\"0.12\", default-features=false, features=[\"json\",\"http2\",\"rustls-tls\"] }\n",
    "tracing = \"0.1\"\n",
    "tracing-subscriber = { version=\"0.3\", features=[\"fmt\",\"env-filter\"] }\n",
    "\"\"\")\n",
    "_write(BUNDLE_DIR / \"gateway\" / \"src\" / \"main.rs\", \"\"\"\n",
    "use std::{net::SocketAddr, time::Instant};\n",
    "use axum::{routing::get, Router};\n",
    "use metrics_exporter_prometheus::PrometheusBuilder;\n",
    "use tracing_subscriber::EnvFilter;\n",
    "\n",
    "#[tokio::main]\n",
    "async fn main() {\n",
    "    tracing_subscriber::fmt().with_env_filter(EnvFilter::from_default_env()).init();\n",
    "    let recorder = PrometheusBuilder::new().install_recorder().unwrap();\n",
    "    let app = Router::new()\n",
    "        .route(\"/up\", get(|| async { axum::Json(serde_json::json!({\"ok\": true})) }))\n",
    "        .route(\"/metrics\", get(move || {\n",
    "            let handle = recorder.handle();\n",
    "            async move { axum::response::Html(handle.render()) }\n",
    "        }));\n",
    "    let addr: SocketAddr = \"0.0.0.0:9910\".parse().unwrap();\n",
    "    tracing::info!(\"listening on {}\", addr);\n",
    "    axum::serve(tokio::net::TcpListener::bind(addr).await.unwrap(), app).await.unwrap();\n",
    "}\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 501,
   "id": "d6426934-db27-4563-9ab8-0b4638c03e11",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'step_timings_in_meta': True,\n",
       " 'bundle_dir': '\\\\tmp\\\\art\\\\bundle',\n",
       " 'bundle_zip': '\\\\tmp\\\\art\\\\release_bundle.zip',\n",
       " 'openapi_exported': True}"
      ]
     },
     "execution_count": 501,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def _sha256(path: Path) -> str:\n",
    "    h = hashlib.sha256()\n",
    "    with path.open(\"rb\") as f:\n",
    "        for chunk in iter(lambda: f.read(65536), b\"\"):\n",
    "            h.update(chunk)\n",
    "    return h.hexdigest()\n",
    "def build_manifest_and_zip(out_dir: Path = ART) -> Dict[str, Any]:\n",
    "    manifest = {\"generated_ts\": int(time.time()), \"files\": []}\n",
    "    for p in sorted(BUNDLE_DIR.rglob(\"*\")):\n",
    "        if p.is_file():\n",
    "            rel = p.relative_to(BUNDLE_DIR).as_posix()\n",
    "            manifest[\"files\"].append({\n",
    "                \"path\": rel,\n",
    "                \"size\": p.stat().st_size,\n",
    "                \"sha256\": _sha256(p),\n",
    "            })\n",
    "    (BUNDLE_DIR / \"MANIFEST.json\").write_text(json.dumps(manifest, indent=2), encoding=\"utf-8\")\n",
    "    zpath = out_dir / \"release_bundle.zip\"\n",
    "    with zipfile.ZipFile(zpath, \"w\", compression=zipfile.ZIP_DEFLATED) as z:\n",
    "        for p in BUNDLE_DIR.rglob(\"*\"):\n",
    "            if p.is_file():\n",
    "                z.write(p, arcname=p.relative_to(BUNDLE_DIR))\n",
    "    return {\"ok\": True, \"manifest\": str(BUNDLE_DIR / \"MANIFEST.json\"), \"zip\": str(zpath)}\n",
    "bundle_info = build_manifest_and_zip()\n",
    "{\n",
    "    \"step_timings_in_meta\": True,\n",
    "    \"bundle_dir\": str(BUNDLE_DIR),\n",
    "    \"bundle_zip\": bundle_info[\"zip\"],\n",
    "    \"openapi_exported\": (BUNDLE_DIR / \"openapi_microservice.json\").exists()\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 502,
   "id": "4e0bd2c1-fe0e-4863-ae3b-4b7d63561f24",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gateway/Cargo.toml\n",
      "[package]\n",
      "name = \"gateway\"\n",
      "version = \"0.1.0\"\n",
      "edition = \"2021\"\n",
      "\n",
      "[dependencies]\n",
      "axum = { version = \"0.7\", features = [\"macros\", \"json\"] }\n",
      "serde = { version = \"1\", features = [\"derive\"] }\n",
      "serde_json = \"1\"\n",
      "tokio = { version = \"1\", features = [\"macros\",\"rt-multi-thread\",\"time\"] }\n",
      "reqwest = { version = \"0.12\", default-features = false, features = [\"json\",\"http2\",\"rustls-tls\"] }\n",
      "tower = \"0.5\"\n",
      "tracing = \"0.1\"\n",
      "tracing-subscriber = { version = \"0.3\", features = [\"fmt\",\"env-filter\"] }\n",
      "metrics = \"0.22\"\n",
      "metrics-exporter-prometheus = \"0.15\"\n",
      "once_cell = \"1.19\"\n",
      "uuid = { version = \"1.10\", features = [\"v4\"] }\n",
      "dashmap = \"6\"\n",
      "parking_lot = \"0.12\"\n",
      "time = { version = \"0.3\", features = [\"macros\"] }\n",
      "\n",
      "================================================================================\n",
      "\n",
      "gateway/src/main.rs\n",
      "use std::{collections::{HashMap, VecDeque}, net::SocketAddr, time::Duration, sync::Arc, fs, io::Write};\n",
      "use axum::{routing::{get, post}, Router, extract::{State}, response::{IntoResponse}, http::{HeaderMap, HeaderValue, StatusCode}};\n",
      "use axum::Json;\n",
      "use once_cell::sync::Lazy;\n",
      "use parking_lot::RwLock;\n",
      "use serde::{Deserialize, Serialize};\n",
      "use metrics::{counter, histogram};\n",
      "use metrics_exporter_prometheus::PrometheusBuilder;\n",
      "use tokio::time::timeout;\n",
      "use tracing::{info, error, warn};\n",
      "use tracing_subscriber::EnvFilter;\n",
      "use uuid::Uuid;\n",
      "\n",
      "#[derive(Clone)]\n",
      "struct AppState {\n",
      "    rag_base: String,\n",
      "    cache: Arc<RwLock<RespCache>>,\n",
      "    breaker: Arc<RwLock<CircuitBreaker>>,\n",
      "    qpm: Arc<RwLock<HashMap<(String,String), i64>>>,\n",
      "    budgets: Arc<RwLock<HashMap<(String,String), f64>>>,\n",
      "    header_allow: Arc<Vec<&'static str>>,\n",
      "}\n",
      "\n",
      "#[derive(Debug, Serialize, Deserialize, Clone)]\n",
      "struct AnswerReq {\n",
      "    query: String,\n",
      "    #[serde(default = \"default_topk\")]\n",
      "    top_k: i64,\n",
      "    #[serde(default)]\n",
      "    use_rerank: Option<bool>,\n",
      "}\n",
      "fn default_topk() -> i64 { 5 }\n",
      "\n",
      "#[derive(Debug, Serialize, Deserialize)]\n",
      "struct StepResult<T: Serialize + Clone> {\n",
      "    ok: bool,\n",
      "    latency_ms: Option<u64>,\n",
      "    payload: Option<T>,\n",
      "    error: Option<String>,\n",
      "}\n",
      "\n",
      "#[derive(Debug, Serialize, Deserialize, Clone)]\n",
      "struct RerankResp { contexts: Vec<serde_json::Value>, latency_ms: Option<u64> }\n",
      "#[derive(Debug, Serialize, Deserialize, Clone)]\n",
      "struct RetrieveResp { results: Vec<serde_json::Value>, latency_ms: Option<u64> }\n",
      "#[derive(Debug, Serialize, Deserialize, Clone)]\n",
      "struct CompleteResp { answer: String, verify: serde_json::Value, latency_ms: Option<u64> }\n",
      "\n",
      "static DISALLOWED_HEADERS: Lazy<Vec<&'static str>> = Lazy::new(|| vec![\"authorization\"]);\n",
      "static HEADER_ALLOWLIST: Lazy<Vec<&'static str>> = Lazy::new(|| vec![\n",
      "    \"x-request-id\",\"x-namespace\",\"x-route\",\"x-verify-v2\",\"x-chaos-disable-ce\",\"x-chaos-force-bm25\"\n",
      "]);\n",
      "\n",
      "// ---- Response Cache (TTL) ----\n",
      "#[derive(Clone)]\n",
      "struct RespCache {\n",
      "    ttl_s: u64,\n",
      "    max_items: usize,\n",
      "    // key -> (insert_epoch_ms, payload)\n",
      "    map: HashMap<String, (u128, serde_json::Value)>\n",
      "}\n",
      "impl RespCache {\n",
      "    fn new(ttl_s: u64, max_items: usize) -> Self {\n",
      "        Self { ttl_s, max_items, map: HashMap::new() }\n",
      "    }\n",
      "    fn make_key(req: &AnswerReq) -> String {\n",
      "        serde_json::json!({\"query\": &req.query, \"top_k\": req.top_k, \"use_rerank\": req.use_rerank}).to_string()\n",
      "    }\n",
      "    fn get(&mut self, key: &str) -> Option<serde_json::Value> {\n",
      "        let now_ms = now_ms();\n",
      "        if let Some((ts, payload)) = self.map.get(key) {\n",
      "            if now_ms.saturating_sub(*ts) <= (self.ttl_s as u128)*1000 {\n",
      "                return Some(payload.clone());\n",
      "            } else {\n",
      "                self.map.remove(key);\n",
      "            }\n",
      "        }\n",
      "        None\n",
      "    }\n",
      "    fn put(&mut self, key: String, payload: serde_json::Value) {\n",
      "        if self.map.len() >= self.max_items {\n",
      "            if let Some(k) = self.map.keys().next().cloned() {\n",
      "                self.map.remove(&k);\n",
      "            }\n",
      "        }\n",
      "        self.map.insert(key, (now_ms(), payload));\n",
      "    }\n",
      "}\n",
      "fn now_ms() -> u128 {\n",
      "    std::time::SystemTime::now().duration_since(std::time::UNIX_EPOCH).unwrap().as_millis()\n",
      "}\n",
      "\n",
      "// ---- Circuit Breaker ----\n",
      "#[derive(Clone)]\n",
      "struct CircuitBreaker {\n",
      "    window_s: u64,\n",
      "    threshold: f64,\n",
      "    min_events: usize,\n",
      "    open_seconds: u64,\n",
      "    events: VecDeque<(u128, bool)>,\n",
      "    open_until_ms: u128,\n",
      "}\n",
      "impl CircuitBreaker {\n",
      "    fn new() -> Self {\n",
      "        Self {\n",
      "            window_s: 60, threshold: 0.25, min_events: 20, open_seconds: 30,\n",
      "            events: VecDeque::new(), open_until_ms: 0\n",
      "        }\n",
      "    }\n",
      "    fn record(&mut self, ok: bool) {\n",
      "        let now = now_ms();\n",
      "        self.gc(now);\n",
      "        self.events.push_back((now, ok));\n",
      "        if now < self.open_until_ms { return; }\n",
      "        if self.events.len() >= self.min_events {\n",
      "            let errs = self.events.iter().filter(|(_, okv)| !*okv).count();\n",
      "            let rate = errs as f64 / (self.events.len() as f64);\n",
      "            if rate >= self.threshold {\n",
      "                self.open_until_ms = now + (self.open_seconds as u128)*1000;\n",
      "                counter!(\"breaker_opens_total\").increment(1);\n",
      "            }\n",
      "        }\n",
      "    }\n",
      "    fn open(&self) -> bool { now_ms() < self.open_until_ms }\n",
      "    fn remaining(&self) -> u64 {\n",
      "        if self.open() { ((self.open_until_ms - now_ms())/1000) as u64 } else { 0 }\n",
      "    }\n",
      "    fn gc(&mut self, now: u128) {\n",
      "        let window_ms = (self.window_s as u128)*1000;\n",
      "        while let Some((ts,_)) = self.events.front().cloned() {\n",
      "            if now.saturating_sub(ts) > window_ms { self.events.pop_front(); } else { break; }\n",
      "        }\n",
      "    }\n",
      "}\n",
      "\n",
      "// ---- Metrics helpers ----\n",
      "// Mirrors Python names where useful.\n",
      "fn observe_gateway_latency_ms(v: f64) { histogram!(\"gateway_answer_latency_ms\", v); }\n",
      "fn per_step_hist(name: &str, v: f64) {\n",
      "    // Use rag_* names to mirror Python gateway's step reporting\n",
      "    match name {\n",
      "        \"bm25\" => histogram!(\"rag_bm25_ms\", v),\n",
      "        \"dense\" => histogram!(\"rag_dense_ms\", v),\n",
      "        \"rerank\" => histogram!(\"rag_rerank_ms\", v),\n",
      "        \"complete\" => histogram!(\"rag_complete_ms\", v),\n",
      "        _ => {}\n",
      "    }\n",
      "}\n",
      "fn fanout_error(step: &str) { counter!(\"gateway_fanout_errors_total\", \"step\" => step.to_string()).increment(1); }\n",
      "fn bind_attempt() { counter!(\"rust_gateway_bind_attempts_total\").increment(1); }\n",
      "\n",
      "// ---- HTTP helpers ----\n",
      "fn get_or_gen_request_id(headers: &HeaderMap) -> String {\n",
      "    if let Some(v) = headers.get(\"x-request-id\") {\n",
      "        if let Ok(s) = v.to_str() { if !s.is_empty() { return s.to_string(); } }\n",
      "    }\n",
      "    Uuid::new_v4().to_string()\n",
      "}\n",
      "fn build_fanout_headers(req_headers: &HeaderMap, req_id: &str) -> HeaderMap {\n",
      "    let mut out = HeaderMap::new();\n",
      "    for k in HEADER_ALLOWLIST.iter() {\n",
      "        if let Some(v) = req_headers.get(*k) {\n",
      "            out.insert(*k, v.clone());\n",
      "        }\n",
      "    }\n",
      "    out.insert(\"x-request-id\", HeaderValue::from_str(req_id).unwrap_or(HeaderValue::from_static(\"bad-uuid\")));\n",
      "    out\n",
      "}\n",
      "\n",
      "// ---- Fan-out to RAG ----\n",
      "async fn post_json(\n",
      "    client: &reqwest::Client,\n",
      "    base: &str,\n",
      "    path: &str,\n",
      "    headers: HeaderMap,\n",
      "    body: serde_json::Value,\n",
      "    step: &str,\n",
      "    step_timeout: Duration,\n",
      ") -> StepResult<serde_json::Value> {\n",
      "    let url = format!(\"{}{}\", base, path);\n",
      "    let t0 = std::time::Instant::now();\n",
      "    let res = timeout(step_timeout, async {\n",
      "        let r = client.post(&url).headers(headers).json(&body).send().await;\n",
      "        match r {\n",
      "            Ok(resp) => {\n",
      "                if !resp.status().is_success() {\n",
      "                    Err(anyhow::anyhow!(\"status {}\", resp.status()))\n",
      "                } else {\n",
      "                    resp.json::<serde_json::Value>().await.map_err(|e| e.into())\n",
      "                }\n",
      "            }\n",
      "            Err(e) => Err(e.into())\n",
      "        }\n",
      "    }).await;\n",
      "\n",
      "    match res {\n",
      "        Ok(Ok(json)) => {\n",
      "            let ms = t0.elapsed().as_millis() as u64;\n",
      "            per_step_hist(step, ms as f64);\n",
      "            StepResult{ ok: true, latency_ms: Some(ms), payload: Some(json), error: None }\n",
      "        }\n",
      "        Ok(Err(e)) => {\n",
      "            fanout_error(step);\n",
      "            StepResult{ ok: false, latency_ms: Some(t0.elapsed().as_millis() as u64), payload: None, error: Some(e.to_string()) }\n",
      "        }\n",
      "        Err(_) => {\n",
      "            fanout_error(step);\n",
      "            StepResult{ ok: false, latency_ms: Some(t0.elapsed().as_millis() as u64), payload: None, error: Some(\"timeout\".into()) }\n",
      "        }\n",
      "    }\n",
      "}\n",
      "\n",
      "#[derive(Serialize, Deserialize)]\n",
      "struct AnswerMeta {\n",
      "    pipeline: String,\n",
      "    latency_ms: u64,\n",
      "    cache: &'static str,\n",
      "    partial_retrieval: serde_json::Value,\n",
      "    route: String,\n",
      "    ns: String,\n",
      "    step_ms: Option<HashMap<String, u64>>,\n",
      "}\n",
      "\n",
      "#[derive(Serialize, Deserialize)]\n",
      "struct AnswerResp {\n",
      "    answer: String,\n",
      "    contexts: Vec<serde_json::Value>,\n",
      "    verify: serde_json::Value,\n",
      "    meta: AnswerMeta,\n",
      "}\n",
      "\n",
      "// ---- Handlers ----\n",
      "#[axum::debug_handler]\n",
      "async fn up_handler() -> impl IntoResponse {\n",
      "    Json(serde_json::json!({\"ok\": true, \"service\": \"gateway\"}))\n",
      "}\n",
      "\n",
      "#[axum::debug_handler]\n",
      "async fn metrics_handler() -> impl IntoResponse {\n",
      "    let handle = PROM_HANDLE.get().unwrap();\n",
      "    axum::response::Response::builder()\n",
      "        .status(StatusCode::OK)\n",
      "        .header(axum::http::header::CONTENT_TYPE, \"text/plain; version=0.0.4\")\n",
      "        .body(handle.render().into())\n",
      "        .unwrap()\n",
      "}\n",
      "\n",
      "#[derive(Default)]\n",
      "struct PromHandle { handle: Option<metrics_exporter_prometheus::PrometheusHandle> }\n",
      "static PROM_HANDLE: Lazy<PromHandleCell> = Lazy::new(|| PromHandleCell{ inner: RwLock::new(None) });\n",
      "struct PromHandleCell{ inner: RwLock<Option<metrics_exporter_prometheus::PrometheusHandle>> }\n",
      "impl PromHandleCell {\n",
      "    fn set(&self, h: metrics_exporter_prometheus::PrometheusHandle) { *self.inner.write() = Some(h); }\n",
      "    fn get(&self) -> Option<metrics_exporter_prometheus::PrometheusHandle> { self.inner.read().clone() }\n",
      "}\n",
      "trait PromHandleExt { fn set_handle(h: metrics_exporter_prometheus::PrometheusHandle); }\n",
      "impl PromHandleExt for PromHandle {\n",
      "    fn set_handle(_h: metrics_exporter_prometheus::PrometheusHandle) {}\n",
      "}\n",
      "impl PromHandleCell {\n",
      "    fn get_handle(&self) -> metrics_exporter_prometheus::PrometheusHandle {\n",
      "        self.inner.read().clone().unwrap()\n",
      "    }\n",
      "}\n",
      "impl PromHandleCell {\n",
      "    fn render(&self) -> String { self.inner.read().as_ref().unwrap().render() }\n",
      "}\n",
      "\n",
      "async fn admin_ping() -> impl IntoResponse {\n",
      "    Json(serde_json::json!({\"ok\": true, \"pong\": true, \"service\": \"gateway\"}))\n",
      "}\n",
      "\n",
      "// In-memory mirrors for budgets/QPM\n",
      "#[derive(Deserialize)]\n",
      "struct QpmSet { namespace: Option<String>, route: Option<String>, qpm: i64 }\n",
      "#[derive(Deserialize)]\n",
      "struct CostSet { namespace: Option<String>, route: Option<String>, daily_usd: f64 }\n",
      "\n",
      "async fn qpm_set(State(st): State<AppState>, Json(req): Json<QpmSet>) -> impl IntoResponse {\n",
      "    let ns = req.namespace.unwrap_or_else(|| \"default\".into()).to_lowercase();\n",
      "    let route = req.route.unwrap_or_else(|| \"answer\".into()).to_lowercase();\n",
      "    st.qpm.write().insert((ns.clone(), route.clone()), req.qpm.max(0));\n",
      "    Json(serde_json::json!({\"ok\": true, \"namespace\": ns, \"route\": route, \"qpm\": req.qpm}))\n",
      "}\n",
      "async fn qpm_summary(State(st): State<AppState>) -> impl IntoResponse {\n",
      "    let map = st.qpm.read();\n",
      "    let mut items = Vec::new();\n",
      "    for ((ns, route), q) in map.iter() {\n",
      "        items.push(serde_json::json!({\"namespace\": ns, \"route\": route, \"qpm\": q}));\n",
      "    }\n",
      "    Json(serde_json::json!({\"ok\": true, \"limits\": items}))\n",
      "}\n",
      "async fn cost_set(State(st): State<AppState>, Json(req): Json<CostSet>) -> impl IntoResponse {\n",
      "    let ns = req.namespace.unwrap_or_else(|| \"default\".into()).to_lowercase();\n",
      "    let route = req.route.unwrap_or_else(|| \"answer\".into()).to_lowercase();\n",
      "    st.budgets.write().insert((ns.clone(), route.clone()), req.daily_usd.max(0.0));\n",
      "    Json(serde_json::json!({\"ok\": true, \"budget\": {\"namespace\": ns, \"route\": route, \"daily_usd\": req.daily_usd}}))\n",
      "}\n",
      "async fn cost_summary(State(st): State<AppState>) -> impl IntoResponse {\n",
      "    let map = st.budgets.read();\n",
      "    let mut items = Vec::new();\n",
      "    for ((ns, route), usd) in map.iter() {\n",
      "        items.push(serde_json::json!({\"namespace\": ns, \"route\": route, \"daily_usd\": usd}));\n",
      "    }\n",
      "    Json(serde_json::json!({\"ok\": true, \"budgets\": items}))\n",
      "}\n",
      "\n",
      "// Router detection (simple; mirror Py heuristics)\n",
      "fn route_for(query: &str, override_hdr: Option<&str>) -> String {\n",
      "    if let Some(ov) = override_hdr { if !ov.trim().is_empty() { return ov.trim().to_lowercase(); } }\n",
      "    let q = query.trim().to_lowercase();\n",
      "    let sql_re = regex_lite::Regex::new(r\"(?is)^\\s*(select|with)\\b\").unwrap();\n",
      "    let math_re = regex_lite::Regex::new(r\"\\b(integrate|derive|differentiate|solve|sum|product|limit|matrix|determinant|eigen)\\b\").unwrap();\n",
      "    let policy_re = regex_lite::Regex::new(r\"\\b(policy|regulation|compliance|gdpr|hipaa|soc2|security)\\b\").unwrap();\n",
      "    if sql_re.is_match(&q) { return \"sql\".into(); }\n",
      "    if math_re.is_match(&q) { return \"math\".into(); }\n",
      "    if policy_re.is_match(&q) { return \"policy\".into(); }\n",
      "    \"factual\".into()\n",
      "}\n",
      "\n",
      "#[axum::debug_handler]\n",
      "async fn answer_handler(\n",
      "    State(st): State<AppState>,\n",
      "    headers: HeaderMap,\n",
      "    Json(body): Json<AnswerReq>,\n",
      ") -> impl IntoResponse {\n",
      "    // Circuit breaker\n",
      "    {\n",
      "        if st.breaker.read().open() {\n",
      "            if let Some(mut v) = st.cache.write().get(&RespCache::make_key(&body)) {\n",
      "                let mut p = v;\n",
      "                if let Some(m) = p.get_mut(\"meta\") {\n",
      "                    if let Some(obj) = m.as_object_mut() {\n",
      "                        obj.insert(\"breaker\".into(), serde_json::json!(\"stale\"));\n",
      "                        obj.insert(\"cache\".into(), serde_json::json!(\"hit\"));\n",
      "                    }\n",
      "                }\n",
      "                return (StatusCode::OK, Json(p));\n",
      "            }\n",
      "            let rem = st.breaker.read().remaining();\n",
      "            return (StatusCode::SERVICE_UNAVAILABLE, Json(serde_json::json!({\"error\":\"breaker_open\",\"retry_after\": rem})));\n",
      "        }\n",
      "    }\n",
      "\n",
      "    let t0 = std::time::Instant::now();\n",
      "    let ns = headers.get(\"x-namespace\").and_then(|v| v.to_str().ok()).unwrap_or(\"default\").to_lowercase();\n",
      "    let route_override = headers.get(\"x-route\").and_then(|v| v.to_str().ok());\n",
      "    let route_name = route_for(&body.query, route_override);\n",
      "    let req_id = get_or_gen_request_id(&headers);\n",
      "    let mut resp_headers = HeaderMap::new();\n",
      "    let _ = resp_headers.insert(\"x-request-id\", HeaderValue::from_str(&req_id).unwrap());\n",
      "\n",
      "    // Cache\n",
      "    let key = RespCache::make_key(&body);\n",
      "    if let Some(mut cached) = st.cache.write().get(&key) {\n",
      "        if let Some(m) = cached.get_mut(\"meta\") {\n",
      "            if let Some(obj) = m.as_object_mut() {\n",
      "                obj.insert(\"cache\".into(), serde_json::json!(\"hit\"));\n",
      "                obj.insert(\"ns\".into(), serde_json::json!(ns.clone()));\n",
      "                obj.insert(\"route\".into(), serde_json::json!(route_name.clone()));\n",
      "            }\n",
      "        }\n",
      "        return (StatusCode::OK, (resp_headers, Json(cached)));\n",
      "    }\n",
      "\n",
      "    // HTTP client\n",
      "    let client = reqwest::Client::builder().pool_idle_timeout(Duration::from_secs(10)).build().unwrap();\n",
      "    let fanout_headers = build_fanout_headers(&headers, &req_id);\n",
      "    let top_k = body.top_k.max(1).min(50) as i64;\n",
      "\n",
      "    // Parallel retrieve\n",
      "    let bm25_f = post_json(&client, &st.rag_base, \"/retrieve_bm25\", fanout_headers.clone(),\n",
      "                           serde_json::json!({\"query\": body.query, \"top_k\": top_k}), \"bm25\",\n",
      "                           Duration::from_millis(step_timeout_ms(\"bm25\")));\n",
      "    let dense_f = post_json(&client, &st.rag_base, \"/retrieve_dense\", fanout_headers.clone(),\n",
      "                           serde_json::json!({\"query\": body.query, \"top_k\": top_k}), \"dense\",\n",
      "                           Duration::from_millis(step_timeout_ms(\"dense\")));\n",
      "    let (bm25_res, dense_res) = tokio::join!(bm25_f, dense_f);\n",
      "\n",
      "    let mut contexts: Vec<serde_json::Value> = Vec::new();\n",
      "    let mut partial = serde_json::json!({\"dense_ok\": false, \"bm25_ok\": false});\n",
      "    if bm25_res.ok {\n",
      "        if let Some(p) = bm25_res.payload { if let Some(arr) = p.get(\"results\").and_then(|x| x.as_array()) {\n",
      "            contexts.extend(arr.clone());\n",
      "            partial[\"bm25_ok\"] = serde_json::json!(true);\n",
      "        }}\n",
      "    }\n",
      "    if dense_res.ok {\n",
      "        if let Some(p) = dense_res.payload { if let Some(arr) = p.get(\"results\").and_then(|x| x.as_array()) {\n",
      "            contexts.extend(arr.clone());\n",
      "            partial[\"dense_ok\"] = serde_json::json!(true);\n",
      "        }}\n",
      "    }\n",
      "    if contexts.is_empty() {\n",
      "        st.breaker.write().record(false);\n",
      "        return (StatusCode::BAD_GATEWAY, Json(serde_json::json!({\"error\":\"preflight_failed: both retrievals failed\"})));\n",
      "    }\n",
      "\n",
      "    // use_rerank logic\n",
      "    let mut use_rerank = body.use_rerank.unwrap_or(true);\n",
      "    if headers.get(\"x-chaos-disable-ce\").is_some() { use_rerank = false; }\n",
      "\n",
      "    // Optional rerank\n",
      "    if use_rerank {\n",
      "        let rr = post_json(&client, &st.rag_base, \"/rerank\", fanout_headers.clone(),\n",
      "                           serde_json::json!({\"query\": body.query, \"contexts\": contexts}),\n",
      "                           \"rerank\", Duration::from_millis(step_timeout_ms(\"rerank\"))).await;\n",
      "        if rr.ok {\n",
      "            if let Some(p) = rr.payload {\n",
      "                if let Some(arr) = p.get(\"contexts\").and_then(|x| x.as_array()) {\n",
      "                    contexts = arr.clone();\n",
      "                }\n",
      "            }\n",
      "        }\n",
      "    }\n",
      "\n",
      "    // Complete\n",
      "    let comp = post_json(&client, &st.rag_base, \"/complete\", fanout_headers.clone(),\n",
      "                         serde_json::json!({\"query\": body.query, \"contexts\": contexts}),\n",
      "                         \"complete\", Duration::from_millis(step_timeout_ms(\"complete\"))).await;\n",
      "    if !comp.ok {\n",
      "        st.breaker.write().record(false);\n",
      "        return (StatusCode::BAD_GATEWAY, Json(serde_json::json!({\"error\": format!(\"complete_failed: {:?}\", comp.error)})));\n",
      "    }\n",
      "\n",
      "    let verify = comp.payload.as_ref().and_then(|p| p.get(\"verify\")).cloned().unwrap_or(serde_json::json!({}));\n",
      "    let answer = comp.payload.as_ref().and_then(|p| p.get(\"answer\")).and_then(|v| v.as_str()).unwrap_or(\"\").to_string();\n",
      "\n",
      "    // step timings\n",
      "    let mut step_ms: HashMap<String,u64> = HashMap::new();\n",
      "    if let Some(ms) = bm25_res.latency_ms { step_ms.insert(\"bm25_ms\".into(), ms); }\n",
      "    if let Some(ms) = dense_res.latency_ms { step_ms.insert(\"dense_ms\".into(), ms); }\n",
      "    if use_rerank {\n",
      "        // nothing extra here; measured inside per_step_hist\n",
      "    }\n",
      "    if let Some(ms) = comp.latency_ms { step_ms.insert(\"complete_ms\".into(), ms); }\n",
      "\n",
      "    let latency_ms = t0.elapsed().as_millis() as u64;\n",
      "    observe_gateway_latency_ms(latency_ms as f64);\n",
      "\n",
      "    let payload = serde_json::json!({\n",
      "        \"answer\": answer,\n",
      "        \"contexts\": contexts.into_iter().take(top_k as usize).collect::<Vec<_>>(),\n",
      "        \"verify\": verify,\n",
      "        \"meta\": {\n",
      "            \"pipeline\": if use_rerank { \"rerank\" } else { \"rrf\" },\n",
      "            \"latency_ms\": latency_ms,\n",
      "            \"cache\": \"miss\",\n",
      "            \"partial_retrieval\": partial,\n",
      "            \"route\": route_name,\n",
      "            \"ns\": ns,\n",
      "            \"step_ms\": step_ms,\n",
      "        }\n",
      "    });\n",
      "\n",
      "    st.breaker.write().record(true);\n",
      "    st.cache.write().put(key, payload.clone());\n",
      "    (StatusCode::OK, (resp_headers, Json(payload)))\n",
      "}\n",
      "\n",
      "fn step_timeout_ms(step: &str) -> u64 {\n",
      "    let bm25 = std::env::var(\"STEP_TIMEOUT_BM25\").ok().and_then(|s| s.parse::<f64>().ok()).unwrap_or(0.8);\n",
      "    let dense = std::env::var(\"STEP_TIMEOUT_DENSE\").ok().and_then(|s| s.parse::<f64>().ok()).unwrap_or(1.2);\n",
      "    let rerank = std::env::var(\"STEP_TIMEOUT_RERANK\").ok().and_then(|s| s.parse::<f64>().ok()).unwrap_or(0.6);\n",
      "    let complete = std::env::var(\"STEP_TIMEOUT_COMPLETE\").ok().and_then(|s| s.parse::<f64>().ok()).unwrap_or(2.0);\n",
      "    let sec = match step { \"bm25\" => bm25, \"dense\" => dense, \"rerank\" => rerank, \"complete\" => complete, _ => 1.0 };\n",
      "    (sec * 1000.0) as u64\n",
      "}\n",
      "\n",
      "// ---- Boot / Bind with auto-increment + PORT.txt ----\n",
      "fn write_port_txt(port: u16) {\n",
      "    let art = std::env::var(\"ART_DIR\").unwrap_or_else(|_| \"/tmp/art\".into());\n",
      "    let dir = std::path::Path::new(&art);\n",
      "    let _ = std::fs::create_dir_all(dir);\n",
      "    let path = dir.join(\"PORT.txt\");\n",
      "    if let Ok(mut f) = std::fs::File::create(path) {\n",
      "        let _ = writeln!(f, \"{}\", port);\n",
      "    }\n",
      "}\n",
      "\n",
      "#[tokio::main]\n",
      "async fn main() {\n",
      "    tracing_subscriber::fmt()\n",
      "        .with_env_filter(EnvFilter::from_default_env().add_directive(\"info\".parse().unwrap()))\n",
      "        .init();\n",
      "\n",
      "    let recorder = PrometheusBuilder::new().install_recorder().expect(\"prom recorder\");\n",
      "    PROM_HANDLE.set(recorder.handle());\n",
      "\n",
      "    // Config\n",
      "    let rag_base = std::env::var(\"RAG_BASE\").unwrap_or_else(|_| \"http://127.0.0.1:9909\".into());\n",
      "    let mut port: u16 = std::env::var(\"GATEWAY_PORT\").ok().and_then(|s| s.parse::<u16>().ok()).unwrap_or(9910);\n",
      "    let host: &str = \"0.0.0.0\";\n",
      "\n",
      "    // Bind attempts with auto-increment\n",
      "    let listener = loop {\n",
      "        bind_attempt();\n",
      "        match tokio::net::TcpListener::bind((host, port)).await {\n",
      "            Ok(l) => break l,\n",
      "            Err(_) => { port += 1; if port > 9999 { panic!(\"No available ports 9910..9999\"); } }\n",
      "        }\n",
      "    };\n",
      "    write_port_txt(port);\n",
      "    info!(\"gateway listening on {host}:{port}, rag_base={rag_base}\");\n",
      "\n",
      "    let state = AppState {\n",
      "        rag_base,\n",
      "        cache: Arc::new(RwLock::new(RespCache::new(60, 512))),\n",
      "        breaker: Arc::new(RwLock::new(CircuitBreaker::new())),\n",
      "        qpm: Arc::new(RwLock::new(HashMap::new())),\n",
      "        budgets: Arc::new(RwLock::new(HashMap::new())),\n",
      "        header_allow: Arc::new(HEADER_ALLOWLIST.clone()),\n",
      "    };\n",
      "\n",
      "    let app = Router::new()\n",
      "        .route(\"/up\", get(up_handler))\n",
      "        .route(\"/metrics\", get(metrics_handler))\n",
      "        .route(\"/admin/ping\", get(admin_ping))\n",
      "        .route(\"/admin/limits/qpm/set\", post(qpm_set))\n",
      "        .route(\"/admin/limits/qpm/summary\", get(qpm_summary))\n",
      "        .route(\"/admin/costs/set\", post(cost_set))\n",
      "        .route(\"/admin/costs/summary\", get(cost_summary))\n",
      "        .route(\"/answer\", post(answer_handler))\n",
      "        .with_state(state);\n",
      "\n",
      "    axum::serve(listener, app).await.unwrap();\n",
      "}\n",
      "\n",
      "// Minimal regex engine without pulling full regex crate; small and fast for our use.\n",
      "mod regex_lite {\n",
      "    pub use regex::Regex;\n",
      "    // Re-exporting full regex; if you want truly \"lite\", swap to regex-lite crate.\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "cargo_toml = r'''\n",
    "[package]\n",
    "name = \"gateway\"\n",
    "version = \"0.1.0\"\n",
    "edition = \"2021\"\n",
    "\n",
    "[dependencies]\n",
    "axum = { version = \"0.7\", features = [\"macros\", \"json\"] }\n",
    "serde = { version = \"1\", features = [\"derive\"] }\n",
    "serde_json = \"1\"\n",
    "tokio = { version = \"1\", features = [\"macros\",\"rt-multi-thread\",\"time\"] }\n",
    "reqwest = { version = \"0.12\", default-features = false, features = [\"json\",\"http2\",\"rustls-tls\"] }\n",
    "tower = \"0.5\"\n",
    "tracing = \"0.1\"\n",
    "tracing-subscriber = { version = \"0.3\", features = [\"fmt\",\"env-filter\"] }\n",
    "metrics = \"0.22\"\n",
    "metrics-exporter-prometheus = \"0.15\"\n",
    "once_cell = \"1.19\"\n",
    "uuid = { version = \"1.10\", features = [\"v4\"] }\n",
    "dashmap = \"6\"\n",
    "parking_lot = \"0.12\"\n",
    "time = { version = \"0.3\", features = [\"macros\"] }\n",
    "'''\n",
    "\n",
    "main_rs = r'''\n",
    "use std::{collections::{HashMap, VecDeque}, net::SocketAddr, time::Duration, sync::Arc, fs, io::Write};\n",
    "use axum::{routing::{get, post}, Router, extract::{State}, response::{IntoResponse}, http::{HeaderMap, HeaderValue, StatusCode}};\n",
    "use axum::Json;\n",
    "use once_cell::sync::Lazy;\n",
    "use parking_lot::RwLock;\n",
    "use serde::{Deserialize, Serialize};\n",
    "use metrics::{counter, histogram};\n",
    "use metrics_exporter_prometheus::PrometheusBuilder;\n",
    "use tokio::time::timeout;\n",
    "use tracing::{info, error, warn};\n",
    "use tracing_subscriber::EnvFilter;\n",
    "use uuid::Uuid;\n",
    "\n",
    "#[derive(Clone)]\n",
    "struct AppState {\n",
    "    rag_base: String,\n",
    "    cache: Arc<RwLock<RespCache>>,\n",
    "    breaker: Arc<RwLock<CircuitBreaker>>,\n",
    "    qpm: Arc<RwLock<HashMap<(String,String), i64>>>,\n",
    "    budgets: Arc<RwLock<HashMap<(String,String), f64>>>,\n",
    "    header_allow: Arc<Vec<&'static str>>,\n",
    "}\n",
    "\n",
    "#[derive(Debug, Serialize, Deserialize, Clone)]\n",
    "struct AnswerReq {\n",
    "    query: String,\n",
    "    #[serde(default = \"default_topk\")]\n",
    "    top_k: i64,\n",
    "    #[serde(default)]\n",
    "    use_rerank: Option<bool>,\n",
    "}\n",
    "fn default_topk() -> i64 { 5 }\n",
    "\n",
    "#[derive(Debug, Serialize, Deserialize)]\n",
    "struct StepResult<T: Serialize + Clone> {\n",
    "    ok: bool,\n",
    "    latency_ms: Option<u64>,\n",
    "    payload: Option<T>,\n",
    "    error: Option<String>,\n",
    "}\n",
    "\n",
    "#[derive(Debug, Serialize, Deserialize, Clone)]\n",
    "struct RerankResp { contexts: Vec<serde_json::Value>, latency_ms: Option<u64> }\n",
    "#[derive(Debug, Serialize, Deserialize, Clone)]\n",
    "struct RetrieveResp { results: Vec<serde_json::Value>, latency_ms: Option<u64> }\n",
    "#[derive(Debug, Serialize, Deserialize, Clone)]\n",
    "struct CompleteResp { answer: String, verify: serde_json::Value, latency_ms: Option<u64> }\n",
    "\n",
    "static DISALLOWED_HEADERS: Lazy<Vec<&'static str>> = Lazy::new(|| vec![\"authorization\"]);\n",
    "static HEADER_ALLOWLIST: Lazy<Vec<&'static str>> = Lazy::new(|| vec![\n",
    "    \"x-request-id\",\"x-namespace\",\"x-route\",\"x-verify-v2\",\"x-chaos-disable-ce\",\"x-chaos-force-bm25\"\n",
    "]);\n",
    "\n",
    "// ---- Response Cache (TTL) ----\n",
    "#[derive(Clone)]\n",
    "struct RespCache {\n",
    "    ttl_s: u64,\n",
    "    max_items: usize,\n",
    "    // key -> (insert_epoch_ms, payload)\n",
    "    map: HashMap<String, (u128, serde_json::Value)>\n",
    "}\n",
    "impl RespCache {\n",
    "    fn new(ttl_s: u64, max_items: usize) -> Self {\n",
    "        Self { ttl_s, max_items, map: HashMap::new() }\n",
    "    }\n",
    "    fn make_key(req: &AnswerReq) -> String {\n",
    "        serde_json::json!({\"query\": &req.query, \"top_k\": req.top_k, \"use_rerank\": req.use_rerank}).to_string()\n",
    "    }\n",
    "    fn get(&mut self, key: &str) -> Option<serde_json::Value> {\n",
    "        let now_ms = now_ms();\n",
    "        if let Some((ts, payload)) = self.map.get(key) {\n",
    "            if now_ms.saturating_sub(*ts) <= (self.ttl_s as u128)*1000 {\n",
    "                return Some(payload.clone());\n",
    "            } else {\n",
    "                self.map.remove(key);\n",
    "            }\n",
    "        }\n",
    "        None\n",
    "    }\n",
    "    fn put(&mut self, key: String, payload: serde_json::Value) {\n",
    "        if self.map.len() >= self.max_items {\n",
    "            if let Some(k) = self.map.keys().next().cloned() {\n",
    "                self.map.remove(&k);\n",
    "            }\n",
    "        }\n",
    "        self.map.insert(key, (now_ms(), payload));\n",
    "    }\n",
    "}\n",
    "fn now_ms() -> u128 {\n",
    "    std::time::SystemTime::now().duration_since(std::time::UNIX_EPOCH).unwrap().as_millis()\n",
    "}\n",
    "\n",
    "// ---- Circuit Breaker ----\n",
    "#[derive(Clone)]\n",
    "struct CircuitBreaker {\n",
    "    window_s: u64,\n",
    "    threshold: f64,\n",
    "    min_events: usize,\n",
    "    open_seconds: u64,\n",
    "    events: VecDeque<(u128, bool)>,\n",
    "    open_until_ms: u128,\n",
    "}\n",
    "impl CircuitBreaker {\n",
    "    fn new() -> Self {\n",
    "        Self {\n",
    "            window_s: 60, threshold: 0.25, min_events: 20, open_seconds: 30,\n",
    "            events: VecDeque::new(), open_until_ms: 0\n",
    "        }\n",
    "    }\n",
    "    fn record(&mut self, ok: bool) {\n",
    "        let now = now_ms();\n",
    "        self.gc(now);\n",
    "        self.events.push_back((now, ok));\n",
    "        if now < self.open_until_ms { return; }\n",
    "        if self.events.len() >= self.min_events {\n",
    "            let errs = self.events.iter().filter(|(_, okv)| !*okv).count();\n",
    "            let rate = errs as f64 / (self.events.len() as f64);\n",
    "            if rate >= self.threshold {\n",
    "                self.open_until_ms = now + (self.open_seconds as u128)*1000;\n",
    "                counter!(\"breaker_opens_total\").increment(1);\n",
    "            }\n",
    "        }\n",
    "    }\n",
    "    fn open(&self) -> bool { now_ms() < self.open_until_ms }\n",
    "    fn remaining(&self) -> u64 {\n",
    "        if self.open() { ((self.open_until_ms - now_ms())/1000) as u64 } else { 0 }\n",
    "    }\n",
    "    fn gc(&mut self, now: u128) {\n",
    "        let window_ms = (self.window_s as u128)*1000;\n",
    "        while let Some((ts,_)) = self.events.front().cloned() {\n",
    "            if now.saturating_sub(ts) > window_ms { self.events.pop_front(); } else { break; }\n",
    "        }\n",
    "    }\n",
    "}\n",
    "\n",
    "// ---- Metrics helpers ----\n",
    "// Mirrors Python names where useful.\n",
    "fn observe_gateway_latency_ms(v: f64) { histogram!(\"gateway_answer_latency_ms\", v); }\n",
    "fn per_step_hist(name: &str, v: f64) {\n",
    "    // Use rag_* names to mirror Python gateway's step reporting\n",
    "    match name {\n",
    "        \"bm25\" => histogram!(\"rag_bm25_ms\", v),\n",
    "        \"dense\" => histogram!(\"rag_dense_ms\", v),\n",
    "        \"rerank\" => histogram!(\"rag_rerank_ms\", v),\n",
    "        \"complete\" => histogram!(\"rag_complete_ms\", v),\n",
    "        _ => {}\n",
    "    }\n",
    "}\n",
    "fn fanout_error(step: &str) { counter!(\"gateway_fanout_errors_total\", \"step\" => step.to_string()).increment(1); }\n",
    "fn bind_attempt() { counter!(\"rust_gateway_bind_attempts_total\").increment(1); }\n",
    "\n",
    "// ---- HTTP helpers ----\n",
    "fn get_or_gen_request_id(headers: &HeaderMap) -> String {\n",
    "    if let Some(v) = headers.get(\"x-request-id\") {\n",
    "        if let Ok(s) = v.to_str() { if !s.is_empty() { return s.to_string(); } }\n",
    "    }\n",
    "    Uuid::new_v4().to_string()\n",
    "}\n",
    "fn build_fanout_headers(req_headers: &HeaderMap, req_id: &str) -> HeaderMap {\n",
    "    let mut out = HeaderMap::new();\n",
    "    for k in HEADER_ALLOWLIST.iter() {\n",
    "        if let Some(v) = req_headers.get(*k) {\n",
    "            out.insert(*k, v.clone());\n",
    "        }\n",
    "    }\n",
    "    out.insert(\"x-request-id\", HeaderValue::from_str(req_id).unwrap_or(HeaderValue::from_static(\"bad-uuid\")));\n",
    "    out\n",
    "}\n",
    "\n",
    "// ---- Fan-out to RAG ----\n",
    "async fn post_json(\n",
    "    client: &reqwest::Client,\n",
    "    base: &str,\n",
    "    path: &str,\n",
    "    headers: HeaderMap,\n",
    "    body: serde_json::Value,\n",
    "    step: &str,\n",
    "    step_timeout: Duration,\n",
    ") -> StepResult<serde_json::Value> {\n",
    "    let url = format!(\"{}{}\", base, path);\n",
    "    let t0 = std::time::Instant::now();\n",
    "    let res = timeout(step_timeout, async {\n",
    "        let r = client.post(&url).headers(headers).json(&body).send().await;\n",
    "        match r {\n",
    "            Ok(resp) => {\n",
    "                if !resp.status().is_success() {\n",
    "                    Err(anyhow::anyhow!(\"status {}\", resp.status()))\n",
    "                } else {\n",
    "                    resp.json::<serde_json::Value>().await.map_err(|e| e.into())\n",
    "                }\n",
    "            }\n",
    "            Err(e) => Err(e.into())\n",
    "        }\n",
    "    }).await;\n",
    "\n",
    "    match res {\n",
    "        Ok(Ok(json)) => {\n",
    "            let ms = t0.elapsed().as_millis() as u64;\n",
    "            per_step_hist(step, ms as f64);\n",
    "            StepResult{ ok: true, latency_ms: Some(ms), payload: Some(json), error: None }\n",
    "        }\n",
    "        Ok(Err(e)) => {\n",
    "            fanout_error(step);\n",
    "            StepResult{ ok: false, latency_ms: Some(t0.elapsed().as_millis() as u64), payload: None, error: Some(e.to_string()) }\n",
    "        }\n",
    "        Err(_) => {\n",
    "            fanout_error(step);\n",
    "            StepResult{ ok: false, latency_ms: Some(t0.elapsed().as_millis() as u64), payload: None, error: Some(\"timeout\".into()) }\n",
    "        }\n",
    "    }\n",
    "}\n",
    "\n",
    "#[derive(Serialize, Deserialize)]\n",
    "struct AnswerMeta {\n",
    "    pipeline: String,\n",
    "    latency_ms: u64,\n",
    "    cache: &'static str,\n",
    "    partial_retrieval: serde_json::Value,\n",
    "    route: String,\n",
    "    ns: String,\n",
    "    step_ms: Option<HashMap<String, u64>>,\n",
    "}\n",
    "\n",
    "#[derive(Serialize, Deserialize)]\n",
    "struct AnswerResp {\n",
    "    answer: String,\n",
    "    contexts: Vec<serde_json::Value>,\n",
    "    verify: serde_json::Value,\n",
    "    meta: AnswerMeta,\n",
    "}\n",
    "\n",
    "// ---- Handlers ----\n",
    "#[axum::debug_handler]\n",
    "async fn up_handler() -> impl IntoResponse {\n",
    "    Json(serde_json::json!({\"ok\": true, \"service\": \"gateway\"}))\n",
    "}\n",
    "\n",
    "#[axum::debug_handler]\n",
    "async fn metrics_handler() -> impl IntoResponse {\n",
    "    let handle = PROM_HANDLE.get().unwrap();\n",
    "    axum::response::Response::builder()\n",
    "        .status(StatusCode::OK)\n",
    "        .header(axum::http::header::CONTENT_TYPE, \"text/plain; version=0.0.4\")\n",
    "        .body(handle.render().into())\n",
    "        .unwrap()\n",
    "}\n",
    "\n",
    "#[derive(Default)]\n",
    "struct PromHandle { handle: Option<metrics_exporter_prometheus::PrometheusHandle> }\n",
    "static PROM_HANDLE: Lazy<PromHandleCell> = Lazy::new(|| PromHandleCell{ inner: RwLock::new(None) });\n",
    "struct PromHandleCell{ inner: RwLock<Option<metrics_exporter_prometheus::PrometheusHandle>> }\n",
    "impl PromHandleCell {\n",
    "    fn set(&self, h: metrics_exporter_prometheus::PrometheusHandle) { *self.inner.write() = Some(h); }\n",
    "    fn get(&self) -> Option<metrics_exporter_prometheus::PrometheusHandle> { self.inner.read().clone() }\n",
    "}\n",
    "trait PromHandleExt { fn set_handle(h: metrics_exporter_prometheus::PrometheusHandle); }\n",
    "impl PromHandleExt for PromHandle {\n",
    "    fn set_handle(_h: metrics_exporter_prometheus::PrometheusHandle) {}\n",
    "}\n",
    "impl PromHandleCell {\n",
    "    fn get_handle(&self) -> metrics_exporter_prometheus::PrometheusHandle {\n",
    "        self.inner.read().clone().unwrap()\n",
    "    }\n",
    "}\n",
    "impl PromHandleCell {\n",
    "    fn render(&self) -> String { self.inner.read().as_ref().unwrap().render() }\n",
    "}\n",
    "\n",
    "async fn admin_ping() -> impl IntoResponse {\n",
    "    Json(serde_json::json!({\"ok\": true, \"pong\": true, \"service\": \"gateway\"}))\n",
    "}\n",
    "\n",
    "// In-memory mirrors for budgets/QPM\n",
    "#[derive(Deserialize)]\n",
    "struct QpmSet { namespace: Option<String>, route: Option<String>, qpm: i64 }\n",
    "#[derive(Deserialize)]\n",
    "struct CostSet { namespace: Option<String>, route: Option<String>, daily_usd: f64 }\n",
    "\n",
    "async fn qpm_set(State(st): State<AppState>, Json(req): Json<QpmSet>) -> impl IntoResponse {\n",
    "    let ns = req.namespace.unwrap_or_else(|| \"default\".into()).to_lowercase();\n",
    "    let route = req.route.unwrap_or_else(|| \"answer\".into()).to_lowercase();\n",
    "    st.qpm.write().insert((ns.clone(), route.clone()), req.qpm.max(0));\n",
    "    Json(serde_json::json!({\"ok\": true, \"namespace\": ns, \"route\": route, \"qpm\": req.qpm}))\n",
    "}\n",
    "async fn qpm_summary(State(st): State<AppState>) -> impl IntoResponse {\n",
    "    let map = st.qpm.read();\n",
    "    let mut items = Vec::new();\n",
    "    for ((ns, route), q) in map.iter() {\n",
    "        items.push(serde_json::json!({\"namespace\": ns, \"route\": route, \"qpm\": q}));\n",
    "    }\n",
    "    Json(serde_json::json!({\"ok\": true, \"limits\": items}))\n",
    "}\n",
    "async fn cost_set(State(st): State<AppState>, Json(req): Json<CostSet>) -> impl IntoResponse {\n",
    "    let ns = req.namespace.unwrap_or_else(|| \"default\".into()).to_lowercase();\n",
    "    let route = req.route.unwrap_or_else(|| \"answer\".into()).to_lowercase();\n",
    "    st.budgets.write().insert((ns.clone(), route.clone()), req.daily_usd.max(0.0));\n",
    "    Json(serde_json::json!({\"ok\": true, \"budget\": {\"namespace\": ns, \"route\": route, \"daily_usd\": req.daily_usd}}))\n",
    "}\n",
    "async fn cost_summary(State(st): State<AppState>) -> impl IntoResponse {\n",
    "    let map = st.budgets.read();\n",
    "    let mut items = Vec::new();\n",
    "    for ((ns, route), usd) in map.iter() {\n",
    "        items.push(serde_json::json!({\"namespace\": ns, \"route\": route, \"daily_usd\": usd}));\n",
    "    }\n",
    "    Json(serde_json::json!({\"ok\": true, \"budgets\": items}))\n",
    "}\n",
    "\n",
    "// Router detection (simple; mirror Py heuristics)\n",
    "fn route_for(query: &str, override_hdr: Option<&str>) -> String {\n",
    "    if let Some(ov) = override_hdr { if !ov.trim().is_empty() { return ov.trim().to_lowercase(); } }\n",
    "    let q = query.trim().to_lowercase();\n",
    "    let sql_re = regex_lite::Regex::new(r\"(?is)^\\s*(select|with)\\b\").unwrap();\n",
    "    let math_re = regex_lite::Regex::new(r\"\\b(integrate|derive|differentiate|solve|sum|product|limit|matrix|determinant|eigen)\\b\").unwrap();\n",
    "    let policy_re = regex_lite::Regex::new(r\"\\b(policy|regulation|compliance|gdpr|hipaa|soc2|security)\\b\").unwrap();\n",
    "    if sql_re.is_match(&q) { return \"sql\".into(); }\n",
    "    if math_re.is_match(&q) { return \"math\".into(); }\n",
    "    if policy_re.is_match(&q) { return \"policy\".into(); }\n",
    "    \"factual\".into()\n",
    "}\n",
    "\n",
    "#[axum::debug_handler]\n",
    "async fn answer_handler(\n",
    "    State(st): State<AppState>,\n",
    "    headers: HeaderMap,\n",
    "    Json(body): Json<AnswerReq>,\n",
    ") -> impl IntoResponse {\n",
    "    // Circuit breaker\n",
    "    {\n",
    "        if st.breaker.read().open() {\n",
    "            if let Some(mut v) = st.cache.write().get(&RespCache::make_key(&body)) {\n",
    "                let mut p = v;\n",
    "                if let Some(m) = p.get_mut(\"meta\") {\n",
    "                    if let Some(obj) = m.as_object_mut() {\n",
    "                        obj.insert(\"breaker\".into(), serde_json::json!(\"stale\"));\n",
    "                        obj.insert(\"cache\".into(), serde_json::json!(\"hit\"));\n",
    "                    }\n",
    "                }\n",
    "                return (StatusCode::OK, Json(p));\n",
    "            }\n",
    "            let rem = st.breaker.read().remaining();\n",
    "            return (StatusCode::SERVICE_UNAVAILABLE, Json(serde_json::json!({\"error\":\"breaker_open\",\"retry_after\": rem})));\n",
    "        }\n",
    "    }\n",
    "\n",
    "    let t0 = std::time::Instant::now();\n",
    "    let ns = headers.get(\"x-namespace\").and_then(|v| v.to_str().ok()).unwrap_or(\"default\").to_lowercase();\n",
    "    let route_override = headers.get(\"x-route\").and_then(|v| v.to_str().ok());\n",
    "    let route_name = route_for(&body.query, route_override);\n",
    "    let req_id = get_or_gen_request_id(&headers);\n",
    "    let mut resp_headers = HeaderMap::new();\n",
    "    let _ = resp_headers.insert(\"x-request-id\", HeaderValue::from_str(&req_id).unwrap());\n",
    "\n",
    "    // Cache\n",
    "    let key = RespCache::make_key(&body);\n",
    "    if let Some(mut cached) = st.cache.write().get(&key) {\n",
    "        if let Some(m) = cached.get_mut(\"meta\") {\n",
    "            if let Some(obj) = m.as_object_mut() {\n",
    "                obj.insert(\"cache\".into(), serde_json::json!(\"hit\"));\n",
    "                obj.insert(\"ns\".into(), serde_json::json!(ns.clone()));\n",
    "                obj.insert(\"route\".into(), serde_json::json!(route_name.clone()));\n",
    "            }\n",
    "        }\n",
    "        return (StatusCode::OK, (resp_headers, Json(cached)));\n",
    "    }\n",
    "\n",
    "    // HTTP client\n",
    "    let client = reqwest::Client::builder().pool_idle_timeout(Duration::from_secs(10)).build().unwrap();\n",
    "    let fanout_headers = build_fanout_headers(&headers, &req_id);\n",
    "    let top_k = body.top_k.max(1).min(50) as i64;\n",
    "\n",
    "    // Parallel retrieve\n",
    "    let bm25_f = post_json(&client, &st.rag_base, \"/retrieve_bm25\", fanout_headers.clone(),\n",
    "                           serde_json::json!({\"query\": body.query, \"top_k\": top_k}), \"bm25\",\n",
    "                           Duration::from_millis(step_timeout_ms(\"bm25\")));\n",
    "    let dense_f = post_json(&client, &st.rag_base, \"/retrieve_dense\", fanout_headers.clone(),\n",
    "                           serde_json::json!({\"query\": body.query, \"top_k\": top_k}), \"dense\",\n",
    "                           Duration::from_millis(step_timeout_ms(\"dense\")));\n",
    "    let (bm25_res, dense_res) = tokio::join!(bm25_f, dense_f);\n",
    "\n",
    "    let mut contexts: Vec<serde_json::Value> = Vec::new();\n",
    "    let mut partial = serde_json::json!({\"dense_ok\": false, \"bm25_ok\": false});\n",
    "    if bm25_res.ok {\n",
    "        if let Some(p) = bm25_res.payload { if let Some(arr) = p.get(\"results\").and_then(|x| x.as_array()) {\n",
    "            contexts.extend(arr.clone());\n",
    "            partial[\"bm25_ok\"] = serde_json::json!(true);\n",
    "        }}\n",
    "    }\n",
    "    if dense_res.ok {\n",
    "        if let Some(p) = dense_res.payload { if let Some(arr) = p.get(\"results\").and_then(|x| x.as_array()) {\n",
    "            contexts.extend(arr.clone());\n",
    "            partial[\"dense_ok\"] = serde_json::json!(true);\n",
    "        }}\n",
    "    }\n",
    "    if contexts.is_empty() {\n",
    "        st.breaker.write().record(false);\n",
    "        return (StatusCode::BAD_GATEWAY, Json(serde_json::json!({\"error\":\"preflight_failed: both retrievals failed\"})));\n",
    "    }\n",
    "\n",
    "    // use_rerank logic\n",
    "    let mut use_rerank = body.use_rerank.unwrap_or(true);\n",
    "    if headers.get(\"x-chaos-disable-ce\").is_some() { use_rerank = false; }\n",
    "\n",
    "    // Optional rerank\n",
    "    if use_rerank {\n",
    "        let rr = post_json(&client, &st.rag_base, \"/rerank\", fanout_headers.clone(),\n",
    "                           serde_json::json!({\"query\": body.query, \"contexts\": contexts}),\n",
    "                           \"rerank\", Duration::from_millis(step_timeout_ms(\"rerank\"))).await;\n",
    "        if rr.ok {\n",
    "            if let Some(p) = rr.payload {\n",
    "                if let Some(arr) = p.get(\"contexts\").and_then(|x| x.as_array()) {\n",
    "                    contexts = arr.clone();\n",
    "                }\n",
    "            }\n",
    "        }\n",
    "    }\n",
    "\n",
    "    // Complete\n",
    "    let comp = post_json(&client, &st.rag_base, \"/complete\", fanout_headers.clone(),\n",
    "                         serde_json::json!({\"query\": body.query, \"contexts\": contexts}),\n",
    "                         \"complete\", Duration::from_millis(step_timeout_ms(\"complete\"))).await;\n",
    "    if !comp.ok {\n",
    "        st.breaker.write().record(false);\n",
    "        return (StatusCode::BAD_GATEWAY, Json(serde_json::json!({\"error\": format!(\"complete_failed: {:?}\", comp.error)})));\n",
    "    }\n",
    "\n",
    "    let verify = comp.payload.as_ref().and_then(|p| p.get(\"verify\")).cloned().unwrap_or(serde_json::json!({}));\n",
    "    let answer = comp.payload.as_ref().and_then(|p| p.get(\"answer\")).and_then(|v| v.as_str()).unwrap_or(\"\").to_string();\n",
    "\n",
    "    // step timings\n",
    "    let mut step_ms: HashMap<String,u64> = HashMap::new();\n",
    "    if let Some(ms) = bm25_res.latency_ms { step_ms.insert(\"bm25_ms\".into(), ms); }\n",
    "    if let Some(ms) = dense_res.latency_ms { step_ms.insert(\"dense_ms\".into(), ms); }\n",
    "    if use_rerank {\n",
    "        // nothing extra here; measured inside per_step_hist\n",
    "    }\n",
    "    if let Some(ms) = comp.latency_ms { step_ms.insert(\"complete_ms\".into(), ms); }\n",
    "\n",
    "    let latency_ms = t0.elapsed().as_millis() as u64;\n",
    "    observe_gateway_latency_ms(latency_ms as f64);\n",
    "\n",
    "    let payload = serde_json::json!({\n",
    "        \"answer\": answer,\n",
    "        \"contexts\": contexts.into_iter().take(top_k as usize).collect::<Vec<_>>(),\n",
    "        \"verify\": verify,\n",
    "        \"meta\": {\n",
    "            \"pipeline\": if use_rerank { \"rerank\" } else { \"rrf\" },\n",
    "            \"latency_ms\": latency_ms,\n",
    "            \"cache\": \"miss\",\n",
    "            \"partial_retrieval\": partial,\n",
    "            \"route\": route_name,\n",
    "            \"ns\": ns,\n",
    "            \"step_ms\": step_ms,\n",
    "        }\n",
    "    });\n",
    "\n",
    "    st.breaker.write().record(true);\n",
    "    st.cache.write().put(key, payload.clone());\n",
    "    (StatusCode::OK, (resp_headers, Json(payload)))\n",
    "}\n",
    "\n",
    "fn step_timeout_ms(step: &str) -> u64 {\n",
    "    let bm25 = std::env::var(\"STEP_TIMEOUT_BM25\").ok().and_then(|s| s.parse::<f64>().ok()).unwrap_or(0.8);\n",
    "    let dense = std::env::var(\"STEP_TIMEOUT_DENSE\").ok().and_then(|s| s.parse::<f64>().ok()).unwrap_or(1.2);\n",
    "    let rerank = std::env::var(\"STEP_TIMEOUT_RERANK\").ok().and_then(|s| s.parse::<f64>().ok()).unwrap_or(0.6);\n",
    "    let complete = std::env::var(\"STEP_TIMEOUT_COMPLETE\").ok().and_then(|s| s.parse::<f64>().ok()).unwrap_or(2.0);\n",
    "    let sec = match step { \"bm25\" => bm25, \"dense\" => dense, \"rerank\" => rerank, \"complete\" => complete, _ => 1.0 };\n",
    "    (sec * 1000.0) as u64\n",
    "}\n",
    "\n",
    "// ---- Boot / Bind with auto-increment + PORT.txt ----\n",
    "fn write_port_txt(port: u16) {\n",
    "    let art = std::env::var(\"ART_DIR\").unwrap_or_else(|_| \"/tmp/art\".into());\n",
    "    let dir = std::path::Path::new(&art);\n",
    "    let _ = std::fs::create_dir_all(dir);\n",
    "    let path = dir.join(\"PORT.txt\");\n",
    "    if let Ok(mut f) = std::fs::File::create(path) {\n",
    "        let _ = writeln!(f, \"{}\", port);\n",
    "    }\n",
    "}\n",
    "\n",
    "#[tokio::main]\n",
    "async fn main() {\n",
    "    tracing_subscriber::fmt()\n",
    "        .with_env_filter(EnvFilter::from_default_env().add_directive(\"info\".parse().unwrap()))\n",
    "        .init();\n",
    "\n",
    "    let recorder = PrometheusBuilder::new().install_recorder().expect(\"prom recorder\");\n",
    "    PROM_HANDLE.set(recorder.handle());\n",
    "\n",
    "    // Config\n",
    "    let rag_base = std::env::var(\"RAG_BASE\").unwrap_or_else(|_| \"http://127.0.0.1:9909\".into());\n",
    "    let mut port: u16 = std::env::var(\"GATEWAY_PORT\").ok().and_then(|s| s.parse::<u16>().ok()).unwrap_or(9910);\n",
    "    let host: &str = \"0.0.0.0\";\n",
    "\n",
    "    // Bind attempts with auto-increment\n",
    "    let listener = loop {\n",
    "        bind_attempt();\n",
    "        match tokio::net::TcpListener::bind((host, port)).await {\n",
    "            Ok(l) => break l,\n",
    "            Err(_) => { port += 1; if port > 9999 { panic!(\"No available ports 9910..9999\"); } }\n",
    "        }\n",
    "    };\n",
    "    write_port_txt(port);\n",
    "    info!(\"gateway listening on {host}:{port}, rag_base={rag_base}\");\n",
    "\n",
    "    let state = AppState {\n",
    "        rag_base,\n",
    "        cache: Arc::new(RwLock::new(RespCache::new(60, 512))),\n",
    "        breaker: Arc::new(RwLock::new(CircuitBreaker::new())),\n",
    "        qpm: Arc::new(RwLock::new(HashMap::new())),\n",
    "        budgets: Arc::new(RwLock::new(HashMap::new())),\n",
    "        header_allow: Arc::new(HEADER_ALLOWLIST.clone()),\n",
    "    };\n",
    "\n",
    "    let app = Router::new()\n",
    "        .route(\"/up\", get(up_handler))\n",
    "        .route(\"/metrics\", get(metrics_handler))\n",
    "        .route(\"/admin/ping\", get(admin_ping))\n",
    "        .route(\"/admin/limits/qpm/set\", post(qpm_set))\n",
    "        .route(\"/admin/limits/qpm/summary\", get(qpm_summary))\n",
    "        .route(\"/admin/costs/set\", post(cost_set))\n",
    "        .route(\"/admin/costs/summary\", get(cost_summary))\n",
    "        .route(\"/answer\", post(answer_handler))\n",
    "        .with_state(state);\n",
    "\n",
    "    axum::serve(listener, app).await.unwrap();\n",
    "}\n",
    "\n",
    "// Minimal regex engine without pulling full regex crate; small and fast for our use.\n",
    "mod regex_lite {\n",
    "    pub use regex::Regex;\n",
    "    // Re-exporting full regex; if you want truly \"lite\", swap to regex-lite crate.\n",
    "}\n",
    "'''\n",
    "print(\"gateway/Cargo.toml\")\n",
    "print(cargo_toml.strip())\n",
    "print(\"\\n\" + \"=\"*80 + \"\\n\")\n",
    "print(\"gateway/src/main.rs\")\n",
    "print(main_rs.strip())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 503,
   "id": "dcbe9a03-616f-433a-8390-0d41f76057fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, sys, time, json, uuid, threading, asyncio, socket, re\n",
    "from pathlib import Path\n",
    "from typing import Any, Dict, List, Optional\n",
    "from dataclasses import dataclass\n",
    "from collections import deque"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 504,
   "id": "9b8f5c20-8283-4073-8f4d-9beb2e742853",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _need(pkg):\n",
    "    try: __import__(pkg); return False\n",
    "    except Exception: return True\n",
    "_missing = [p for p in (\"fastapi\",\"uvicorn\",\"httpx\") if _need(p)]\n",
    "if _missing:\n",
    "    import subprocess\n",
    "    print(\" pip install\", \" \".join(_missing))\n",
    "    subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", *(_missing)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 505,
   "id": "e36f0492-33b6-477e-9f9a-cba70e07a9fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "from fastapi import FastAPI, Request, Body, HTTPException\n",
    "from fastapi.responses import JSONResponse, PlainTextResponse\n",
    "from pydantic import BaseModel, Field\n",
    "import httpx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 506,
   "id": "4d9b9e0d-7f32-4070-be77-1dfdd2e7e6f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "_METRICS = False\n",
    "try:\n",
    "    from prometheus_client import CollectorRegistry, Histogram, Counter, generate_latest, CONTENT_TYPE_LATEST\n",
    "    REG = CollectorRegistry()\n",
    "    GATEWAY_LAT_MS = Histogram(\"gateway_answer_latency_ms\", \"Overall latency (ms)\", registry=REG, buckets=[10,20,40,80,160,320,640,1280,2560])\n",
    "    RAG_BM25_MS   = Histogram(\"rag_bm25_ms\",   \"BM25 step (ms)\", registry=REG, buckets=[10,20,40,80,160,320,640,1280])\n",
    "    RAG_DENSE_MS  = Histogram(\"rag_dense_ms\",  \"Dense step (ms)\", registry=REG, buckets=[10,20,40,80,160,320,640,1280])\n",
    "    RAG_RERANK_MS = Histogram(\"rag_rerank_ms\", \"Rerank step (ms)\", registry=REG, buckets=[10,20,40,80,160,320,640,1280])\n",
    "    RAG_COMPLETE_MS=Histogram(\"rag_complete_ms\",\"Complete step (ms)\",registry=REG, buckets=[20,40,80,160,320,640,1280,2560])\n",
    "    FANOUT_ERRORS = Counter(\"gateway_fanout_errors_total\", \"Fanout errors\", [\"step\"], registry=REG)\n",
    "    _METRICS = True\n",
    "except Exception:\n",
    "    class _N:\n",
    "        def observe(self,*a,**k): ...\n",
    "        def labels(self,*a,**k): return self\n",
    "        def inc(self,*a,**k): ...\n",
    "    GATEWAY_LAT_MS = RAG_BM25_MS = RAG_DENSE_MS = RAG_RERANK_MS = RAG_COMPLETE_MS = FANOUT_ERRORS = _N()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 507,
   "id": "fe1bc63d-ef0e-499c-ac85-7237f9ff2ca6",
   "metadata": {},
   "outputs": [],
   "source": [
    "HOST = \"127.0.0.1\"\n",
    "DEFAULT_RAG_PORT = 9909\n",
    "RAG_BASE = os.getenv(\"RAG_BASE\", f\"http://{HOST}:{DEFAULT_RAG_PORT}\")  \n",
    "GATEWAY_START_PORT = int(os.getenv(\"GATEWAY_PORT\", \"9910\"))            \n",
    "GATEWAY_MAX_TRIES = 30                                               \n",
    "STEP_TIMEOUTS = {  \n",
    "    \"bm25\": float(os.getenv(\"STEP_TIMEOUT_BM25\", \"0.5\")),\n",
    "    \"dense\": float(os.getenv(\"STEP_TIMEOUT_DENSE\",\"0.8\")),\n",
    "    \"rerank\":float(os.getenv(\"STEP_TIMEOUT_RERANK\",\"0.5\")),\n",
    "    \"complete\":float(os.getenv(\"STEP_TIMEOUT_COMPLETE\",\"1.6\")),\n",
    "}\n",
    "HTTP_LIMITS = httpx.Limits(max_keepalive_connections=2, max_connections=8)  \n",
    "ALLOW_HEADERS = {\"x-request-id\",\"x-namespace\",\"x-route\",\"x-verify-v2\"}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 508,
   "id": "f9ca5d02-48dc-4213-b049-eebaaa375ed3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _port_open(host: str, port: int, timeout_s: float=0.2) -> bool:\n",
    "    s=socket.socket(); s.settimeout(timeout_s)\n",
    "    try: s.connect((host,port)); s.close(); return True\n",
    "    except Exception: s.close(); return False\n",
    "def find_free_port(start: int, tries: int = 30, host: str = HOST) -> Optional[int]:\n",
    "    for i in range(tries):\n",
    "        p = start + i\n",
    "        with socket.socket() as s:\n",
    "            try:\n",
    "                s.bind((host, p))\n",
    "                return p\n",
    "            except Exception:\n",
    "                continue\n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 509,
   "id": "9a7574f4-f1ef-4656-9138-e15d7b284c9c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "_SQL    = re.compile(r\"(?is)^\\s*(select|with)\\b\")\n",
    "_MATH   = re.compile(r\"\\b(integrate|derive|differentiate|solve|sum|product|limit|matrix|determinant|eigen)\\b\", re.I)\n",
    "_POLICY = re.compile(r\"\\b(policy|regulation|compliance|gdpr|hipaa|soc2|security)\\b\", re.I)\n",
    "def route_for(q: str, override: Optional[str]=None)->str:\n",
    "    if override: return override.strip().lower()\n",
    "    q=(q or \"\").strip()\n",
    "    if _SQL.search(q): return \"sql\"\n",
    "    if _MATH.search(q): return \"math\"\n",
    "    if _POLICY.search(q): return \"policy\"\n",
    "    return \"factual\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 510,
   "id": "2ce760e0-976e-4567-834a-c213caf1bef1",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CircuitBreaker:\n",
    "    def __init__(self, window=30, threshold=0.35, min_events=10, open_seconds=15):\n",
    "        self.win=int(window); self.th=threshold; self.min=min_events; self.open_s=open_seconds\n",
    "        self._events=deque(); self._open_until=0.0\n",
    "    def _gc(self):\n",
    "        now=time.time()\n",
    "        while self._events and (now-self._events[0][0])>self.win: self._events.popleft()\n",
    "    def record(self, ok: bool):\n",
    "        now=time.time(); self._gc(); self._events.append((now,bool(ok)))\n",
    "        if now < self._open_until: return\n",
    "        if len(self._events)>=self.min:\n",
    "            errs=sum(1 for _,ok in self._events if not ok)\n",
    "            rate=errs/max(1,len(self._events))\n",
    "            if rate>=self.th: self._open_until=now+self.open_s\n",
    "    def open(self)->bool: return time.time()<self._open_until\n",
    "    def remaining(self)->int: return max(0, int(self._open_until-time.time()))\n",
    "CB = CircuitBreaker()\n",
    "class RespCache(dict):\n",
    "    def __init__(self, ttl_s=30, max_items=64): super().__init__(); self.ttl_s=ttl_s; self.max_items=max_items\n",
    "    def make_key(self, body: Dict[str,Any]) -> str:\n",
    "        return json.dumps({k: body.get(k) for k in (\"query\",\"top_k\",\"use_rerank\")}, sort_keys=True)\n",
    "    def get(self, key: str):\n",
    "        v = super().get(key); \n",
    "        if not v: return None\n",
    "        ts, payload = v\n",
    "        if time.time()-ts > self.ttl_s:\n",
    "            try: super().pop(key, None)\n",
    "            except: ...\n",
    "            return None\n",
    "        return payload\n",
    "    def put(self, key: str, payload: Dict[str,Any]):\n",
    "        if len(self) >= self.max_items:\n",
    "            try: self.pop(next(iter(self.keys())))\n",
    "            except: ...\n",
    "        super().__setitem__(key, (time.time(), payload))\n",
    "RESP_CACHE = RespCache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 511,
   "id": "f052c9be-66b5-4163-976a-8628c65c1a88",
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class StepResult:\n",
    "    ok: bool\n",
    "    latency_ms: Optional[int]=None\n",
    "    payload: Optional[Dict[str,Any]]=None\n",
    "    error: Optional[str]=None\n",
    "class RagClient:\n",
    "    def __init__(self, base: str):\n",
    "        self.base=base.rstrip(\"/\")\n",
    "        self.client=httpx.AsyncClient(base_url=self.base, timeout=2.0, limits=HTTP_LIMITS)\n",
    "    def set_base(self, base: str):\n",
    "        self.base = base.rstrip(\"/\")\n",
    "    def _fanout_headers(self, req: Request)->Dict[str,str]:\n",
    "        out={k:v for k,v in req.headers.items() if k.lower() in ALLOW_HEADERS}\n",
    "        out.setdefault(\"x-request-id\", req.headers.get(\"x-request-id\", uuid.uuid4().hex))\n",
    "        return out\n",
    "    async def _post(self, req: Request, path: str, body: Dict[str,Any], step: str, tmo: float)->StepResult:\n",
    "        url=f\"{self.base}{path}\"\n",
    "        headers=self._fanout_headers(req)\n",
    "        t0=time.time()\n",
    "        try:\n",
    "            async with asyncio.timeout(tmo):\n",
    "                r=await self.client.post(url, json=body, headers=headers)\n",
    "            r.raise_for_status()\n",
    "            ms=int((time.time()-t0)*1000)\n",
    "            if step==\"bm25\":   RAG_BM25_MS.observe(ms)\n",
    "            if step==\"dense\":  RAG_DENSE_MS.observe(ms)\n",
    "            if step==\"rerank\": RAG_RERANK_MS.observe(ms)\n",
    "            if step==\"complete\": RAG_COMPLETE_MS.observe(ms)\n",
    "            return StepResult(True, ms, r.json(), None)\n",
    "        except Exception as e:\n",
    "            FANOUT_ERRORS.labels(step=step).inc() if _METRICS else None\n",
    "            return StepResult(False, int((time.time()-t0)*1000), None, str(e))\n",
    "    async def bm25(self, req: Request, q: str, k: int)->StepResult:\n",
    "        return await self._post(req, \"/retrieve_bm25\", {\"query\":q,\"top_k\":k}, \"bm25\", STEP_TIMEOUTS[\"bm25\"])\n",
    "    async def dense(self, req: Request, q: str, k: int)->StepResult:\n",
    "        return await self._post(req, \"/retrieve_dense\", {\"query\":q,\"top_k\":k}, \"dense\", STEP_TIMEOUTS[\"dense\"])\n",
    "    async def rerank(self, req: Request, q: str, ctx: List[Dict[str,Any]])->StepResult:\n",
    "        return await self._post(req, \"/rerank\", {\"query\":q,\"contexts\":ctx}, \"rerank\", STEP_TIMEOUTS[\"rerank\"])\n",
    "    async def complete(self, req: Request, q: str, ctx: List[Dict[str,Any]])->StepResult:\n",
    "        return await self._post(req, \"/complete\", {\"query\":q,\"contexts\":ctx}, \"complete\", STEP_TIMEOUTS[\"complete\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 512,
   "id": "a01634a0-4a05-4c01-9ef3-e4d935b4a436",
   "metadata": {},
   "outputs": [],
   "source": [
    "app = FastAPI(title=\"Ultra-Light Gateway\", version=\"0.2\")\n",
    "rag = RagClient(RAG_BASE)\n",
    "_WEIGHTS = {\"sql\":1.0,\"math\":1.0,\"policy\":1.0,\"factual\":1.0}\n",
    "_REQ_LAT = deque(maxlen=500); _FAITH = deque(maxlen=500)\n",
    "class AnswerReq(BaseModel):\n",
    "    query: str = Field(...)\n",
    "    top_k: int = Field(5, ge=1, le=20)\n",
    "    use_rerank: Optional[bool] = None\n",
    "@app.get(\"/up\")\n",
    "def up():\n",
    "    return {\"ok\": True, \"service\":\"gateway\", \"ts\": int(time.time())}\n",
    "@app.get(\"/metrics\")\n",
    "def metrics():\n",
    "    if not _METRICS: return PlainTextResponse(\"metrics disabled\", status_code=404)\n",
    "    return PlainTextResponse(generate_latest(REG), media_type=CONTENT_TYPE_LATEST)\n",
    "@app.get(\"/admin/ping\")\n",
    "def ping():\n",
    "    return {\"ok\": True, \"pong\": True}\n",
    "class RLUpdateReq(BaseModel):\n",
    "    weights: Dict[str, float]\n",
    "@app.post(\"/rl/update\")\n",
    "def rl_update(req: RLUpdateReq):\n",
    "    for k,v in (req.weights or {}).items():\n",
    "        _WEIGHTS[k.strip().lower()] = max(0.0, min(1.0, float(v)))\n",
    "    return {\"ok\": True, \"weights\": dict(_WEIGHTS)}\n",
    "def _p95(xs: List[float])->Optional[float]:\n",
    "    if not xs: return None\n",
    "    xs=sorted(xs); i=(len(xs)-1)*0.95; lo=int(i); hi=min(len(xs)-1, lo+1)\n",
    "    return xs[lo] if lo==hi else xs[lo]+(xs[hi]-xs[lo])*(i-lo)\n",
    "@app.get(\"/canary/status\")\n",
    "def canary_status():\n",
    "    lat = list(_REQ_LAT)\n",
    "    faith = [x for x in list(_FAITH) if isinstance(x,(int,float))]\n",
    "    return {\"ok\": True, \"weights\": dict(_WEIGHTS),\n",
    "            \"window\": {\"count\": len(lat), \"p95_ms\": _p95(lat), \"faith_mean\": (sum(faith)/len(faith)) if faith else None}}\n",
    "def _route(q: str, override: Optional[str])->str:\n",
    "    return route_for(q, override)\n",
    "@app.post(\"/answer\")\n",
    "async def answer(request: Request, body: AnswerReq = Body(...)):\n",
    "    if CB.open():\n",
    "        key = RESP_CACHE.make_key(body.model_dump())\n",
    "        hit = RESP_CACHE.get(key)\n",
    "        if hit:\n",
    "            p = dict(hit); p.setdefault(\"meta\",{}).update({\"breaker\":\"stale\",\"cache\":\"hit\"})\n",
    "            return JSONResponse(p)\n",
    "        raise HTTPException(status_code=503, detail={\"error\":\"breaker_open\",\"retry_after\": CB.remaining()})\n",
    "    t0=time.time()\n",
    "    q = (body.query or \"\").strip()\n",
    "    if not q: raise HTTPException(400, \"query required\")\n",
    "    key = RESP_CACHE.make_key(body.model_dump())\n",
    "    hit = RESP_CACHE.get(key)\n",
    "    if hit:\n",
    "        p = dict(hit); p.setdefault(\"meta\",{}).update({\"cache\":\"hit\"})\n",
    "        return JSONResponse(p)\n",
    "    ns = request.headers.get(\"x-namespace\",\"default\").strip().lower()\n",
    "    route_override = request.headers.get(\"x-route\")\n",
    "    route_name = _route(q, route_override)\n",
    "    top_k = int(min(max(body.top_k,1), 10)) \n",
    "    try:\n",
    "        bm25_task = asyncio.create_task(rag.bm25(request, q, top_k))\n",
    "        dense_task = asyncio.create_task(rag.dense(request, q, top_k))\n",
    "        bm25_res, dense_res = await asyncio.gather(bm25_task, dense_task)\n",
    "    except Exception as e:\n",
    "        FANOUT_ERRORS.labels(step=\"concurrency\").inc() if _METRICS else None\n",
    "        bm25_res = dense_res = StepResult(False, error=str(e))\n",
    "    contexts: List[Dict[str,Any]] = []\n",
    "    partial = {\"bm25_ok\": False, \"dense_ok\": False}\n",
    "    if bm25_res.ok: partial[\"bm25_ok\"]=True; contexts.extend((bm25_res.payload or {}).get(\"results\") or [])\n",
    "    if dense_res.ok: partial[\"dense_ok\"]=True; contexts.extend((dense_res.payload or {}).get(\"results\") or [])\n",
    "    use_rerank = body.use_rerank\n",
    "    if use_rerank is None:\n",
    "        ce_prob = float(_WEIGHTS.get(route_name, 1.0))\n",
    "        use_rerank = (ce_prob >= 0.5)\n",
    "    if use_rerank and contexts:\n",
    "        rr = await rag.rerank(request, q, contexts)\n",
    "        if rr.ok and isinstance(rr.payload, dict):\n",
    "            contexts = rr.payload.get(\"contexts\", contexts)\n",
    "    comp = await rag.complete(request, q, contexts if contexts else [])\n",
    "    if not comp.ok:\n",
    "        CB.record(False)\n",
    "        raise HTTPException(502, f\"complete_failed: {comp.error}\")\n",
    "    verify = (comp.payload or {}).get(\"verify\", {})\n",
    "    faith = verify.get(\"faithfulness\")\n",
    "    if isinstance(faith,(int,float)): _FAITH.append(float(faith))\n",
    "    payload = {\n",
    "        \"answer\": (comp.payload or {}).get(\"answer\",\"\"),\n",
    "        \"contexts\": (contexts or [])[:top_k],\n",
    "        \"verify\": verify,\n",
    "        \"meta\": {\n",
    "            \"pipeline\": \"rerank\" if (use_rerank and contexts) else \"rrf\" if contexts else \"complete_only\",\n",
    "            \"latency_ms\": int((time.time()-t0)*1000),\n",
    "            \"cache\": \"miss\",\n",
    "            \"partial_retrieval\": partial,\n",
    "            \"route\": route_name,\n",
    "            \"ns\": ns\n",
    "        }\n",
    "    }\n",
    "    try:\n",
    "        ms = payload[\"meta\"][\"latency_ms\"]\n",
    "        if _METRICS: GATEWAY_LAT_MS.observe(ms)\n",
    "        _REQ_LAT.append(ms)\n",
    "    except Exception: ...\n",
    "    RESP_CACHE.put(key, payload)\n",
    "    CB.record(True)\n",
    "    return JSONResponse(payload)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 513,
   "id": "1d156fcb-134d-4065-9821-8f4afd03d3a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def probe_rag(base: str) -> Dict[str, Any]:\n",
    "    try:\n",
    "        import urllib.request as u, json as _j\n",
    "        req = u.Request(f\"{base}/retrieve_bm25\", data=_j.dumps({\"query\":\"probe\",\"top_k\":1}).encode(\"utf-8\"),\n",
    "                        headers={\"Content-Type\":\"application/json\"}, method=\"POST\")\n",
    "        with u.urlopen(req, timeout=1.5) as r:\n",
    "            if r.status==200:\n",
    "                js = _j.loads(r.read().decode())\n",
    "                if isinstance(js, dict) and \"results\" in js: return {\"ok\": True, \"compatible\": True}\n",
    "    except Exception:\n",
    "        pass\n",
    "    return {\"ok\": True, \"compatible\": False}\n",
    "def start_mock_rag(host=HOST, start_port=DEFAULT_RAG_PORT) -> str:\n",
    "    port = find_free_port(start_port, tries=40, host=host)\n",
    "    if port is None: raise RuntimeError(\"no free port for mock RAG\")\n",
    "    from fastapi import FastAPI\n",
    "    import uvicorn as _uv\n",
    "    mock = FastAPI(title=\"Mock RAG\")\n",
    "    @mock.post(\"/retrieve_bm25\")\n",
    "    def _bm25(body: Dict[str,Any]):\n",
    "        q=body.get(\"query\",\"\"); k=int(body.get(\"top_k\",5)); time.sleep(0.02)\n",
    "        return {\"results\":[{\"id\":f\"bm25:{i}\",\"text\":f\"{q} bm25 #{i}\"} for i in range(k)], \"latency_ms\":20}\n",
    "    @mock.post(\"/retrieve_dense\")\n",
    "    def _dense(body: Dict[str,Any]):\n",
    "        q=body.get(\"query\",\"\"); k=int(body.get(\"top_k\",5)); time.sleep(0.05)\n",
    "        return {\"results\":[{\"id\":f\"dense:{i}\",\"text\":f\"{q} dense #{i}\"} for i in range(k)], \"latency_ms\":50}\n",
    "    @mock.post(\"/rerank\")\n",
    "    def _rerank(body: Dict[str,Any]):\n",
    "        ctx=body.get(\"contexts\",[]); time.sleep(0.03)\n",
    "        return {\"contexts\":ctx, \"latency_ms\":30}\n",
    "    @mock.post(\"/complete\")\n",
    "    def _complete(body: Dict[str,Any]):\n",
    "        q=body.get(\"query\",\"\"); time.sleep(0.08)\n",
    "        return {\"answer\": f\"Answer to: {q}\", \"verify\":{\"faithfulness\":0.9}, \"latency_ms\":80}\n",
    "    def _run(): _uv.run(mock, host=host, port=port, log_level=\"warning\")\n",
    "    threading.Thread(target=_run, name=\"mock-rag\", daemon=True).start()\n",
    "    return f\"http://{host}:{port}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 514,
   "id": "76af2c6a-d21c-48b5-8ec9-e885214cf8d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def start_gateway(host=HOST, start_port=GATEWAY_START_PORT) -> Optional[int]:\n",
    "    import uvicorn as _uv\n",
    "    port = find_free_port(start_port, tries=GATEWAY_MAX_TRIES, host=host)\n",
    "    if port is None:\n",
    "        print(\"no free gateway port\"); return None\n",
    "    def _run(): _uv.run(app, host=host, port=port, log_level=\"warning\")\n",
    "    threading.Thread(target=_run, name=\"gateway\", daemon=True).start()\n",
    "    for _ in range(80):\n",
    "        try:\n",
    "            import urllib.request as u\n",
    "            with u.urlopen(f\"http://{host}:{port}/up\", timeout=0.5) as r:\n",
    "                if r.status==200: return port\n",
    "        except Exception:\n",
    "            time.sleep(0.2)\n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 515,
   "id": "e471387d-8c91-412e-b08f-67de5fe080f1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Using compatible RAG at http://127.0.0.1:8001\n",
      " Gateway at http://127.0.0.1:9911  | RAG: http://127.0.0.1:8001  | metrics: on\n",
      "Smoke: 200 {\"answer\":\"Q: tiny smoke\\nA: Rust serves fast; RAG retrieves & grounds.\\n\\nContext:\\n- Rust pipelines are fast and memory safe using async with Tokio and Axum.\\ \n",
      "ultra-light gateway running. leave this cell alone while you hit the endpoints.\n"
     ]
    }
   ],
   "source": [
    "probe = probe_rag(RAG_BASE)\n",
    "if not probe[\"compatible\"]:\n",
    "    if RAG_BASE != f\"http://{HOST}:{DEFAULT_RAG_PORT}\":\n",
    "        print(f\"RAG at {RAG_BASE} is not compatible with /retrieve_bm25  starting local mock.\")\n",
    "    else:\n",
    "        print(\" No compatible RAG detected  starting local mock.\")\n",
    "    new_base = start_mock_rag()\n",
    "    rag.set_base(new_base)\n",
    "    RAG_BASE = new_base\n",
    "    print(f\" Mock RAG at {RAG_BASE}\")\n",
    "else:\n",
    "    print(f\" Using compatible RAG at {RAG_BASE}\")\n",
    "gw_port = start_gateway()\n",
    "if gw_port is None:\n",
    "    raise SystemExit(\"Gateway failed to bind  re-run the cell.\")\n",
    "print(f\" Gateway at http://{HOST}:{gw_port}  | RAG: {RAG_BASE}  | metrics: {'on' if _METRICS else 'off'}\")\n",
    "def _post_json(url, obj, headers=None, t=2.0):\n",
    "    import urllib.request as u, json as _j\n",
    "    data=_j.dumps(obj).encode(\"utf-8\")\n",
    "    req=u.Request(url, data=data, headers={\"Content-Type\":\"application/json\", **(headers or {})}, method=\"POST\")\n",
    "    with u.urlopen(req, timeout=t) as r: return r.status, r.read().decode()\n",
    "try:\n",
    "    s,b = _post_json(f\"http://{HOST}:{gw_port}/answer\",\n",
    "                     {\"query\":\"tiny smoke\", \"top_k\": 2},\n",
    "                     headers={\"X-Namespace\":\"default\"})\n",
    "    print(\"Smoke:\", s, b[:160], \"\")\n",
    "except Exception as e:\n",
    "    print(\"Smoke failed:\", e)\n",
    "print(\"ultra-light gateway running. leave this cell alone while you hit the endpoints.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 516,
   "id": "f312823e-9f88-441f-8556-b0f7206cfa49",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wrote:\n",
      " - C:\\Users\\aniru\\gateway\\Cargo.toml\n",
      " - C:\\Users\\aniru\\gateway\\src\\main.rs\n",
      "\n",
      "Next steps (local dev):\n",
      "  cd gateway && cargo build --release\n",
      "  RAG_BASE=http://127.0.0.1:9909 GATEWAY_PORT=9910 ./target/release/gateway\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "GATEWAY_DIR = Path(\"gateway\")\n",
    "SRC_DIR = GATEWAY_DIR / \"src\"\n",
    "SRC_DIR.mkdir(parents=True, exist_ok=True)\n",
    "cargo_toml_fixed = r'''\n",
    "[package]\n",
    "name = \"gateway\"\n",
    "version = \"0.1.0\"\n",
    "edition = \"2021\"\n",
    "\n",
    "[dependencies]\n",
    "axum = { version = \"0.7\", features = [\"macros\", \"json\"] }\n",
    "serde = { version = \"1\", features = [\"derive\"] }\n",
    "serde_json = \"1\"\n",
    "tokio = { version = \"1\", features = [\"macros\",\"rt-multi-thread\",\"time\"] }\n",
    "reqwest = { version = \"0.12\", default-features = false, features = [\"json\",\"http2\",\"rustls-tls\"] }\n",
    "tower = \"0.5\"\n",
    "tracing = \"0.1\"\n",
    "tracing-subscriber = { version = \"0.3\", features = [\"fmt\",\"env-filter\"] }\n",
    "metrics = \"0.22\"\n",
    "metrics-exporter-prometheus = \"0.15\"\n",
    "once_cell = \"1.19\"\n",
    "uuid = { version = \"1.10\", features = [\"v4\"] }\n",
    "dashmap = \"6\"\n",
    "parking_lot = \"0.12\"\n",
    "time = { version = \"0.3\", features = [\"macros\"] }\n",
    "anyhow = \"1\"\n",
    "regex = \"1\"\n",
    "rand = \"0.8\"\n",
    "'''\n",
    "\n",
    "main_rs_fixed = r'''\n",
    "use std::{\n",
    "    collections::{HashMap, VecDeque},\n",
    "    net::SocketAddr,\n",
    "    sync::Arc,\n",
    "    time::{Duration, Instant},\n",
    "    fs, io::Write,\n",
    "};\n",
    "use axum::{\n",
    "    routing::{get, post},\n",
    "    extract::{State},\n",
    "    response::{IntoResponse, Response},\n",
    "    http::{HeaderMap, HeaderValue, StatusCode, header},\n",
    "    Json, Router\n",
    "};\n",
    "use once_cell::sync::Lazy;\n",
    "use parking_lot::RwLock;\n",
    "use serde::{Deserialize, Serialize};\n",
    "use metrics::{counter, histogram};\n",
    "use metrics_exporter_prometheus::{PrometheusBuilder, PrometheusHandle};\n",
    "use tokio::time::timeout;\n",
    "use tracing::{info, error};\n",
    "use tracing_subscriber::EnvFilter;\n",
    "use uuid::Uuid;\n",
    "use rand::Rng;\n",
    "use anyhow::anyhow;\n",
    "\n",
    "#[derive(Clone)]\n",
    "struct AppState {\n",
    "    rag_base: String,\n",
    "    cache: Arc<RwLock<RespCache>>,\n",
    "    breaker: Arc<RwLock<CircuitBreaker>>,\n",
    "    qpm: Arc<RwLock<HashMap<(String,String), i64>>>,\n",
    "    budgets: Arc<RwLock<HashMap<(String,String), f64>>>,\n",
    "    weights: Arc<RwLock<HashMap<String, f64>>>, // CE probability by route\n",
    "}\n",
    "\n",
    "#[derive(Debug, Serialize, Deserialize, Clone)]\n",
    "struct AnswerReq {\n",
    "    query: String,\n",
    "    #[serde(default = \"default_topk\")]\n",
    "    top_k: i64,\n",
    "    #[serde(default)]\n",
    "    use_rerank: Option<bool>,\n",
    "}\n",
    "fn default_topk() -> i64 { 5 }\n",
    "\n",
    "#[derive(Debug, Serialize, Deserialize)]\n",
    "struct StepResult<T: Serialize + Clone> {\n",
    "    ok: bool,\n",
    "    latency_ms: Option<u64>,\n",
    "    payload: Option<T>,\n",
    "    error: Option<String>,\n",
    "}\n",
    "\n",
    "#[derive(Debug, Serialize, Deserialize, Clone)]\n",
    "struct RerankResp { contexts: Vec<serde_json::Value>, latency_ms: Option<u64> }\n",
    "#[derive(Debug, Serialize, Deserialize, Clone)]\n",
    "struct RetrieveResp { results: Vec<serde_json::Value>, latency_ms: Option<u64> }\n",
    "#[derive(Debug, Serialize, Deserialize, Clone)]\n",
    "struct CompleteResp { answer: String, verify: serde_json::Value, latency_ms: Option<u64> }\n",
    "\n",
    "static DISALLOWED_HEADERS: Lazy<Vec<&'static str>> = Lazy::new(|| vec![\"authorization\"]);\n",
    "static HEADER_ALLOWLIST: Lazy<Vec<&'static str>> = Lazy::new(|| vec![\n",
    "    \"x-request-id\",\"x-namespace\",\"x-route\",\"x-verify-v2\",\"x-chaos-disable-ce\",\"x-chaos-force-bm25\"\n",
    "]);\n",
    "\n",
    "// Prometheus recorder handle\n",
    "static PROM_HANDLE: Lazy<RwLock<Option<PrometheusHandle>>> = Lazy::new(|| RwLock::new(None));\n",
    "\n",
    "// ---- Response Cache (TTL) ----\n",
    "#[derive(Clone)]\n",
    "struct RespCache {\n",
    "    ttl_s: u64,\n",
    "    max_items: usize,\n",
    "    // key -> (insert_epoch_ms, payload)\n",
    "    map: HashMap<String, (u128, serde_json::Value)>\n",
    "}\n",
    "impl RespCache {\n",
    "    fn new(ttl_s: u64, max_items: usize) -> Self {\n",
    "        Self { ttl_s, max_items, map: HashMap::new() }\n",
    "    }\n",
    "    fn make_key(req: &AnswerReq) -> String {\n",
    "        serde_json::json!({\"query\": &req.query, \"top_k\": req.top_k, \"use_rerank\": req.use_rerank}).to_string()\n",
    "    }\n",
    "    fn now_ms() -> u128 {\n",
    "        std::time::SystemTime::now()\n",
    "            .duration_since(std::time::UNIX_EPOCH).unwrap()\n",
    "            .as_millis()\n",
    "    }\n",
    "    fn get(&mut self, key: &str) -> Option<serde_json::Value> {\n",
    "        let now_ms = Self::now_ms();\n",
    "        if let Some((ts, payload)) = self.map.get(key) {\n",
    "            if now_ms.saturating_sub(*ts) <= (self.ttl_s as u128)*1000 {\n",
    "                return Some(payload.clone());\n",
    "            } else {\n",
    "                self.map.remove(key);\n",
    "            }\n",
    "        }\n",
    "        None\n",
    "    }\n",
    "    fn put(&mut self, key: String, payload: serde_json::Value) {\n",
    "        if self.map.len() >= self.max_items {\n",
    "            if let Some(k) = self.map.keys().next().cloned() {\n",
    "                self.map.remove(&k);\n",
    "            }\n",
    "        }\n",
    "        self.map.insert(key, (Self::now_ms(), payload));\n",
    "    }\n",
    "}\n",
    "\n",
    "// ---- Circuit Breaker ----\n",
    "#[derive(Clone)]\n",
    "struct CircuitBreaker {\n",
    "    window_s: u64,\n",
    "    threshold: f64,\n",
    "    min_events: usize,\n",
    "    open_seconds: u64,\n",
    "    events: VecDeque<(u128, bool)>,\n",
    "    open_until_ms: u128,\n",
    "}\n",
    "impl CircuitBreaker {\n",
    "    fn new() -> Self {\n",
    "        Self {\n",
    "            window_s: 60, threshold: 0.25, min_events: 20, open_seconds: 30,\n",
    "            events: VecDeque::new(), open_until_ms: 0\n",
    "        }\n",
    "    }\n",
    "    fn now_ms() -> u128 {\n",
    "        std::time::SystemTime::now().duration_since(std::time::UNIX_EPOCH).unwrap().as_millis()\n",
    "    }\n",
    "    fn record(&mut self, ok: bool) {\n",
    "        let now = Self::now_ms();\n",
    "        self.gc(now);\n",
    "        self.events.push_back((now, ok));\n",
    "        if now < self.open_until_ms { return; }\n",
    "        if self.events.len() >= self.min_events {\n",
    "            let errs = self.events.iter().filter(|(_, okv)| !*okv).count();\n",
    "            let rate = errs as f64 / (self.events.len() as f64);\n",
    "            if rate >= self.threshold {\n",
    "                self.open_until_ms = now + (self.open_seconds as u128)*1000;\n",
    "                counter!(\"breaker_opens_total\").increment(1);\n",
    "            }\n",
    "        }\n",
    "    }\n",
    "    fn open(&self) -> bool { Self::now_ms() < self.open_until_ms }\n",
    "    fn remaining(&self) -> u64 {\n",
    "        if self.open() { ((self.open_until_ms - Self::now_ms())/1000) as u64 } else { 0 }\n",
    "    }\n",
    "    fn gc(&mut self, now: u128) {\n",
    "        let window_ms = (self.window_s as u128)*1000;\n",
    "        while let Some((ts,_)) = self.events.front().cloned() {\n",
    "            if now.saturating_sub(ts) > window_ms { self.events.pop_front(); } else { break; }\n",
    "        }\n",
    "    }\n",
    "}\n",
    "\n",
    "// ---- Metrics helpers ----\n",
    "fn observe_gateway_latency_ms(v: f64) { histogram!(\"gateway_answer_latency_ms\", v); }\n",
    "fn per_step_hist(name: &str, v: f64) {\n",
    "    match name {\n",
    "        \"bm25\" => histogram!(\"rag_bm25_ms\", v),\n",
    "        \"dense\" => histogram!(\"rag_dense_ms\", v),\n",
    "        \"rerank\" => histogram!(\"rag_rerank_ms\", v),\n",
    "        \"complete\" => histogram!(\"rag_complete_ms\", v),\n",
    "        _ => {}\n",
    "    }\n",
    "}\n",
    "fn fanout_error(step: &str) { counter!(\"gateway_fanout_errors_total\", \"step\" => step.to_string()).increment(1); }\n",
    "fn bind_attempt() { counter!(\"rust_gateway_bind_attempts_total\").increment(1); }\n",
    "\n",
    "// ---- HTTP helpers ----\n",
    "fn get_or_gen_request_id(headers: &HeaderMap) -> String {\n",
    "    if let Some(v) = headers.get(\"x-request-id\") {\n",
    "        if let Ok(s) = v.to_str() { if !s.is_empty() { return s.to_string(); } }\n",
    "    }\n",
    "    Uuid::new_v4().to_string()\n",
    "}\n",
    "fn build_fanout_headers(req_headers: &HeaderMap, req_id: &str) -> HeaderMap {\n",
    "    let mut out = HeaderMap::new();\n",
    "    for k in HEADER_ALLOWLIST.iter() {\n",
    "        if let Some(v) = req_headers.get(*k) {\n",
    "            out.insert(*k, v.clone());\n",
    "        }\n",
    "    }\n",
    "    out.insert(\"x-request-id\", HeaderValue::from_str(req_id).unwrap_or(HeaderValue::from_static(\"bad-uuid\")));\n",
    "    out\n",
    "}\n",
    "\n",
    "// ---- Fan-out to RAG ----\n",
    "async fn post_json(\n",
    "    client: &reqwest::Client,\n",
    "    base: &str,\n",
    "    path: &str,\n",
    "    headers: HeaderMap,\n",
    "    body: serde_json::Value,\n",
    "    step: &str,\n",
    "    step_timeout: Duration,\n",
    ") -> StepResult<serde_json::Value> {\n",
    "    let url = format!(\"{}{}\", base, path);\n",
    "    let t0 = Instant::now();\n",
    "    let res = timeout(step_timeout, async {\n",
    "        let r = client.post(&url).headers(headers).json(&body).send().await;\n",
    "        match r {\n",
    "            Ok(resp) => {\n",
    "                if !resp.status().is_success() {\n",
    "                    Err(anyhow!(\"status {}\", resp.status()))\n",
    "                } else {\n",
    "                    resp.json::<serde_json::Value>().await.map_err(|e| e.into())\n",
    "                }\n",
    "            }\n",
    "            Err(e) => Err(e.into())\n",
    "        }\n",
    "    }).await;\n",
    "\n",
    "    match res {\n",
    "        Ok(Ok(json)) => {\n",
    "            let ms = t0.elapsed().as_millis() as u64;\n",
    "            per_step_hist(step, ms as f64);\n",
    "            StepResult{ ok: true, latency_ms: Some(ms), payload: Some(json), error: None }\n",
    "        }\n",
    "        Ok(Err(e)) => {\n",
    "            fanout_error(step);\n",
    "            StepResult{ ok: false, latency_ms: Some(t0.elapsed().as_millis() as u64), payload: None, error: Some(e.to_string()) }\n",
    "        }\n",
    "        Err(_) => {\n",
    "            fanout_error(step);\n",
    "            StepResult{ ok: false, latency_ms: Some(t0.elapsed().as_millis() as u64), payload: None, error: Some(\"timeout\".into()) }\n",
    "        }\n",
    "    }\n",
    "}\n",
    "\n",
    "#[derive(Serialize, Deserialize)]\n",
    "struct AnswerMeta {\n",
    "    pipeline: String,\n",
    "    latency_ms: u64,\n",
    "    cache: &'static str,\n",
    "    partial_retrieval: serde_json::Value,\n",
    "    route: String,\n",
    "    ns: String,\n",
    "    step_ms: Option<HashMap<String, u64>>,\n",
    "}\n",
    "\n",
    "#[derive(Serialize, Deserialize)]\n",
    "struct AnswerResp {\n",
    "    answer: String,\n",
    "    contexts: Vec<serde_json::Value>,\n",
    "    verify: serde_json::Value,\n",
    "    meta: AnswerMeta,\n",
    "}\n",
    "\n",
    "// ---- Handlers ----\n",
    "async fn up_handler() -> impl IntoResponse {\n",
    "    Json(serde_json::json!({\"ok\": true, \"service\": \"gateway\"}))\n",
    "}\n",
    "\n",
    "async fn metrics_handler() -> impl IntoResponse {\n",
    "    let body = PROM_HANDLE.read().as_ref().unwrap().render();\n",
    "    Response::builder()\n",
    "        .status(StatusCode::OK)\n",
    "        .header(header::CONTENT_TYPE, \"text/plain; version=0.0.4\")\n",
    "        .body(body.into())\n",
    "        .unwrap()\n",
    "}\n",
    "\n",
    "async fn admin_ping() -> impl IntoResponse {\n",
    "    Json(serde_json::json!({\"ok\": true, \"pong\": true, \"service\": \"gateway\"}))\n",
    "}\n",
    "\n",
    "// In-memory mirrors for budgets/QPM\n",
    "#[derive(Deserialize)]\n",
    "struct QpmSet { namespace: Option<String>, route: Option<String>, qpm: i64 }\n",
    "#[derive(Deserialize)]\n",
    "struct CostSet { namespace: Option<String>, route: Option<String>, daily_usd: f64 }\n",
    "\n",
    "async fn qpm_set(State(st): State<AppState>, Json(req): Json<QpmSet>) -> impl IntoResponse {\n",
    "    let ns = req.namespace.unwrap_or_else(|| \"default\".into()).to_lowercase();\n",
    "    let route = req.route.unwrap_or_else(|| \"answer\".into()).to_lowercase();\n",
    "    st.qpm.write().insert((ns.clone(), route.clone()), req.qpm.max(0));\n",
    "    Json(serde_json::json!({\"ok\": true, \"namespace\": ns, \"route\": route, \"qpm\": req.qpm}))\n",
    "}\n",
    "async fn qpm_summary(State(st): State<AppState>) -> impl IntoResponse {\n",
    "    let map = st.qpm.read();\n",
    "    let mut items = Vec::new();\n",
    "    for ((ns, route), q) in map.iter() {\n",
    "        items.push(serde_json::json!({\"namespace\": ns, \"route\": route, \"qpm\": q}));\n",
    "    }\n",
    "    Json(serde_json::json!({\"ok\": true, \"limits\": items}))\n",
    "}\n",
    "async fn cost_set(State(st): State<AppState>, Json(req): Json<CostSet>) -> impl IntoResponse {\n",
    "    let ns = req.namespace.unwrap_or_else(|| \"default\".into()).to_lowercase();\n",
    "    let route = req.route.unwrap_or_else(|| \"answer\".into()).to_lowercase();\n",
    "    st.budgets.write().insert((ns.clone(), route.clone()), req.daily_usd.max(0.0));\n",
    "    Json(serde_json::json!({\"ok\": true, \"budget\": {\"namespace\": ns, \"route\": route, \"daily_usd\": req.daily_usd}}))\n",
    "}\n",
    "async fn cost_summary(State(st): State<AppState>) -> impl IntoResponse {\n",
    "    let map = st.budgets.read();\n",
    "    let mut items = Vec::new();\n",
    "    for ((ns, route), usd) in map.iter() {\n",
    "        items.push(serde_json::json!({\"namespace\": ns, \"route\": route, \"daily_usd\": usd}));\n",
    "    }\n",
    "    Json(serde_json::json!({\"ok\": true, \"budgets\": items}))\n",
    "}\n",
    "\n",
    "// RL / Canary weights\n",
    "#[derive(Deserialize)]\n",
    "struct RLUpdate { weights: HashMap<String, f64> }\n",
    "async fn rl_update(State(st): State<AppState>, Json(req): Json<RLUpdate>) -> impl IntoResponse {\n",
    "    let mut w = st.weights.write();\n",
    "    for (k, v) in req.weights.into_iter() {\n",
    "        let val = v.clamp(0.0, 1.0);\n",
    "        w.insert(k.to_lowercase(), val);\n",
    "    }\n",
    "    Json(serde_json::json!({\"ok\": true, \"weights\": &*w}))\n",
    "}\n",
    "\n",
    "#[derive(Serialize)]\n",
    "struct WindowStats { count: usize, p95_ms: Option<f64> }\n",
    "static REQ_LAT_MS: Lazy<RwLock<VecDeque<f64>>> = Lazy::new(|| RwLock::new(VecDeque::with_capacity(5000)));\n",
    "fn push_latency(ms: f64) {\n",
    "    let mut q = REQ_LAT_MS.write();\n",
    "    if q.len() >= 5000 { q.pop_front(); }\n",
    "    q.push_back(ms);\n",
    "}\n",
    "fn p95(v: &[f64]) -> Option<f64> {\n",
    "    if v.is_empty() { return None; }\n",
    "    let mut s = v.to_vec();\n",
    "    s.sort_by(|a,b| a.partial_cmp(b).unwrap());\n",
    "    let i = (s.len() - 1) as f64 * 0.95;\n",
    "    let lo = i.floor() as usize;\n",
    "    let hi = i.ceil() as usize;\n",
    "    if lo == hi { Some(s[lo]) } else { Some(s[lo] + (s[hi]-s[lo])*(i - lo as f64)) }\n",
    "}\n",
    "async fn canary_status() -> impl IntoResponse {\n",
    "    let q = REQ_LAT_MS.read();\n",
    "    let vec: Vec<f64> = q.iter().copied().collect();\n",
    "    let stats = WindowStats { count: vec.len(), p95_ms: p95(&vec) };\n",
    "    Json(serde_json::json!({\"ok\": true, \"window\": stats}))\n",
    "}\n",
    "\n",
    "// Router detection (simple; mirrors Py heuristics)\n",
    "fn route_for(query: &str, override_hdr: Option<&str>) -> String {\n",
    "    if let Some(ov) = override_hdr { if !ov.trim().is_empty() { return ov.trim().to_lowercase(); } }\n",
    "    let q = query.trim().to_lowercase();\n",
    "    let sql_re = regex_lite::Regex::new(r\"(?is)^\\s*(select|with)\\b\").unwrap();\n",
    "    let math_re = regex_lite::Regex::new(r\"\\b(integrate|derive|differentiate|solve|sum|product|limit|matrix|determinant|eigen)\\b\").unwrap();\n",
    "    let policy_re = regex_lite::Regex::new(r\"\\b(policy|regulation|compliance|gdpr|hipaa|soc2|security)\\b\").unwrap();\n",
    "    if sql_re.is_match(&q) { return \"sql\".into(); }\n",
    "    if math_re.is_match(&q) { return \"math\".into(); }\n",
    "    if policy_re.is_match(&q) { return \"policy\".into(); }\n",
    "    \"factual\".into()\n",
    "}\n",
    "\n",
    "#[axum::debug_handler]\n",
    "async fn answer_handler(\n",
    "    State(st): State<AppState>,\n",
    "    headers: HeaderMap,\n",
    "    Json(body): Json<AnswerReq>,\n",
    ") -> impl IntoResponse {\n",
    "    // Breaker quick path\n",
    "    {\n",
    "        let br = st.breaker.read();\n",
    "        if br.open() {\n",
    "            let rem = br.remaining();\n",
    "            return (\n",
    "                StatusCode::SERVICE_UNAVAILABLE,\n",
    "                Json(serde_json::json!({\"error\":\"breaker_open\",\"retry_after\": rem}))\n",
    "            );\n",
    "        }\n",
    "    }\n",
    "\n",
    "    let t0 = Instant::now();\n",
    "    let ns = headers.get(\"x-namespace\").and_then(|v| v.to_str().ok()).unwrap_or(\"default\").to_lowercase();\n",
    "    let route_override = headers.get(\"x-route\").and_then(|v| v.to_str().ok());\n",
    "    let route_name = route_for(&body.query, route_override);\n",
    "    let req_id = get_or_gen_request_id(&headers);\n",
    "\n",
    "    // Cache\n",
    "    let key = RespCache::make_key(&body);\n",
    "    if let Some(mut cached) = st.cache.write().get(&key) {\n",
    "        if let Some(m) = cached.get_mut(\"meta\") {\n",
    "            if let Some(obj) = m.as_object_mut() {\n",
    "                obj.insert(\"cache\".into(), serde_json::json!(\"hit\"));\n",
    "                obj.insert(\"ns\".into(), serde_json::json!(ns.clone()));\n",
    "                obj.insert(\"route\".into(), serde_json::json!(route_name.clone()));\n",
    "            }\n",
    "        }\n",
    "        return (StatusCode::OK, Json(cached));\n",
    "    }\n",
    "\n",
    "    // HTTP client & fanout headers\n",
    "    let client = reqwest::Client::builder().pool_idle_timeout(Duration::from_secs(10)).build().unwrap();\n",
    "    let fanout_headers = build_fanout_headers(&headers, &req_id);\n",
    "    let top_k = body.top_k.clamp(1, 50) as i64;\n",
    "\n",
    "    // Parallel retrieve\n",
    "    let bm25_f = post_json(&client, &st.rag_base, \"/retrieve_bm25\", fanout_headers.clone(),\n",
    "                           serde_json::json!({\"query\": body.query, \"top_k\": top_k}), \"bm25\",\n",
    "                           Duration::from_millis(step_timeout_ms(\"bm25\")));\n",
    "    let dense_f = post_json(&client, &st.rag_base, \"/retrieve_dense\", fanout_headers.clone(),\n",
    "                           serde_json::json!({\"query\": body.query, \"top_k\": top_k}), \"dense\",\n",
    "                           Duration::from_millis(step_timeout_ms(\"dense\")));\n",
    "    let (bm25_res, dense_res) = tokio::join!(bm25_f, dense_f);\n",
    "\n",
    "    let mut contexts: Vec<serde_json::Value> = Vec::new();\n",
    "    let mut partial = serde_json::json!({\"dense_ok\": false, \"bm25_ok\": false});\n",
    "    if bm25_res.ok {\n",
    "        if let Some(p) = bm25_res.payload { if let Some(arr) = p.get(\"results\").and_then(|x| x.as_array()) {\n",
    "            contexts.extend(arr.clone());\n",
    "            partial[\"bm25_ok\"] = serde_json::json!(true);\n",
    "        }}\n",
    "    }\n",
    "    if dense_res.ok {\n",
    "        if let Some(p) = dense_res.payload { if let Some(arr) = p.get(\"results\").and_then(|x| x.as_array()) {\n",
    "            contexts.extend(arr.clone());\n",
    "            partial[\"dense_ok\"] = serde_json::json!(true);\n",
    "        }}\n",
    "    }\n",
    "    if contexts.is_empty() {\n",
    "        st.breaker.write().record(false);\n",
    "        return (StatusCode::BAD_GATEWAY, Json(serde_json::json!({\"error\":\"preflight_failed: both retrievals failed\"})));\n",
    "    }\n",
    "\n",
    "    // Decide CE usage: explicit > chaos header > weighted probability\n",
    "    let mut use_rerank = body.use_rerank.unwrap_or(true);\n",
    "    if headers.get(\"x-chaos-disable-ce\").is_some() { use_rerank = false; }\n",
    "    if body.use_rerank.is_none() {\n",
    "        // weighted probability by route\n",
    "        let ce_prob = st.weights.read().get(&route_name).copied().unwrap_or(1.0).clamp(0.0, 1.0);\n",
    "        let roll: f64 = rand::thread_rng().gen();\n",
    "        use_rerank = roll < ce_prob;\n",
    "    }\n",
    "\n",
    "    // Optional rerank\n",
    "    if use_rerank {\n",
    "        let rr = post_json(&client, &st.rag_base, \"/rerank\", fanout_headers.clone(),\n",
    "                           serde_json::json!({\"query\": body.query, \"contexts\": contexts}),\n",
    "                           \"rerank\", Duration::from_millis(step_timeout_ms(\"rerank\"))).await;\n",
    "        if rr.ok {\n",
    "            if let Some(p) = rr.payload {\n",
    "                if let Some(arr) = p.get(\"contexts\").and_then(|x| x.as_array()) {\n",
    "                    contexts = arr.clone();\n",
    "                }\n",
    "            }\n",
    "        }\n",
    "    }\n",
    "\n",
    "    // Complete\n",
    "    let comp = post_json(&client, &st.rag_base, \"/complete\", fanout_headers.clone(),\n",
    "                         serde_json::json!({\"query\": body.query, \"contexts\": contexts}),\n",
    "                         \"complete\", Duration::from_millis(step_timeout_ms(\"complete\"))).await;\n",
    "    if !comp.ok {\n",
    "        st.breaker.write().record(false);\n",
    "        return (StatusCode::BAD_GATEWAY, Json(serde_json::json!({\"error\": format!(\"complete_failed: {:?}\", comp.error)})));\n",
    "    }\n",
    "\n",
    "    let verify = comp.payload.as_ref().and_then(|p| p.get(\"verify\")).cloned().unwrap_or(serde_json::json!({}));\n",
    "    let answer = comp.payload.as_ref().and_then(|p| p.get(\"answer\")).and_then(|v| v.as_str()).unwrap_or(\"\").to_string();\n",
    "\n",
    "    // step timings\n",
    "    let mut step_ms: HashMap<String,u64> = HashMap::new();\n",
    "    if let Some(ms) = bm25_res.latency_ms { step_ms.insert(\"bm25_ms\".into(), ms); }\n",
    "    if let Some(ms) = dense_res.latency_ms { step_ms.insert(\"dense_ms\".into(), ms); }\n",
    "    if let Some(ms) = comp.latency_ms { step_ms.insert(\"complete_ms\".into(), ms); }\n",
    "\n",
    "    let latency_ms = t0.elapsed().as_millis() as u64;\n",
    "    observe_gateway_latency_ms(latency_ms as f64);\n",
    "    push_latency(latency_ms as f64);\n",
    "\n",
    "    let payload = serde_json::json!({\n",
    "        \"answer\": answer,\n",
    "        \"contexts\": contexts.into_iter().take(top_k as usize).collect::<Vec<_>>(),\n",
    "        \"verify\": verify,\n",
    "        \"meta\": {\n",
    "            \"pipeline\": if use_rerank { \"rerank\" } else { \"rrf\" },\n",
    "            \"latency_ms\": latency_ms,\n",
    "            \"cache\": \"miss\",\n",
    "            \"partial_retrieval\": partial,\n",
    "            \"route\": route_name,\n",
    "            \"ns\": ns,\n",
    "            \"step_ms\": step_ms,\n",
    "        }\n",
    "    });\n",
    "\n",
    "    st.breaker.write().record(true);\n",
    "    st.cache.write().put(key, payload.clone());\n",
    "    (StatusCode::OK, Json(payload))\n",
    "}\n",
    "\n",
    "fn step_timeout_ms(step: &str) -> u64 {\n",
    "    let bm25 = std::env::var(\"STEP_TIMEOUT_BM25\").ok().and_then(|s| s.parse::<f64>().ok()).unwrap_or(0.8);\n",
    "    let dense = std::env::var(\"STEP_TIMEOUT_DENSE\").ok().and_then(|s| s.parse::<f64>().ok()).unwrap_or(1.2);\n",
    "    let rerank = std::env::var(\"STEP_TIMEOUT_RERANK\").ok().and_then(|s| s.parse::<f64>().ok()).unwrap_or(0.6);\n",
    "    let complete = std::env::var(\"STEP_TIMEOUT_COMPLETE\").ok().and_then(|s| s.parse::<f64>().ok()).unwrap_or(2.0);\n",
    "    let sec = match step { \"bm25\" => bm25, \"dense\" => dense, \"rerank\" => rerank, \"complete\" => complete, _ => 1.0 };\n",
    "    (sec * 1000.0) as u64\n",
    "}\n",
    "\n",
    "fn write_port_txt(port: u16) {\n",
    "    let art = std::env::var(\"ART_DIR\").unwrap_or_else(|_| \"/tmp/art\".into());\n",
    "    let dir = std::path::Path::new(&art);\n",
    "    let _ = std::fs::create_dir_all(dir);\n",
    "    let path = dir.join(\"PORT.txt\");\n",
    "    if let Ok(mut f) = std::fs::File::create(path) {\n",
    "        let _ = writeln!(f, \"{}\", port);\n",
    "    }\n",
    "}\n",
    "\n",
    "#[tokio::main]\n",
    "async fn main() {\n",
    "    tracing_subscriber::fmt()\n",
    "        .with_env_filter(EnvFilter::from_default_env().add_directive(\"info\".parse().unwrap()))\n",
    "        .init();\n",
    "\n",
    "    // Prometheus recorder\n",
    "    let recorder = PrometheusBuilder::new().install_recorder().expect(\"prom recorder\");\n",
    "    *PROM_HANDLE.write() = Some(recorder.handle());\n",
    "\n",
    "    // Config\n",
    "    let rag_base = std::env::var(\"RAG_BASE\").unwrap_or_else(|_| \"http://127.0.0.1:9909\".into());\n",
    "    let mut port: u16 = std::env::var(\"GATEWAY_PORT\").ok().and_then(|s| s.parse::<u16>().ok()).unwrap_or(9910);\n",
    "    let host: &str = \"0.0.0.0\";\n",
    "\n",
    "    // Bind attempts with auto-increment\n",
    "    let listener = loop {\n",
    "        bind_attempt();\n",
    "        match tokio::net::TcpListener::bind((host, port)).await {\n",
    "            Ok(l) => break l,\n",
    "            Err(_) => { port += 1; if port > 9999 { panic!(\"No available ports 9910..9999\"); } }\n",
    "        }\n",
    "    };\n",
    "    write_port_txt(port);\n",
    "    info!(\"gateway listening on {host}:{port}, rag_base={rag_base}\");\n",
    "\n",
    "    let state = AppState {\n",
    "        rag_base,\n",
    "        cache: Arc::new(RwLock::new(RespCache::new(60, 512))),\n",
    "        breaker: Arc::new(RwLock::new(CircuitBreaker::new())),\n",
    "        qpm: Arc::new(RwLock::new(HashMap::new())),\n",
    "        budgets: Arc::new(RwLock::new(HashMap::new())),\n",
    "        weights: Arc::new(RwLock::new(HashMap::from([\n",
    "            (\"sql\".into(), 1.0), (\"math\".into(), 1.0), (\"policy\".into(), 1.0), (\"factual\".into(), 1.0)\n",
    "        ]))),\n",
    "    };\n",
    "\n",
    "    let app = Router::new()\n",
    "        .route(\"/up\", get(up_handler))\n",
    "        .route(\"/metrics\", get(metrics_handler))\n",
    "        .route(\"/admin/ping\", get(admin_ping))\n",
    "        .route(\"/admin/limits/qpm/set\", post(qpm_set))\n",
    "        .route(\"/admin/limits/qpm/summary\", get(qpm_summary))\n",
    "        .route(\"/admin/costs/set\", post(cost_set))\n",
    "        .route(\"/admin/costs/summary\", get(cost_summary))\n",
    "        .route(\"/rl/update\", post(rl_update))\n",
    "        .route(\"/canary/status\", get(canary_status))\n",
    "        .route(\"/answer\", post(answer_handler))\n",
    "        .with_state(state);\n",
    "\n",
    "    axum::serve(listener, app).await.unwrap();\n",
    "}\n",
    "\n",
    "// Minimal shim; swap to truly-light crate if desired\n",
    "mod regex_lite {\n",
    "    pub use regex::Regex;\n",
    "}\n",
    "'''\n",
    "(GATEWAY_DIR / \"Cargo.toml\").write_text(cargo_toml_fixed.strip() + \"\\n\", encoding=\"utf-8\")\n",
    "(SRC_DIR / \"main.rs\").write_text(main_rs_fixed.strip() + \"\\n\", encoding=\"utf-8\")\n",
    "print(\"Wrote:\")\n",
    "print(\" -\", (GATEWAY_DIR / \"Cargo.toml\").resolve())\n",
    "print(\" -\", (SRC_DIR / \"main.rs\").resolve())\n",
    "print(\"\\nNext steps (local dev):\")\n",
    "print(\"  cd gateway && cargo build --release\")\n",
    "print(\"  RAG_BASE=http://127.0.0.1:9909 GATEWAY_PORT=9910 ./target/release/gateway\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 517,
   "id": "6e9343e1-ecd0-457b-a92e-d3e55cecf2fc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extended gateway written to: C:\\Users\\aniru\\gateway\\src\\main.rs\n",
      "\n",
      "Rebuild:\n",
      "  cd gateway && cargo build --release\n",
      "Run:\n",
      "  RAG_BASE=http://127.0.0.1:9909 GATEWAY_PORT=9910 ./target/release/gateway\n",
      "\n",
      "Smoke:\n",
      "  curl -s localhost:9910/up | jq\n",
      "Chaos:\n",
      "  curl -s localhost:9910/admin/chaos/get | jq\n",
      "  curl -s -X POST localhost:9910/admin/chaos/set -H 'content-type: application/json' -d '{\"disable_ce\":true}' | jq\n",
      "Weights snapshot:\n",
      "  curl -s -X POST localhost:9910/rl/promote -H 'content-type: application/json' -d '{}' | jq\n",
      "Rollback:\n",
      "  curl -s -X POST localhost:9910/rl/rollback -H 'content-type: application/json' -d '{}' | jq\n",
      "Counts:\n",
      "  curl -s localhost:9910/canary/strategy_counts | jq\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "GATEWAY_DIR = Path(\"gateway\")\n",
    "SRC = GATEWAY_DIR / \"src\" / \"main.rs\"\n",
    "rs = Path(SRC).read_text(encoding=\"utf-8\")\n",
    "inject_helpers = r'''\n",
    "// ==================== ADDED: persistence, chaos, strategy counts ====================\n",
    "fn art_dir() -> std::path::PathBuf {\n",
    "    std::env::var(\"ART_DIR\").map(std::path::PathBuf::from).unwrap_or_else(|_| \"/tmp/art\".into())\n",
    "}\n",
    "fn weights_file() -> std::path::PathBuf { art_dir().join(\"weights.json\") }\n",
    "fn weights_history_dir() -> std::path::PathBuf { art_dir().join(\"weights_versions\") }\n",
    "fn weights_history_file() -> std::path::PathBuf { weights_history_dir().join(\"history.jsonl\") }\n",
    "\n",
    "fn write_json(path: &std::path::Path, v: &serde_json::Value) {\n",
    "    if let Some(p) = path.parent() { let _ = std::fs::create_dir_all(p); }\n",
    "    if let Ok(mut f) = std::fs::File::create(path) {\n",
    "        let _ = write!(f, \"{}\\n\", v.to_string());\n",
    "    }\n",
    "}\n",
    "fn read_json(path: &std::path::Path) -> serde_json::Value {\n",
    "    std::fs::read_to_string(path).ok()\n",
    "        .and_then(|s| serde_json::from_str::<serde_json::Value>(&s).ok())\n",
    "        .unwrap_or_else(|| serde_json::json!({}))\n",
    "}\n",
    "\n",
    "// chaos toggles\n",
    "#[derive(Debug, Clone, Copy, Serialize, Deserialize)]\n",
    "struct ChaosState { disable_ce: bool, force_bm25: bool, verifier_v2: bool }\n",
    "static CHAOS: Lazy<RwLock<ChaosState>> = Lazy::new(|| RwLock::new(ChaosState{\n",
    "    disable_ce:false, force_bm25:false, verifier_v2:false\n",
    "}));\n",
    "\n",
    "// per-strategy counters\n",
    "static STRAT_COUNTS: Lazy<RwLock<HashMap<&'static str, u64>>> = Lazy::new(|| {\n",
    "    let mut m = HashMap::new();\n",
    "    m.insert(\"rerank\", 0);\n",
    "    m.insert(\"rrf\", 0);\n",
    "    RwLock::new(m)\n",
    "});\n",
    "fn strat_inc(name: &'static str) {\n",
    "    let mut m = STRAT_COUNTS.write();\n",
    "    *m.entry(name).or_insert(0) += 1;\n",
    "}\n",
    "'''\n",
    "if \"// ==================== ADDED: persistence, chaos, strategy counts ====================\" not in rs:\n",
    "    anchor = 'static PROM_HANDLE'\n",
    "    idx = rs.find(anchor)\n",
    "    if idx == -1:\n",
    "        anchor = 'static HEADER_ALLOWLIST'\n",
    "        idx = rs.find(anchor)\n",
    "    if idx != -1:\n",
    "        insert_at = rs.find(\";\", idx)\n",
    "        insert_at = rs.find(\"\\n\", insert_at) + 1\n",
    "        rs = rs[:insert_at] + inject_helpers + rs[insert_at:]\n",
    "add_routes = r'''\n",
    "// ---- Chaos admin ----\n",
    "#[derive(Deserialize)]\n",
    "struct ChaosSetReq { disable_ce: Option<bool>, force_bm25: Option<bool>, verifier_v2: Option<bool> }\n",
    "async fn chaos_get() -> impl IntoResponse {\n",
    "    let c = *CHAOS.read();\n",
    "    Json(serde_json::json!({\"ok\": true, \"chaos\": {\"disable_ce\":c.disable_ce, \"force_bm25\":c.force_bm25, \"verifier_v2\":c.verifier_v2}}))\n",
    "}\n",
    "async fn chaos_set(Json(req): Json<ChaosSetReq>) -> impl IntoResponse {\n",
    "    let mut c = CHAOS.write();\n",
    "    if let Some(v)=req.disable_ce { c.disable_ce = v; }\n",
    "    if let Some(v)=req.force_bm25 { c.force_bm25 = v; }\n",
    "    if let Some(v)=req.verifier_v2 { c.verifier_v2 = v; }\n",
    "    Json(serde_json::json!({\"ok\": true, \"chaos\": {\"disable_ce\":c.disable_ce, \"force_bm25\":c.force_bm25, \"verifier_v2\":c.verifier_v2}}))\n",
    "}\n",
    "\n",
    "// ---- Strategy counts ----\n",
    "async fn canary_strategy_counts() -> impl IntoResponse {\n",
    "    let m = STRAT_COUNTS.read();\n",
    "    Json(serde_json::json!({\"ok\": true, \"counts\": {\"rerank\": m.get(\"rerank\").copied().unwrap_or(0), \"rrf\": m.get(\"rrf\").copied().unwrap_or(0)}}))\n",
    "}\n",
    "\n",
    "// ---- Weights promote/rollback with versioning ----\n",
    "#[derive(Deserialize)]\n",
    "struct PromoteReq { tag: Option<String> }\n",
    "#[derive(Deserialize)]\n",
    "struct RollbackReq { tag: Option<String> }\n",
    "\n",
    "async fn rl_promote(State(st): State<AppState>, Json(req): Json<PromoteReq>) -> impl IntoResponse {\n",
    "    let tag = req.tag.unwrap_or_else(|| {\n",
    "        let ts = chrono::Utc::now().format(\"%Y%m%d-%H%M%S\").to_string();\n",
    "        ts\n",
    "    });\n",
    "    let w = st.weights.read().clone();\n",
    "    let snapshot = serde_json::json!({\"tag\": tag, \"ts\": chrono::Utc::now().timestamp(), \"weights\": w});\n",
    "    write_json(&weights_history_dir(), &serde_json::json!({})); // ensure dir\n",
    "    // append line\n",
    "    {\n",
    "        let hist_path = weights_history_file();\n",
    "        if let Some(parent) = hist_path.parent() { let _ = std::fs::create_dir_all(parent); }\n",
    "        if let Ok(mut f) = std::fs::OpenOptions::new().create(true).append(true).open(hist_path) {\n",
    "            let _ = writeln!(f, \"{}\", snapshot.to_string());\n",
    "        }\n",
    "    }\n",
    "    write_json(&weights_file(), &serde_json::json!(w));\n",
    "    Json(serde_json::json!({\"ok\": true, \"snapshot\": snapshot}))\n",
    "}\n",
    "\n",
    "async fn rl_rollback(State(st): State<AppState>, Json(req): Json<RollbackReq>) -> impl IntoResponse {\n",
    "    let hist_path = weights_history_file();\n",
    "    let mut chosen: Option<serde_json::Value> = None;\n",
    "    if let Ok(s) = std::fs::read_to_string(&hist_path) {\n",
    "        let lines: Vec<&str> = s.lines().collect();\n",
    "        if let Some(tag) = req.tag {\n",
    "            for ln in lines.iter().rev() {\n",
    "                if let Ok(v) = serde_json::from_str::<serde_json::Value>(ln) {\n",
    "                    if v.get(\"tag\").and_then(|t| t.as_str()) == Some(tag.as_str()) {\n",
    "                        chosen = Some(v); break;\n",
    "                    }\n",
    "                }\n",
    "            }\n",
    "        } else if lines.len() >= 2 {\n",
    "            // roll back to previous\n",
    "            if let Ok(v) = serde_json::from_str::<serde_json::Value>(lines[lines.len()-2]) {\n",
    "                chosen = Some(v);\n",
    "            }\n",
    "        } else if let Some(last) = lines.last() {\n",
    "            if let Ok(v) = serde_json::from_str::<serde_json::Value>(last) {\n",
    "                chosen = Some(v);\n",
    "            }\n",
    "        }\n",
    "    }\n",
    "    if let Some(v) = chosen {\n",
    "        if let Some(wobj) = v.get(\"weights\") {\n",
    "            if let Ok(map) = serde_json::from_value::<HashMap<String,f64>>(wobj.clone()) {\n",
    "                *st.weights.write() = map.clone();\n",
    "                write_json(&weights_file(), &serde_json::json!(map));\n",
    "                return Json(serde_json::json!({\"ok\": true, \"rolled_to\": v.get(\"tag\"), \"weights\": map}));\n",
    "            }\n",
    "        }\n",
    "    }\n",
    "    (StatusCode::BAD_REQUEST, Json(serde_json::json!({\"ok\": false, \"error\":\"no suitable snapshot\"})))\n",
    "}\n",
    "'''\n",
    "if \"// ---- Chaos admin ----\" not in rs:\n",
    "    insert_anchor = \"\\n#[tokio::main]\\nasync fn main()\"\n",
    "    rs = rs.replace(insert_anchor, add_routes + insert_anchor)\n",
    "route_wiring = r'''\n",
    "        .route(\"/admin/chaos/get\", get(chaos_get))\n",
    "        .route(\"/admin/chaos/set\", post(chaos_set))\n",
    "        .route(\"/canary/strategy_counts\", get(canary_strategy_counts))\n",
    "        .route(\"/rl/promote\", post(rl_promote))\n",
    "        .route(\"/rl/rollback\", post(rl_rollback))\n",
    "'''\n",
    "if '/admin/chaos/get' not in rs:\n",
    "    rs = rs.replace(\n",
    "        '.route(\"/answer\", post(answer_handler))',\n",
    "        '.route(\"/answer\", post(answer_handler))' + route_wiring\n",
    "    )\n",
    "boot_load = r'''\n",
    "    // Load persisted weights if present\n",
    "    let wf = weights_file();\n",
    "    if wf.exists() {\n",
    "        if let Ok(s) = std::fs::read_to_string(&wf) {\n",
    "            if let Ok(v) = serde_json::from_str::<HashMap<String,f64>>(&s) {\n",
    "                *state.weights.write() = v;\n",
    "            }\n",
    "        }\n",
    "    }\n",
    "'''\n",
    "if \"Load persisted weights if present\" not in rs:\n",
    "    rs = rs.replace(\n",
    "        'let state = AppState {',\n",
    "        'let state = AppState {'\n",
    "    )\n",
    "    rs = rs.replace(\n",
    "        '    let app = Router::new()',\n",
    "        boot_load + '\\n    let app = Router::new()'\n",
    "    )\n",
    "answer_patch_mark = \"async fn answer_handler(\"\n",
    "if answer_patch_mark in rs and \"CHAOS.read()\" not in rs:\n",
    "    rs = rs.replace(\n",
    "        '// Decide CE usage: explicit > chaos header > weighted probability',\n",
    "        '// Decide CE usage: explicit > chaos header > weighted probability'\n",
    "    )\n",
    "    rs = rs.replace(\n",
    "        '    let req_id = get_or_gen_request_id(&headers);',\n",
    "        '    let req_id = get_or_gen_request_id(&headers);\\n    let mut resp_headers = HeaderMap::new();\\n    let _ = resp_headers.insert(\"x-request-id\", HeaderValue::from_str(&req_id).unwrap());\\n    let chaos = *CHAOS.read();'\n",
    "    )\n",
    "    rs = rs.replace(\n",
    "        '    let mut use_rerank = body.use_rerank.unwrap_or(true);',\n",
    "        '    let mut use_rerank = body.use_rerank.unwrap_or(true);\\n    if chaos.disable_ce { use_rerank = false; }'\n",
    "    )\n",
    "    rs = rs.replace(\n",
    "        '    let (bm25_res, dense_res) = tokio::join!(bm25_f, dense_f);',\n",
    "        '    let (bm25_res, mut dense_res) = tokio::join!(bm25_f, dense_f);\\n    if chaos.force_bm25 { dense_res = StepResult{ ok:false, latency_ms:dense_res.latency_ms, payload:None, error:Some(\"chaos_force_bm25\".into()) }; }'\n",
    "    )\n",
    "    rs = rs.replace(\n",
    "        '(StatusCode::OK, Json(cached))',\n",
    "        '(StatusCode::OK, (resp_headers.clone(), Json(cached)))'\n",
    "    )\n",
    "    rs = rs.replace(\n",
    "        '(StatusCode::BAD_GATEWAY, Json(serde_json::json!({\"error\":\"preflight_failed: both retrievals failed\"})))',\n",
    "        '(StatusCode::BAD_GATEWAY, (resp_headers.clone(), Json(serde_json::json!({\"error\":\"preflight_failed: both retrievals failed\"}))))'\n",
    "    )\n",
    "    rs = rs.replace(\n",
    "        '(StatusCode::BAD_GATEWAY, Json(serde_json::json!({\"error\": format!(\"complete_failed: {:?}\", comp.error)})))',\n",
    "        '(StatusCode::BAD_GATEWAY, (resp_headers.clone(), Json(serde_json::json!({\"error\": format!(\"complete_failed: {:?}\", comp.error)}))))'\n",
    "    )\n",
    "    rs = rs.replace(\n",
    "        '(StatusCode::OK, Json(payload))',\n",
    "        'if use_rerank { strat_inc(\"rerank\"); } else { strat_inc(\"rrf\"); }\\n    (StatusCode::OK, (resp_headers, Json(payload)))'\n",
    "    )\n",
    "    rs = rs.replace(\n",
    "        'return (\\n                StatusCode::SERVICE_UNAVAILABLE,\\n                Json(serde_json::json!({\"error\":\"breaker_open\",\"retry_after\": rem}))\\n            );',\n",
    "        'return (\\n                StatusCode::SERVICE_UNAVAILABLE,\\n                (HeaderMap::new(), Json(serde_json::json!({\"error\":\"breaker_open\",\"retry_after\": rem})))\\n            );'\n",
    "    )\n",
    "cargo = (GATEWAY_DIR / \"Cargo.toml\").read_text(encoding=\"utf-8\")\n",
    "if 'chrono = ' not in cargo:\n",
    "    cargo = cargo.rstrip() + '\\nchrono = { version = \"0.4\", default-features = false, features = [\"clock\"] }\\n'\n",
    "    (GATEWAY_DIR / \"Cargo.toml\").write_text(cargo, encoding=\"utf-8\")\n",
    "Path(SRC).write_text(rs, encoding=\"utf-8\")\n",
    "print(\"Extended gateway written to:\", SRC.resolve())\n",
    "print(\"\\nRebuild:\\n  cd gateway && cargo build --release\")\n",
    "print(\"Run:\\n  RAG_BASE=http://127.0.0.1:9909 GATEWAY_PORT=9910 ./target/release/gateway\")\n",
    "print(\"\\nSmoke:\\n  curl -s localhost:9910/up | jq\")\n",
    "print(\"Chaos:\\n  curl -s localhost:9910/admin/chaos/get | jq\\n  curl -s -X POST localhost:9910/admin/chaos/set -H 'content-type: application/json' -d '{\\\"disable_ce\\\":true}' | jq\")\n",
    "print(\"Weights snapshot:\\n  curl -s -X POST localhost:9910/rl/promote -H 'content-type: application/json' -d '{}' | jq\")\n",
    "print(\"Rollback:\\n  curl -s -X POST localhost:9910/rl/rollback -H 'content-type: application/json' -d '{}' | jq\")\n",
    "print(\"Counts:\\n  curl -s localhost:9910/canary/strategy_counts | jq\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 518,
   "id": "069bd6ce-2c15-4f64-b321-c107edc6e332",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Patched gateway\\src\\main.rs | changes_applied=True\n",
      "\n",
      "Rebuild:\n",
      "  cd gateway && cargo build --release\n",
      "Run:\n",
      "  RAG_BASE=http://127.0.0.1:9909 GATEWAY_PORT=9910 ./target/release/gateway\n",
      "\n",
      "Quick checks:\n",
      "  curl -i -s -XPOST localhost:9910/answer -H 'content-type: application/json' -d '{\"query\":\"smoke\",\"top_k\":2}' | head\n",
      "  curl -s localhost:9910/admin/chaos/set -H 'content-type: application/json' -d '{\"verifier_v2\":true}' | jq\n",
      "  curl -s localhost:9910/rl/promote -H 'content-type: application/json' -d '{}' | jq\n",
      "  curl -s localhost:9910/canary/strategy_counts | jq\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "import re\n",
    "SRC = Path(\"gateway/src/main.rs\")\n",
    "rs = SRC.read_text(encoding=\"utf-8\")\n",
    "changes = 0\n",
    "rs_new = rs.replace(\n",
    "    'write_json(&weights_history_dir(), &serde_json::json!({})); // ensure dir',\n",
    "    'let _ = std::fs::create_dir_all(weights_history_dir());'\n",
    ")\n",
    "changes += (rs_new != rs)\n",
    "rs = rs_new\n",
    "rs_new = rs.replace('if wf.exists() {', 'if wf.as_path().exists() {')\n",
    "changes += (rs_new != rs)\n",
    "rs = rs_new\n",
    "start_pat = r'async fn answer_handler\\([\\s\\S]*?\\)\\s*->\\s*impl IntoResponse\\s*\\{\\n'\n",
    "m = re.search(start_pat, rs)\n",
    "if m:\n",
    "    insert_at = m.end()\n",
    "    prelude = (\n",
    "        '    // request-id early so even breaker-open responses carry it\\n'\n",
    "        '    let req_id = get_or_gen_request_id(&headers);\\n'\n",
    "        '    let mut resp_headers = HeaderMap::new();\\n'\n",
    "        '    let _ = resp_headers.insert(\"x-request-id\", HeaderValue::from_str(&req_id).unwrap());\\n'\n",
    "    )\n",
    "    rs = rs[:insert_at] + prelude + rs[insert_at:]\n",
    "    changes += 1\n",
    "rs = rs.replace(\n",
    "    'let req_id = get_or_gen_request_id(&headers);\\n'\n",
    "    '    let mut resp_headers = HeaderMap::new();\\n'\n",
    "    '    let _ = resp_headers.insert(\"x-request-id\", HeaderValue::from_str(&req_id).unwrap());\\n'\n",
    "    '    let chaos = *CHAOS.read();',\n",
    "    'let chaos = *CHAOS.read();'\n",
    ")\n",
    "rs = rs.replace(\n",
    "    'return (\\n                StatusCode::SERVICE_UNAVAILABLE,\\n                (HeaderMap::new(), Json(serde_json::json!({\"error\":\"breaker_open\",\"retry_after\": rem})))\\n            );',\n",
    "    'return (\\n                StatusCode::SERVICE_UNAVAILABLE,\\n                (resp_headers.clone(), Json(serde_json::json!({\"error\":\"breaker_open\",\"retry_after\": rem})))\\n            );'\n",
    ")\n",
    "rs = rs.replace(\n",
    "    'let fanout_headers = build_fanout_headers(&headers, &req_id);',\n",
    "    'let mut fanout_headers = build_fanout_headers(&headers, &req_id);'\n",
    ")\n",
    "if 'if chaos.verifier_v2 {' not in rs:\n",
    "    rs = rs.replace(\n",
    "        'let top_k = body.top_k.clamp(1, 50) as i64;',\n",
    "        'let top_k = body.top_k.clamp(1, 50) as i64;\\n'\n",
    "        '    if chaos.verifier_v2 {\\n'\n",
    "        '        fanout_headers.insert(\"x-verify-v2\", HeaderValue::from_static(\"1\"));\\n'\n",
    "        '    }'\n",
    "    )\n",
    "    changes += 1\n",
    "if 'gateway_strategy_selected_total' not in rs:\n",
    "    rs = rs.replace(\n",
    "        'fn strat_inc(name: \\'static str) {\\n'\n",
    "        '    let mut m = STRAT_COUNTS.write();\\n'\n",
    "        '    *m.entry(name).or_insert(0) += 1;\\n'\n",
    "        '}',\n",
    "        'fn strat_inc(name: \\'static str) {\\n'\n",
    "        '    let mut m = STRAT_COUNTS.write();\\n'\n",
    "        '    *m.entry(name).or_insert(0) += 1;\\n'\n",
    "        '    counter!(\"gateway_strategy_selected_total\", \"strategy\" => name.to_string()).increment(1);\\n'\n",
    "        '}'\n",
    "    )\n",
    "    changes += 1\n",
    "cargo = Path(\"gateway/Cargo.toml\")\n",
    "ct = cargo.read_text(encoding=\"utf-8\")\n",
    "if \"chrono =\" not in ct:\n",
    "    ct = ct.rstrip() + '\\nchrono = { version = \"0.4\", default-features = false, features = [\"clock\"] }\\n'\n",
    "    cargo.write_text(ct, encoding=\"utf-8\")\n",
    "SRC.write_text(rs, encoding=\"utf-8\")\n",
    "print(f\"Patched {SRC} | changes_applied={bool(changes)}\")\n",
    "print(\"\\nRebuild:\\n  cd gateway && cargo build --release\")\n",
    "print(\"Run:\\n  RAG_BASE=http://127.0.0.1:9909 GATEWAY_PORT=9910 ./target/release/gateway\")\n",
    "print(\"\\nQuick checks:\")\n",
    "print(\"  curl -i -s -XPOST localhost:9910/answer -H 'content-type: application/json' -d '{\\\"query\\\":\\\"smoke\\\",\\\"top_k\\\":2}' | head\")\n",
    "print(\"  curl -s localhost:9910/admin/chaos/set -H 'content-type: application/json' -d '{\\\"verifier_v2\\\":true}' | jq\")\n",
    "print(\"  curl -s localhost:9910/rl/promote -H 'content-type: application/json' -d '{}' | jq\")\n",
    "print(\"  curl -s localhost:9910/canary/strategy_counts | jq\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 519,
   "id": "b4e49913-9f32-4934-9fbb-891c374b8551",
   "metadata": {},
   "outputs": [],
   "source": [
    "import threading\n",
    "import time\n",
    "import socket\n",
    "import contextlib\n",
    "import os\n",
    "import uvicorn\n",
    "_GATEWAY_THREAD = None\n",
    "_GATEWAY_SERVER = None\n",
    "_GATEWAY_URL = None\n",
    "_GATEWAY_PORT = None\n",
    "_GATEWAY_HOST = \"127.0.0.1\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 520,
   "id": "0e4ffb78-1776-4980-9622-5b451772e5ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _port_in_use(host: str, port: int) -> bool:\n",
    "    with contextlib.closing(socket.socket(socket.AF_INET, socket.SOCK_STREAM)) as s:\n",
    "        s.settimeout(0.15)\n",
    "        return s.connect_ex((host, port)) == 0\n",
    "def _pick_free_port(start: int = 9910, end: int = 9999, host: str = \"127.0.0.1\") -> int:\n",
    "    for p in range(start, end + 1):\n",
    "        if not _port_in_use(host, p):\n",
    "            return p\n",
    "    raise RuntimeError(\"no free port in range\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 521,
   "id": "cc9b663d-f213-4185-8e69-b7733bb83387",
   "metadata": {},
   "outputs": [],
   "source": [
    "def start_in_notebook(\n",
    "    host: str = \"127.0.0.1\",\n",
    "    port: int = 9910,\n",
    "    mock_rag: bool = True,\n",
    "    auto_port: bool = True,\n",
    "    log_level: str = \"info\",\n",
    ") -> str:\n",
    "    \"\"\"\n",
    "    Start uvicorn in a background thread.\n",
    "    Returns the URL of the running gateway.\n",
    "    Safe to call multiple times; it will just return existing URL.\n",
    "    \"\"\"\n",
    "    global _GATEWAY_THREAD, _GATEWAY_SERVER, _GATEWAY_URL, _GATEWAY_PORT, _GATEWAY_HOST\n",
    "    if _GATEWAY_THREAD is not None and _GATEWAY_THREAD.is_alive():\n",
    "        return _GATEWAY_URL\n",
    "    _GATEWAY_HOST = host\n",
    "    if mock_rag:\n",
    "        base = start_mock_rag(host=host, start_port=9909)\n",
    "        rag.set_base(base)\n",
    "        os.environ[\"RAG_BASE\"] = base\n",
    "    else:\n",
    "        base = os.getenv(\"RAG_BASE\", f\"http://{host}:9909\")\n",
    "        rag.set_base(base)\n",
    "    chosen_port = port\n",
    "    if auto_port and _port_in_use(host, chosen_port):\n",
    "        chosen_port = _pick_free_port(host=host)\n",
    "    config = uvicorn.Config(\n",
    "        app,\n",
    "        host=host,\n",
    "        port=chosen_port,\n",
    "        log_level=log_level,\n",
    "        lifespan=\"on\",\n",
    "    )\n",
    "    server = uvicorn.Server(config)\n",
    "    def _run():\n",
    "        server.run()\n",
    "    th = threading.Thread(target=_run, name=\"gateway-uvicorn\", daemon=True)\n",
    "    th.start()\n",
    "    url = f\"http://{host}:{chosen_port}\"\n",
    "    for _ in range(50):  \n",
    "        try:\n",
    "            import urllib.request as u, json\n",
    "            with u.urlopen(f\"{url}/up\", timeout=0.2) as r:\n",
    "                if r.status == 200:\n",
    "                    _GATEWAY_THREAD = th\n",
    "                    _GATEWAY_SERVER = server\n",
    "                    _GATEWAY_URL = url\n",
    "                    _GATEWAY_PORT = chosen_port\n",
    "                    print(f\"Gateway running (background) at {url}\")\n",
    "                    print(\" Stop it with:  stop_gateway()\")\n",
    "                    return url\n",
    "        except Exception:\n",
    "            time.sleep(0.1)\n",
    "    raise RuntimeError(\"gateway failed to start in background\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 522,
   "id": "3b3923e7-f1b5-4806-b447-280f0755f768",
   "metadata": {},
   "outputs": [],
   "source": [
    "def stop_gateway():\n",
    "    \"\"\"\n",
    "    Gracefully stop the background gateway if running.\n",
    "    \"\"\"\n",
    "    global _GATEWAY_THREAD, _GATEWAY_SERVER, _GATEWAY_URL, _GATEWAY_PORT\n",
    "    srv = _GATEWAY_SERVER\n",
    "    th = _GATEWAY_THREAD\n",
    "    if srv is None:\n",
    "        return\n",
    "    srv.should_exit = True\n",
    "    srv.force_exit = True\n",
    "    for _ in range(30):\n",
    "        if th is not None and not th.is_alive():\n",
    "            break\n",
    "        time.sleep(0.1)\n",
    "    _GATEWAY_THREAD = None\n",
    "    _GATEWAY_SERVER = None\n",
    "    _GATEWAY_URL = None\n",
    "    _GATEWAY_PORT = None\n",
    "    print(\"Gateway stopped.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 523,
   "id": "10edefe3-36da-4f2e-98a4-0024cd8657f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def gateway_url() -> str:\n",
    "    return _GATEWAY_URL or \"not running\"\n",
    "def gateway_port() -> int:\n",
    "    return _GATEWAY_PORT or -1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 524,
   "id": "a7b28885-887f-4f91-aecb-cbeebfe71502",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:     Started server process [17912]\n",
      "INFO:     Waiting for application startup.\n",
      "INFO:     Application startup complete.\n",
      "INFO:     Uvicorn running on http://127.0.0.1:9913 (Press CTRL+C to quit)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:     127.0.0.1:54421 - \"GET /up HTTP/1.1\" 200 OK\n",
      "Gateway running (background) at http://127.0.0.1:9913\n",
      " Stop it with:  stop_gateway()\n",
      "http://127.0.0.1:9913\n",
      "INFO:     127.0.0.1:54424 - \"POST /retrieve_bm25 HTTP/1.1\" 200 OK\n",
      "INFO:     127.0.0.1:54423 - \"POST /retrieve_dense HTTP/1.1\" 200 OK\n",
      "INFO:     127.0.0.1:54424 - \"POST /rerank HTTP/1.1\" 200 OK\n",
      "INFO:     127.0.0.1:54424 - \"POST /complete HTTP/1.1\" 200 OK\n",
      "INFO:     127.0.0.1:54422 - \"POST /answer HTTP/1.1\" 200 OK\n",
      "{'answer': 'Answer to: smoke test', 'contexts': [{'id': 'bm25:0', 'text': 'smoke test bm25 #0'}, {'id': 'bm25:1', 'text': 'smoke test bm25 #1'}], 'verify': {'faithfulness': 0.9}, 'meta': {'pipeline': 'rerank', 'latency_ms': 186, 'cache': 'miss', 'partial_retrieval': {'bm25_ok': True, 'dense_ok': True}, 'route': 'factual', 'ns': 'default'}}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:     Shutting down\n",
      "INFO:     Finished server process [17912]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gateway stopped.\n"
     ]
    }
   ],
   "source": [
    "url = start_in_notebook(mock_rag=True, auto_port=True)\n",
    "print(url)\n",
    "import requests\n",
    "r = requests.post(f\"{url}/answer\", json={\"query\": \"smoke test\", \"top_k\": 2})\n",
    "print(r.json())\n",
    "stop_gateway()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 525,
   "id": "c34a52dd-a06d-4521-ae43-608834a32612",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import json\n",
    "import threading\n",
    "from typing import Any, Dict, List, Optional\n",
    "import requests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 526,
   "id": "c3c7380d-00ee-4e88-96f0-a7897daabbe5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " using gateway at: http://127.0.0.1:9910\n"
     ]
    }
   ],
   "source": [
    "def _need_gateway() -> str:\n",
    "    u = gateway_url() if \"gateway_url\" in globals() else None\n",
    "    if not u or u == \"not running\":\n",
    "        u = \"http://127.0.0.1:9910\"\n",
    "    return u.rstrip(\"/\")\n",
    "GW = _need_gateway()\n",
    "print(f\" using gateway at: {GW}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 527,
   "id": "0f5b6e9d-5ce1-4e82-aee5-a05ad93db6fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "def gw_answer(\n",
    "    query: str,\n",
    "    top_k: int = 3,\n",
    "    ns: str = \"default\",\n",
    "    route: Optional[str] = None,\n",
    "    headers: Optional[Dict[str, str]] = None,\n",
    ") -> Dict[str, Any]:\n",
    "    h = {\"content-type\": \"application/json\", \"x-namespace\": ns}\n",
    "    if route:\n",
    "        h[\"x-route\"] = route\n",
    "    if headers:\n",
    "        h.update(headers)\n",
    "    r = requests.post(f\"{GW}/answer\", headers=h, json={\"query\": query, \"top_k\": top_k})\n",
    "    try:\n",
    "        return r.json()\n",
    "    except Exception:\n",
    "        return {\"error\": f\"bad response {r.status_code}\", \"text\": r.text}\n",
    "def gw_chaos_get() -> Dict[str, Any]:\n",
    "    r = requests.get(f\"{GW}/admin/chaos/get\")\n",
    "    return r.json()\n",
    "def gw_chaos_set(**kw) -> Dict[str, Any]:\n",
    "    r = requests.post(f\"{GW}/admin/chaos/set\", json=kw)\n",
    "    return r.json()\n",
    "def gw_rl_update(weights: Dict[str, float]) -> Dict[str, Any]:\n",
    "    r = requests.post(f\"{GW}/rl/update\", json={\"weights\": weights})\n",
    "    return r.json()\n",
    "def gw_rl_promote(tag: Optional[str] = None) -> Dict[str, Any]:\n",
    "    body = {} if tag is None else {\"tag\": tag}\n",
    "    r = requests.post(f\"{GW}/rl/promote\", json=body)\n",
    "    return r.json()\n",
    "def gw_rl_rollback(tag: Optional[str] = None) -> Dict[str, Any]:\n",
    "    body = {} if tag is None else {\"tag\": tag}\n",
    "    r = requests.post(f\"{GW}/rl/rollback\", json=body)\n",
    "    return r.json()\n",
    "def gw_strategy_counts() -> Dict[str, Any]:\n",
    "    r = requests.get(f\"{GW}/canary/strategy_counts\")\n",
    "    return r.json()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 528,
   "id": "8519f614-448b-47b7-945e-a7d9a62b2aa2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Task was destroyed but it is pending!\n",
      "task: <Task pending name='Task-2123' coro=<LifespanOn.main() running at C:\\Users\\aniru\\AppData\\Roaming\\Python\\Python312\\site-packages\\uvicorn\\lifespan\\on.py:86> wait_for=<Future pending cb=[Task.__wakeup()]>>\n",
      "ERROR:    Traceback (most recent call last):\n",
      "  File \"C:\\Users\\aniru\\AppData\\Roaming\\Python\\Python312\\site-packages\\starlette\\routing.py\", line 701, in lifespan\n",
      "    await receive()\n",
      "GeneratorExit\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def hit_once(i: int, q: str, ns: str, route: Optional[str], out: List[float]):\n",
    "    t0 = time.time()\n",
    "    resp = gw_answer(q, top_k=3, ns=ns, route=route)\n",
    "    dt = (time.time() - t0) * 1000.0\n",
    "    out.append(dt)\n",
    "    if i < 3:  \n",
    "        print(f\"[{i}] {dt:.1f} ms  meta={resp.get('meta',{})}\")\n",
    "def loadgen(\n",
    "    q: str = \"smoke test\",\n",
    "    ns: str = \"default\",\n",
    "    route: Optional[str] = None,\n",
    "    n: int = 30,\n",
    "    threads: int = 4,\n",
    "):\n",
    "    \"\"\"\n",
    "    quick-n-dirty notebook loadgen\n",
    "    - spawns `threads` threads\n",
    "    - total of `n` requests\n",
    "    - returns list of latencies (ms)\n",
    "    \"\"\"\n",
    "    lat: List[float] = []\n",
    "    lock = threading.Lock()\n",
    "    def worker(items: List[int]):\n",
    "        for i in items:\n",
    "            t0 = time.time()\n",
    "            resp = gw_answer(q, ns=ns, route=route)\n",
    "            dt = (time.time() - t0) * 1000.0\n",
    "            with lock:\n",
    "                lat.append(dt)\n",
    "            if i < 3:\n",
    "                print(f\"[worker] {i}  {dt:.1f} ms | pipe={resp.get('meta',{}).get('pipeline')}\")\n",
    "    per = max(1, n // threads)\n",
    "    ths = []\n",
    "    cur = 0\n",
    "    for _ in range(threads):\n",
    "        chunk = list(range(cur, min(cur + per, n)))\n",
    "        cur += per\n",
    "        if not chunk:\n",
    "            break\n",
    "        t = threading.Thread(target=worker, args=(chunk,), daemon=True)\n",
    "        t.start()\n",
    "        ths.append(t)\n",
    "    for t in ths:\n",
    "        t.join()\n",
    "    lat_sorted = sorted(lat)\n",
    "    p95 = lat_sorted[int(len(lat_sorted) * 0.95) - 1] if lat_sorted else None\n",
    "    print(f\"\\nloadgen done: n={len(lat)} p95{p95:.1f} ms\" if p95 else \"loadgen done (no data)\")\n",
    "    return lat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 529,
   "id": "93754b82-818d-4d82-8cee-5340912521a6",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- SMOKE: /up ---\n",
      "{\n",
      "  \"ok\": true,\n",
      "  \"service\": \"gateway\",\n",
      "  \"ts\": 1762114605\n",
      "}\n",
      "\n",
      "--- SMOKE: /answer ---\n",
      "{\n",
      "  \"detail\": \"retrieval_failed: 'Request' object is not callable\"\n",
      "} ...\n",
      "\n",
      "\n",
      "--- CHAOS: enable verifier_v2 ---\n",
      "{\n",
      "  \"ok\": true,\n",
      "  \"chaos\": {\n",
      "    \"disable_ce\": false,\n",
      "    \"force_bm25\": false,\n",
      "    \"verifier_v2\": true\n",
      "  }\n",
      "}\n",
      "\n",
      "--- RL: set factual=0.35, sql=1.0 ---\n",
      "{\n",
      "  \"ok\": true,\n",
      "  \"weights\": {\n",
      "    \"sql\": 1.0,\n",
      "    \"math\": 1.0,\n",
      "    \"policy\": 1.0,\n",
      "    \"factual\": 0.35\n",
      "  }\n",
      "}\n",
      "\n",
      "--- Strategy counts (after a couple hits) ---\n",
      "{\n",
      "  \"ok\": true,\n",
      "  \"counts\": {\n",
      "    \"rerank\": 2,\n",
      "    \"rrf\": 1\n",
      "  }\n",
      "}\n",
      "\n",
      "--- LOADGEN (short) ---\n",
      "[worker] 0  18.2 ms | pipe=None\n",
      "[worker] 1  12.9 ms | pipe=None\n",
      "[worker] 2  15.6 ms | pipe=None\n",
      "\n",
      "loadgen done: n=18 p9522.3 ms\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n--- SMOKE: /up ---\")\n",
    "try:\n",
    "    up = requests.get(f\"{GW}/up\", timeout=1.5).json()\n",
    "    print(json.dumps(up, indent=2))\n",
    "except Exception as e:\n",
    "    print(\"could not hit /up:\", e)\n",
    "print(\"\\n--- SMOKE: /answer ---\")\n",
    "resp = gw_answer(\"what is the current gateway pipeline?\", top_k=3)\n",
    "print(json.dumps(resp, indent=2)[:400], \"...\\n\")\n",
    "print(\"\\n--- CHAOS: enable verifier_v2 ---\")\n",
    "print(json.dumps(gw_chaos_set(verifier_v2=True), indent=2))\n",
    "print(\"\\n--- RL: set factual=0.35, sql=1.0 ---\")\n",
    "print(json.dumps(gw_rl_update({\"factual\": 0.35, \"sql\": 1.0}), indent=2))\n",
    "print(\"\\n--- Strategy counts (after a couple hits) ---\")\n",
    "_ = gw_answer(\"SELECT * FROM users LIMIT 5\", route=None)\n",
    "_ = gw_answer(\"policy for HIPAA compliant storage\", route=None)\n",
    "print(json.dumps(gw_strategy_counts(), indent=2))\n",
    "print(\"\\n--- LOADGEN (short) ---\")\n",
    "_ = loadgen(q=\"explain RAG fan-out pipeline\", n=20, threads=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 530,
   "id": "04f2c40d-46b9-4ee3-ab15-3491f8ecc1ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "import statistics\n",
    "def loadgen(\n",
    "    q: str = \"explain RAG fan-out pipeline\",\n",
    "    ns: str = \"default\",\n",
    "    route: Optional[str] = None,\n",
    "    n: int = 30,\n",
    "    threads: int = 4,\n",
    "    extra_headers: Optional[Dict[str, str]] = None,\n",
    "):\n",
    "    \"\"\"\n",
    "    quick-n-dirty notebook loadgen\n",
    "    - spawns `threads` threads\n",
    "    - total of `n` requests\n",
    "    - returns list of latencies (ms)\n",
    "    \"\"\"\n",
    "    lat: List[float] = []\n",
    "    lock = threading.Lock()\n",
    "    def worker(items: List[int]):\n",
    "        for i in items:\n",
    "            t0 = time.time()\n",
    "            resp = gw_answer(q, top_k=3, ns=ns, route=route, headers=extra_headers)\n",
    "            dt = (time.time() - t0) * 1000.0\n",
    "            with lock:\n",
    "                lat.append(dt)\n",
    "            if i < 3:\n",
    "                meta = resp.get(\"meta\", {})\n",
    "                print(\n",
    "                    f\"[worker] {i}  {dt:.1f} ms | pipe={meta.get('pipeline')} | steps={meta.get('step_ms')}\"\n",
    "                )\n",
    "    per = max(1, n // threads)\n",
    "    ths = []\n",
    "    cur = 0\n",
    "    for _ in range(threads):\n",
    "        chunk = list(range(cur, min(cur + per, n)))\n",
    "        cur += per\n",
    "        if not chunk:\n",
    "            break\n",
    "        t = threading.Thread(target=worker, args=(chunk,), daemon=True)\n",
    "        t.start()\n",
    "        ths.append(t)\n",
    "    for t in ths:\n",
    "        t.join()\n",
    "    if not lat:\n",
    "        print(\"loadgen done (no data)\")\n",
    "        return []\n",
    "    lat_sorted = sorted(lat)\n",
    "    idx = max(0, int(len(lat_sorted) * 0.95) - 1)\n",
    "    p95 = lat_sorted[idx]\n",
    "    avg = statistics.mean(lat_sorted)\n",
    "    print(f\"\\nloadgen done: n={len(lat_sorted)} avg={avg:.1f} ms p95{p95:.1f} ms\")\n",
    "    return lat_sorted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 531,
   "id": "5e4b17e9-5879-4872-82b5-ea5838ecc059",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- METRICS PING ---\n",
      "metrics status: 200\n",
      " metrics exposed (Prometheus style)\n",
      "\n",
      "--- CHAOS DEMO (disable_ce) ---\n",
      "{\n",
      "  \"ok\": true,\n",
      "  \"chaos\": {\n",
      "    \"disable_ce\": true,\n",
      "    \"force_bm25\": false,\n",
      "    \"verifier_v2\": true\n",
      "  }\n",
      "}\n",
      "{\n",
      "  \"ok\": true,\n",
      "  \"chaos\": {\n",
      "    \"disable_ce\": true,\n",
      "    \"force_bm25\": false,\n",
      "    \"verifier_v2\": true\n",
      "  }\n",
      "}\n",
      "\n",
      "--- RL PROMOTE ---\n",
      "{\n",
      "  \"ok\": true,\n",
      "  \"snapshot\": {\n",
      "    \"tag\": \"20251102-131646\",\n",
      "    \"ts\": 1762114606,\n",
      "    \"weights\": {\n",
      "      \"sql\": 1.0,\n",
      "      \"math\": 1.0,\n",
      "      \"policy\": 1.0,\n",
      "      \"factual\": 0.35\n",
      "    }\n",
      "  }\n",
      "}\n",
      "\n",
      "--- RL ROLLBACK ---\n",
      "{\n",
      "  \"ok\": true,\n",
      "  \"weights\": {\n",
      "    \"sql\": 1.0,\n",
      "    \"math\": 1.0,\n",
      "    \"policy\": 1.0,\n",
      "    \"factual\": 0.35\n",
      "  },\n",
      "  \"rolled_to\": \"20251030-100601\"\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n--- METRICS PING ---\")\n",
    "try:\n",
    "    m = requests.get(f\"{GW}/metrics\", timeout=1.5)\n",
    "    print(\"metrics status:\", m.status_code)\n",
    "    if m.status_code == 200:\n",
    "        print(\" metrics exposed (Prometheus style)\")\n",
    "    else:\n",
    "        print(\" metrics endpoint not 200, maybe python gateway or metrics disabled\")\n",
    "except Exception as e:\n",
    "    print(\"metrics check failed:\", e)\n",
    "print(\"\\n--- CHAOS DEMO (disable_ce) ---\")\n",
    "print(json.dumps(gw_chaos_set(disable_ce=True), indent=2))\n",
    "_ = gw_answer(\"test chaos pipeline\", top_k=2)\n",
    "print(json.dumps(gw_chaos_get(), indent=2))\n",
    "_ = gw_chaos_set(disable_ce=False)\n",
    "print(\"\\n--- RL PROMOTE ---\")\n",
    "promo = gw_rl_promote()\n",
    "print(json.dumps(promo, indent=2))\n",
    "print(\"\\n--- RL ROLLBACK ---\")\n",
    "rollback = gw_rl_rollback()\n",
    "print(json.dumps(rollback, indent=2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 532,
   "id": "73f32af6-855d-407e-8221-bc6fd1b943fd",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== SCENARIO 1: baseline (CE default) ===\n",
      "[worker] 0  7.1 ms | pipe=None | steps=None\n",
      "[worker] 1  12.0 ms | pipe=None | steps=None\n",
      "[worker] 2  11.4 ms | pipe=None | steps=None\n",
      "\n",
      "loadgen done: n=18 avg=9.8 ms p9512.5 ms\n",
      "\n",
      "=== SCENARIO 2: chaos.disable_ce = true ===\n",
      "[worker] 0  6.0 ms | pipe=None | steps=None\n",
      "[worker] 1  10.7 ms | pipe=None | steps=None\n",
      "[worker] 2  9.0 ms | pipe=None | steps=None\n",
      "\n",
      "loadgen done: n=18 avg=9.8 ms p9511.3 ms\n",
      "\n",
      "=== SUMMARY ===\n",
      "baseline p95: 12.5 ms\n",
      "canary   p95: 11.3 ms\n",
      "\n",
      "strategy counts after test:\n",
      "{\n",
      "  \"ok\": true,\n",
      "  \"counts\": {\n",
      "    \"rerank\": 14,\n",
      "    \"rrf\": 63\n",
      "  }\n",
      "}\n",
      "\n",
      " continuation cell done. go hit the gateway.\n"
     ]
    }
   ],
   "source": [
    "def compare_scenarios():\n",
    "    print(\"\\n=== SCENARIO 1: baseline (CE default) ===\")\n",
    "    base = loadgen(q=\"how do we fan-out to bm25+dense?\", n=20, threads=3)\n",
    "    print(\"\\n=== SCENARIO 2: chaos.disable_ce = true ===\")\n",
    "    gw_chaos_set(disable_ce=True)\n",
    "    canary = loadgen(\n",
    "        q=\"how do we fan-out to bm25+dense?\",\n",
    "        n=20,\n",
    "        threads=3,\n",
    "        extra_headers=None,  \n",
    "    )\n",
    "    gw_chaos_set(disable_ce=False)\n",
    "    def safe_p95(xs: List[float]) -> float:\n",
    "        if not xs:\n",
    "            return float(\"nan\")\n",
    "        xs_sorted = sorted(xs)\n",
    "        idx = max(0, int(len(xs_sorted) * 0.95) - 1)\n",
    "        return xs_sorted[idx]\n",
    "    print(\"\\n=== SUMMARY ===\")\n",
    "    print(f\"baseline p95: {safe_p95(base):.1f} ms\")\n",
    "    print(f\"canary   p95: {safe_p95(canary):.1f} ms\")\n",
    "    print(\"\\nstrategy counts after test:\")\n",
    "    print(json.dumps(gw_strategy_counts(), indent=2))\n",
    "compare_scenarios()\n",
    "print(\"\\n continuation cell done. go hit the gateway.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 533,
   "id": "2168e6ef-e1e8-4998-803b-9723cb077c46",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import time\n",
    "from pathlib import Path\n",
    "from typing import Any, Dict, List, Optional\n",
    "ART_DIR = Path(os.getenv(\"ART_DIR\", \"/tmp/art\"))\n",
    "ART_DIR.mkdir(parents=True, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 534,
   "id": "2dc769de-a150-4cf1-a392-bc1df907894b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fetch_json(url: str, method: str = \"GET\", body: Optional[dict] = None, timeout: float = 2.5) -> Dict[str, Any]:\n",
    "    try:\n",
    "        if method == \"GET\":\n",
    "            r = requests.get(url, timeout=timeout)\n",
    "        else:\n",
    "            r = requests.post(url, json=body or {}, timeout=timeout)\n",
    "        return r.json()\n",
    "    except Exception as e:\n",
    "        return {\"ok\": False, \"error\": str(e), \"url\": url}\n",
    "def gateway_dump() -> Dict[str, Any]:\n",
    "    base = _need_gateway()\n",
    "    snap = {\n",
    "        \"ts\": int(time.time()),\n",
    "        \"base\": base,\n",
    "        \"up\": fetch_json(f\"{base}/up\"),\n",
    "        \"metrics_status\": None,\n",
    "        \"chaos\": fetch_json(f\"{base}/admin/chaos/get\"),\n",
    "        \"rl_weights\": fetch_json(f\"{base}/rl/update\", method=\"POST\", body={\"weights\": {}}), \n",
    "        \"strategy_counts\": fetch_json(f\"{base}/canary/strategy_counts\"),\n",
    "    }\n",
    "    try:\n",
    "        m = requests.get(f\"{base}/metrics\", timeout=1.5)\n",
    "        snap[\"metrics_status\"] = m.status_code\n",
    "    except Exception as e:\n",
    "        snap[\"metrics_status\"] = f\"metrics_err: {e}\"\n",
    "    return snap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 535,
   "id": "29f60f1c-bbd0-4ed9-bd9b-edf91eb0a326",
   "metadata": {},
   "outputs": [],
   "source": [
    "def scenario_hit(\n",
    "    query: str,\n",
    "    *,\n",
    "    ns: str = \"default\",\n",
    "    route: Optional[str] = None,\n",
    "    headers: Optional[Dict[str, str]] = None,\n",
    "    top_k: int = 3,\n",
    "    label: str = \"default\",\n",
    ") -> Dict[str, Any]:\n",
    "    t0 = time.time()\n",
    "    resp = gw_answer(query, top_k=top_k, ns=ns, route=route, headers=headers)\n",
    "    dt = (time.time() - t0) * 1000.0\n",
    "    meta = resp.get(\"meta\", {})\n",
    "    out = {\n",
    "        \"label\": label,\n",
    "        \"lat_ms\": dt,\n",
    "        \"pipeline\": meta.get(\"pipeline\"),\n",
    "        \"step_ms\": meta.get(\"step_ms\"),\n",
    "        \"ns\": meta.get(\"ns\"),\n",
    "        \"route\": meta.get(\"route\"),\n",
    "        \"partial\": meta.get(\"partial_retrieval\"),\n",
    "        \"raw\": resp,\n",
    "    }\n",
    "    print(f\"[scenario:{label}] {dt:.1f} ms | pipe={out['pipeline']} | ns={out['ns']} | route={out['route']}\")\n",
    "    return out\n",
    "def run_scenario_matrix() -> Dict[str, Any]:\n",
    "    base = _need_gateway()\n",
    "    print(f\"\\n=== scenario matrix @ {base} ===\")\n",
    "    rows: List[Dict[str, Any]] = []\n",
    "    rows.append(\n",
    "        scenario_hit(\"what is the current RAG pipeline?\", label=\"default\")\n",
    "    )\n",
    "    rows.append(\n",
    "        scenario_hit(\"SELECT * FROM sales LIMIT 10\", route=\"sql\", label=\"route=sql\")\n",
    "    )\n",
    "    gw_chaos_set(disable_ce=True)\n",
    "    rows.append(\n",
    "        scenario_hit(\n",
    "            \"HIPAA compliant storage rules for PHI\",\n",
    "            headers={\"x-chaos-disable-ce\": \"1\"},\n",
    "            label=\"policy+chaos_disable_ce\",\n",
    "        )\n",
    "    )\n",
    "    gw_chaos_set(disable_ce=False)\n",
    "    gw_chaos_set(verifier_v2=True)\n",
    "    rows.append(\n",
    "        scenario_hit(\n",
    "            \"explain isolation forest for anomaly detection\",\n",
    "            headers={\"x-verify-v2\": \"1\"},\n",
    "            label=\"verifier_v2\",\n",
    "        )\n",
    "    )\n",
    "    gw_chaos_set(verifier_v2=False)\n",
    "    rows.append(\n",
    "        scenario_hit(\n",
    "            \"explain dense retriever vs bm25 for RAG\",\n",
    "            ns=\"tenant-acme\",\n",
    "            label=\"ns=tenant-acme\",\n",
    "        )\n",
    "    )\n",
    "    return {\n",
    "        \"ts\": int(time.time()),\n",
    "        \"scenarios\": rows,\n",
    "        \"strategy_counts_after\": gw_strategy_counts(),\n",
    "        \"chaos_after\": gw_chaos_get(),\n",
    "    }\n",
    "def save_run(snap: Dict[str, Any], matrix: Dict[str, Any]) -> Path:\n",
    "    out = {\n",
    "        \"ts\": int(time.time()),\n",
    "        \"gateway\": snap,\n",
    "        \"matrix\": matrix,\n",
    "    }\n",
    "    out_path = ART_DIR / \"gateway_smoke.json\"\n",
    "    out_path.write_text(json.dumps(out, indent=2), encoding=\"utf-8\")\n",
    "    print(f\"\\n wrote {out_path}\")\n",
    "    return out_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 536,
   "id": "dcdd5ddb-fc31-4af8-a643-b93cf688b64a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== scenario matrix @ http://127.0.0.1:9910 ===\n",
      "[scenario:default] 3.8 ms | pipe=None | ns=None | route=None\n",
      "[scenario:route=sql] 5.0 ms | pipe=None | ns=None | route=None\n",
      "[scenario:policy+chaos_disable_ce] 7.8 ms | pipe=None | ns=None | route=None\n",
      "[scenario:verifier_v2] 4.4 ms | pipe=None | ns=None | route=None\n",
      "[scenario:ns=tenant-acme] 0.7 ms | pipe=None | ns=None | route=None\n",
      "\n",
      " wrote \\tmp\\art\\gateway_smoke.json\n",
      "\n",
      "--- PREVIEW (strategy counts) ---\n",
      "{\n",
      "  \"ok\": true,\n",
      "  \"counts\": {\n",
      "    \"rerank\": 17,\n",
      "    \"rrf\": 66\n",
      "  }\n",
      "}\n",
      "\n",
      "--- if you want to compare two runs: ---\n",
      "old = json.load(open('/tmp/art/gateway_smoke.json'))  # current\n",
      "# then re-run the cell after changes and diff\n"
     ]
    }
   ],
   "source": [
    "snap = gateway_dump()\n",
    "matrix = run_scenario_matrix()\n",
    "out_path = save_run(snap, matrix)\n",
    "print(\"\\n--- PREVIEW (strategy counts) ---\")\n",
    "print(json.dumps(matrix[\"strategy_counts_after\"], indent=2))\n",
    "print(\"\\n--- if you want to compare two runs: ---\")\n",
    "print(\"old = json.load(open('/tmp/art/gateway_smoke.json'))  # current\")\n",
    "print(\"# then re-run the cell after changes and diff\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 537,
   "id": "8433dbab-e3b2-4461-8202-02c30f6f2b1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import time\n",
    "from pathlib import Path\n",
    "from typing import Any, Dict, List, Optional\n",
    "ART_DIR = Path(os.getenv(\"ART_DIR\", \"/tmp/art\"))\n",
    "ART_DIR.mkdir(parents=True, exist_ok=True)\n",
    "RUNS_DIR = ART_DIR / \"gateway_runs\"\n",
    "RUNS_DIR.mkdir(parents=True, exist_ok=True)\n",
    "LATEST_PATH = ART_DIR / \"gateway_smoke.json\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 538,
   "id": "319562db-c038-44fb-9bf9-56e959f601d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _now_ts() -> int:\n",
    "    return int(time.time())\n",
    "def _ts_str() -> str:\n",
    "    return time.strftime(\"%Y%m%d-%H%M%S\", time.localtime())\n",
    "def archive_latest(latest: Path = LATEST_PATH) -> Optional[Path]:\n",
    "    \"\"\"\n",
    "    Move /tmp/art/gateway_smoke.json  /tmp/art/gateway_runs/<ts>.json\n",
    "    so we can diff later.\n",
    "    \"\"\"\n",
    "    if not latest.exists():\n",
    "        print(\" no latest gateway_smoke.json to archive\")\n",
    "        return None\n",
    "    ts = _ts_str()\n",
    "    out = RUNS_DIR / f\"gateway_smoke_{ts}.json\"\n",
    "    out.write_text(latest.read_text(encoding=\"utf-8\"), encoding=\"utf-8\")\n",
    "    print(f\" archived current run  {out}\")\n",
    "    return out\n",
    "def list_runs(limit: int = 15) -> List[Path]:\n",
    "    files = sorted(RUNS_DIR.glob(\"gateway_smoke_*.json\"), reverse=True)\n",
    "    return files[:limit]\n",
    "def load_run(path: Path) -> Dict[str, Any]:\n",
    "    return json.loads(path.read_text(encoding=\"utf-8\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 539,
   "id": "d5e04b25-cda7-4edd-8418-bbd60fe0524b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _safe_get_p95(run: Dict[str, Any]) -> Optional[float]:\n",
    "    \"\"\"\n",
    "    Try to pull p95 from a couple likely places.\n",
    "    \"\"\"\n",
    "    base = run.get(\"gateway\", {}).get(\"base\")\n",
    "    if not base:\n",
    "        return None\n",
    "    try:\n",
    "        import requests\n",
    "        r = requests.get(f\"{base}/canary/status\", timeout=1.5)\n",
    "        js = r.json()\n",
    "        return js.get(\"window\", {}).get(\"p95_ms\")\n",
    "    except Exception:\n",
    "        return None\n",
    "def compare_runs(new_run: Dict[str, Any], old_run: Dict[str, Any]) -> Dict[str, Any]:\n",
    "    \"\"\"\n",
    "    Super small diff: chaos, strategy counts, base URL, and p95 (if we can fetch it).\n",
    "    \"\"\"\n",
    "    base_new = new_run.get(\"gateway\", {}).get(\"base\")\n",
    "    base_old = old_run.get(\"gateway\", {}).get(\"base\")\n",
    "    chaos_new = new_run.get(\"gateway\", {}).get(\"chaos\", {})\n",
    "    chaos_old = old_run.get(\"gateway\", {}).get(\"chaos\", {})\n",
    "    strat_new = new_run.get(\"matrix\", {}).get(\"strategy_counts_after\", {})\n",
    "    strat_old = old_run.get(\"matrix\", {}).get(\"strategy_counts_after\", {})\n",
    "    p95_new = _safe_get_p95(new_run)\n",
    "    p95_old = _safe_get_p95(old_run)\n",
    "    return {\n",
    "        \"base_old\": base_old,\n",
    "        \"base_new\": base_new,\n",
    "        \"chaos_old\": chaos_old,\n",
    "        \"chaos_new\": chaos_new,\n",
    "        \"strategy_old\": strat_old,\n",
    "        \"strategy_new\": strat_new,\n",
    "        \"p95_old\": p95_old,\n",
    "        \"p95_new\": p95_new,\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 540,
   "id": "6fa1ab8d-93e7-4cbb-a586-8cfbbbf01b4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_diff(diff: Dict[str, Any]) -> None:\n",
    "    print(\"\\n=== GATEWAY DIFF ===\")\n",
    "    if diff[\"base_old\"] != diff[\"base_new\"]:\n",
    "        print(f\"base: {diff['base_old']}  {diff['base_new']}\")\n",
    "    else:\n",
    "        print(f\"base: {diff['base_new']}\")\n",
    "    print(\"\\nchaos:\")\n",
    "    print(\"  old:\", json.dumps(diff[\"chaos_old\"], indent=2))\n",
    "    print(\"  new:\", json.dumps(diff[\"chaos_new\"], indent=2))\n",
    "    print(\"\\nstrategy counts:\")\n",
    "    print(\"  old:\", json.dumps(diff[\"strategy_old\"], indent=2))\n",
    "    print(\"  new:\", json.dumps(diff[\"strategy_new\"], indent=2))\n",
    "    if diff[\"p95_old\"] is not None or diff[\"p95_new\"] is not None:\n",
    "        print(\"\\np95 (ms):\", diff[\"p95_old\"], \"\", diff[\"p95_new\"])\n",
    "    print(\"====================\\n\")\n",
    "def enforce_slo(\n",
    "    diff: Dict[str, Any],\n",
    "    *,\n",
    "    max_p95_ms: float = 1800.0,\n",
    "    require_strategy_growth: bool = False,\n",
    ") -> None:\n",
    "    \"\"\"\n",
    "    Angry little gate.\n",
    "    - fails if p95_new > max_p95_ms\n",
    "    - optionally fails if strategy counts didn't move\n",
    "    \"\"\"\n",
    "    p95_new = diff.get(\"p95_new\")\n",
    "    if p95_new is not None and p95_new > max_p95_ms:\n",
    "        raise RuntimeError(f\"SLO FAIL: p95={p95_new:.1f} ms > {max_p95_ms} ms\")\n",
    "    if require_strategy_growth:\n",
    "        old_counts = diff.get(\"strategy_old\", {}).get(\"counts\", {})\n",
    "        new_counts = diff.get(\"strategy_new\", {}).get(\"counts\", {})\n",
    "        if old_counts == new_counts:\n",
    "            raise RuntimeError(\"SLO FAIL: strategy counts did not move (no traffic?)\")\n",
    "    print(\"SLO \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 541,
   "id": "3994eb69-4de7-475f-a8d0-daca1f4d4f1e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " archived current run  \\tmp\\art\\gateway_runs\\gateway_smoke_20251102-131650.json\n",
      "\n",
      "=== GATEWAY DIFF ===\n",
      "base: http://127.0.0.1:9910\n",
      "\n",
      "chaos:\n",
      "  old: {\n",
      "  \"ok\": true,\n",
      "  \"chaos\": {\n",
      "    \"disable_ce\": false,\n",
      "    \"force_bm25\": false,\n",
      "    \"verifier_v2\": true\n",
      "  }\n",
      "}\n",
      "  new: {\n",
      "  \"ok\": true,\n",
      "  \"chaos\": {\n",
      "    \"disable_ce\": false,\n",
      "    \"force_bm25\": false,\n",
      "    \"verifier_v2\": true\n",
      "  }\n",
      "}\n",
      "\n",
      "strategy counts:\n",
      "  old: {\n",
      "  \"ok\": true,\n",
      "  \"counts\": {\n",
      "    \"rerank\": 15,\n",
      "    \"rrf\": 68\n",
      "  }\n",
      "}\n",
      "  new: {\n",
      "  \"ok\": true,\n",
      "  \"counts\": {\n",
      "    \"rerank\": 17,\n",
      "    \"rrf\": 66\n",
      "  }\n",
      "}\n",
      "====================\n",
      "\n",
      "SLO \n",
      "\n",
      " continuation+diff cell done.\n",
      "Tip: open \\tmp\\art\\gateway_runs to inspect historical JSONs.\n"
     ]
    }
   ],
   "source": [
    "archived_path = archive_latest()\n",
    "runs = list_runs(limit=2)\n",
    "if len(runs) < 2:\n",
    "    print(\" need at least 2 archived runs to diff. run another smoke cell first.\")\n",
    "else:\n",
    "    newest = load_run(runs[0])\n",
    "    previous = load_run(runs[1])\n",
    "    d = compare_runs(newest, previous)\n",
    "    print_diff(d)\n",
    "    enforce_slo(d, max_p95_ms=1800.0, require_strategy_growth=False)\n",
    "print(\"\\n continuation+diff cell done.\")\n",
    "print(\"Tip: open\", RUNS_DIR, \"to inspect historical JSONs.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 542,
   "id": "d9af94fd-4318-4168-addb-c10576ea773a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import time\n",
    "from pathlib import Path\n",
    "from typing import Any, Dict, List, Optional\n",
    "ART_DIR = Path(os.getenv(\"ART_DIR\", \"/tmp/art\"))\n",
    "RUNS_DIR = ART_DIR / \"gateway_runs\"\n",
    "RUNS_DIR.mkdir(parents=True, exist_ok=True)\n",
    "AGG_PATH = ART_DIR / \"gateway_runs_agg.json\"\n",
    "CI_SUMMARY_PATH = ART_DIR / \"gateway_ci_summary.json\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 543,
   "id": "9856bc3e-f2ba-494f-afb9-656824b283d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_json(p: Path) -> Dict[str, Any]:\n",
    "    try:\n",
    "        return json.loads(p.read_text(encoding=\"utf-8\"))\n",
    "    except Exception as e:\n",
    "        return {\"ok\": False, \"error\": str(e), \"path\": str(p)}\n",
    "def prune_old_runs(keep: int = 30) -> List[Path]:\n",
    "    \"\"\"\n",
    "    Keep only the latest `keep` archived runs.\n",
    "    \"\"\"\n",
    "    files = sorted(RUNS_DIR.glob(\"gateway_smoke_*.json\"), reverse=True)\n",
    "    to_delete = files[keep:]\n",
    "    for f in to_delete:\n",
    "        try:\n",
    "            f.unlink()\n",
    "        except Exception:\n",
    "            pass\n",
    "    if to_delete:\n",
    "        print(f\" pruned {len(to_delete)} old run(s)\")\n",
    "    else:\n",
    "        print(\" nothing to prune\")\n",
    "    return files[:keep]\n",
    "def _find_p95_in_run(run: Dict[str, Any]) -> Optional[float]:\n",
    "    win = run.get(\"matrix\", {}).get(\"strategy_counts_after\", {}).get(\"window\")\n",
    "    if isinstance(win, dict):\n",
    "        p95 = win.get(\"p95_ms\")\n",
    "        if isinstance(p95, (int, float)):\n",
    "            return float(p95)\n",
    "    base = run.get(\"gateway\", {}).get(\"base\")\n",
    "    if not base:\n",
    "        return None\n",
    "    try:\n",
    "        r = requests.get(f\"{base}/canary/status\", timeout=1.5)\n",
    "        js = r.json()\n",
    "        return js.get(\"window\", {}).get(\"p95_ms\")\n",
    "    except Exception:\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 544,
   "id": "e2a010f6-7d66-4b31-b7f4-30f5a0cbd302",
   "metadata": {},
   "outputs": [],
   "source": [
    "def aggregate_runs(max_runs: int = 50) -> Dict[str, Any]:\n",
    "    files = sorted(RUNS_DIR.glob(\"gateway_smoke_*.json\"), reverse=True)[:max_runs]\n",
    "    agg: List[Dict[str, Any]] = []\n",
    "    for f in files:\n",
    "        run = load_json(f)\n",
    "        ts = run.get(\"ts\")\n",
    "        chaos = run.get(\"gateway\", {}).get(\"chaos\", {})\n",
    "        p95 = _find_p95_in_run(run)\n",
    "        strat = run.get(\"matrix\", {}).get(\"strategy_counts_after\", {})\n",
    "        agg.append({\n",
    "            \"ts\": ts,\n",
    "            \"file\": f.name,\n",
    "            \"p95_ms\": p95,\n",
    "            \"chaos\": chaos,\n",
    "            \"strategy_counts\": strat,\n",
    "        })\n",
    "    out = {\"generated_ts\": int(time.time()), \"runs\": agg}\n",
    "    AGG_PATH.write_text(json.dumps(out, indent=2), encoding=\"utf-8\")\n",
    "    print(f\" wrote aggregated runs  {AGG_PATH}\")\n",
    "    return out\n",
    "def replay_scenario_from_run(\n",
    "    run_path: Path,\n",
    "    *,\n",
    "    scenario_label: str = \"default\",\n",
    "    override_base: Optional[str] = None,\n",
    ") -> Dict[str, Any]:\n",
    "    \"\"\"\n",
    "    Take an old run, pick 1 scenario, re-hit current gateway with same query/headers/ns.\n",
    "    Good for 'did the fix actually work?' checks.\n",
    "    \"\"\"\n",
    "    run = load_json(run_path)\n",
    "    base = override_base or _need_gateway()\n",
    "    scenarios = run.get(\"matrix\", {}).get(\"scenarios\", [])\n",
    "    chosen = None\n",
    "    for sc in scenarios:\n",
    "        if sc.get(\"label\") == scenario_label:\n",
    "            chosen = sc\n",
    "            break\n",
    "    if not chosen:\n",
    "        raise RuntimeError(f\"scenario '{scenario_label}' not found in {run_path.name}\")\n",
    "    query = chosen.get(\"raw\", {}).get(\"answer\") or chosen.get(\"raw\", {}).get(\"query\")\n",
    "    if not query:\n",
    "        query = \"replay smoke\"\n",
    "    headers = {}\n",
    "    if chosen.get(\"raw\", {}).get(\"meta\", {}).get(\"ns\"):\n",
    "        headers[\"x-namespace\"] = chosen[\"raw\"][\"meta\"][\"ns\"]\n",
    "    if chosen.get(\"raw\", {}).get(\"meta\", {}).get(\"route\"):\n",
    "        headers[\"x-route\"] = chosen[\"raw\"][\"meta\"][\"route\"]\n",
    "    t0 = time.time()\n",
    "    resp = gw_answer(query, headers=headers or None)\n",
    "    dt = (time.time() - t0) * 1000.0\n",
    "    print(f\"[replay] {run_path.name}:{scenario_label}  {dt:.1f} ms | meta={resp.get('meta', {})}\")\n",
    "    return {\n",
    "        \"run\": run_path.name,\n",
    "        \"label\": scenario_label,\n",
    "        \"lat_ms\": dt,\n",
    "        \"resp\": resp,\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 545,
   "id": "22b176ad-49dc-462f-9f6e-1afbfdd325f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def export_ci_summary() -> Path:\n",
    "    \"\"\"\n",
    "    Spits out a single JSON blob CI can read.\n",
    "    - latest p95\n",
    "    - latest strategy counts\n",
    "    - chaos toggles\n",
    "    - gateway base\n",
    "    \"\"\"\n",
    "    latest = ART_DIR / \"gateway_smoke.json\"\n",
    "    if not latest.exists():\n",
    "        raise RuntimeError(\"no gateway_smoke.json yet  run the smoke cell first\")\n",
    "    cur = json.loads(latest.read_text(encoding=\"utf-8\"))\n",
    "    base = cur.get(\"gateway\", {}).get(\"base\")\n",
    "    chaos = cur.get(\"gateway\", {}).get(\"chaos\")\n",
    "    strat = cur.get(\"matrix\", {}).get(\"strategy_counts_after\")\n",
    "    p95 = _find_p95_in_run(cur)\n",
    "    out = {\n",
    "        \"ts\": int(time.time()),\n",
    "        \"base\": base,\n",
    "        \"p95_ms\": p95,\n",
    "        \"chaos\": chaos,\n",
    "        \"strategy_counts\": strat,\n",
    "    }\n",
    "    CI_SUMMARY_PATH.write_text(json.dumps(out, indent=2), encoding=\"utf-8\")\n",
    "    print(f\" CI summary  {CI_SUMMARY_PATH}\")\n",
    "    return CI_SUMMARY_PATH"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 546,
   "id": "5a243f86-e255-425d-97d2-3c89dbcb7c37",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== maintenance / aggregation ===\n",
      " nothing to prune\n",
      " wrote aggregated runs  \\tmp\\art\\gateway_runs_agg.json\n",
      "\n",
      "=== export CI summary ===\n",
      " CI summary  \\tmp\\art\\gateway_ci_summary.json\n",
      "\n",
      "=== optional: replay latest archived scenario ===\n",
      "[replay] gateway_smoke_20251102-131650.json:default  3.0 ms | meta={}\n",
      "\n",
      " continuation+aggregation cell done.\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n=== maintenance / aggregation ===\")\n",
    "kept = prune_old_runs(keep=40)\n",
    "agg = aggregate_runs(max_runs=50)\n",
    "print(\"\\n=== export CI summary ===\")\n",
    "export_ci_summary()\n",
    "print(\"\\n=== optional: replay latest archived scenario ===\")\n",
    "if kept:\n",
    "    try:\n",
    "        replay_scenario_from_run(kept[0], scenario_label=\"default\")\n",
    "    except Exception as e:\n",
    "        print(\"replay failed:\", e)\n",
    "print(\"\\n continuation+aggregation cell done.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 547,
   "id": "910b5acd-dab1-4be1-9c23-317444041714",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " ART_DIR = \\tmp\\art\n",
      " RUNS_DIR = \\tmp\\art\\gateway_runs\n",
      " AGG_PATH = \\tmp\\art\\gateway_runs_agg.json\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "import time\n",
    "from pathlib import Path\n",
    "from typing import Any, Dict, List, Optional\n",
    "ART_DIR = Path(os.getenv(\"ART_DIR\", \"/tmp/art\"))\n",
    "RUNS_DIR = ART_DIR / \"gateway_runs\"\n",
    "AGG_PATH = ART_DIR / \"gateway_runs_agg.json\"\n",
    "LATEST = ART_DIR / \"gateway_smoke.json\"\n",
    "print(f\" ART_DIR = {ART_DIR}\")\n",
    "print(f\" RUNS_DIR = {RUNS_DIR}\")\n",
    "print(f\" AGG_PATH = {AGG_PATH}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 548,
   "id": "cd79b8ef-81a9-44b8-a394-e3c72535b458",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _load_json_safe(p: Path) -> Optional[Dict[str, Any]]:\n",
    "    if not p.exists():\n",
    "        return None\n",
    "    try:\n",
    "        return json.loads(p.read_text(encoding=\"utf-8\"))\n",
    "    except Exception as e:\n",
    "        print(f\" failed to read {p}: {e}\")\n",
    "        return None\n",
    "def list_archived_runs(limit: int = 30) -> List[Path]:\n",
    "    files = sorted(RUNS_DIR.glob(\"gateway_smoke_*.json\"), reverse=True)\n",
    "    return files[:limit]\n",
    "def show_history(limit: int = 15) -> List[Dict[str, Any]]:\n",
    "    agg = _load_json_safe(AGG_PATH)\n",
    "    if not agg:\n",
    "        print(\" no aggregated runs yet  run the big cell above first.\")\n",
    "        return []\n",
    "    rows = agg.get(\"runs\", [])[:limit]\n",
    "    print(\"\\n=== gateway history (newest first) ===\")\n",
    "    for r in rows:\n",
    "        ts = r.get(\"ts\")\n",
    "        ts_str = time.strftime(\"%Y-%m-%d %H:%M:%S\", time.localtime(ts)) if ts else \"?\"\n",
    "        p95 = r.get(\"p95_ms\")\n",
    "        chaos = r.get(\"chaos\", {}).get(\"chaos\", r.get(\"chaos\", {}))\n",
    "        cz = []\n",
    "        if chaos.get(\"disable_ce\"): cz.append(\"disable_ce\")\n",
    "        if chaos.get(\"force_bm25\"): cz.append(\"force_bm25\")\n",
    "        if chaos.get(\"verifier_v2\"): cz.append(\"verifier_v2\")\n",
    "        print(f\"- {ts_str} | p95={p95} ms | chaos={','.join(cz) if cz else '-'} | file={r.get('file')}\")\n",
    "    return rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 549,
   "id": "3bb737ee-27f1-447e-bdea-a27fff8259e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def detect_regression(threshold: float = 1.25) -> Optional[str]:\n",
    "    \"\"\"\n",
    "    super dumb regression detector:\n",
    "    - take last 8 p95 values\n",
    "    - last one must not be > median * threshold\n",
    "    \"\"\"\n",
    "    agg = _load_json_safe(AGG_PATH)\n",
    "    if not agg:\n",
    "        return \"no data\"\n",
    "    vals = [r.get(\"p95_ms\") for r in agg.get(\"runs\", []) if r.get(\"p95_ms\") is not None]\n",
    "    if len(vals) < 4:\n",
    "        return \"not enough data\"\n",
    "    latest = vals[0]\n",
    "    hist = vals[1:9]  \n",
    "    hist_clean = [v for v in hist if isinstance(v, (int, float))]\n",
    "    if not hist_clean:\n",
    "        return \"no past p95 to compare\"\n",
    "    hist_clean.sort()\n",
    "    mid = hist_clean[len(hist_clean)//2]\n",
    "    if latest is not None and mid is not None and latest > mid * threshold:\n",
    "        msg = f\" regression? latest p95={latest:.1f} ms > median({mid:.1f}) * {threshold}\"\n",
    "        print(msg)\n",
    "        return msg\n",
    "    print(f\" no obvious regression: latest={latest:.1f} ms, median={mid:.1f} ms\")\n",
    "    return None\n",
    "def diff_two_archives(idx_new: int = 0, idx_old: int = 1) -> None:\n",
    "    \"\"\"\n",
    "    Quick diff on two archived runs by index (0 = newest)\n",
    "    \"\"\"\n",
    "    files = list_archived_runs(limit=20)\n",
    "    if idx_new >= len(files) or idx_old >= len(files):\n",
    "        print(\" not enough archived runs\")\n",
    "        return\n",
    "    new_run = json.loads(files[idx_new].read_text(encoding=\"utf-8\"))\n",
    "    old_run = json.loads(files[idx_old].read_text(encoding=\"utf-8\"))\n",
    "    def _safe_p95_from_run(run: Dict[str, Any]) -> Optional[float]:\n",
    "        win = run.get(\"matrix\", {}).get(\"strategy_counts_after\", {}).get(\"window\")\n",
    "        if isinstance(win, dict):\n",
    "            return win.get(\"p95_ms\")\n",
    "        return None\n",
    "    p95_new = _safe_p95_from_run(new_run)\n",
    "    p95_old = _safe_p95_from_run(old_run)\n",
    "    chaos_new = new_run.get(\"gateway\", {}).get(\"chaos\", {})\n",
    "    chaos_old = old_run.get(\"gateway\", {}).get(\"chaos\", {})\n",
    "    strat_new = new_run.get(\"matrix\", {}).get(\"strategy_counts_after\", {})\n",
    "    strat_old = old_run.get(\"matrix\", {}).get(\"strategy_counts_after\", {})\n",
    "    print(\"\\n=== ARCHIVE DIFF ===\")\n",
    "    print(f\"new: {files[idx_new].name}\")\n",
    "    print(f\"old: {files[idx_old].name}\")\n",
    "    print(f\"p95: {p95_old}  {p95_new}\")\n",
    "    print(\"\\nchaos:\")\n",
    "    print(\"old:\", json.dumps(chaos_old, indent=2))\n",
    "    print(\"new:\", json.dumps(chaos_new, indent=2))\n",
    "    print(\"\\nstrategy counts:\")\n",
    "    print(\"old:\", json.dumps(strat_old, indent=2))\n",
    "    print(\"new:\", json.dumps(strat_new, indent=2))\n",
    "    print(\"=== end diff ===\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 550,
   "id": "45d6a881-3de8-4dfb-ba93-725000f1122d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== gateway history (newest first) ===\n",
      "- 2025-11-02 13:16:49 | p95=None ms | chaos=verifier_v2 | file=gateway_smoke_20251102-131650.json\n",
      "- 2025-10-30 10:06:05 | p95=None ms | chaos=verifier_v2 | file=gateway_smoke_20251030-100608.json\n",
      "- 2025-10-29 23:59:43 | p95=None ms | chaos=verifier_v2 | file=gateway_smoke_20251030-000220.json\n",
      "\n",
      "Tip:\n",
      "  diff_two_archives(0, 1)   # newest vs previous\n",
      "  stop_running_gateway_if_any()  # to kill bg gateway\n"
     ]
    }
   ],
   "source": [
    "def stop_running_gateway_if_any():\n",
    "    \"\"\"\n",
    "    If you started uvicorn in background via start_in_notebook(),\n",
    "    this will try to stop it.\n",
    "    \"\"\"\n",
    "    if \"stop_gateway\" in globals():\n",
    "        try:\n",
    "            stop_gateway()\n",
    "        except Exception as e:\n",
    "            print(\" stop_gateway failed:\", e)\n",
    "    else:\n",
    "        print(\" no stop_gateway() in globals\")\n",
    "history = show_history(limit=15)\n",
    "detect_regression(threshold=1.25)\n",
    "print(\"\\nTip:\")\n",
    "print(\"  diff_two_archives(0, 1)   # newest vs previous\")\n",
    "print(\"  stop_running_gateway_if_any()  # to kill bg gateway\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 551,
   "id": "17946ea4-c09a-48aa-832e-a99164dfb776",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " using ART_DIR=\\tmp\\art\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "import time\n",
    "from pathlib import Path\n",
    "from typing import Any, Dict, List, Optional\n",
    "import math\n",
    "ART_DIR = Path(os.getenv(\"ART_DIR\", \"/tmp/art\"))\n",
    "RUNS_DIR = ART_DIR / \"gateway_runs\"\n",
    "AGG_PATH = ART_DIR / \"gateway_runs_agg.json\"\n",
    "LATEST = ART_DIR / \"gateway_smoke.json\"\n",
    "CSV_EXPORT = ART_DIR / \"gateway_runs_agg.csv\"\n",
    "print(f\" using ART_DIR={ART_DIR}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 552,
   "id": "751e39b9-b04f-40d6-af6b-d459f5c8d6f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def annotate_latest(note: str, tag: Optional[str] = None) -> Optional[Path]:\n",
    "    \"\"\"\n",
    "    Add a human note to the latest run JSON.\n",
    "    This is so future-you knows *why* p95 spiked.\n",
    "    \"\"\"\n",
    "    if not LATEST.exists():\n",
    "        print(\" no latest run to annotate\")\n",
    "        return None\n",
    "    data = json.loads(LATEST.read_text(encoding=\"utf-8\"))\n",
    "    meta = data.setdefault(\"_notes\", [])\n",
    "    meta.append({\n",
    "        \"ts\": int(time.time()),\n",
    "        \"note\": note,\n",
    "        \"tag\": tag,\n",
    "    })\n",
    "    LATEST.write_text(json.dumps(data, indent=2), encoding=\"utf-8\")\n",
    "    print(f\" annotated latest run with: {note!r} (tag={tag})\")\n",
    "    return LATEST\n",
    "def annotate_archive(file: Path, note: str, tag: Optional[str] = None) -> None:\n",
    "    data = json.loads(file.read_text(encoding=\"utf-8\"))\n",
    "    meta = data.setdefault(\"_notes\", [])\n",
    "    meta.append({\n",
    "        \"ts\": int(time.time()),\n",
    "        \"note\": note,\n",
    "        \"tag\": tag,\n",
    "    })\n",
    "    file.write_text(json.dumps(data, indent=2), encoding=\"utf-8\")\n",
    "    print(f\" annotated {file.name} with: {note!r}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 553,
   "id": "b0ab89e1-28d0-44e5-8726-16b72dd682f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_archives(limit: int = 40) -> List[Dict[str, Any]]:\n",
    "    files = sorted(RUNS_DIR.glob(\"gateway_smoke_*.json\"), reverse=True)[:limit]\n",
    "    out = []\n",
    "    for f in files:\n",
    "        try:\n",
    "            js = json.loads(f.read_text(encoding=\"utf-8\"))\n",
    "            js[\"_file\"] = f\n",
    "            out.append(js)\n",
    "        except Exception as e:\n",
    "            print(f\" failed to load {f}: {e}\")\n",
    "    return out\n",
    "def _extract_p95(run: Dict[str, Any]) -> Optional[float]:\n",
    "    win = run.get(\"matrix\", {}).get(\"strategy_counts_after\", {}).get(\"window\")\n",
    "    if isinstance(win, dict):\n",
    "        p = win.get(\"p95_ms\")\n",
    "        if isinstance(p, (int, float)):\n",
    "            return float(p)\n",
    "    return None\n",
    "def find_slow_runs(threshold_ms: float = 1500.0, limit: int = 40) -> List[Dict[str, Any]]:\n",
    "    runs = load_archives(limit=limit)\n",
    "    slow = []\n",
    "    for r in runs:\n",
    "        p95 = _extract_p95(r)\n",
    "        if p95 is not None and p95 > threshold_ms:\n",
    "            slow.append({\"file\": r[\"_file\"].name, \"ts\": r.get(\"ts\"), \"p95_ms\": p95, \"chaos\": r.get(\"gateway\", {}).get(\"chaos\")})\n",
    "    if not slow:\n",
    "        print(f\" no runs over {threshold_ms} ms\")\n",
    "    else:\n",
    "        print(f\" slow runs (> {threshold_ms} ms):\")\n",
    "        for s in slow:\n",
    "            ts = s[\"ts\"]\n",
    "            ts_str = time.strftime(\"%Y-%m-%d %H:%M:%S\", time.localtime(ts)) if ts else \"?\"\n",
    "            print(f\"- {ts_str} | p95={s['p95_ms']} ms | chaos={s['chaos']} | {s['file']}\")\n",
    "    return slow\n",
    "def find_runs_with_chaos(flag: str, limit: int = 40) -> List[Dict[str, Any]]:\n",
    "    \"\"\"\n",
    "    flag  {\"disable_ce\", \"force_bm25\", \"verifier_v2\"}\n",
    "    \"\"\"\n",
    "    runs = load_archives(limit=limit)\n",
    "    hit = []\n",
    "    for r in runs:\n",
    "        cz = r.get(\"gateway\", {}).get(\"chaos\", {}).get(\"chaos\") or r.get(\"gateway\", {}).get(\"chaos\") or {}\n",
    "        if cz.get(flag):\n",
    "            hit.append({\"file\": r[\"_file\"].name, \"ts\": r.get(\"ts\"), \"p95_ms\": _extract_p95(r)})\n",
    "    if not hit:\n",
    "        print(f\" no runs with chaos.{flag}=true\")\n",
    "    else:\n",
    "        print(f\" runs with chaos.{flag}=true:\")\n",
    "        for h in hit:\n",
    "            ts = h[\"ts\"]\n",
    "            ts_str = time.strftime(\"%Y-%m-%d %H:%M:%S\", time.localtime(ts)) if ts else \"?\"\n",
    "            print(f\"- {ts_str} | p95={h['p95_ms']} ms | file={h['file']}\")\n",
    "    return hit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 554,
   "id": "9fdb7cc7-bd20-4bfe-b413-8e77b44ec007",
   "metadata": {},
   "outputs": [],
   "source": [
    "def export_agg_to_csv() -> Optional[Path]:\n",
    "    agg = None\n",
    "    try:\n",
    "        agg = json.loads(AGG_PATH.read_text(encoding=\"utf-8\"))\n",
    "    except Exception as e:\n",
    "        print(\" no agg json:\", e)\n",
    "        return None\n",
    "    rows = agg.get(\"runs\", [])\n",
    "    with CSV_EXPORT.open(\"w\", encoding=\"utf-8\") as f:\n",
    "        f.write(\"ts,ts_human,p95_ms,disable_ce,force_bm25,verifier_v2,file\\n\")\n",
    "        for r in rows:\n",
    "            ts = r.get(\"ts\")\n",
    "            ts_human = time.strftime(\"%Y-%m-%d %H:%M:%S\", time.localtime(ts)) if ts else \"\"\n",
    "            chaos = r.get(\"chaos\", {}).get(\"chaos\", r.get(\"chaos\", {}))\n",
    "            f.write(\n",
    "                f\"{ts},{ts_human},{r.get('p95_ms')},\"\n",
    "                f\"{int(bool(chaos.get('disable_ce')))},\"\n",
    "                f\"{int(bool(chaos.get('force_bm25')))},\"\n",
    "                f\"{int(bool(chaos.get('verifier_v2')))},\"\n",
    "                f\"{r.get('file','')}\\n\"\n",
    "            )\n",
    "    print(f\" CSV exported  {CSV_EXPORT}\")\n",
    "    return CSV_EXPORT\n",
    "def plot_p95_history(limit: int = 30):\n",
    "    \"\"\"\n",
    "    Quick and dirty chart. If you're in Jupyter it will just show up.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        import matplotlib.pyplot as plt\n",
    "    except Exception as e:\n",
    "        print(\" matplotlib not available:\", e)\n",
    "        return\n",
    "    agg = _load_json_safe(AGG_PATH)\n",
    "    if not agg:\n",
    "        print(\" no agg to plot\")\n",
    "        return\n",
    "    rows = agg.get(\"runs\", [])[:limit]\n",
    "    if not rows:\n",
    "        print(\" no rows\")\n",
    "        return\n",
    "    xs = list(range(len(rows)))[::-1]  \n",
    "    p95s = [r.get(\"p95_ms\") for r in rows][::-1]\n",
    "    labels = [time.strftime(\"%m-%d %H:%M\", time.localtime(r.get(\"ts\"))) if r.get(\"ts\") else \"\" for r in rows][::-1]\n",
    "    plt.figure()\n",
    "    plt.plot(xs, p95s, marker=\"o\")\n",
    "    plt.xticks(xs, labels, rotation=45, ha=\"right\")\n",
    "    plt.title(\"gateway p95 over recent runs\")\n",
    "    plt.ylabel(\"p95 (ms)\")\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 555,
   "id": "2088a795-02f1-43c2-aaa2-dc8388603e23",
   "metadata": {},
   "outputs": [],
   "source": [
    "def auto_heal_if_slow(max_p95_ms: float = 1800.0) -> None:\n",
    "    \"\"\"\n",
    "    - read latest\n",
    "    - if slow  suggest / apply chaos change\n",
    "    this doesn't do magic, just gives you the next lever\n",
    "    \"\"\"\n",
    "    latest = _load_json_safe(LATEST)\n",
    "    if not latest:\n",
    "        print(\" no latest.json, nothing to heal\")\n",
    "        return\n",
    "    p95 = None\n",
    "    win = latest.get(\"matrix\", {}).get(\"strategy_counts_after\", {}).get(\"window\")\n",
    "    if isinstance(win, dict):\n",
    "        p95 = win.get(\"p95_ms\")\n",
    "    if p95 is None:\n",
    "        print(\" latest has no p95, skipping\")\n",
    "        return\n",
    "    if p95 <= max_p95_ms:\n",
    "        print(f\" p95={p95:.1f} ms  {max_p95_ms} ms  no action\")\n",
    "        return\n",
    "    print(f\" p95={p95:.1f} ms > {max_p95_ms} ms  consider mitigation\")\n",
    "    base = None\n",
    "    gw = latest.get(\"gateway\", {})\n",
    "    base = gw.get(\"base\")\n",
    "    if base and \"gw_chaos_set\" in globals():\n",
    "        try:\n",
    "            print(\" toggling chaos.disable_ce=true on live gateway...\")\n",
    "            out = gw_chaos_set(disable_ce=True)\n",
    "            print(\"gateway said:\", out)\n",
    "        except Exception as e:\n",
    "            print(\" tried to toggle chaos but failed:\", e)\n",
    "    else:\n",
    "        print(\" no live gateway helper in scope, just fix it manually:\")\n",
    "        print(\"   curl -s -X POST $GW/admin/chaos/set -d '{\\\"disable_ce\\\":true}' -H 'content-type: application/json'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 556,
   "id": "c75e068f-f2eb-4e16-8a78-35b9919593b0",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== latest history ===\n",
      "\n",
      "=== gateway history (newest first) ===\n",
      "- 2025-11-02 13:16:49 | p95=None ms | chaos=verifier_v2 | file=gateway_smoke_20251102-131650.json\n",
      "- 2025-10-30 10:06:05 | p95=None ms | chaos=verifier_v2 | file=gateway_smoke_20251030-100608.json\n",
      "- 2025-10-29 23:59:43 | p95=None ms | chaos=verifier_v2 | file=gateway_smoke_20251030-000220.json\n",
      "\n",
      "=== detect regression (median-based) ===\n",
      "\n",
      "=== slow runs (>1500ms) ===\n",
      " no runs over 1500.0 ms\n",
      "\n",
      "=== runs with chaos.disable_ce ===\n",
      " no runs with chaos.disable_ce=true\n",
      "\n",
      "=== export CSV ===\n",
      " CSV exported  \\tmp\\art\\gateway_runs_agg.csv\n",
      "\n",
      "Tip: call plot_p95_history() to see a timeline.\n",
      "Tip: annotate_latest('testing new dense retriever', tag='exp-dense-v3')\n",
      "Tip: auto_heal_if_slow(1600.0)\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n=== latest history ===\")\n",
    "_ = show_history(limit=12)\n",
    "print(\"\\n=== detect regression (median-based) ===\")\n",
    "_ = detect_regression(threshold=1.25)\n",
    "print(\"\\n=== slow runs (>1500ms) ===\")\n",
    "_ = find_slow_runs(1500.0, limit=30)\n",
    "print(\"\\n=== runs with chaos.disable_ce ===\")\n",
    "_ = find_runs_with_chaos(\"disable_ce\", limit=30)\n",
    "print(\"\\n=== export CSV ===\")\n",
    "export_agg_to_csv()\n",
    "print(\"\\nTip: call plot_p95_history() to see a timeline.\")\n",
    "print(\"Tip: annotate_latest('testing new dense retriever', tag='exp-dense-v3')\")\n",
    "print(\"Tip: auto_heal_if_slow(1600.0)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 557,
   "id": "3b251665-21f0-4343-a6cd-bdfdf600f82f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " using ART_DIR=\\tmp\\art\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "import time\n",
    "from pathlib import Path\n",
    "from typing import Any, Dict, List, Optional\n",
    "ART_DIR = Path(os.getenv(\"ART_DIR\", \"/tmp/art\"))\n",
    "RUNS_DIR = ART_DIR / \"gateway_runs\"\n",
    "AGG_PATH = ART_DIR / \"gateway_runs_agg.json\"\n",
    "LATEST = ART_DIR / \"gateway_smoke.json\"\n",
    "REPORT_PATH = ART_DIR / \"gateway_report.md\"\n",
    "print(f\" using ART_DIR={ART_DIR}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 558,
   "id": "62f74554-dcac-46ea-8f30-664a2da7de7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _load_json_safe(p: Path) -> Optional[Dict[str, Any]]:\n",
    "    if not p.exists():\n",
    "        return None\n",
    "    try:\n",
    "        return json.loads(p.read_text(encoding=\"utf-8\"))\n",
    "    except Exception as e:\n",
    "        print(f\" failed to read {p}: {e}\")\n",
    "        return None\n",
    "def load_latest() -> Optional[Dict[str, Any]]:\n",
    "    return _load_json_safe(LATEST)\n",
    "def load_archives(limit: int = 40) -> List[Dict[str, Any]]:\n",
    "    files = sorted(RUNS_DIR.glob(\"gateway_smoke_*.json\"), reverse=True)[:limit]\n",
    "    out = []\n",
    "    for f in files:\n",
    "        try:\n",
    "            js = json.loads(f.read_text(encoding=\"utf-8\"))\n",
    "            js[\"_file\"] = f\n",
    "            out.append(js)\n",
    "        except Exception as e:\n",
    "            print(f\" failed to load {f}: {e}\")\n",
    "    return out\n",
    "def _extract_p95_from_run(run: Dict[str, Any]) -> Optional[float]:\n",
    "    win = run.get(\"matrix\", {}).get(\"strategy_counts_after\", {}).get(\"window\")\n",
    "    if isinstance(win, dict):\n",
    "        p = win.get(\"p95_ms\")\n",
    "        if isinstance(p, (int, float)):\n",
    "            return float(p)\n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 559,
   "id": "9ecebe12-f182-4162-b24d-646a15b62aa8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " using ART_DIR=\\tmp\\art\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "import time\n",
    "from pathlib import Path\n",
    "from typing import Any, Dict, List, Optional, Tuple\n",
    "ART_DIR = Path(os.getenv(\"ART_DIR\", \"/tmp/art\"))\n",
    "RUNS_DIR = ART_DIR / \"gateway_runs\"\n",
    "AGG_PATH = ART_DIR / \"gateway_runs_agg.json\"\n",
    "LATEST = ART_DIR / \"gateway_smoke.json\"\n",
    "REPORT_MD = ART_DIR / \"gateway_report.md\"\n",
    "REPORT_HTML = ART_DIR / \"gateway_report.html\"\n",
    "print(f\" using ART_DIR={ART_DIR}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 560,
   "id": "2184e1ac-9105-48eb-ad86-1a346f5ac7d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _read_json(path: Path) -> Optional[Dict[str, Any]]:\n",
    "    if not path.exists():\n",
    "        return None\n",
    "    try:\n",
    "        return json.loads(path.read_text(encoding=\"utf-8\"))\n",
    "    except Exception as e:\n",
    "        print(\" read failed:\", path, e)\n",
    "        return None\n",
    "def _ts_human(ts: Optional[int]) -> str:\n",
    "    if not ts:\n",
    "        return \"?\"\n",
    "    return time.strftime(\"%Y-%m-%d %H:%M:%S\", time.localtime(ts))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 561,
   "id": "8bcf6120-52d7-414f-8d9c-198659df5469",
   "metadata": {},
   "outputs": [],
   "source": [
    "def slice_runs(\n",
    "    since_ts: Optional[int] = None,\n",
    "    chaos_flag: Optional[str] = None,\n",
    "    max_items: int = 50,\n",
    ") -> List[Dict[str, Any]]:\n",
    "    \"\"\"\n",
    "    - since_ts: only keep runs with ts >= since_ts\n",
    "    - chaos_flag: filter to runs where chaos.<flag> == true\n",
    "    \"\"\"\n",
    "    agg = _read_json(AGG_PATH)\n",
    "    if not agg:\n",
    "        print(\" no agg json to slice\")\n",
    "        return []\n",
    "    rows = agg.get(\"runs\", [])\n",
    "    out: List[Dict[str, Any]] = []\n",
    "    for r in rows:\n",
    "        ts = r.get(\"ts\")\n",
    "        if since_ts is not None and (ts is None or ts < since_ts):\n",
    "            continue\n",
    "        chaos = r.get(\"chaos\", {}).get(\"chaos\", r.get(\"chaos\", {})) or {}\n",
    "        if chaos_flag:\n",
    "            if not chaos.get(chaos_flag, False):\n",
    "                continue\n",
    "        out.append(r)\n",
    "        if len(out) >= max_items:\n",
    "            break\n",
    "    print(f\" sliced  {len(out)} run(s)\")\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 562,
   "id": "f8bc1074-bc75-4ea6-b9a5-ff86775e3b35",
   "metadata": {},
   "outputs": [],
   "source": [
    "def detect_flappy_p95(window: int = 10, min_delta_ms: float = 250.0) -> List[Tuple[int, float, float]]:\n",
    "    \"\"\"\n",
    "    Return list of (index, prev_p95, cur_p95) where abs(diff) >= min_delta_ms,\n",
    "    meaning p95 is moving a lot run to run.\n",
    "    \"\"\"\n",
    "    agg = _read_json(AGG_PATH)\n",
    "    if not agg:\n",
    "        print(\" no agg json\")\n",
    "        return []\n",
    "    rows = agg.get(\"runs\", [])[:window]\n",
    "    deltas: List[Tuple[int, float, float]] = []\n",
    "    prev = None\n",
    "    for i, r in enumerate(rows):\n",
    "        p = r.get(\"p95_ms\")\n",
    "        if isinstance(p, (int, float)) and prev is not None:\n",
    "            if abs(p - prev) >= min_delta_ms:\n",
    "                deltas.append((i, prev, p))\n",
    "        if isinstance(p, (int, float)):\n",
    "            prev = p\n",
    "    if deltas:\n",
    "        print(\" flappy p95 detected:\")\n",
    "        for i, a, b in deltas:\n",
    "            print(f\"  - run[{i}] ={b-a:+.1f} ms\")\n",
    "    else:\n",
    "        print(\" no flappy p95 in recent window\")\n",
    "    return deltas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 563,
   "id": "3cccd2a8-f594-4ad6-ae72-a6bb0e503a77",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_html_report(limit: int = 30) -> Path:\n",
    "    agg = _read_json(AGG_PATH)\n",
    "    latest = _read_json(LATEST)\n",
    "    html: List[str] = []\n",
    "    html.append(\"<!doctype html><html><head><meta charset='utf-8'><title>Gateway Report</title>\")\n",
    "    html.append(\"<style>body{font-family:system-ui,Segoe UI,Roboto,Helvetica,Arial,sans-serif;padding:1.5rem;background:#0f172a;color:#e2e8f0}table{border-collapse:collapse;width:100%;margin-top:1rem}th,td{border:1px solid #1f2937;padding:0.5rem 0.75rem}th{background:#1f2937;text-align:left}code{background:#1f2937;padding:0.15rem 0.4rem;border-radius:4px}</style>\")\n",
    "    html.append(\"</head><body>\")\n",
    "    html.append(f\"<h1>Gateway Report</h1>\")\n",
    "    html.append(f\"<p><small>generated {time.strftime('%Y-%m-%d %H:%M:%S', time.localtime())}</small></p>\")\n",
    "    html.append(\"<h2>Latest run</h2>\")\n",
    "    if latest:\n",
    "        lp95 = (\n",
    "            latest.get(\"matrix\", {})\n",
    "            .get(\"strategy_counts_after\", {})\n",
    "            .get(\"window\", {})\n",
    "            .get(\"p95_ms\")\n",
    "        )\n",
    "        chaos = latest.get(\"gateway\", {}).get(\"chaos\", {}).get(\"chaos\") or latest.get(\"gateway\", {}).get(\"chaos\") or {}\n",
    "        base = latest.get(\"gateway\", {}).get(\"base\")\n",
    "        html.append(f\"<p>p95: <strong>{lp95}</strong> ms</p>\")\n",
    "        html.append(f\"<p>base: <code>{base}</code></p>\")\n",
    "        html.append(\"<p>chaos:</p>\")\n",
    "        html.append(\"<ul>\")\n",
    "        for k, v in (chaos or {}).items():\n",
    "            html.append(f\"<li>{k}: {v}</li>\")\n",
    "        html.append(\"</ul>\")\n",
    "    else:\n",
    "        html.append(\"<p><em>no latest.json</em></p>\")\n",
    "    html.append(\"<h2>Recent history</h2>\")\n",
    "    if agg:\n",
    "        rows = agg.get(\"runs\", [])[:limit]\n",
    "        html.append(\"<table>\")\n",
    "        html.append(\"<tr><th>when</th><th>p95 (ms)</th><th>chaos</th><th>file</th></tr>\")\n",
    "        for r in rows:\n",
    "            ts = r.get(\"ts\")\n",
    "            when = _ts_human(ts)\n",
    "            p95 = r.get(\"p95_ms\")\n",
    "            cz = r.get(\"chaos\", {}).get(\"chaos\", r.get(\"chaos\", {})) or {}\n",
    "            flags = \",\".join([k for k, v in cz.items() if v]) or \"-\"\n",
    "            html.append(f\"<tr><td>{when}</td><td>{p95}</td><td>{flags}</td><td>{r.get('file','')}</td></tr>\")\n",
    "        html.append(\"</table>\")\n",
    "    else:\n",
    "        html.append(\"<p><em>no agg.json yet</em></p>\")\n",
    "    html.append(\"<p style='margin-top:2rem;opacity:0.6;font-size:0.8rem'>generated by notebook gateway harness</p>\")\n",
    "    html.append(\"</body></html>\")\n",
    "    REPORT_HTML.write_text(\"\\n\".join(html), encoding=\"utf-8\")\n",
    "    print(f\" HTML report  {REPORT_HTML}\")\n",
    "    return REPORT_HTML"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 564,
   "id": "0bd69805-8957-4da8-9c7d-20ef68961998",
   "metadata": {},
   "outputs": [],
   "source": [
    "def last_n_slow(n: int = 5, over_ms: float = 1500.0) -> List[Dict[str, Any]]:\n",
    "    agg = _read_json(AGG_PATH)\n",
    "    if not agg:\n",
    "        print(\" no agg\")\n",
    "        return []\n",
    "    out = []\n",
    "    for r in agg.get(\"runs\", []):\n",
    "        p = r.get(\"p95_ms\")\n",
    "        if isinstance(p, (int, float)) and p > over_ms:\n",
    "            out.append(r)\n",
    "        if len(out) >= n:\n",
    "            break\n",
    "    if not out:\n",
    "        print(f\" no runs over {over_ms} ms\")\n",
    "    else:\n",
    "        print(f\" last slow runs (> {over_ms} ms):\")\n",
    "        for r in out:\n",
    "            print(f\"- {_ts_human(r.get('ts'))} | p95={r.get('p95_ms')} | file={r.get('file')}\")\n",
    "    return out\n",
    "def link_report_paths():\n",
    "    print(f\"md:   {REPORT_MD}\")\n",
    "    print(f\"html: {REPORT_HTML}\")\n",
    "    print(f\"json: {LATEST}\")\n",
    "    print(f\"agg:  {AGG_PATH}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 565,
   "id": "8c43f828-50f0-4d7d-96fa-388d93874198",
   "metadata": {},
   "outputs": [],
   "source": [
    "def df_from_agg(limit: int = 60):\n",
    "    try:\n",
    "        import pandas as pd\n",
    "    except Exception:\n",
    "        print(\" pandas not installed; skip df\")\n",
    "        return None\n",
    "    agg = _read_json(AGG_PATH)\n",
    "    if not agg:\n",
    "        print(\" no agg\")\n",
    "        return None\n",
    "    rows = agg.get(\"runs\", [])[:limit]\n",
    "    norm = []\n",
    "    for r in rows:\n",
    "        cz = r.get(\"chaos\", {}).get(\"chaos\", r.get(\"chaos\", {})) or {}\n",
    "        norm.append({\n",
    "            \"ts\": r.get(\"ts\"),\n",
    "            \"when\": _ts_human(r.get(\"ts\")),\n",
    "            \"p95_ms\": r.get(\"p95_ms\"),\n",
    "            \"disable_ce\": bool(cz.get(\"disable_ce\")),\n",
    "            \"force_bm25\": bool(cz.get(\"force_bm25\")),\n",
    "            \"verifier_v2\": bool(cz.get(\"verifier_v2\")),\n",
    "            \"file\": r.get(\"file\"),\n",
    "        })\n",
    "    df = pd.DataFrame(norm)\n",
    "    try:\n",
    "        from caas_jupyter_tools import display_dataframe_to_user\n",
    "        display_dataframe_to_user(\"gateway_runs\", df)\n",
    "    except Exception:\n",
    "        pass\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 566,
   "id": "87f3000b-ac16-4a15-b930-b19768c72bb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def suggest_fix_for_latest(max_p95_ms: float = 1600.0) -> Dict[str, Any]:\n",
    "    latest = _read_json(LATEST)\n",
    "    if not latest:\n",
    "        return {\"ok\": False, \"reason\": \"no latest\"}\n",
    "    win = latest.get(\"matrix\", {}).get(\"strategy_counts_after\", {}).get(\"window\") or {}\n",
    "    p95 = win.get(\"p95_ms\")\n",
    "    chaos = latest.get(\"gateway\", {}).get(\"chaos\", {}).get(\"chaos\") or latest.get(\"gateway\", {}).get(\"chaos\") or {}\n",
    "    if not isinstance(p95, (int, float)):\n",
    "        return {\"ok\": False, \"reason\": \"no p95\"}\n",
    "    if p95 <= max_p95_ms:\n",
    "        print(f\" latest p95={p95:.1f}  {max_p95_ms} ms  no changes\")\n",
    "        return {\"ok\": True, \"p95\": p95, \"action\": \"none\"}\n",
    "    hints = []\n",
    "    if not chaos.get(\"disable_ce\"):\n",
    "        hints.append(\"POST /admin/chaos/set {\\\"disable_ce\\\": true}\")\n",
    "    if not chaos.get(\"force_bm25\"):\n",
    "        hints.append(\"POST /admin/chaos/set {\\\"force_bm25\\\": true}\")\n",
    "    if not hints:\n",
    "        hints.append(\"reduce STEP_TIMEOUT_* envs or check RAG backend\")\n",
    "    out = {\n",
    "        \"ok\": True,\n",
    "        \"p95\": p95,\n",
    "        \"suggest\": hints,\n",
    "    }\n",
    "    print(\" high p95  suggestions:\")\n",
    "    for h in hints:\n",
    "        print(\"  -\", h)\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 567,
   "id": "bf7b2ac9-6d02-4547-84e0-f0fd1ef6e4fe",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== recent runs (min view) ===\n",
      " sliced  3 run(s)\n",
      "\n",
      "=== flappy detector ===\n",
      " no flappy p95 in recent window\n",
      "\n",
      "=== build HTML ===\n",
      " HTML report  \\tmp\\art\\gateway_report.html\n",
      "\n",
      "=== last slow ===\n",
      " no runs over 1500.0 ms\n",
      "\n",
      "=== suggestion for latest ===\n",
      "\n",
      "use link_report_paths() to see where stuff was written\n",
      "md:   \\tmp\\art\\gateway_report.md\n",
      "html: \\tmp\\art\\gateway_report.html\n",
      "json: \\tmp\\art\\gateway_smoke.json\n",
      "agg:  \\tmp\\art\\gateway_runs_agg.json\n",
      "\n",
      " continuation block done.\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n=== recent runs (min view) ===\")\n",
    "_ = slice_runs(max_items=10)\n",
    "print(\"\\n=== flappy detector ===\")\n",
    "_ = detect_flappy_p95(window=12, min_delta_ms=250.0)\n",
    "print(\"\\n=== build HTML ===\")\n",
    "_ = build_html_report(limit=25)\n",
    "print(\"\\n=== last slow ===\")\n",
    "_ = last_n_slow(n=5, over_ms=1500.0)\n",
    "print(\"\\n=== suggestion for latest ===\")\n",
    "_ = suggest_fix_for_latest(max_p95_ms=1600.0)\n",
    "print(\"\\nuse link_report_paths() to see where stuff was written\")\n",
    "link_report_paths()\n",
    "print(\"\\n continuation block done.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 568,
   "id": "bb6657f0-9307-4c32-939f-3ea03744fdca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " using ART_DIR=\\tmp\\art\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "import time\n",
    "from pathlib import Path\n",
    "from typing import Any, Dict, List, Optional\n",
    "ART_DIR = Path(os.getenv(\"ART_DIR\", \"/tmp/art\"))\n",
    "RUNS_DIR = ART_DIR / \"gateway_runs\"\n",
    "LATEST = ART_DIR / \"gateway_smoke.json\"\n",
    "EXP_PATH = ART_DIR / \"gateway_experiments.json\"\n",
    "print(f\" using ART_DIR={ART_DIR}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 569,
   "id": "87647bea-21f8-4ed8-8f6b-8e4a5617e1ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _read_json(path: Path) -> Optional[Dict[str, Any]]:\n",
    "    if not path.exists():\n",
    "        return None\n",
    "    try:\n",
    "        return json.loads(path.read_text(encoding=\"utf-8\"))\n",
    "    except Exception as e:\n",
    "        print(f\" failed to read {path}: {e}\")\n",
    "        return None\n",
    "def _now() -> int:\n",
    "    return int(time.time())\n",
    "def _ts_human(ts: int) -> str:\n",
    "    return time.strftime(\"%Y-%m-%d %H:%M:%S\", time.localtime(ts))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 570,
   "id": "88e69078-b77a-4ed2-8e5a-7b5bd848261f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_latest_run() -> Optional[Dict[str, Any]]:\n",
    "    return _read_json(LATEST)\n",
    "def load_prev_run() -> Optional[Dict[str, Any]]:\n",
    "    files = sorted(RUNS_DIR.glob(\"gateway_smoke_*.json\"), reverse=True)\n",
    "    if len(files) < 2:\n",
    "        return None\n",
    "    return json.loads(files[1].read_text(encoding=\"utf-8\"))\n",
    "def extract_p95(run: Dict[str, Any]) -> Optional[float]:\n",
    "    win = (\n",
    "        run.get(\"matrix\", {})\n",
    "           .get(\"strategy_counts_after\", {})\n",
    "           .get(\"window\", {})\n",
    "    )\n",
    "    if isinstance(win, dict):\n",
    "        p = win.get(\"p95_ms\")\n",
    "        if isinstance(p, (int, float)):\n",
    "            return float(p)\n",
    "    return None\n",
    "def health_summary(max_p95_ms: float = 1800.0) -> Dict[str, Any]:\n",
    "    curr = load_latest_run()\n",
    "    prev = load_prev_run()\n",
    "    if not curr:\n",
    "        print(\" no latest run yet\")\n",
    "        return {\"ok\": False, \"reason\": \"no-latest\"}\n",
    "    p_curr = extract_p95(curr)\n",
    "    p_prev = extract_p95(prev) if prev else None\n",
    "    chaos = (\n",
    "        curr.get(\"gateway\", {})\n",
    "            .get(\"chaos\", {})\n",
    "            .get(\"chaos\")\n",
    "        or curr.get(\"gateway\", {}).get(\"chaos\")\n",
    "        or {}\n",
    "    )\n",
    "    ok = p_curr is not None and p_curr <= max_p95_ms\n",
    "    out = {\n",
    "        \"ts\": _now(),\n",
    "        \"p95_curr\": p_curr,\n",
    "        \"p95_prev\": p_prev,\n",
    "        \"chaos\": chaos,\n",
    "        \"ok\": ok,\n",
    "        \"max_allowed\": max_p95_ms,\n",
    "    }\n",
    "    print(\"\\n=== gateway health ===\")\n",
    "    print(f\"current p95: {p_curr} ms\")\n",
    "    print(f\"previous p95: {p_prev} ms\")\n",
    "    print(f\"chaos: {chaos}\")\n",
    "    print(\"status:\", \" OK\" if ok else \" SLOW\")\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 571,
   "id": "13404385-34dd-4872-b2ab-98a062fb47d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def chaos_drill(\n",
    "    query: str = \"explain RAG fan-out pipeline\",\n",
    "    disable_ce: bool = True,\n",
    "    force_bm25: bool = False,\n",
    "    ns: str = \"default\",\n",
    ") -> Dict[str, Any]:\n",
    "    if \"gw_answer\" not in globals():\n",
    "        print(\" no live gateway funcs, skipping actual hits  just returning plan\")\n",
    "        return {\n",
    "            \"ok\": True,\n",
    "            \"plan\": {\n",
    "                \"set\": {\"disable_ce\": disable_ce, \"force_bm25\": force_bm25},\n",
    "                \"hit\": query,\n",
    "                \"unset\": {\"disable_ce\": False, \"force_bm25\": False},\n",
    "            },\n",
    "        }\n",
    "    print(\"\\n=== chaos drill ===\")\n",
    "    before = gw_chaos_get()\n",
    "    print(\"before:\", json.dumps(before, indent=2))\n",
    "    after_set = gw_chaos_set(disable_ce=disable_ce, force_bm25=force_bm25)\n",
    "    print(\"after set:\", json.dumps(after_set, indent=2))\n",
    "    t0 = time.time()\n",
    "    resp = gw_answer(query, ns=ns, top_k=3)\n",
    "    dt = (time.time() - t0) * 1000.0\n",
    "    print(f\"hit  {dt:.1f} ms | meta={resp.get('meta',{})}\")\n",
    "    _ = gw_chaos_set(disable_ce=False, force_bm25=False)\n",
    "    final = gw_chaos_get()\n",
    "    return {\n",
    "        \"ok\": True,\n",
    "        \"lat_ms\": dt,\n",
    "        \"before\": before,\n",
    "        \"after_set\": after_set,\n",
    "        \"resp\": resp,\n",
    "        \"final\": final,\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 572,
   "id": "8f9aba94-90cc-4a48-b387-ce42fddc87b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_perf_experiment(\n",
    "    name: str,\n",
    "    q: str = \"explain RAG fan-out pipeline\",\n",
    "    n: int = 25,\n",
    "    threads: int = 4,\n",
    "    chaos: Optional[Dict[str, bool]] = None,\n",
    ") -> Dict[str, Any]:\n",
    "    \"\"\"\n",
    "    - optionally set chaos\n",
    "    - run loadgen()\n",
    "    - restore chaos\n",
    "    - save to gateway_experiments.json\n",
    "    \"\"\"\n",
    "    if \"loadgen\" not in globals():\n",
    "        raise RuntimeError(\"loadgen(...) not in scope, run the previous cell first\")\n",
    "    print(f\"\\n=== perf experiment: {name} ===\")\n",
    "    chaos_before = gw_chaos_get() if \"gw_chaos_get\" in globals() else None\n",
    "    if chaos:\n",
    "        print(\" applying chaos:\", chaos)\n",
    "        gw_chaos_set(**chaos)\n",
    "    lat_sorted = loadgen(q=q, n=n, threads=threads)\n",
    "    if chaos:\n",
    "        print(\" restoring chaos\")\n",
    "        gw_chaos_set(disable_ce=False, force_bm25=False, verifier_v2=False)\n",
    "    rec = {\n",
    "        \"ts\": _now(),\n",
    "        \"name\": name,\n",
    "        \"query\": q,\n",
    "        \"n\": n,\n",
    "        \"threads\": threads,\n",
    "        \"chaos\": chaos or {},\n",
    "        \"p95_ms\": lat_sorted[int(len(lat_sorted) * 0.95) - 1] if lat_sorted else None,\n",
    "        \"avg_ms\": sum(lat_sorted) / len(lat_sorted) if lat_sorted else None,\n",
    "    }\n",
    "    exp: List[Dict[str, Any]] = []\n",
    "    if EXP_PATH.exists():\n",
    "        try:\n",
    "            exp = json.loads(EXP_PATH.read_text(encoding=\"utf-8\"))\n",
    "        except Exception:\n",
    "            exp = []\n",
    "    exp.insert(0, rec)\n",
    "    EXP_PATH.write_text(json.dumps(exp, indent=2), encoding=\"utf-8\")\n",
    "    print(f\" wrote experiment  {EXP_PATH}\")\n",
    "    return rec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 573,
   "id": "d16a5a76-e898-407f-bd4c-12a9be5ce7df",
   "metadata": {},
   "outputs": [],
   "source": [
    "def curl_snippets(base: Optional[str] = None) -> None:\n",
    "    if base is None:\n",
    "        base = _need_gateway() if \"_need_gateway\" in globals() else \"http://127.0.0.1:9910\"\n",
    "    print(\"\\n=== curl ===\")\n",
    "    print(f\"GW={base}\")\n",
    "    print(\"curl -s $GW/up | jq\")\n",
    "    print(\"curl -s $GW/metrics | head -200\")\n",
    "    print(\"curl -s -X POST $GW/answer -H 'content-type: application/json' -d '{\\\"query\\\": \\\"smoke\\\", \\\"top_k\\\": 2}' | jq\")\n",
    "    print(\"curl -s $GW/admin/chaos/get | jq\")\n",
    "    print(\"curl -s -X POST $GW/admin/chaos/set -H 'content-type: application/json' -d '{\\\"disable_ce\\\": true}' | jq\")\n",
    "    print(\"curl -s -X POST $GW/rl/update -H 'content-type: application/json' -d '{\\\"weights\\\": {\\\"factual\\\": 0.5}}' | jq\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 574,
   "id": "b0064a27-d158-430d-8b20-4ed09d4b21e7",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== gateway health ===\n",
      "current p95: None ms\n",
      "previous p95: None ms\n",
      "chaos: {'disable_ce': False, 'force_bm25': False, 'verifier_v2': True}\n",
      "status:  SLOW\n",
      "\n",
      "=== curl ===\n",
      "GW=http://127.0.0.1:9910\n",
      "curl -s $GW/up | jq\n",
      "curl -s $GW/metrics | head -200\n",
      "curl -s -X POST $GW/answer -H 'content-type: application/json' -d '{\"query\": \"smoke\", \"top_k\": 2}' | jq\n",
      "curl -s $GW/admin/chaos/get | jq\n",
      "curl -s -X POST $GW/admin/chaos/set -H 'content-type: application/json' -d '{\"disable_ce\": true}' | jq\n",
      "curl -s -X POST $GW/rl/update -H 'content-type: application/json' -d '{\"weights\": {\"factual\": 0.5}}' | jq\n",
      "\n",
      "=== quick A/B (rerank vs chaos.disable_ce) ===\n",
      "A pipeline: None\n",
      "B pipeline: None\n",
      "\n",
      " continuation cell loaded. run `run_perf_experiment(...)` to log more runs.\n"
     ]
    }
   ],
   "source": [
    "health = health_summary(max_p95_ms=1800.0)\n",
    "curl_snippets()\n",
    "print(\"\\n=== quick A/B (rerank vs chaos.disable_ce) ===\")\n",
    "if \"gw_answer\" in globals():\n",
    "    a = gw_answer(\"how to build a RAG gateway?\", top_k=3)\n",
    "    gw_chaos_set(disable_ce=True)\n",
    "    b = gw_answer(\"how to build a RAG gateway?\", top_k=3)\n",
    "    gw_chaos_set(disable_ce=False)\n",
    "    print(\"A pipeline:\", a.get(\"meta\", {}).get(\"pipeline\"))\n",
    "    print(\"B pipeline:\", b.get(\"meta\", {}).get(\"pipeline\"))\n",
    "else:\n",
    "    print(\" live gateway not in scope, skipping A/B\")\n",
    "print(\"\\n continuation cell loaded. run `run_perf_experiment(...)` to log more runs.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 575,
   "id": "9c9b51e9-9ade-4aa3-a96f-9a37de98e428",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " using EXP_PATH=\\tmp\\art\\gateway_experiments.json\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "import time\n",
    "from pathlib import Path\n",
    "from typing import Any, Dict, List, Optional\n",
    "ART_DIR = Path(os.getenv(\"ART_DIR\", \"/tmp/art\"))\n",
    "EXP_PATH = ART_DIR / \"gateway_experiments.json\"\n",
    "EXP_REPORT_MD = ART_DIR / \"gateway_experiments.md\"\n",
    "print(f\" using EXP_PATH={EXP_PATH}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 576,
   "id": "a5bdad66-0504-4b7f-8a5b-8173e0a7d50a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _exp_load() -> List[Dict[str, Any]]:\n",
    "    if not EXP_PATH.exists():\n",
    "        return []\n",
    "    try:\n",
    "        data = json.loads(EXP_PATH.read_text(encoding=\"utf-8\"))\n",
    "        if isinstance(data, list):\n",
    "            return data\n",
    "    except Exception as e:\n",
    "        print(\" failed to read experiments:\", e)\n",
    "    return []\n",
    "def _exp_save(exps: List[Dict[str, Any]]) -> None:\n",
    "    EXP_PATH.write_text(json.dumps(exps, indent=2), encoding=\"utf-8\")\n",
    "def list_experiments(limit: int = 25) -> List[Dict[str, Any]]:\n",
    "    exps = _exp_load()\n",
    "    exps = exps[:limit]\n",
    "    print(\"\\n=== experiments (newest first) ===\")\n",
    "    for i, e in enumerate(exps):\n",
    "        ts = e.get(\"ts\")\n",
    "        when = time.strftime(\"%Y-%m-%d %H:%M:%S\", time.localtime(ts)) if ts else \"?\"\n",
    "        print(\n",
    "            f\"{i:2d}. {when} | {e.get('name')} | p95={e.get('p95_ms')} ms | avg={e.get('avg_ms')} ms | chaos={e.get('chaos')}\"\n",
    "        )\n",
    "    return exps\n",
    "def filter_experiments(name: str) -> List[Dict[str, Any]]:\n",
    "    exps = _exp_load()\n",
    "    hits = [e for e in exps if e.get(\"name\") == name]\n",
    "    print(f\"\\n=== experiments for name={name!r} ===\")\n",
    "    for e in hits:\n",
    "        ts = e.get(\"ts\")\n",
    "        when = time.strftime(\"%Y-%m-%d %H:%M:%S\", time.localtime(ts)) if ts else \"?\"\n",
    "        print(f\"- {when} | p95={e.get('p95_ms')} ms | chaos={e.get('chaos')}\")\n",
    "    if not hits:\n",
    "        print(\"(none)\")\n",
    "    return hits\n",
    "def best_experiment(name: Optional[str] = None) -> Optional[Dict[str, Any]]:\n",
    "    exps = _exp_load()\n",
    "    if name is not None:\n",
    "        exps = [e for e in exps if e.get(\"name\") == name]\n",
    "    exps = [e for e in exps if e.get(\"p95_ms\") is not None]\n",
    "    if not exps:\n",
    "        print(\" no experiments found for that filter\")\n",
    "        return None\n",
    "    best = min(exps, key=lambda e: e[\"p95_ms\"])\n",
    "    print(\n",
    "        f\" best: {best.get('name')} @ p95={best.get('p95_ms')} ms | chaos={best.get('chaos')} | ts={best.get('ts')}\"\n",
    "    )\n",
    "    return best\n",
    "def compare_experiments(idx_a: int = 0, idx_b: int = 1) -> None:\n",
    "    exps = _exp_load()\n",
    "    if idx_a >= len(exps) or idx_b >= len(exps):\n",
    "        print(\" not enough experiments to compare\")\n",
    "        return\n",
    "    a = exps[idx_a]\n",
    "    b = exps[idx_b]\n",
    "    def _when(e): \n",
    "        return time.strftime(\"%Y-%m-%d %H:%M:%S\", time.localtime(e.get(\"ts\"))) if e.get(\"ts\") else \"?\"\n",
    "    print(\"\\n=== EXPERIMENT DIFF ===\")\n",
    "    print(f\"A[{idx_a}] {a.get('name')} @ {_when(a)}\")\n",
    "    print(f\"  p95={a.get('p95_ms')} ms | avg={a.get('avg_ms')} ms | chaos={a.get('chaos')}\")\n",
    "    print(f\"B[{idx_b}] {b.get('name')} @ {_when(b)}\")\n",
    "    print(f\"  p95={b.get('p95_ms')} ms | avg={b.get('avg_ms')} ms | chaos={b.get('chaos')}\")\n",
    "    if a.get(\"p95_ms\") is not None and b.get(\"p95_ms\") is not None:\n",
    "        delta = b[\"p95_ms\"] - a[\"p95_ms\"]\n",
    "        sign = \" faster\" if delta < 0 else \" slower\"\n",
    "        print(f\"\\n p95 = {delta:.1f} ms  {sign}\")\n",
    "    print(\"=== end diff ===\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 577,
   "id": "76013f95-4702-42b6-bdbe-96c0f50e85ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_default_suite(q: str = \"explain RAG fan-out pipeline\") -> List[Dict[str, Any]]:\n",
    "    \"\"\"\n",
    "    1. baseline\n",
    "    2. chaos.disable_ce=true\n",
    "    3. chaos.force_bm25=true\n",
    "    \"\"\"\n",
    "    if \"run_perf_experiment\" not in globals():\n",
    "        raise RuntimeError(\"run_perf_experiment(...) not defined  run previous cell first\")\n",
    "    results: List[Dict[str, Any]] = []\n",
    "    print(\"\\n suite: baseline\")\n",
    "    results.append(\n",
    "        run_perf_experiment(\n",
    "            name=\"baseline\",\n",
    "            q=q,\n",
    "            n=20,\n",
    "            threads=3,\n",
    "            chaos=None,\n",
    "        )\n",
    "    )\n",
    "    print(\"\\n suite: chaos.disable_ce\")\n",
    "    results.append(\n",
    "        run_perf_experiment(\n",
    "            name=\"chaos_disable_ce\",\n",
    "            q=q,\n",
    "            n=20,\n",
    "            threads=3,\n",
    "            chaos={\"disable_ce\": True},\n",
    "        )\n",
    "    )\n",
    "    print(\"\\n suite: chaos.force_bm25\")\n",
    "    results.append(\n",
    "        run_perf_experiment(\n",
    "            name=\"chaos_force_bm25\",\n",
    "            q=q,\n",
    "            n=20,\n",
    "            threads=3,\n",
    "            chaos={\"force_bm25\": True},\n",
    "        )\n",
    "    )\n",
    "    print(\"\\n suite done\")\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 578,
   "id": "be5d2f10-0495-42d4-a31b-673a3dbc43a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def export_experiments_md(limit: int = 40) -> Path:\n",
    "    exps = _exp_load()[:limit]\n",
    "    lines: List[str] = []\n",
    "    lines.append(\"# Gateway Perf Experiments\")\n",
    "    lines.append(f\"_generated: {time.strftime('%Y-%m-%d %H:%M:%S', time.localtime())}_\")\n",
    "    lines.append(\"\")\n",
    "    if not exps:\n",
    "        lines.append(\"_no experiments yet_\")\n",
    "    else:\n",
    "        lines.append(\"| when | name | p95 (ms) | avg (ms) | chaos |\")\n",
    "        lines.append(\"|------|------|----------|----------|--------|\")\n",
    "        for e in exps:\n",
    "            ts = e.get(\"ts\")\n",
    "            when = time.strftime(\"%Y-%m-%d %H:%M:%S\", time.localtime(ts)) if ts else \"?\"\n",
    "            chaos = e.get(\"chaos\") or {}\n",
    "            chaos_flags = \",\".join([k for k, v in chaos.items() if v]) or \"-\"\n",
    "            lines.append(\n",
    "                f\"| {when} | {e.get('name','-')} | {e.get('p95_ms')} | {e.get('avg_ms')} | {chaos_flags} |\"\n",
    "            )\n",
    "    EXP_REPORT_MD.write_text(\"\\n\".join(lines), encoding=\"utf-8\")\n",
    "    print(f\" experiments report  {EXP_REPORT_MD}\")\n",
    "    return EXP_REPORT_MD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 579,
   "id": "f786fa63-319c-4dfb-837d-b2f48601198d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_experiments(name_filter: Optional[str] = None, limit: int = 50) -> None:\n",
    "    \"\"\"\n",
    "    super quick trend view  only if matplotlib is around\n",
    "    \"\"\"\n",
    "    try:\n",
    "        import matplotlib.pyplot as plt\n",
    "    except Exception as e:\n",
    "        print(\" matplotlib not available:\", e)\n",
    "        return\n",
    "    exps = _exp_load()[:limit]\n",
    "    if name_filter:\n",
    "        exps = [e for e in exps if e.get(\"name\") == name_filter]\n",
    "    if not exps:\n",
    "        print(\" no experiments for that filter\")\n",
    "        return\n",
    "    xs = list(range(len(exps)))[::-1]\n",
    "    p95s = [e.get(\"p95_ms\") for e in exps][::-1]\n",
    "    labels = [\n",
    "        time.strftime(\"%m-%d %H:%M\", time.localtime(e.get(\"ts\"))) if e.get(\"ts\") else \"\"\n",
    "        for e in exps\n",
    "    ][::-1]\n",
    "    plt.figure()\n",
    "    plt.plot(xs, p95s, marker=\"o\")\n",
    "    plt.xticks(xs, labels, rotation=45, ha=\"right\")\n",
    "    plt.title(f\"gateway experiments p95 ({name_filter or 'all'})\")\n",
    "    plt.ylabel(\"p95 (ms)\")\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 580,
   "id": "0c4d8990-c612-47e6-9d56-da83f0260d20",
   "metadata": {},
   "outputs": [],
   "source": [
    "def apply_best_chaos_from_experiments(name: str) -> Dict[str, Any]:\n",
    "    \"\"\"\n",
    "    Find best run for `name` and push that chaos to the live gateway.\n",
    "    \"\"\"\n",
    "    best = best_experiment(name)\n",
    "    if not best:\n",
    "        return {\"ok\": False, \"reason\": \"no-best\"}\n",
    "    chaos = best.get(\"chaos\") or {}\n",
    "    if not chaos:\n",
    "        print(\" best run had no chaos  nothing to apply\")\n",
    "        return {\"ok\": True, \"applied\": {}}\n",
    "    if \"gw_chaos_set\" not in globals():\n",
    "        print(\" live gateway funcs not in scope, cannot apply\")\n",
    "        return {\"ok\": False, \"reason\": \"no-gateway\"}\n",
    "    print(f\" applying chaos from best run: {chaos}\")\n",
    "    out = gw_chaos_set(**chaos)\n",
    "    return {\"ok\": True, \"applied\": chaos, \"gateway\": out}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 581,
   "id": "a534d60c-2785-4918-9a17-224025961ded",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== experiment dashboard ===\n",
      "\n",
      "=== experiments (newest first) ===\n",
      " experiments report  \\tmp\\art\\gateway_experiments.md\n",
      "Tip:\n",
      "  run_default_suite()                    # baseline + 2 chaos variants\n",
      "  filter_experiments('baseline')         # see baselines\n",
      "  best_experiment('chaos_disable_ce')    # find fastest chaos\n",
      "  apply_best_chaos_from_experiments('chaos_disable_ce')\n",
      "  plot_experiments()                     # if matplotlib present\n",
      "\n",
      " continuation loaded.\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n=== experiment dashboard ===\")\n",
    "_ = list_experiments(limit=20)\n",
    "export_experiments_md(limit=40)\n",
    "print(\"Tip:\")\n",
    "print(\"  run_default_suite()                    # baseline + 2 chaos variants\")\n",
    "print(\"  filter_experiments('baseline')         # see baselines\")\n",
    "print(\"  best_experiment('chaos_disable_ce')    # find fastest chaos\")\n",
    "print(\"  apply_best_chaos_from_experiments('chaos_disable_ce')\")\n",
    "print(\"  plot_experiments()                     # if matplotlib present\")\n",
    "print(\"\\n continuation loaded.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 582,
   "id": "282b532d-bbae-4817-8a55-4c434ece05b6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " (cont) using ART_DIR=\\tmp\\art\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "import time\n",
    "from pathlib import Path\n",
    "from typing import Any, Dict, List, Optional, Tuple\n",
    "ART_DIR = Path(os.getenv(\"ART_DIR\", \"/tmp/art\"))\n",
    "EXP_PATH = ART_DIR / \"gateway_experiments.json\"\n",
    "EXP_HTML = ART_DIR / \"gateway_experiments.html\"\n",
    "EXP_CI = ART_DIR / \"gateway_experiments_ci.json\"\n",
    "print(f\" (cont) using ART_DIR={ART_DIR}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 583,
   "id": "d9c889d2-9884-4897-b64b-c9d772d50c0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _exp_load_all() -> List[Dict[str, Any]]:\n",
    "    if not EXP_PATH.exists():\n",
    "        return []\n",
    "    try:\n",
    "        data = json.loads(EXP_PATH.read_text(encoding=\"utf-8\"))\n",
    "        return data if isinstance(data, list) else []\n",
    "    except Exception as e:\n",
    "        print(\" failed to read experiments:\", e)\n",
    "        return []\n",
    "def _exp_append(rec: Dict[str, Any]) -> None:\n",
    "    exps = _exp_load_all()\n",
    "    exps.insert(0, rec)\n",
    "    EXP_PATH.write_text(json.dumps(exps, indent=2), encoding=\"utf-8\")\n",
    "def _ts_human(ts: Optional[int]) -> str:\n",
    "    if not ts:\n",
    "        return \"?\"\n",
    "    return time.strftime(\"%Y-%m-%d %H:%M:%S\", time.localtime(ts))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 584,
   "id": "703f67cb-c11d-4319-9f49-1ea6b2be5455",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_experiment_matrix(\n",
    "    name_prefix: str = \"matrix\",\n",
    "    queries: Optional[List[str]] = None,\n",
    "    chaos_variants: Optional[List[Dict[str, bool]]] = None,\n",
    "    n: int = 20,\n",
    "    threads: int = 3,\n",
    ") -> List[Dict[str, Any]]:\n",
    "    \"\"\"\n",
    "    Run several experiments in a grid:\n",
    "      for q in queries:\n",
    "        for chaos in chaos_variants:\n",
    "            run_perf_experiment(...)\n",
    "    Returns list of experiment records (newest first).\n",
    "    \"\"\"\n",
    "    if \"run_perf_experiment\" not in globals():\n",
    "        raise RuntimeError(\"run_perf_experiment(...) not defined  run the previous cell first\")\n",
    "\n",
    "    if not queries:\n",
    "        queries = [\n",
    "            \"explain RAG fan-out pipeline\",\n",
    "            \"SELECT id, name FROM customers LIMIT 5\",\n",
    "            \"HIPAA policy for PHI storage\",\n",
    "        ]\n",
    "    if not chaos_variants:\n",
    "        chaos_variants = [\n",
    "            {},  \n",
    "            {\"disable_ce\": True},\n",
    "            {\"force_bm25\": True},\n",
    "            {\"verifier_v2\": True},\n",
    "        ]\n",
    "    print(\"\\n=== experiment matrix ===\")\n",
    "    out: List[Dict[str, Any]] = []\n",
    "    for qi, q in enumerate(queries):\n",
    "        print(f\"\\n query[{qi}]: {q!r}\")\n",
    "        for ci, ch in enumerate(chaos_variants):\n",
    "            label_bits = []\n",
    "            for k, v in ch.items():\n",
    "                if v:\n",
    "                    label_bits.append(k)\n",
    "            chaos_name = \"+\".join(label_bits) if label_bits else \"baseline\"\n",
    "            exp_name = f\"{name_prefix}:{chaos_name}\"\n",
    "            rec = run_perf_experiment(\n",
    "                name=exp_name,\n",
    "                q=q,\n",
    "                n=n,\n",
    "                threads=threads,\n",
    "                chaos=ch if ch else None,\n",
    "            )\n",
    "            out.append(rec)\n",
    "    print(\"\\n matrix done\")\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 585,
   "id": "c6d6bd57-5ce7-42ef-a36a-aa6aeade4fa8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def rank_experiments(limit: int = 80) -> List[Dict[str, Any]]:\n",
    "    \"\"\"\n",
    "    Return experiments sorted by p95 asc (fastest first).\n",
    "    \"\"\"\n",
    "    exps = _exp_load_all()[:limit]\n",
    "    exps = [e for e in exps if e.get(\"p95_ms\") is not None]\n",
    "    exps.sort(key=lambda e: e[\"p95_ms\"])\n",
    "    print(\"\\n=== top experiments (fastest first) ===\")\n",
    "    for e in exps[:15]:\n",
    "        print(\n",
    "            f\"- {e['p95_ms']:7.1f} ms | {e.get('name')} | {e.get('chaos')} | {_ts_human(e.get('ts'))}\"\n",
    "        )\n",
    "    return exps\n",
    "def experiments_ci_gate(\n",
    "    baseline_name: str = \"baseline\",\n",
    "    max_p95_ms: float = 1800.0,\n",
    ") -> int:\n",
    "    \"\"\"\n",
    "    CI-style check:\n",
    "    - ensure there's at least one baseline\n",
    "    - ensure best baseline p95 <= max_p95_ms\n",
    "    - write summary JSON for CI\n",
    "    Returns 0 on pass, 1 on fail.\n",
    "    \"\"\"\n",
    "    exps = _exp_load_all()\n",
    "    baselines = [e for e in exps if e.get(\"name\") == baseline_name and e.get(\"p95_ms\") is not None]\n",
    "    if not baselines:\n",
    "        print(f\" CI: no '{baseline_name}' experiment found\")\n",
    "        out = {\"ok\": False, \"reason\": \"no-baseline\", \"ts\": int(time.time())}\n",
    "        EXP_CI.write_text(json.dumps(out, indent=2), encoding=\"utf-8\")\n",
    "        return 1\n",
    "    best_base = min(baselines, key=lambda e: e[\"p95_ms\"])\n",
    "    ok = best_base[\"p95_ms\"] <= max_p95_ms\n",
    "    out = {\n",
    "        \"ok\": ok,\n",
    "        \"ts\": int(time.time()),\n",
    "        \"baseline_p95\": best_base[\"p95_ms\"],\n",
    "        \"baseline_ts\": best_base[\"ts\"],\n",
    "        \"max_p95_ms\": max_p95_ms,\n",
    "        \"baseline_chaos\": best_base.get(\"chaos\"),\n",
    "    }\n",
    "    EXP_CI.write_text(json.dumps(out, indent=2), encoding=\"utf-8\")\n",
    "    if ok:\n",
    "        print(f\" CI: baseline p95={best_base['p95_ms']:.1f} ms  {max_p95_ms} ms\")\n",
    "        return 0\n",
    "    else:\n",
    "        print(f\" CI: baseline p95={best_base['p95_ms']:.1f} ms > {max_p95_ms} ms\")\n",
    "        return 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 586,
   "id": "e3aea04e-3336-4509-81cb-4bf9175397e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def export_experiments_html(limit: int = 80) -> Path:\n",
    "    exps = _exp_load_all()[:limit]\n",
    "    html: List[str] = []\n",
    "    html.append(\"<!doctype html><html><head><meta charset='utf-8'><title>Gateway Experiments</title>\")\n",
    "    html.append(\"<style>body{font-family:system-ui,Segoe UI,Roboto,Helvetica,Arial,sans-serif;background:#020617;color:#e2e8f0;padding:1.5rem}table{border-collapse:collapse;width:100%;margin-top:1rem}th,td{border:1px solid #1e293b;padding:0.4rem 0.5rem}th{background:#0f172a;text-align:left}code{background:#0f172a;padding:0.1rem 0.35rem;border-radius:4px}h1{margin-bottom:0.5rem}</style>\")\n",
    "    html.append(\"</head><body>\")\n",
    "    html.append(\"<h1>Gateway Perf Experiments</h1>\")\n",
    "    html.append(f\"<p><small>generated {time.strftime('%Y-%m-%d %H:%M:%S', time.localtime())}</small></p>\")\n",
    "    if not exps:\n",
    "        html.append(\"<p><em>No experiments yet</em></p>\")\n",
    "    else:\n",
    "        html.append(\"<table>\")\n",
    "        html.append(\"<tr><th>when</th><th>name</th><th>p95 (ms)</th><th>avg (ms)</th><th>chaos</th></tr>\")\n",
    "        for e in exps:\n",
    "            chaos = e.get(\"chaos\") or {}\n",
    "            flags = \",\".join([k for k, v in chaos.items() if v]) or \"-\"\n",
    "            html.append(\n",
    "                f\"<tr><td>{_ts_human(e.get('ts'))}</td><td>{e.get('name')}</td><td>{e.get('p95_ms')}</td><td>{e.get('avg_ms')}</td><td>{flags}</td></tr>\"\n",
    "            )\n",
    "        html.append(\"</table>\")\n",
    "    html.append(\"<p style='margin-top:2rem;opacity:0.6;font-size:0.75rem'>notebook-generated  keep your RAG fast </p>\")\n",
    "    html.append(\"</body></html>\")\n",
    "    EXP_HTML.write_text(\"\\n\".join(html), encoding=\"utf-8\")\n",
    "    print(f\" experiments HTML  {EXP_HTML}\")\n",
    "    return EXP_HTML"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 587,
   "id": "f9700fa3-f8d9-4f4b-9799-871cbb473ba4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def auto_apply_best(name: str = \"chaos_disable_ce\") -> Dict[str, Any]:\n",
    "    \"\"\"\n",
    "    - find the fastest experiment with this name\n",
    "    - if it has chaos flags  push them to live gateway\n",
    "    \"\"\"\n",
    "    exps = _exp_load_all()\n",
    "    cand = [e for e in exps if e.get(\"name\") == name and e.get(\"p95_ms\") is not None]\n",
    "    if not cand:\n",
    "        print(f\" no experiments named {name!r}\")\n",
    "        return {\"ok\": False, \"reason\": \"no-experiments\"}\n",
    "    best = min(cand, key=lambda e: e[\"p95_ms\"])\n",
    "    chaos = best.get(\"chaos\") or {}\n",
    "    print(f\" best {name}: p95={best['p95_ms']:.1f} ms | chaos={chaos}\")\n",
    "    if not chaos:\n",
    "        print(\" best has no chaos  nothing to apply\")\n",
    "        return {\"ok\": True, \"applied\": {}}\n",
    "    if \"gw_chaos_set\" not in globals():\n",
    "        print(\" no live gateway helpers  not applied\")\n",
    "        return {\"ok\": False, \"reason\": \"no-gw\", \"best\": best}\n",
    "    applied = gw_chaos_set(**chaos)\n",
    "    return {\"ok\": True, \"applied\": chaos, \"gateway\": applied, \"best\": best}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 588,
   "id": "ed1b81db-f131-4b99-af3c-905afa9fc388",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== experiment continuation ready ===\n",
      "Try:\n",
      "  run_experiment_matrix(name_prefix='nightly')     # batch\n",
      "  rank_experiments()                               # see best\n",
      "  experiments_ci_gate(baseline_name='baseline')    # CI-ish check\n",
      "  export_experiments_html()                        # pretty report\n",
      "  auto_apply_best('chaos_disable_ce')              # push to live gateway\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n=== experiment continuation ready ===\")\n",
    "print(\"Try:\")\n",
    "print(\"  run_experiment_matrix(name_prefix='nightly')     # batch\")\n",
    "print(\"  rank_experiments()                               # see best\")\n",
    "print(\"  experiments_ci_gate(baseline_name='baseline')    # CI-ish check\")\n",
    "print(\"  export_experiments_html()                        # pretty report\")\n",
    "print(\"  auto_apply_best('chaos_disable_ce')              # push to live gateway\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 589,
   "id": "f8e33f1a-5dcd-4564-85b8-0d29868b8700",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " using ART_DIR=\\tmp\\art\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import re\n",
    "import json\n",
    "import time\n",
    "from pathlib import Path\n",
    "from typing import Any, Dict, List, Optional, Tuple\n",
    "ART_DIR = Path(os.getenv(\"ART_DIR\", \"/tmp/art\"))\n",
    "ART_DIR.mkdir(parents=True, exist_ok=True)\n",
    "PROM_SNAPSHOT = ART_DIR / \"gateway_prometheus.txt\"\n",
    "PROM_PARSED = ART_DIR / \"gateway_prometheus.json\"\n",
    "ROUTE_DIAG = ART_DIR / \"gateway_route_diag.json\"\n",
    "LAT_HIST = ART_DIR / \"gateway_latency_hist.json\"\n",
    "print(f\" using ART_DIR={ART_DIR}\")\n",
    "METRIC_PAT = re.compile(\n",
    "    r\"^(?P<name>[a-zA-Z_:][a-zA-Z0-9_:]*)(\\{(?P<labels>[^}]*)\\})?\\s+(?P<value>[-+]?\\d*\\.?\\d+(e[-+]?\\d+)?)\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 590,
   "id": "201a3a88-e253-4f05-928f-cb8eda447748",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _gw_base() -> str:\n",
    "    if \"gateway_url\" in globals():\n",
    "        u = gateway_url()\n",
    "        if u and u != \"not running\":\n",
    "            return u\n",
    "    return \"http://127.0.0.1:9910\"\n",
    "def scrape_prometheus(base: Optional[str] = None, timeout: float = 2.5) -> Optional[str]:\n",
    "    \"\"\"\n",
    "    GET /metrics and drop raw text in /tmp/art/gateway_prometheus.txt\n",
    "    \"\"\"\n",
    "    import requests\n",
    "    base = base or _gw_base()\n",
    "    url = f\"{base}/metrics\"\n",
    "    try:\n",
    "        r = requests.get(url, timeout=timeout)\n",
    "        r.raise_for_status()\n",
    "    except Exception as e:\n",
    "        print(f\" metrics scrape failed: {e}\")\n",
    "        return None\n",
    "    PROM_SNAPSHOT.write_text(r.text, encoding=\"utf-8\")\n",
    "    print(f\" scraped metrics  {PROM_SNAPSHOT}\")\n",
    "    return r.text\n",
    "def _parse_labels(s: str) -> Dict[str, str]:\n",
    "    out = {}\n",
    "    if not s:\n",
    "        return out\n",
    "    for part in s.split(\",\"):\n",
    "        if \"=\" not in part:\n",
    "            continue\n",
    "        k, v = part.split(\"=\", 1)\n",
    "        v = v.strip().strip('\"')\n",
    "        out[k.strip()] = v\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 591,
   "id": "a02da7db-dd45-4785-a8e0-a6813d27cb61",
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_prometheus_text(text: str) -> List[Dict[str, Any]]:\n",
    "    \"\"\"\n",
    "    Turn Prom text into Python dicts:\n",
    "    [{name:..., labels:{...}, value:float}, ...]\n",
    "    \"\"\"\n",
    "    out: List[Dict[str, Any]] = []\n",
    "    for line in text.splitlines():\n",
    "        line = line.strip()\n",
    "        if not line or line.startswith(\"#\"):\n",
    "            continue\n",
    "        m = METRIC_PAT.match(line)\n",
    "        if not m:\n",
    "            continue\n",
    "        name = m.group(\"name\")\n",
    "        labels = _parse_labels(m.group(\"labels\"))\n",
    "        val = float(m.group(\"value\"))\n",
    "        out.append({\"name\": name, \"labels\": labels, \"value\": val})\n",
    "    return out\n",
    "def load_or_scrape_prom() -> List[Dict[str, Any]]:\n",
    "    txt = None\n",
    "    if PROM_SNAPSHOT.exists():\n",
    "        txt = PROM_SNAPSHOT.read_text(encoding=\"utf-8\")\n",
    "    else:\n",
    "        txt = scrape_prometheus()\n",
    "    if not txt:\n",
    "        return []\n",
    "    items = parse_prometheus_text(txt)\n",
    "    PROM_PARSED.write_text(json.dumps(items, indent=2), encoding=\"utf-8\")\n",
    "    print(f\" parsed metrics  {PROM_PARSED}\")\n",
    "    return items"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 592,
   "id": "16fdf45b-1e77-4b7d-9470-7b34363c258f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def pick_histogram_buckets(metrics: List[Dict[str, Any]], prefix: str) -> Dict[str, float]:\n",
    "    \"\"\"\n",
    "    Grab histo buckets like: gateway_answer_latency_ms_bucket{le=\"0.5\"} ...\n",
    "    return {le: value}\n",
    "    \"\"\"\n",
    "    out = {}\n",
    "    for m in metrics:\n",
    "        if m[\"name\"] != prefix:\n",
    "            continue\n",
    "        le = m[\"labels\"].get(\"le\")\n",
    "        if le is None:\n",
    "            continue\n",
    "        out[le] = m[\"value\"]\n",
    "    return out\n",
    "def detect_hot_routes(metrics: List[Dict[str, Any]]) -> Dict[str, Any]:\n",
    "    \"\"\"\n",
    "    Look at `gateway_strategy_selected_total{strategy=...}` and tell us who's getting traffic.\n",
    "    \"\"\"\n",
    "    strat_counts: Dict[str, float] = {}\n",
    "    for m in metrics:\n",
    "        if m[\"name\"] != \"gateway_strategy_selected_total\":\n",
    "            continue\n",
    "        strat = m[\"labels\"].get(\"strategy\", \"unknown\")\n",
    "        strat_counts[strat] = strat_counts.get(strat, 0.0) + m[\"value\"]\n",
    "    fanout_errs: Dict[str, float] = {}\n",
    "    for m in metrics:\n",
    "        if m[\"name\"] != \"gateway_fanout_errors_total\":\n",
    "            continue\n",
    "        step = m[\"labels\"].get(\"step\", \"unknown\")\n",
    "        fanout_errs[step] = fanout_errs.get(step, 0.0) + m[\"value\"]\n",
    "    diag = {\n",
    "        \"ts\": int(time.time()),\n",
    "        \"strategies\": strat_counts,\n",
    "        \"fanout_errors\": fanout_errs,\n",
    "    }\n",
    "    ROUTE_DIAG.write_text(json.dumps(diag, indent=2), encoding=\"utf-8\")\n",
    "    print(f\" route diag  {ROUTE_DIAG}\")\n",
    "    return diag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 593,
   "id": "044dc92f-6934-4f11-8e59-c1f5f25a061c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def reconstruct_latency(metrics: List[Dict[str, Any]]) -> Dict[str, Any]:\n",
    "    \"\"\"\n",
    "    Rebuild latency view from Prom  this is coarse but good enough for notebook diffing.\n",
    "    \"\"\"\n",
    "    buckets = pick_histogram_buckets(metrics, \"gateway_answer_latency_ms_bucket\")\n",
    "    out = {\n",
    "        \"ts\": int(time.time()),\n",
    "        \"buckets\": buckets,\n",
    "    }\n",
    "    LAT_HIST.write_text(json.dumps(out, indent=2), encoding=\"utf-8\")\n",
    "    print(f\" latency hist  {LAT_HIST}\")\n",
    "    return out\n",
    "def suggest_from_metrics(metrics: List[Dict[str, Any]]) -> Dict[str, Any]:\n",
    "    \"\"\"\n",
    "    Notebook smartass mode:\n",
    "    - if fanout_errors_total for 'dense' is high  suggest chaos.force_bm25=true\n",
    "    - if strategy_selected_total says rerank >> rrf  suggest lowering CE weight\n",
    "    \"\"\"\n",
    "    diag = detect_hot_routes(metrics)\n",
    "    fanout = diag[\"fanout_errors\"]\n",
    "    strategies = diag[\"strategies\"]\n",
    "    hints: List[str] = []\n",
    "    if fanout.get(\"dense\", 0) > 0:\n",
    "        hints.append(\"dense step is erroring  try POST /admin/chaos/set {\\\"force_bm25\\\": true}\")\n",
    "    rerank = strategies.get(\"rerank\", 0)\n",
    "    rrf = strategies.get(\"rrf\", 0)\n",
    "    if rerank > 0 and rrf >= 0 and rerank > (rrf * 4 + 5):\n",
    "        hints.append(\"rerank dominates  consider lowering /rl/update factual:0.5 or chaos.disable_ce=true\")\n",
    "    out = {\n",
    "        \"ts\": int(time.time()),\n",
    "        \"hints\": hints,\n",
    "        \"fanout\": fanout,\n",
    "        \"strategies\": strategies,\n",
    "    }\n",
    "    print(\"\\n=== metric-based suggestions ===\")\n",
    "    if hints:\n",
    "        for h in hints:\n",
    "            print(\"  -\", h)\n",
    "    else:\n",
    "        print(\"  (none  gateway looks ok from metrics)\")\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 594,
   "id": "82ab3fa7-7f32-4c94-9106-b11f0bd986ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "def dump_all_telemetry() -> Path:\n",
    "    \"\"\"\n",
    "    end-to-end: scrape  parse  diag  write 1 JSON\n",
    "    \"\"\"\n",
    "    metrics = load_or_scrape_prom()\n",
    "    diag = detect_hot_routes(metrics)\n",
    "    hist = reconstruct_latency(metrics)\n",
    "    sugg = suggest_from_metrics(metrics)\n",
    "    out = {\n",
    "        \"ts\": int(time.time()),\n",
    "        \"metrics_file\": str(PROM_SNAPSHOT),\n",
    "        \"parsed_file\": str(PROM_PARSED),\n",
    "        \"route_diag\": diag,\n",
    "        \"latency_hist\": hist,\n",
    "        \"suggest\": sugg,\n",
    "    }\n",
    "    out_path = ART_DIR / \"gateway_telemetry.json\"\n",
    "    out_path.write_text(json.dumps(out, indent=2), encoding=\"utf-8\")\n",
    "    print(f\" telemetry bundle  {out_path}\")\n",
    "    return out_path\n",
    "def gw_ctl(cmd: str = \"status\") -> None:\n",
    "    cmd = cmd.lower().strip()\n",
    "    if cmd == \"status\":\n",
    "        print(\" health:\")\n",
    "        if \"health_summary\" in globals():\n",
    "            health_summary()\n",
    "        else:\n",
    "            print(\"health_summary() not in scope\")\n",
    "    elif cmd == \"metrics\":\n",
    "        dump_all_telemetry()\n",
    "    elif cmd == \"chaos:on\":\n",
    "        if \"gw_chaos_set\" in globals():\n",
    "            print(gw_chaos_set(disable_ce=True))\n",
    "        else:\n",
    "            print(\"no gw_chaos_set() in scope\")\n",
    "    elif cmd == \"chaos:off\":\n",
    "        if \"gw_chaos_set\" in globals():\n",
    "            print(gw_chaos_set(disable_ce=False, force_bm25=False, verifier_v2=False))\n",
    "        else:\n",
    "            print(\"no gw_chaos_set() in scope\")\n",
    "    else:\n",
    "        print(\" unknown cmd. try: status | metrics | chaos:on | chaos:off\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 595,
   "id": "9d95d86a-2b68-4f16-b5bc-9e519d122d26",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== telemetry snapshot ===\n",
      " parsed metrics  \\tmp\\art\\gateway_prometheus.json\n",
      " route diag  \\tmp\\art\\gateway_route_diag.json\n",
      " latency hist  \\tmp\\art\\gateway_latency_hist.json\n",
      " route diag  \\tmp\\art\\gateway_route_diag.json\n",
      "\n",
      "=== metric-based suggestions ===\n",
      "  (none  gateway looks ok from metrics)\n",
      " telemetry bundle  \\tmp\\art\\gateway_telemetry.json\n",
      "\n",
      "Tip:\n",
      "  gw_ctl('status')\n",
      "  gw_ctl('metrics')\n",
      "  gw_ctl('chaos:on')  # emergency brake\n",
      "  gw_ctl('chaos:off')\n",
      "\n",
      " continuation telemetry cell done.\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n=== telemetry snapshot ===\")\n",
    "_ = dump_all_telemetry()\n",
    "print(\"\\nTip:\")\n",
    "print(\"  gw_ctl('status')\")\n",
    "print(\"  gw_ctl('metrics')\")\n",
    "print(\"  gw_ctl('chaos:on')  # emergency brake\")\n",
    "print(\"  gw_ctl('chaos:off')\")\n",
    "print(\"\\n continuation telemetry cell done.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 596,
   "id": "e1153844-3fa0-4075-b54c-1fef062abe31",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " continuation  reading from \\tmp\\art\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "import time\n",
    "from pathlib import Path\n",
    "from typing import Any, Dict, List, Optional, Tuple\n",
    "ART_DIR = Path(os.getenv(\"ART_DIR\", \"/tmp/art\"))\n",
    "PROM_SNAPSHOT = ART_DIR / \"gateway_prometheus.txt\"\n",
    "PROM_PARSED = ART_DIR / \"gateway_prometheus.json\"\n",
    "ROUTE_DIAG = ART_DIR / \"gateway_route_diag.json\"\n",
    "LAT_HIST = ART_DIR / \"gateway_latency_hist.json\"\n",
    "PROM_DIFF = ART_DIR / \"gateway_prom_diff.json\"\n",
    "PROM_REPORT_MD = ART_DIR / \"gateway_prom_report.md\"\n",
    "PROM_REPORT_HTML = ART_DIR / \"gateway_prom_report.html\"\n",
    "print(f\" continuation  reading from {ART_DIR}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 597,
   "id": "0580c1df-d066-4f9e-ae86-1c7ebfce27d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _read_json_safe(p: Path) -> Optional[Dict[str, Any]]:\n",
    "    if not p.exists():\n",
    "        return None\n",
    "    try:\n",
    "        return json.loads(p.read_text(encoding=\"utf-8\"))\n",
    "    except Exception as e:\n",
    "        print(f\" failed to read {p}: {e}\")\n",
    "        return None\n",
    "def _ts_human(ts: Optional[int]) -> str:\n",
    "    if not ts:\n",
    "        return \"?\"\n",
    "    return time.strftime(\"%Y-%m-%d %H:%M:%S\", time.localtime(ts))\n",
    "def _gw_base() -> str:\n",
    "    if \"gateway_url\" in globals():\n",
    "        u = gateway_url()\n",
    "        if u and u != \"not running\":\n",
    "            return u\n",
    "    return \"http://127.0.0.1:9910\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 598,
   "id": "9f37113a-4132-4401-9f32-6a2e4b497122",
   "metadata": {},
   "outputs": [],
   "source": [
    "PROM_RUNS_DIR = ART_DIR / \"gateway_prom_runs\"\n",
    "PROM_RUNS_DIR.mkdir(parents=True, exist_ok=True)\n",
    "def archive_prometheus_snapshot() -> Optional[Path]:\n",
    "    if not PROM_SNAPSHOT.exists():\n",
    "        print(\" no gateway_prometheus.txt to archive  run dump_all_telemetry() first\")\n",
    "        return None\n",
    "    ts = time.strftime(\"%Y%m%d-%H%M%S\", time.localtime())\n",
    "    out = PROM_RUNS_DIR / f\"metrics_{ts}.txt\"\n",
    "    out.write_text(PROM_SNAPSHOT.read_text(encoding=\"utf-8\"), encoding=\"utf-8\")\n",
    "    print(f\" archived metrics  {out}\")\n",
    "    return out\n",
    "def list_prom_archives(limit: int = 25) -> List[Path]:\n",
    "    files = sorted(PROM_RUNS_DIR.glob(\"metrics_*.txt\"), reverse=True)\n",
    "    return files[:limit]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 599,
   "id": "852508ad-f01c-4991-a8a3-6a3caa2fba2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_prom_file(path: Path) -> List[Dict[str, Any]]:\n",
    "    if not path.exists():\n",
    "        return []\n",
    "    text = path.read_text(encoding=\"utf-8\")\n",
    "    METRIC_PAT = re.compile(\n",
    "        r\"^(?P<name>[a-zA-Z_:][a-zA-Z0-9_:]*)(\\{(?P<labels>[^}]*)\\})?\\s+(?P<value>[-+]?\\d*\\.?\\d+(e[-+]?\\d+)?)\"\n",
    "    )\n",
    "    def _parse_labels(s: str) -> Dict[str, str]:\n",
    "        out = {}\n",
    "        if not s:\n",
    "            return out\n",
    "        for part in s.split(\",\"):\n",
    "            if \"=\" not in part:\n",
    "                continue\n",
    "            k, v = part.split(\"=\", 1)\n",
    "            v = v.strip().strip('\"')\n",
    "            out[k.strip()] = v\n",
    "        return out\n",
    "    out: List[Dict[str, Any]] = []\n",
    "    for line in text.splitlines():\n",
    "        line = line.strip()\n",
    "        if not line or line.startswith(\"#\"):\n",
    "            continue\n",
    "        m = METRIC_PAT.match(line)\n",
    "        if not m:\n",
    "            continue\n",
    "        name = m.group(\"name\")\n",
    "        labels = _parse_labels(m.group(\"labels\"))\n",
    "        val = float(m.group(\"value\"))\n",
    "        out.append({\"name\": name, \"labels\": labels, \"value\": val})\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 600,
   "id": "dad5ced9-f3a1-4fad-bba0-ad01e2f0ebd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def diff_prom_archives(idx_new: int = 0, idx_old: int = 1) -> Optional[Dict[str, Any]]:\n",
    "    files = list_prom_archives(limit=40)\n",
    "    if idx_new >= len(files) or idx_old >= len(files):\n",
    "        print(\" not enough prom archives to diff\")\n",
    "        return None\n",
    "    p_new = files[idx_new]\n",
    "    p_old = files[idx_old]\n",
    "    m_new = parse_prom_file(p_new)\n",
    "    m_old = parse_prom_file(p_old)\n",
    "    def _key(m: Dict[str, Any]) -> str:\n",
    "        lbl = m.get(\"labels\") or {}\n",
    "        lbls = \",\".join(f\"{k}={lbl[k]}\" for k in sorted(lbl))\n",
    "        return f\"{m['name']}|{lbls}\"\n",
    "    idx_new_m = { _key(m): m for m in m_new }\n",
    "    idx_old_m = { _key(m): m for m in m_old }\n",
    "    added = {}\n",
    "    removed = {}\n",
    "    changed = {}\n",
    "    for k, v in idx_new_m.items():\n",
    "        if k not in idx_old_m:\n",
    "            added[k] = v\n",
    "        else:\n",
    "            old_v = idx_old_m[k]\n",
    "            if abs(v[\"value\"] - old_v[\"value\"]) > 1e-6:\n",
    "                changed[k] = {\n",
    "                    \"old\": old_v[\"value\"],\n",
    "                    \"new\": v[\"value\"],\n",
    "                    \"delta\": v[\"value\"] - old_v[\"value\"],\n",
    "                }\n",
    "    for k, v in idx_old_m.items():\n",
    "        if k not in idx_new_m:\n",
    "            removed[k] = v\n",
    "    out = {\n",
    "        \"ts\": int(time.time()),\n",
    "        \"new_file\": p_new.name,\n",
    "        \"old_file\": p_old.name,\n",
    "        \"added\": added,\n",
    "        \"removed\": removed,\n",
    "        \"changed\": changed,\n",
    "    }\n",
    "    PROM_DIFF.write_text(json.dumps(out, indent=2), encoding=\"utf-8\")\n",
    "    print(f\" prom diff  {PROM_DIFF}\")\n",
    "    big_deltas = {k: v for k, v in changed.items() if abs(v[\"delta\"]) >= 1.0}\n",
    "    if big_deltas:\n",
    "        print(\" big metric jumps:\")\n",
    "        for k, v in big_deltas.items():\n",
    "            print(f\"  - {k}: {v['old']}  {v['new']} (={v['delta']:+.1f})\")\n",
    "    else:\n",
    "        print(\" no big metric jumps\")\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 601,
   "id": "2b1e9d7e-b1f6-4e5e-9d4f-c99ec4b3d919",
   "metadata": {},
   "outputs": [],
   "source": [
    "def watch_metrics(\n",
    "    secs: int = 20,\n",
    "    interval: float = 2.0,\n",
    "    show_hints: bool = True,\n",
    ") -> None:\n",
    "    \"\"\"\n",
    "    Poll /metrics every `interval` seconds for `secs` seconds.\n",
    "    Good for \"I just toggled chaos, did the errors drop?\"\n",
    "    \"\"\"\n",
    "    end = time.time() + secs\n",
    "    base = _gw_base()\n",
    "    print(f\" watching {base}/metrics for {secs}s \")\n",
    "    last_dense_err = None\n",
    "    while time.time() < end:\n",
    "        txt = scrape_prometheus(base=base, timeout=2.0)\n",
    "        if not txt:\n",
    "            time.sleep(interval)\n",
    "            continue\n",
    "        items = parse_prometheus_text(txt)\n",
    "        diag = detect_hot_routes(items)\n",
    "        dense_err = diag[\"fanout_errors\"].get(\"dense\", 0)\n",
    "        if show_hints:\n",
    "            sugg = suggest_from_metrics(items)\n",
    "        if last_dense_err is not None and dense_err > last_dense_err:\n",
    "            print(f\" dense errors increased: {last_dense_err}  {dense_err}\")\n",
    "        last_dense_err = dense_err\n",
    "        time.sleep(interval)\n",
    "    print(\" watch finished.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 602,
   "id": "6d2ed6e6-de24-4ba8-96d6-4b06e3e9d502",
   "metadata": {},
   "outputs": [],
   "source": [
    "GATEWAY_AGG = ART_DIR / \"gateway_runs_agg.json\"\n",
    "def build_prom_report_md(limit: int = 30) -> Path:\n",
    "    prom = _read_json_safe(PROM_PARSED) or []\n",
    "    diag = _read_json_safe(ROUTE_DIAG) or {}\n",
    "    lat = _read_json_safe(LAT_HIST) or {}\n",
    "    agg = _read_json_safe(GATEWAY_AGG) or {}\n",
    "    lines: List[str] = []\n",
    "    lines.append(\"# Gateway Telemetry Report\")\n",
    "    lines.append(f\"_generated: {time.strftime('%Y-%m-%d %H:%M:%S', time.localtime())}_\")\n",
    "    lines.append(\"\")\n",
    "    lines.append(\"## Strategy & Fanout (from metrics)\")\n",
    "    lines.append(\"```json\")\n",
    "    lines.append(json.dumps(diag, indent=2))\n",
    "    lines.append(\"```\")\n",
    "    lines.append(\"\")\n",
    "    lines.append(\"## Latency buckets (from metrics)\")\n",
    "    lines.append(\"```json\")\n",
    "    lines.append(json.dumps(lat, indent=2))\n",
    "    lines.append(\"```\")\n",
    "    lines.append(\"\")\n",
    "    lines.append(\"## Recent gateway runs (p95)\")\n",
    "    rows = agg.get(\"runs\", [])[:limit]\n",
    "    if rows:\n",
    "        lines.append(\"| when | p95 (ms) | chaos | file |\")\n",
    "        lines.append(\"|------|----------|--------|------|\")\n",
    "        for r in rows:\n",
    "            ts = r.get(\"ts\")\n",
    "            when = _ts_human(ts)\n",
    "            p95 = r.get(\"p95_ms\")\n",
    "            cz = r.get(\"chaos\", {}).get(\"chaos\", r.get(\"chaos\", {})) or {}\n",
    "            cz_flags = \",\".join([k for k, v in cz.items() if v]) or \"-\"\n",
    "            lines.append(f\"| {when} | {p95} | {cz_flags} | {r.get('file','')} |\")\n",
    "    else:\n",
    "        lines.append(\"_no gateway agg yet_\")\n",
    "    PROM_REPORT_MD.write_text(\"\\n\".join(lines), encoding=\"utf-8\")\n",
    "    print(f\" prom report  {PROM_REPORT_MD}\")\n",
    "    return PROM_REPORT_MD\n",
    "def build_prom_report_html(limit: int = 30) -> Path:\n",
    "    diag = _read_json_safe(ROUTE_DIAG) or {}\n",
    "    lat = _read_json_safe(LAT_HIST) or {}\n",
    "    agg = _read_json_safe(GATEWAY_AGG) or {}\n",
    "    html: List[str] = []\n",
    "    html.append(\"<!doctype html><html><head><meta charset='utf-8'><title>Gateway Telemetry</title>\")\n",
    "    html.append(\"<style>body{font-family:system-ui,Segoe UI,Roboto,Helvetica,Arial,sans-serif;background:#020617;color:#e2e8f0;padding:1.5rem}h1{margin-bottom:.5rem}pre{background:#0f172a;padding:.75rem;border-radius:.5rem;overflow:auto}</style>\")\n",
    "    html.append(\"</head><body>\")\n",
    "    html.append(\"<h1>Gateway Telemetry</h1>\")\n",
    "    html.append(f\"<p><small>generated {time.strftime('%Y-%m-%d %H:%M:%S', time.localtime())}</small></p>\")\n",
    "    html.append(\"<h2>Strategies</h2>\")\n",
    "    html.append(\"<pre>\")\n",
    "    html.append(json.dumps(diag, indent=2))\n",
    "    html.append(\"</pre>\")\n",
    "    html.append(\"<h2>Latency buckets</h2>\")\n",
    "    html.append(\"<pre>\")\n",
    "    html.append(json.dumps(lat, indent=2))\n",
    "    html.append(\"</pre>\")\n",
    "    html.append(\"<h2>Recent runs (p95)</h2>\")\n",
    "    rows = agg.get(\"runs\", [])[:limit]\n",
    "    if rows:\n",
    "        html.append(\"<table style='border-collapse:collapse;width:100%;margin-top:1rem'>\")\n",
    "        html.append(\"<tr><th>when</th><th>p95 (ms)</th><th>chaos</th><th>file</th></tr>\")\n",
    "        for r in rows:\n",
    "            ts = r.get(\"ts\")\n",
    "            when = _ts_human(ts)\n",
    "            p95 = r.get(\"p95_ms\")\n",
    "            cz = r.get(\"chaos\", {}).get(\"chaos\", r.get(\"chaos\", {})) or {}\n",
    "            flags = \",\".join([k for k, v in cz.items() if v]) or \"-\"\n",
    "            html.append(f\"<tr><td>{when}</td><td>{p95}</td><td>{flags or '-'}</td><td>{r.get('file','')}</td></tr>\")\n",
    "        html.append(\"</table>\")\n",
    "    else:\n",
    "        html.append(\"<p><em>no gateway agg yet</em></p>\")\n",
    "    html.append(\"<p style='margin-top:2rem;opacity:.5;font-size:.75rem'>auto-generated from notebook telemetry</p>\")\n",
    "    html.append(\"</body></html>\")\n",
    "    PROM_REPORT_HTML.write_text(\"\\n\".join(html), encoding=\"utf-8\")\n",
    "    print(f\" prom HTML  {PROM_REPORT_HTML}\")\n",
    "    return PROM_REPORT_HTML"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 603,
   "id": "6c411923-0e87-4fc0-960c-ef2fdee7ffe2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def telemetry_pain() -> Dict[str, Any]:\n",
    "    metrics = _read_json_safe(PROM_PARSED) or []\n",
    "    diag = _read_json_safe(ROUTE_DIAG) or {}\n",
    "    lat = _read_json_safe(LAT_HIST) or {}\n",
    "    hints = []\n",
    "    fanout = (diag or {}).get(\"fanout_errors\", {})\n",
    "    if fanout.get(\"dense\", 0) > 0:\n",
    "        hints.append(\"dense errors > 0  chaos.force_bm25=true\")\n",
    "    if fanout.get(\"complete\", 0) > 0:\n",
    "        hints.append(\"completion errors > 0  check RAG /complete backend\")\n",
    "    buckets = (lat or {}).get(\"buckets\", {})\n",
    "    over_1s = None\n",
    "    if buckets:\n",
    "        total = buckets.get(\"+Inf\") or buckets.get(\"Inf\") or None\n",
    "        over_1s = None\n",
    "        for k, v in buckets.items():\n",
    "            if k in (\"+Inf\", \"Inf\"):\n",
    "                continue\n",
    "            try:\n",
    "                if float(k) >= 1000.0:\n",
    "                    over_1s = (total - v) if total is not None else None\n",
    "                    break\n",
    "            except ValueError:\n",
    "                pass\n",
    "        if over_1s is not None and total is not None and (over_1s / total) > 0.4:\n",
    "            hints.append(\"40% of requests over 1s  look at STEP_TIMEOUT_* or disable CE\")\n",
    "    out = {\n",
    "        \"ts\": int(time.time()),\n",
    "        \"fanout_errors\": fanout,\n",
    "        \"latency_buckets\": buckets,\n",
    "        \"hints\": hints,\n",
    "    }\n",
    "    print(\"\\n=== pain ===\")\n",
    "    if hints:\n",
    "        for h in hints:\n",
    "            print(\"  -\", h)\n",
    "    else:\n",
    "        print(\"  (none )\")\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 604,
   "id": "68cee12d-512e-4570-a8a2-e7e5493b4447",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== prom continuation ===\n",
      " archived metrics  \\tmp\\art\\gateway_prom_runs\\metrics_20251102-131803.txt\n",
      " prom diff  \\tmp\\art\\gateway_prom_diff.json\n",
      " no big metric jumps\n",
      " prom report  \\tmp\\art\\gateway_prom_report.md\n",
      " prom HTML  \\tmp\\art\\gateway_prom_report.html\n",
      "\n",
      "=== pain ===\n",
      "  (none )\n",
      "\n",
      "Tip:\n",
      "  watch_metrics(20, 2.0)          # live poll\n",
      "  diff_prom_archives(0, 2)        # diff newest vs 3rd newest\n",
      "  telemetry_pain()                # 1-shot what's wrong\n",
      "  list_prom_archives()            # see all scrapes\n",
      "\n",
      " prom continuation cell done.\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n=== prom continuation ===\")\n",
    "archived = archive_prometheus_snapshot()\n",
    "_ = diff_prom_archives(0, 1)  \n",
    "_ = build_prom_report_md()\n",
    "_ = build_prom_report_html()\n",
    "_ = telemetry_pain()\n",
    "print(\"\\nTip:\")\n",
    "print(\"  watch_metrics(20, 2.0)          # live poll\")\n",
    "print(\"  diff_prom_archives(0, 2)        # diff newest vs 3rd newest\")\n",
    "print(\"  telemetry_pain()                # 1-shot what's wrong\")\n",
    "print(\"  list_prom_archives()            # see all scrapes\")\n",
    "print(\"\\n prom continuation cell done.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 605,
   "id": "a1e6cc93-4438-44b9-acb8-9285de2c26e5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " prom TS will live in \\tmp\\art\\gateway_prom_timeseries.json\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "import time\n",
    "import re\n",
    "from pathlib import Path\n",
    "from typing import Any, Dict, List, Optional, Tuple\n",
    "ART_DIR = Path(os.getenv(\"ART_DIR\", \"/tmp/art\"))\n",
    "PROM_RUNS_DIR = ART_DIR / \"gateway_prom_runs\"\n",
    "PROM_TS_PATH = ART_DIR / \"gateway_prom_timeseries.json\"\n",
    "PROM_TS_CSV = ART_DIR / \"gateway_prom_timeseries.csv\"\n",
    "PROM_DIFF = ART_DIR / \"gateway_prom_diff.json\"  \n",
    "print(f\" prom TS will live in {PROM_TS_PATH}\")\n",
    "METRIC_LINE = re.compile(\n",
    "    r\"^(?P<name>[a-zA-Z_:][a-zA-Z0-9_:]*)(\\{(?P<labels>[^}]*)\\})?\\s+(?P<value>[-+]?\\d*\\.?\\d+(e[-+]?\\d+)?)\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 606,
   "id": "5db54e33-0b5e-4d36-ba26-504a49cf5742",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _parse_labels(s: str) -> Dict[str, str]:\n",
    "    out = {}\n",
    "    if not s:\n",
    "        return out\n",
    "    for part in s.split(\",\"):\n",
    "        if \"=\" not in part:\n",
    "            continue\n",
    "        k, v = part.split(\"=\", 1)\n",
    "        out[k.strip()] = v.strip().strip('\"')\n",
    "    return out\n",
    "def parse_prom_text(txt: str) -> List[Dict[str, Any]]:\n",
    "    rows: List[Dict[str, Any]] = []\n",
    "    for line in txt.splitlines():\n",
    "        line = line.strip()\n",
    "        if not line or line.startswith(\"#\"):\n",
    "            continue\n",
    "        m = METRIC_LINE.match(line)\n",
    "        if not m:\n",
    "            continue\n",
    "        name = m.group(\"name\")\n",
    "        labels = _parse_labels(m.group(\"labels\"))\n",
    "        val = float(m.group(\"value\"))\n",
    "        rows.append({\"name\": name, \"labels\": labels, \"value\": val})\n",
    "    return rows\n",
    "def _gw_base() -> str:\n",
    "    if \"gateway_url\" in globals():\n",
    "        u = gateway_url()\n",
    "        if u and u != \"not running\":\n",
    "            return u\n",
    "    return \"http://127.0.0.1:9910\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 607,
   "id": "4ec43755-4315-4bed-bbca-e53bab96f0a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def scrape_and_append_ts(base: Optional[str] = None, timeout: float = 2.5) -> Dict[str, Any]:\n",
    "    \"\"\"\n",
    "    - scrape /metrics\n",
    "    - parse\n",
    "    - append one snapshot into gateway_prom_timeseries.json\n",
    "    format:\n",
    "      {\n",
    "        \"snapshots\": [\n",
    "          {\n",
    "            \"ts\": 1730000000,\n",
    "            \"metrics\": [ {name, labels, value}, ... ]\n",
    "          },\n",
    "          ...\n",
    "        ]\n",
    "      }\n",
    "    \"\"\"\n",
    "    import requests\n",
    "    base = base or _gw_base()\n",
    "    url = f\"{base}/metrics\"\n",
    "    try:\n",
    "        r = requests.get(url, timeout=timeout)\n",
    "        r.raise_for_status()\n",
    "    except Exception as e:\n",
    "        print(f\" metrics scrape failed: {e}\")\n",
    "        return {\"ok\": False, \"error\": str(e)}\n",
    "    txt = r.text\n",
    "    parsed = parse_prom_text(txt)\n",
    "    ts_obj = {\"ts\": int(time.time()), \"metrics\": parsed}\n",
    "    if PROM_TS_PATH.exists():\n",
    "        data = json.loads(PROM_TS_PATH.read_text(encoding=\"utf-8\"))\n",
    "    else:\n",
    "        data = {\"snapshots\": []}\n",
    "    data[\"snapshots\"].append(ts_obj)\n",
    "    data[\"snapshots\"] = data[\"snapshots\"][-120:]\n",
    "    PROM_TS_PATH.write_text(json.dumps(data, indent=2), encoding=\"utf-8\")\n",
    "    print(f\" appended scrape to {PROM_TS_PATH} (total={len(data['snapshots'])})\")\n",
    "    return {\"ok\": True, \"count\": len(data[\"snapshots\"])}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 608,
   "id": "75846197-b2d7-4b92-a31f-a30511b7aa86",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _metric_key(m: Dict[str, Any]) -> str:\n",
    "    lbl = m.get(\"labels\") or {}\n",
    "    parts = \",\".join(f\"{k}={lbl[k]}\" for k in sorted(lbl))\n",
    "    return f\"{m['name']}|{parts}\"\n",
    "def load_ts() -> Dict[str, Any]:\n",
    "    if not PROM_TS_PATH.exists():\n",
    "        return {\"snapshots\": []}\n",
    "    return json.loads(PROM_TS_PATH.read_text(encoding=\"utf-8\"))\n",
    "def query_metric_over_time(\n",
    "    metric: str,\n",
    "    label_filter: Optional[Dict[str, str]] = None,\n",
    "    limit: int = 120,\n",
    ") -> List[Dict[str, Any]]:\n",
    "    \"\"\"\n",
    "    returns list of {ts, value} for the metric+labels (first match per snapshot)\n",
    "    label_filter = {\"strategy\": \"rerank\"} etc.\n",
    "    \"\"\"\n",
    "    data = load_ts()\n",
    "    snaps = data.get(\"snapshots\", [])[-limit:]\n",
    "    rows: List[Dict[str, Any]] = []\n",
    "    for snap in snaps:\n",
    "        ts = snap[\"ts\"]\n",
    "        val = None\n",
    "        for m in snap[\"metrics\"]:\n",
    "            if m[\"name\"] != metric:\n",
    "                continue\n",
    "            if label_filter:\n",
    "                ok = True\n",
    "                for k, v in label_filter.items():\n",
    "                    if m[\"labels\"].get(k) != v:\n",
    "                        ok = False\n",
    "                        break\n",
    "                if not ok:\n",
    "                    continue\n",
    "            val = m[\"value\"]\n",
    "            break\n",
    "        rows.append({\"ts\": ts, \"value\": val})\n",
    "    return rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 609,
   "id": "533ac038-e31a-41d1-85a1-687efd4c787c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_deltas(\n",
    "    metric: str,\n",
    "    label_filter: Optional[Dict[str, str]] = None,\n",
    "    limit: int = 60,\n",
    ") -> List[Dict[str, Any]]:\n",
    "    series = query_metric_over_time(metric, label_filter=label_filter, limit=limit)\n",
    "    out: List[Dict[str, Any]] = []\n",
    "    prev_val = None\n",
    "    prev_ts = None\n",
    "    for point in series:\n",
    "        ts = point[\"ts\"]\n",
    "        val = point[\"value\"]\n",
    "        if val is None or prev_val is None:\n",
    "            out.append({\"ts\": ts, \"delta\": None, \"rate_per_s\": None})\n",
    "        else:\n",
    "            dval = val - prev_val\n",
    "            dts = max(1, ts - prev_ts)  \n",
    "            rate = dval / dts\n",
    "            out.append({\"ts\": ts, \"delta\": dval, \"rate_per_s\": rate})\n",
    "        prev_val = val\n",
    "        prev_ts = ts\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 610,
   "id": "580de0ec-3d23-4116-9daa-f745186d257d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prom_slo_check(\n",
    "    *,\n",
    "    max_dense_error_rate: float = 0.1,\n",
    "    max_complete_error_rate: float = 0.1,\n",
    "    enforce_rerank_ratio: float = 4.0,\n",
    ") -> Dict[str, Any]:\n",
    "    \"\"\"\n",
    "    - look at last deltas for gateway_fanout_errors_total{step=\"dense\"} and {step=\"complete\"}\n",
    "    - look at strategy counts rerank vs rrf\n",
    "    - spit suggestions\n",
    "    \"\"\"\n",
    "    dense_deltas = compute_deltas(\"gateway_fanout_errors_total\", {\"step\": \"dense\"}, limit=15)\n",
    "    complete_deltas = compute_deltas(\"gateway_fanout_errors_total\", {\"step\": \"complete\"}, limit=15)\n",
    "    strat_rerank = query_metric_over_time(\"gateway_strategy_selected_total\", {\"strategy\": \"rerank\"}, limit=15)\n",
    "    strat_rrf = query_metric_over_time(\"gateway_strategy_selected_total\", {\"strategy\": \"rrf\"}, limit=15)\n",
    "    latest_dense = next((d for d in reversed(dense_deltas) if d[\"delta\"] is not None), None)\n",
    "    latest_complete = next((d for d in reversed(complete_deltas) if d[\"delta\"] is not None), None)\n",
    "    latest_rerank = next((d for d in reversed(strat_rerank) if d[\"value\"] is not None), None)\n",
    "    latest_rrf = next((d for d in reversed(strat_rrf) if d[\"value\"] is not None), None)\n",
    "    hints: List[str] = []\n",
    "    status = \"ok\"\n",
    "    if latest_dense and latest_dense[\"delta\"] and latest_dense[\"delta\"] > 0:\n",
    "        hints.append(\"dense errors increasing  try chaos.force_bm25=true\")\n",
    "        status = \"warn\"\n",
    "    if latest_complete and latest_complete[\"delta\"] and latest_complete[\"delta\"] > 0:\n",
    "        hints.append(\"complete errors increasing  check backend /complete\")\n",
    "        status = \"warn\"\n",
    "    if latest_rerank and latest_rrf and latest_rrf[\"value\"] is not None:\n",
    "        rr = latest_rerank[\"value\"]\n",
    "        rf = latest_rrf[\"value\"]\n",
    "        if rf == 0:\n",
    "            if rr > 0:\n",
    "                hints.append(\"all traffic is rerank  lower CE or rl.update factual=0.5\")\n",
    "                status = \"warn\"\n",
    "        else:\n",
    "            ratio = rr / rf\n",
    "            if ratio > enforce_rerank_ratio:\n",
    "                hints.append(f\"rerank:rrf = {ratio:.1f}:1  consider nerfing rerank weights\")\n",
    "                status = \"warn\"\n",
    "    out = {\n",
    "        \"ts\": int(time.time()),\n",
    "        \"status\": status,\n",
    "        \"latest_dense_delta\": latest_dense,\n",
    "        \"latest_complete_delta\": latest_complete,\n",
    "        \"hints\": hints,\n",
    "    }\n",
    "    print(\"\\n=== prom SLO check ===\")\n",
    "    print(json.dumps(out, indent=2))\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 611,
   "id": "82a494d7-63b4-4188-946c-3e5aaf35e442",
   "metadata": {},
   "outputs": [],
   "source": [
    "def export_prom_ts_csv(limit: int = 120) -> Optional[Path]:\n",
    "    data = load_ts()\n",
    "    snaps = data.get(\"snapshots\", [])[-limit:]\n",
    "    if not snaps:\n",
    "        print(\" no TS to export\")\n",
    "        return None\n",
    "    buckets = set()\n",
    "    rows_tmp: List[Dict[str, Any]] = []\n",
    "    for snap in snaps:\n",
    "        ts = snap[\"ts\"]\n",
    "        row: Dict[str, Any] = {\"ts\": ts, \"when\": time.strftime(\"%Y-%m-%d %H:%M:%S\", time.localtime(ts))}\n",
    "        for m in snap[\"metrics\"]:\n",
    "            n = m[\"name\"]\n",
    "            lbl = m[\"labels\"]\n",
    "            val = m[\"value\"]\n",
    "            if n == \"gateway_answer_latency_ms_bucket\":\n",
    "                le = lbl.get(\"le\", \"\")\n",
    "                key = f\"lat_le_{le}\"\n",
    "                row[key] = val\n",
    "                buckets.add(key)\n",
    "            elif n == \"gateway_strategy_selected_total\":\n",
    "                strat = lbl.get(\"strategy\", \"unknown\")\n",
    "                key = f\"strat_{strat}\"\n",
    "                row[key] = val\n",
    "            elif n == \"gateway_fanout_errors_total\":\n",
    "                step = lbl.get(\"step\", \"unknown\")\n",
    "                key = f\"fanout_{step}\"\n",
    "                row[key] = val\n",
    "        rows_tmp.append(row)\n",
    "    headers = [\"ts\", \"when\"] + sorted(list(buckets)) + [\n",
    "        \"strat_rerank\",\n",
    "        \"strat_rrf\",\n",
    "        \"fanout_dense\",\n",
    "        \"fanout_complete\",\n",
    "        \"fanout_bm25\",\n",
    "    ]\n",
    "    with PROM_TS_CSV.open(\"w\", encoding=\"utf-8\") as f:\n",
    "        f.write(\",\".join(headers) + \"\\n\")\n",
    "        for row in rows_tmp:\n",
    "            vals = []\n",
    "            for h in headers:\n",
    "                v = row.get(h, \"\")\n",
    "                vals.append(str(v))\n",
    "            f.write(\",\".join(vals) + \"\\n\")\n",
    "    print(f\" prom TS CSV  {PROM_TS_CSV}\")\n",
    "    return PROM_TS_CSV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 612,
   "id": "ee1ad2af-a9fa-4d08-ab48-a8b922ce0944",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prom_ctl(cmd: str = \"scrape\") -> None:\n",
    "    cmd = cmd.strip().lower()\n",
    "    if cmd == \"scrape\":\n",
    "        scrape_and_append_ts()\n",
    "    elif cmd == \"slo\":\n",
    "        prom_slo_check()\n",
    "    elif cmd == \"csv\":\n",
    "        export_prom_ts_csv()\n",
    "    elif cmd == \"watch\":\n",
    "        for _ in range(5):\n",
    "            scrape_and_append_ts()\n",
    "            prom_slo_check()\n",
    "            time.sleep(2.0)\n",
    "    else:\n",
    "        print(\" unknown cmd, try: scrape | slo | csv | watch\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 613,
   "id": "4b95cd14-913f-487e-83fd-5bdc666b126f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " appended scrape to \\tmp\\art\\gateway_prom_timeseries.json (total=2)\n",
      "\n",
      "=== prom SLO check ===\n",
      "{\n",
      "  \"ts\": 1762114686,\n",
      "  \"status\": \"ok\",\n",
      "  \"latest_dense_delta\": null,\n",
      "  \"latest_complete_delta\": null,\n",
      "  \"hints\": []\n",
      "}\n",
      " prom TS CSV  \\tmp\\art\\gateway_prom_timeseries.csv\n",
      "\n",
      " prom TS continuation ready.\n",
      "Try:\n",
      "  prom_ctl('scrape')\n",
      "  prom_ctl('slo')\n",
      "  prom_ctl('csv')\n",
      "  compute_deltas('gateway_fanout_errors_total', {'step':'dense'})[:5]\n",
      "  query_metric_over_time('gateway_strategy_selected_total', {'strategy':'rerank'})[-5:]\n"
     ]
    }
   ],
   "source": [
    "scrape_and_append_ts()\n",
    "prom_slo_check()\n",
    "export_prom_ts_csv()\n",
    "print(\"\\n prom TS continuation ready.\")\n",
    "print(\"Try:\")\n",
    "print(\"  prom_ctl('scrape')\")\n",
    "print(\"  prom_ctl('slo')\")\n",
    "print(\"  prom_ctl('csv')\")\n",
    "print(\"  compute_deltas('gateway_fanout_errors_total', {'step':'dense'})[:5]\")\n",
    "print(\"  query_metric_over_time('gateway_strategy_selected_total', {'strategy':'rerank'})[-5:]\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 614,
   "id": "cb8509a7-0f05-49bd-88a4-220da01befe1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " using ART_DIR=\\tmp\\art\n",
      " admin token present: False\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "import time\n",
    "from pathlib import Path\n",
    "from typing import Any, Dict, List, Optional\n",
    "ART_DIR = Path(os.getenv(\"ART_DIR\", \"/tmp/art\"))\n",
    "ART_DIR.mkdir(parents=True, exist_ok=True)\n",
    "OPS_PATH = ART_DIR / \"gateway_ops.md\"\n",
    "ADMIN_TOKEN = os.getenv(\"GATEWAY_ADMIN_TOKEN\", \"\").strip()  \n",
    "print(f\" using ART_DIR={ART_DIR}\")\n",
    "print(f\" admin token present: {bool(ADMIN_TOKEN)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 615,
   "id": "a464f4ce-4b4e-4e50-a0b6-dd0983efb6c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _gw_base() -> str:\n",
    "    if \"gateway_url\" in globals():\n",
    "        u = gateway_url()\n",
    "        if u and u != \"not running\":\n",
    "            return u\n",
    "    return \"http://127.0.0.1:9910\"\n",
    "def _admin_headers(extra: Optional[Dict[str, str]] = None) -> Dict[str, str]:\n",
    "    h = {\"content-type\": \"application/json\"}\n",
    "    if ADMIN_TOKEN:\n",
    "        h[\"x-admin-token\"] = ADMIN_TOKEN\n",
    "    if extra:\n",
    "        h.update(extra)\n",
    "    return h\n",
    "def gw_admin_get(path: str, *, timeout: float = 2.5) -> Dict[str, Any]:\n",
    "    \"\"\"\n",
    "    GET to admin-ish endpoints, with optional token.\n",
    "    Works even if server doesn't require token yet.\n",
    "    \"\"\"\n",
    "    import requests\n",
    "    base = _gw_base()\n",
    "    url = f\"{base}{path}\"\n",
    "    try:\n",
    "        r = requests.get(url, headers=_admin_headers(), timeout=timeout)\n",
    "        try:\n",
    "            js = r.json()\n",
    "        except Exception:\n",
    "            js = {\"text\": r.text}\n",
    "        js[\"_status\"] = r.status_code\n",
    "        return js\n",
    "    except Exception as e:\n",
    "        return {\"ok\": False, \"error\": str(e), \"_url\": url}\n",
    "def gw_admin_post(path: str, body: Dict[str, Any], *, timeout: float = 2.5) -> Dict[str, Any]:\n",
    "    import requests\n",
    "    base = _gw_base()\n",
    "    url = f\"{base}{path}\"\n",
    "    try:\n",
    "        r = requests.post(url, headers=_admin_headers(), json=body, timeout=timeout)\n",
    "        try:\n",
    "            js = r.json()\n",
    "        except Exception:\n",
    "            js = {\"text\": r.text}\n",
    "        js[\"_status\"] = r.status_code\n",
    "        return js\n",
    "    except Exception as e:\n",
    "        return {\"ok\": False, \"error\": str(e), \"_url\": url}\n",
    "def check_admin_protection() -> Dict[str, Any]:\n",
    "    \"\"\"\n",
    "    quick probe: if server now wants a token, this will tell you.\n",
    "    \"\"\"\n",
    "    res = gw_admin_get(\"/admin/chaos/get\")\n",
    "    code = res.get(\"_status\")\n",
    "    if code == 401 or code == 403:\n",
    "        print(\" gateway wants an admin token. set GATEWAY_ADMIN_TOKEN in env & re-run.\")\n",
    "    else:\n",
    "        print(f\" admin reachable (status={code})\")\n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 616,
   "id": "458cd117-968a-4feb-a9fd-c2182e20d1fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "PROM_PARSED = ART_DIR / \"gateway_prometheus.json\"\n",
    "ROUTE_DIAG = ART_DIR / \"gateway_route_diag.json\"\n",
    "def _read_json_safe(p: Path) -> Optional[Dict[str, Any]]:\n",
    "    if not p.exists():\n",
    "        return None\n",
    "    try:\n",
    "        return json.loads(p.read_text(encoding=\"utf-8\"))\n",
    "    except Exception as e:\n",
    "        print(f\" failed to read {p}: {e}\")\n",
    "        return None\n",
    "def auto_mitigate_from_metrics(\n",
    "    dense_err_thresh: int = 0,\n",
    "    rerank_dom_ratio: float = 4.0,\n",
    "    apply: bool = True,\n",
    ") -> Dict[str, Any]:\n",
    "    \"\"\"\n",
    "    - if dense fanout errors > dense_err_thresh  suggest/force chaos.force_bm25\n",
    "    - if rerank >> rrf  suggest/force chaos.disable_ce\n",
    "    \"\"\"\n",
    "    prom = _read_json_safe(PROM_PARSED) or []\n",
    "    diag = _read_json_safe(ROUTE_DIAG) or {}\n",
    "    fanout = diag.get(\"fanout_errors\", {})\n",
    "    strategies = diag.get(\"strategies\", {})\n",
    "    actions: List[str] = []\n",
    "    to_apply: Dict[str, bool] = {}\n",
    "    dense_errs = fanout.get(\"dense\", 0)\n",
    "    if dense_errs > dense_err_thresh:\n",
    "        actions.append(f\"dense errors {dense_errs} > {dense_err_thresh}  force_bm25\")\n",
    "        to_apply[\"force_bm25\"] = True\n",
    "    rerank_ct = strategies.get(\"rerank\", 0)\n",
    "    rrf_ct = strategies.get(\"rrf\", 0)\n",
    "    if rrf_ct == 0 and rerank_ct > 0:\n",
    "        actions.append(\"rrf=0 but rerank>0  disable_ce\")\n",
    "        to_apply[\"disable_ce\"] = True\n",
    "    elif rrf_ct > 0:\n",
    "        ratio = rerank_ct / rrf_ct\n",
    "        if ratio > rerank_dom_ratio:\n",
    "            actions.append(f\"rerank/rrf ratio={ratio:.1f} > {rerank_dom_ratio}  disable_ce\")\n",
    "            to_apply[\"disable_ce\"] = True\n",
    "    out = {\n",
    "        \"ts\": int(time.time()),\n",
    "        \"fanout\": fanout,\n",
    "        \"strategies\": strategies,\n",
    "        \"actions\": actions,\n",
    "        \"to_apply\": to_apply,\n",
    "    }\n",
    "    if apply and to_apply:\n",
    "        if \"gw_chaos_set\" in globals():\n",
    "            print(\" applying chaos:\", to_apply)\n",
    "            applied = gw_chaos_set(**to_apply)\n",
    "            out[\"applied\"] = applied\n",
    "        else:\n",
    "            print(\" would apply:\", to_apply, \"(but no gw_chaos_set in scope)\")\n",
    "    elif not actions:\n",
    "        print(\" metrics look ok  no auto-mitigation\")\n",
    "    else:\n",
    "        print(\" actions suggested but not applied (apply=False)\")\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 617,
   "id": "8ad1833b-e1bf-4e5c-9cd0-0ca5a23f795f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_ops_cheatsheet() -> Path:\n",
    "    base = _gw_base()\n",
    "    lines: List[str] = []\n",
    "    lines.append(\"# Gateway Ops\")\n",
    "    lines.append(f\"_generated: {time.strftime('%Y-%m-%d %H:%M:%S', time.localtime())}_\")\n",
    "    lines.append(\"\")\n",
    "    lines.append(\"## Health\")\n",
    "    lines.append(f\"- `curl -s {base}/up | jq`\")\n",
    "    lines.append(f\"- `curl -s {base}/metrics | head -200`\")\n",
    "    lines.append(\"\")\n",
    "    lines.append(\"## Chaos\")\n",
    "    if ADMIN_TOKEN:\n",
    "        lines.append(f\"- `curl -s {base}/admin/chaos/get -H \\\"x-admin-token: {ADMIN_TOKEN}\\\" | jq`\")\n",
    "        lines.append(f\"- `curl -s -X POST {base}/admin/chaos/set -H \\\"x-admin-token: {ADMIN_TOKEN}\\\" -H 'content-type: application/json' -d '{{\\\"disable_ce\\\":true}}' | jq`\")\n",
    "    else:\n",
    "        lines.append(f\"- `curl -s {base}/admin/chaos/get | jq`\")\n",
    "        lines.append(f\"- `curl -s -X POST {base}/admin/chaos/set -H 'content-type: application/json' -d '{{\\\"disable_ce\\\":true}}' | jq`\")\n",
    "    lines.append(\"\")\n",
    "    lines.append(\"## RL Weights\")\n",
    "    if ADMIN_TOKEN:\n",
    "        lines.append(f\"- `curl -s -X POST {base}/rl/promote -H 'content-type: application/json' -H \\\"x-admin-token: {ADMIN_TOKEN}\\\" -d '{{}}' | jq`\")\n",
    "        lines.append(f\"- `curl -s -X POST {base}/rl/rollback -H 'content-type: application/json' -H \\\"x-admin-token: {ADMIN_TOKEN}\\\" -d '{{}}' | jq`\")\n",
    "    else:\n",
    "        lines.append(f\"- `curl -s -X POST {base}/rl/promote -H 'content-type: application/json' -d '{{}}' | jq`\")\n",
    "        lines.append(f\"- `curl -s -X POST {base}/rl/rollback -H 'content-type: application/json' -d '{{}}' | jq`\")\n",
    "    lines.append(\"\")\n",
    "    lines.append(\"## Strategy Mix\")\n",
    "    lines.append(f\"- `curl -s {base}/canary/strategy_counts | jq`\")\n",
    "    lines.append(\"\")\n",
    "    lines.append(\"## Emergency (slow / dense errors)\")\n",
    "    if ADMIN_TOKEN:\n",
    "        lines.append(f\"- `curl -s -X POST {base}/admin/chaos/set -H \\\"x-admin-token: {ADMIN_TOKEN}\\\" -H 'content-type: application/json' -d '{{\\\"force_bm25\\\":true}}' | jq`\")\n",
    "    else:\n",
    "        lines.append(f\"- `curl -s -X POST {base}/admin/chaos/set -H 'content-type: application/json' -d '{{\\\"force_bm25\\\":true}}' | jq`\")\n",
    "    OPS_PATH.write_text(\"\\n\".join(lines), encoding=\"utf-8\")\n",
    "    print(f\" ops cheatsheet  {OPS_PATH}\")\n",
    "    return OPS_PATH"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 618,
   "id": "1bd465b6-0a34-456c-a1bb-8a2b0a2aee26",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " admin reachable (status=200)\n",
      " ops cheatsheet  \\tmp\\art\\gateway_ops.md\n",
      " continuation (admin+ops+auto-mitigation) loaded.\n"
     ]
    }
   ],
   "source": [
    "def headless_ci_run(max_p95_ms: float = 1800.0) -> int:\n",
    "    \"\"\"\n",
    "    call this from CI or CLI:\n",
    "    - check admin\n",
    "    - auto-mitigate (non-fatal)\n",
    "    - enforce p95\n",
    "    \"\"\"\n",
    "    print(\"\\n=== headless CI run ===\")\n",
    "    check_admin_protection()\n",
    "    auto_mitigate_from_metrics(apply=False)  \n",
    "    if \"ci_gate_from_agg\" in globals():\n",
    "        code = ci_gate_from_agg(max_p95_ms=max_p95_ms)\n",
    "    else:\n",
    "        print(\" ci_gate_from_agg(...) not in scope, skipping perf gate\")\n",
    "        code = 0\n",
    "    write_ops_cheatsheet()\n",
    "    print(f\" CI exit code would be: {code}\")\n",
    "    return code\n",
    "_ = check_admin_protection()\n",
    "_ = write_ops_cheatsheet()\n",
    "print(\" continuation (admin+ops+auto-mitigation) loaded.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 619,
   "id": "1bf2762b-1c4e-4dff-933d-ed9aa3488e75",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " continuation (deploy + per-namespace chaos) using ART_DIR=\\tmp\\art\n",
      " gateway base = http://127.0.0.1:9910\n",
      " admin token set: False\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "import time\n",
    "from pathlib import Path\n",
    "from typing import Any, Dict, List, Optional\n",
    "ART_DIR = Path(os.getenv(\"ART_DIR\", \"/tmp/art\"))\n",
    "ART_DIR.mkdir(parents=True, exist_ok=True)\n",
    "print(f\" continuation (deploy + per-namespace chaos) using ART_DIR={ART_DIR}\")\n",
    "ADMIN_TOKEN = os.getenv(\"GATEWAY_ADMIN_TOKEN\", \"\").strip()\n",
    "GATEWAY_BASE = os.getenv(\"GATEWAY_BASE\", \"\").strip()\n",
    "if not GATEWAY_BASE:\n",
    "    if \"gateway_url\" in globals():\n",
    "        u = gateway_url()\n",
    "        if u and u != \"not running\":\n",
    "            GATEWAY_BASE = u\n",
    "if not GATEWAY_BASE:\n",
    "    GATEWAY_BASE = \"http://127.0.0.1:9910\"\n",
    "print(f\" gateway base = {GATEWAY_BASE}\")\n",
    "print(f\" admin token set: {bool(ADMIN_TOKEN)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 620,
   "id": "d6b93d37-5055-4117-9006-9dfe157e8ea8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _admin_headers(extra: Optional[Dict[str, str]] = None) -> Dict[str, str]:\n",
    "    h = {\"content-type\": \"application/json\"}\n",
    "    if ADMIN_TOKEN:\n",
    "        h[\"x-admin-token\"] = ADMIN_TOKEN\n",
    "    if extra:\n",
    "        h.update(extra)\n",
    "    return h"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 621,
   "id": "54e9f551-4bb7-497a-a2a6-4448606a700d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " per-namespace chaos helpers loaded (chaos_set_ns('tenant-acme', disable_ce=True))\n"
     ]
    }
   ],
   "source": [
    "def chaos_set_global(**flags) -> Dict[str, Any]:\n",
    "    \"\"\"Set chaos globally (current behavior).\"\"\"\n",
    "    import requests\n",
    "    r = requests.post(\n",
    "        f\"{GATEWAY_BASE}/admin/chaos/set\",\n",
    "        headers=_admin_headers(),\n",
    "        json=flags,\n",
    "        timeout=2.5,\n",
    "    )\n",
    "    try:\n",
    "        js = r.json()\n",
    "    except Exception:\n",
    "        js = {\"text\": r.text}\n",
    "    js[\"_status\"] = r.status_code\n",
    "    return js\n",
    "def chaos_set_ns(ns: str, **flags) -> Dict[str, Any]:\n",
    "    \"\"\"\n",
    "    Try per-namespace chaos: /admin/chaos/set?ns=<ns>\n",
    "    If gateway doesn't support it yet, it'll just 404/400.\n",
    "    \"\"\"\n",
    "    import requests\n",
    "    url = f\"{GATEWAY_BASE}/admin/chaos/set?ns={ns}\"\n",
    "    r = requests.post(\n",
    "        url,\n",
    "        headers=_admin_headers(),\n",
    "        json=flags,\n",
    "        timeout=2.5,\n",
    "    )\n",
    "    try:\n",
    "        js = r.json()\n",
    "    except Exception:\n",
    "        js = {\"text\": r.text}\n",
    "    js[\"_status\"] = r.status_code\n",
    "    return js\n",
    "def chaos_show() -> Dict[str, Any]:\n",
    "    import requests\n",
    "    r = requests.get(f\"{GATEWAY_BASE}/admin/chaos/get\", headers=_admin_headers(), timeout=2.5)\n",
    "    try:\n",
    "        js = r.json()\n",
    "    except Exception:\n",
    "        js = {\"text\": r.text}\n",
    "    js[\"_status\"] = r.status_code\n",
    "    return js\n",
    "print(\" per-namespace chaos helpers loaded (chaos_set_ns('tenant-acme', disable_ce=True))\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 622,
   "id": "741efadb-ff3b-419d-88bd-1a73a604ba0d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " wrote dockerfile  \\tmp\\art\\Dockerfile.gateway\n",
      " wrote compose  \\tmp\\art\\docker-compose.gateway.yml\n",
      " wrote k8s deploy  \\tmp\\art\\k8s-gateway-deployment.yaml\n",
      " wrote k8s svc     \\tmp\\art\\k8s-gateway-service.yaml\n",
      " wrote gha workflow  \\tmp\\art\\gateway-ci.yml\n"
     ]
    }
   ],
   "source": [
    "DOCKERFILE_PATH = ART_DIR / \"Dockerfile.gateway\"\n",
    "COMPOSE_PATH = ART_DIR / \"docker-compose.gateway.yml\"\n",
    "K8S_DEPLOY_PATH = ART_DIR / \"k8s-gateway-deployment.yaml\"\n",
    "K8S_SVC_PATH = ART_DIR / \"k8s-gateway-service.yaml\"\n",
    "GH_ACTION_PATH = ART_DIR / \"gateway-ci.yml\"\n",
    "def write_dockerfile() -> Path:\n",
    "    content = f\"\"\"# --- build ---\n",
    "FROM rust:1.80 as builder\n",
    "WORKDIR /app\n",
    "COPY gateway/ ./gateway/\n",
    "WORKDIR /app/gateway\n",
    "RUN cargo build --release\n",
    "\n",
    "# --- runtime ---\n",
    "FROM gcr.io/distroless/cc as runtime\n",
    "WORKDIR /app\n",
    "COPY --from=builder /app/gateway/target/release/gateway /app/gateway\n",
    "ENV GATEWAY_PORT=9910\n",
    "ENV RAG_BASE=http://mock-rag:9909\n",
    "# ENV GATEWAY_ADMIN_TOKEN=changeme\n",
    "EXPOSE 9910\n",
    "ENTRYPOINT [\"/app/gateway\"]\n",
    "\"\"\"\n",
    "    DOCKERFILE_PATH.write_text(content, encoding=\"utf-8\")\n",
    "    print(f\" wrote dockerfile  {DOCKERFILE_PATH}\")\n",
    "    return DOCKERFILE_PATH\n",
    "def write_compose() -> Path:\n",
    "    content = f\"\"\"version: \"3.9\"\n",
    "services:\n",
    "  gateway:\n",
    "    build:\n",
    "      context: .\n",
    "      dockerfile: {DOCKERFILE_PATH.name}\n",
    "    environment:\n",
    "      - RAG_BASE=http://mock-rag:9909\n",
    "      - GATEWAY_PORT=9910\n",
    "      # - GATEWAY_ADMIN_TOKEN={ADMIN_TOKEN or \"changeme\"}\n",
    "    ports:\n",
    "      - \"9910:9910\"\n",
    "    depends_on:\n",
    "      - mock-rag\n",
    "\n",
    "  mock-rag:\n",
    "    image: ghcr.io/openai/echo-server:latest\n",
    "    ports:\n",
    "      - \"9909:80\"\n",
    "\"\"\"\n",
    "    COMPOSE_PATH.write_text(content, encoding=\"utf-8\")\n",
    "    print(f\" wrote compose  {COMPOSE_PATH}\")\n",
    "    return COMPOSE_PATH\n",
    "def write_k8s() -> List[Path]:\n",
    "    deploy = f\"\"\"apiVersion: apps/v1\n",
    "kind: Deployment\n",
    "metadata:\n",
    "  name: rag-gateway\n",
    "  labels:\n",
    "    app: rag-gateway\n",
    "spec:\n",
    "  replicas: 1\n",
    "  selector:\n",
    "    matchLabels:\n",
    "      app: rag-gateway\n",
    "  template:\n",
    "    metadata:\n",
    "      labels:\n",
    "        app: rag-gateway\n",
    "    spec:\n",
    "      containers:\n",
    "        - name: gateway\n",
    "          image: your-registry/rag-gateway:latest\n",
    "          imagePullPolicy: IfNotPresent\n",
    "          env:\n",
    "            - name: RAG_BASE\n",
    "              value: \"http://mock-rag.default.svc.cluster.local:9909\"\n",
    "            - name: GATEWAY_PORT\n",
    "              value: \"9910\"\n",
    "          ports:\n",
    "            - containerPort: 9910\n",
    "          readinessProbe:\n",
    "            httpGet:\n",
    "              path: /up\n",
    "              port: 9910\n",
    "            initialDelaySeconds: 3\n",
    "            periodSeconds: 5\n",
    "          livenessProbe:\n",
    "            httpGet:\n",
    "              path: /up\n",
    "              port: 9910\n",
    "            initialDelaySeconds: 10\n",
    "            periodSeconds: 10\n",
    "\"\"\"\n",
    "    svc = \"\"\"apiVersion: v1\n",
    "kind: Service\n",
    "metadata:\n",
    "  name: rag-gateway\n",
    "spec:\n",
    "  selector:\n",
    "    app: rag-gateway\n",
    "  ports:\n",
    "    - protocol: TCP\n",
    "      port: 9910\n",
    "      targetPort: 9910\n",
    "      name: http\n",
    "\"\"\"\n",
    "    K8S_DEPLOY_PATH.write_text(deploy, encoding=\"utf-8\")\n",
    "    K8S_SVC_PATH.write_text(svc, encoding=\"utf-8\")\n",
    "    print(f\" wrote k8s deploy  {K8S_DEPLOY_PATH}\")\n",
    "    print(f\" wrote k8s svc     {K8S_SVC_PATH}\")\n",
    "    return [K8S_DEPLOY_PATH, K8S_SVC_PATH]\n",
    "def write_gh_action() -> Path:\n",
    "    content = f\"\"\"name: gateway-ci\n",
    "on:\n",
    "  push:\n",
    "    paths:\n",
    "      - \"gateway/**\"\n",
    "      - \".github/workflows/gateway-ci.yml\"\n",
    "  pull_request:\n",
    "    paths:\n",
    "      - \"gateway/**\"\n",
    "jobs:\n",
    "  build-and-test:\n",
    "    runs-on: ubuntu-latest\n",
    "    steps:\n",
    "      - uses: actions/checkout@v4\n",
    "\n",
    "      - name: Install Rust\n",
    "        uses: dtolnay/rust-toolchain@stable\n",
    "\n",
    "      - name: Build gateway\n",
    "        run: |\n",
    "          cd gateway\n",
    "          cargo build --release\n",
    "\n",
    "      - name: Run smoke notebook step (headless)\n",
    "        run: |\n",
    "          python - << 'PY'\n",
    "from pathlib import Path\n",
    "import json, os, sys\n",
    "ART_DIR = Path(\"/tmp/art\")\n",
    "agg = ART_DIR / \"gateway_runs_agg.json\"\n",
    "if agg.exists():\n",
    "    js = json.loads(agg.read_text())\n",
    "    runs = js.get(\"runs\", [])\n",
    "    if runs:\n",
    "        latest = runs[0]\n",
    "        p95 = latest.get(\"p95_ms\")\n",
    "        if p95 is not None and p95 > 1800.0:\n",
    "            print(f\"p95 too high: {{p95}} ms\")\n",
    "            sys.exit(1)\n",
    "print(\"ci-smoke ok\")\n",
    "PY\n",
    "\"\"\"\n",
    "    GH_ACTION_PATH.write_text(content, encoding=\"utf-8\")\n",
    "    print(f\" wrote gha workflow  {GH_ACTION_PATH}\")\n",
    "    return GH_ACTION_PATH\n",
    "_ = write_dockerfile()\n",
    "_ = write_compose()\n",
    "_ = write_k8s()\n",
    "_ = write_gh_action()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 623,
   "id": "87621d76-d567-4967-99db-809797353021",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " ops bundle  \\tmp\\art\\gateway_bundle.json\n",
      "\n",
      " continuation (per-ns chaos + deploy scaffolding + bundle) done.\n",
      "Try:\n",
      "  chaos_set_ns('tenant-acme', disable_ce=True)\n",
      "  # copy /tmp/art/gateway-ci.yml  .github/workflows/gateway-ci.yml\n",
      "  # docker compose -f /tmp/art/docker-compose.gateway.yml up --build\n"
     ]
    }
   ],
   "source": [
    "BUNDLE_PATH = ART_DIR / \"gateway_bundle.json\"\n",
    "def build_ops_bundle() -> Path:\n",
    "    files = {\n",
    "        \"ops_md\": str((ART_DIR / \"gateway_ops.md\")),\n",
    "        \"report_md\": str((ART_DIR / \"gateway_report.md\")),\n",
    "        \"report_html\": str((ART_DIR / \"gateway_report.html\")),\n",
    "        \"experiments_md\": str((ART_DIR / \"gateway_experiments.md\")),\n",
    "        \"experiments_html\": str((ART_DIR / \"gateway_experiments.html\")),\n",
    "        \"prom_report_md\": str((ART_DIR / \"gateway_prom_report.md\")),\n",
    "        \"prom_report_html\": str((ART_DIR / \"gateway_prom_report.html\")),\n",
    "        \"dockerfile\": str(DOCKERFILE_PATH),\n",
    "        \"compose\": str(COMPOSE_PATH),\n",
    "        \"k8s_deploy\": str(K8S_DEPLOY_PATH),\n",
    "        \"k8s_svc\": str(K8S_SVC_PATH),\n",
    "        \"gha\": str(GH_ACTION_PATH),\n",
    "    }\n",
    "    payload = {\n",
    "        \"ts\": int(time.time()),\n",
    "        \"gateway_base\": GATEWAY_BASE,\n",
    "        \"admin_token_present\": bool(ADMIN_TOKEN),\n",
    "        \"files\": files,\n",
    "    }\n",
    "    BUNDLE_PATH.write_text(json.dumps(payload, indent=2), encoding=\"utf-8\")\n",
    "    print(f\" ops bundle  {BUNDLE_PATH}\")\n",
    "    return BUNDLE_PATH\n",
    "_ = build_ops_bundle()\n",
    "print(\"\\n continuation (per-ns chaos + deploy scaffolding + bundle) done.\")\n",
    "print(\"Try:\")\n",
    "print(\"  chaos_set_ns('tenant-acme', disable_ce=True)\")\n",
    "print(\"  # copy /tmp/art/gateway-ci.yml  .github/workflows/gateway-ci.yml\")\n",
    "print(\"  # docker compose -f /tmp/art/docker-compose.gateway.yml up --build\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 624,
   "id": "16248ef7-bfab-4ef3-921b-58fc884713e2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " continuation (validator + helm-ish) using ART_DIR=\\tmp\\art\n",
      " gateway base = http://127.0.0.1:9910\n",
      " admin token set: False\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "import time\n",
    "from pathlib import Path\n",
    "from typing import Any, Dict, List, Optional\n",
    "ART_DIR = Path(os.getenv(\"ART_DIR\", \"/tmp/art\"))\n",
    "ART_DIR.mkdir(parents=True, exist_ok=True)\n",
    "print(f\" continuation (validator + helm-ish) using ART_DIR={ART_DIR}\")\n",
    "GATEWAY_BASE = os.getenv(\"GATEWAY_BASE\", \"\").strip()\n",
    "if not GATEWAY_BASE:\n",
    "    if \"gateway_url\" in globals():\n",
    "        u = gateway_url()\n",
    "        if u and u != \"not running\":\n",
    "            GATEWAY_BASE = u\n",
    "if not GATEWAY_BASE:\n",
    "    GATEWAY_BASE = \"http://127.0.0.1:9910\"\n",
    "ADMIN_TOKEN = os.getenv(\"GATEWAY_ADMIN_TOKEN\", \"\").strip()\n",
    "print(f\" gateway base = {GATEWAY_BASE}\")\n",
    "print(f\" admin token set: {bool(ADMIN_TOKEN)}\")\n",
    "MANIFEST_PATH = ART_DIR / \"gateway_ops_manifest.json\"\n",
    "HELM_VALUES_PATH = ART_DIR / \"gateway-values.yaml\"\n",
    "CHECKLIST_PATH = ART_DIR / \"gateway_checklist.md\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 625,
   "id": "1e915dfa-f63b-4bc2-b04c-b362be105c28",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _admin_headers(extra: Optional[Dict[str, str]] = None) -> Dict[str, str]:\n",
    "    h = {\"content-type\": \"application/json\"}\n",
    "    if ADMIN_TOKEN:\n",
    "        h[\"x-admin-token\"] = ADMIN_TOKEN\n",
    "    if extra:\n",
    "        h.update(extra)\n",
    "    return h\n",
    "def _http_get(path: str, *, timeout: float = 2.5) -> Dict[str, Any]:\n",
    "    import requests\n",
    "    url = path if path.startswith(\"http\") else f\"{GATEWAY_BASE}{path}\"\n",
    "    try:\n",
    "        r = requests.get(url, headers=_admin_headers(), timeout=timeout)\n",
    "        try:\n",
    "            js = r.json()\n",
    "        except Exception:\n",
    "            js = {\"text\": r.text}\n",
    "        js[\"_status\"] = r.status_code\n",
    "        js[\"_url\"] = url\n",
    "        return js\n",
    "    except Exception as e:\n",
    "        return {\"ok\": False, \"error\": str(e), \"_url\": url}\n",
    "def _http_post(path: str, body: Dict[str, Any], *, timeout: float = 2.5, headers: Optional[Dict[str, str]] = None) -> Dict[str, Any]:\n",
    "    import requests\n",
    "    url = path if path.startswith(\"http\") else f\"{GATEWAY_BASE}{path}\"\n",
    "    hh = _admin_headers()\n",
    "    if headers:\n",
    "        hh.update(headers)\n",
    "    try:\n",
    "        r = requests.post(url, headers=hh, json=body, timeout=timeout)\n",
    "        try:\n",
    "            js = r.json()\n",
    "        except Exception:\n",
    "            js = {\"text\": r.text}\n",
    "        js[\"_status\"] = r.status_code\n",
    "        js[\"_url\"] = url\n",
    "        return js\n",
    "    except Exception as e:\n",
    "        return {\"ok\": False, \"error\": str(e), \"_url\": url}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 626,
   "id": "e2c8d005-fd9b-4921-afd3-09fc3becc201",
   "metadata": {},
   "outputs": [],
   "source": [
    "def validate_gateway(echo_query: str = \"smoke test\", ns: str = \"default\") -> Dict[str, Any]:\n",
    "    \"\"\"\n",
    "    One-shot \"is this thing alive and semi-useful?\" check.\n",
    "    - /up\n",
    "    - /metrics\n",
    "    - /admin/chaos/get  (may 401/403  we record it)\n",
    "    - /answer\n",
    "    \"\"\"\n",
    "    print(\"\\n=== gateway e2e validate ===\")\n",
    "    up = _http_get(\"/up\")\n",
    "    print(\"up:\", up.get(\"_status\"), up.get(\"status\") or up.get(\"ok\"))\n",
    "    metrics = _http_get(\"/metrics\")\n",
    "    print(\"metrics:\", metrics.get(\"_status\"))\n",
    "    chaos = _http_get(\"/admin/chaos/get\")\n",
    "    print(\"chaos:\", chaos.get(\"_status\"))\n",
    "    ans_headers = {\"x-namespace\": ns}\n",
    "    ans = _http_post(\"/answer\", {\"query\": echo_query, \"top_k\": 2}, headers=ans_headers)\n",
    "    print(\"answer:\", ans.get(\"_status\"))\n",
    "    ok = (\n",
    "        up.get(\"_status\") == 200 and\n",
    "        ans.get(\"_status\") == 200\n",
    "    )\n",
    "    out = {\n",
    "        \"ts\": int(time.time()),\n",
    "        \"base\": GATEWAY_BASE,\n",
    "        \"up\": up,\n",
    "        \"metrics\": metrics,\n",
    "        \"chaos\": chaos,\n",
    "        \"answer\": ans,\n",
    "        \"ok\": ok,\n",
    "    }\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 627,
   "id": "b58aa2c9-800b-4533-9112-954a93f52921",
   "metadata": {},
   "outputs": [],
   "source": [
    "def smoke_namespaces(\n",
    "    namespaces: List[str],\n",
    "    query: str = \"explain RAG fan-out pipeline\",\n",
    "    top_k: int = 3,\n",
    ") -> List[Dict[str, Any]]:\n",
    "    \"\"\"\n",
    "    hit /answer for each ns to ensure per-ns chaos / routing doesn't explode\n",
    "    \"\"\"\n",
    "    print(\"\\n=== ns smoke ===\")\n",
    "    out: List[Dict[str, Any]] = []\n",
    "    for ns in namespaces:\n",
    "        print(f\" hitting ns={ns!r}\")\n",
    "        ans = _http_post(\n",
    "            \"/answer\",\n",
    "            {\"query\": query, \"top_k\": top_k},\n",
    "            headers={\"x-namespace\": ns},\n",
    "        )\n",
    "        meta = ans.get(\"meta\", {})\n",
    "        out.append(\n",
    "            {\n",
    "                \"ns\": ns,\n",
    "                \"status\": ans.get(\"_status\"),\n",
    "                \"pipeline\": meta.get(\"pipeline\"),\n",
    "                \"route\": meta.get(\"route\"),\n",
    "                \"partial\": meta.get(\"partial_retrieval\"),\n",
    "            }\n",
    "        )\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 628,
   "id": "d1e8a834-ae02-4154-99e6-35219594f04c",
   "metadata": {},
   "outputs": [],
   "source": [
    "ROTATED_TOKEN_PATH = ART_DIR / \"gateway_admin_token.txt\"\n",
    "def rotate_admin_token(new_token: Optional[str] = None) -> str:\n",
    "    \"\"\"\n",
    "    This does NOT tell the gateway to rotate;\n",
    "    it generates/persists a new token for your env/scripts.\n",
    "    You still have to tell the gateway (via config/env) to accept it.\n",
    "    \"\"\"\n",
    "    import secrets\n",
    "    tok = new_token or secrets.token_hex(16)\n",
    "    ROTATED_TOKEN_PATH.write_text(tok, encoding=\"utf-8\")\n",
    "    print(f\" wrote new admin token  {ROTATED_TOKEN_PATH}\")\n",
    "    return tok"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 629,
   "id": "5913a162-a42a-4c7a-93fe-351846053884",
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_helm_values(\n",
    "    image: str = \"your-registry/rag-gateway:latest\",\n",
    "    admin_token: Optional[str] = None,\n",
    "    rag_base: Optional[str] = None,\n",
    "    port: int = 9910,\n",
    ") -> Path:\n",
    "    \"\"\"\n",
    "    generate a values.yaml so you can `helm install rag-gw -f gateway-values.yaml`\n",
    "    \"\"\"\n",
    "    rag = rag_base or os.getenv(\"RAG_BASE\", \"http://mock-rag.default.svc.cluster.local:9909\")\n",
    "    token = admin_token or ADMIN_TOKEN or \"changeme\"\n",
    "    lines = []\n",
    "    lines.append(\"image:\")\n",
    "    lines.append(f\"  repository: {image}\")\n",
    "    lines.append(\"  tag: latest\")\n",
    "    lines.append(\"\")\n",
    "    lines.append(\"service:\")\n",
    "    lines.append(f\"  port: {port}\")\n",
    "    lines.append(\"\")\n",
    "    lines.append(\"env:\")\n",
    "    lines.append(f\"  RAG_BASE: \\\"{rag}\\\"\")\n",
    "    lines.append(f\"  GATEWAY_PORT: \\\"{port}\\\"\")\n",
    "    lines.append(f\"  GATEWAY_ADMIN_TOKEN: \\\"{token}\\\"\")\n",
    "    lines.append(\"\")\n",
    "    lines.append(\"resources:\")\n",
    "    lines.append(\"  limits:\")\n",
    "    lines.append(\"    cpu: 500m\")\n",
    "    lines.append(\"    memory: 512Mi\")\n",
    "    lines.append(\"  requests:\")\n",
    "    lines.append(\"    cpu: 250m\")\n",
    "    lines.append(\"    memory: 256Mi\")\n",
    "    HELM_VALUES_PATH.write_text(\"\\n\".join(lines), encoding=\"utf-8\")\n",
    "    print(f\" helm values  {HELM_VALUES_PATH}\")\n",
    "    return HELM_VALUES_PATH"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 630,
   "id": "dac708af-6a46-4572-97df-d4e6a46408c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_ops_checklist() -> Path:\n",
    "    base = GATEWAY_BASE\n",
    "    lines = []\n",
    "    lines.append(\"# Gateway Checklist\")\n",
    "    lines.append(\"\")\n",
    "    lines.append(\"1. **Health**\")\n",
    "    lines.append(f\"   - `curl -s {base}/up | jq`\")\n",
    "    lines.append(f\"   - `curl -s {base}/metrics | head -200`\")\n",
    "    lines.append(\"\")\n",
    "    lines.append(\"2. **Admin**\")\n",
    "    if ADMIN_TOKEN:\n",
    "        lines.append(f\"   - `curl -s {base}/admin/chaos/get -H \\\"x-admin-token: {ADMIN_TOKEN}\\\" | jq`\")\n",
    "    else:\n",
    "        lines.append(f\"   - `curl -s {base}/admin/chaos/get | jq`  # (no token set)\")\n",
    "    lines.append(\"\")\n",
    "    lines.append(\"3. **Smoke**\")\n",
    "    lines.append(f\"   - `curl -s -X POST {base}/answer -H 'content-type: application/json' -d '{{\\\"query\\\":\\\"smoke\\\",\\\"top_k\\\":2}}' | jq`\")\n",
    "    lines.append(\"\")\n",
    "    lines.append(\"4. **Perf (optional)**\")\n",
    "    lines.append(\"   - run notebook `loadgen(...)` or CLI bench\")\n",
    "    lines.append(\"\")\n",
    "    lines.append(\"5. **Metrics sanity**\")\n",
    "    lines.append(f\"   - `curl -s {base}/metrics | grep gateway_answer_latency_ms_bucket | head -20`\")\n",
    "    CHECKLIST_PATH.write_text(\"\\n\".join(lines), encoding=\"utf-8\")\n",
    "    print(f\" checklist  {CHECKLIST_PATH}\")\n",
    "    return CHECKLIST_PATH"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 631,
   "id": "855d62ba-aa48-43ee-8fad-6a86854485d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ci_preflight(\n",
    "    *,\n",
    "    namespaces: Optional[List[str]] = None,\n",
    "    max_p95_ms: float = 1800.0,\n",
    ") -> int:\n",
    "    \"\"\"\n",
    "    meant to be run in GH/Argo:\n",
    "    - validate gateway alive\n",
    "    - hit all namespaces\n",
    "    - (if notebook ci_gate_from_agg exists) enforce p95\n",
    "    returns exit code 0/1\n",
    "    \"\"\"\n",
    "    print(\"\\n=== ci preflight ===\")\n",
    "    res = validate_gateway()\n",
    "    if not res[\"ok\"]:\n",
    "        print(\" gateway basic validation failed\")\n",
    "        return 1\n",
    "    if namespaces:\n",
    "        ns_res = smoke_namespaces(namespaces)\n",
    "        bad = [r for r in ns_res if r[\"status\"] != 200]\n",
    "        if bad:\n",
    "            print(\" some namespaces failed:\", bad)\n",
    "            return 1\n",
    "    code = 0\n",
    "    if \"ci_gate_from_agg\" in globals():\n",
    "        code = ci_gate_from_agg(max_p95_ms=max_p95_ms)\n",
    "    else:\n",
    "        print(\" ci_gate_from_agg not in scope, skipping perf enforcement\")\n",
    "    manifest = {\n",
    "        \"ts\": int(time.time()),\n",
    "        \"gateway_base\": GATEWAY_BASE,\n",
    "        \"namespaces\": namespaces or [],\n",
    "        \"validate\": res,\n",
    "        \"exit_code\": code,\n",
    "    }\n",
    "    MANIFEST_PATH.write_text(json.dumps(manifest, indent=2), encoding=\"utf-8\")\n",
    "    print(f\" ops manifest  {MANIFEST_PATH}\")\n",
    "    print(f\" ci preflight exit={code}\")\n",
    "    return code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 632,
   "id": "1b609025-9247-41a0-83fd-767efb160dbf",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " helm values  \\tmp\\art\\gateway-values.yaml\n",
      " checklist  \\tmp\\art\\gateway_checklist.md\n",
      "\n",
      "=== ns smoke ===\n",
      " hitting ns='default'\n",
      " hitting ns='tenant-acme'\n",
      " hitting ns='tenant-beta'\n",
      "ns-smoke: [{'ns': 'default', 'status': 502, 'pipeline': None, 'route': None, 'partial': None}, {'ns': 'tenant-acme', 'status': 502, 'pipeline': None, 'route': None, 'partial': None}, {'ns': 'tenant-beta', 'status': 502, 'pipeline': None, 'route': None, 'partial': None}]\n",
      "\n",
      "=== ci preflight ===\n",
      "\n",
      "=== gateway e2e validate ===\n",
      "up: 200 True\n",
      "metrics: 200\n",
      "chaos: 200\n",
      "answer: 502\n",
      " gateway basic validation failed\n",
      "ci-preflight exit: 1\n",
      "\n",
      " continuation (validator + helm-values + ci-preflight) done.\n"
     ]
    }
   ],
   "source": [
    "_ = write_helm_values()\n",
    "_ = write_ops_checklist()\n",
    "smoke = smoke_namespaces([\"default\", \"tenant-acme\", \"tenant-beta\"])\n",
    "print(\"ns-smoke:\", smoke)\n",
    "pref = ci_preflight(namespaces=[\"default\", \"tenant-acme\"])\n",
    "print(\"ci-preflight exit:\", pref)\n",
    "print(\"\\n continuation (validator + helm-values + ci-preflight) done.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 633,
   "id": "debeee6f-b1f4-49b4-96de-528df21086c1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " continuation (audit+drift+sla) using ART_DIR=\\tmp\\art\n",
      " gateway base = http://127.0.0.1:9910\n",
      " admin token set: False\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "import time\n",
    "from pathlib import Path\n",
    "from typing import Any, Dict, List, Optional, Tuple\n",
    "ART_DIR = Path(os.getenv(\"ART_DIR\", \"/tmp/art\"))\n",
    "ART_DIR.mkdir(parents=True, exist_ok=True)\n",
    "print(f\" continuation (audit+drift+sla) using ART_DIR={ART_DIR}\")\n",
    "GATEWAY_BASE = os.getenv(\"GATEWAY_BASE\", \"\").strip()\n",
    "if not GATEWAY_BASE:\n",
    "    if \"gateway_url\" in globals():\n",
    "        u = gateway_url()\n",
    "        if u and u != \"not running\":\n",
    "            GATEWAY_BASE = u\n",
    "if not GATEWAY_BASE:\n",
    "    GATEWAY_BASE = \"http://127.0.0.1:9910\"\n",
    "ADMIN_TOKEN = os.getenv(\"GATEWAY_ADMIN_TOKEN\", \"\").strip()\n",
    "AUDIT_LOG = ART_DIR / \"gateway_audit.jsonl\"\n",
    "DESIRED_STATE_PATH = ART_DIR / \"gateway_desired_state.json\"\n",
    "DRIFT_REPORT_PATH = ART_DIR / \"gateway_drift_report.json\"\n",
    "SLA_RUNS_PATH = ART_DIR / \"gateway_sla_runs.json\"\n",
    "ONCALL_MD = ART_DIR / \"gateway_oncall.md\"\n",
    "print(f\" gateway base = {GATEWAY_BASE}\")\n",
    "print(f\" admin token set: {bool(ADMIN_TOKEN)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 634,
   "id": "ede004e6-667c-4044-a6d1-3ccbdefe06e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _admin_headers(extra: Optional[Dict[str, str]] = None) -> Dict[str, str]:\n",
    "    h = {\"content-type\": \"application/json\"}\n",
    "    if ADMIN_TOKEN:\n",
    "        h[\"x-admin-token\"] = ADMIN_TOKEN\n",
    "    if extra:\n",
    "        h.update(extra)\n",
    "    return h\n",
    "def _http_get(path: str, *, timeout: float = 2.5) -> Dict[str, Any]:\n",
    "    import requests\n",
    "    url = path if path.startswith(\"http\") else f\"{GATEWAY_BASE}{path}\"\n",
    "    r = requests.get(url, headers=_admin_headers(), timeout=timeout)\n",
    "    try:\n",
    "        js = r.json()\n",
    "    except Exception:\n",
    "        js = {\"text\": r.text}\n",
    "    js[\"_status\"] = r.status_code\n",
    "    js[\"_url\"] = url\n",
    "    return js\n",
    "def _http_post(path: str, body: Dict[str, Any], *, timeout: float = 2.5, headers: Optional[Dict[str, str]] = None) -> Dict[str, Any]:\n",
    "    import requests\n",
    "    url = path if path.startswith(\"http\") else f\"{GATEWAY_BASE}{path}\"\n",
    "    hh = _admin_headers()\n",
    "    if headers:\n",
    "        hh.update(headers)\n",
    "    r = requests.post(url, headers=hh, json=body, timeout=timeout)\n",
    "    try:\n",
    "        js = r.json()\n",
    "    except Exception:\n",
    "        js = {\"text\": r.text}\n",
    "    js[\"_status\"] = r.status_code\n",
    "    js[\"_url\"] = url\n",
    "    return js"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 635,
   "id": "70272533-e60f-4a7d-bde7-530429231d19",
   "metadata": {},
   "outputs": [],
   "source": [
    "def audit_admin(action: str, payload: Dict[str, Any], resp: Dict[str, Any]) -> None:\n",
    "    \"\"\"\n",
    "    Append to /tmp/art/gateway_audit.jsonl so we know WHO/WHAT changed chaos/rl.\n",
    "    \"\"\"\n",
    "    rec = {\n",
    "        \"ts\": int(time.time()),\n",
    "        \"action\": action,\n",
    "        \"payload\": payload,\n",
    "        \"resp_status\": resp.get(\"_status\"),\n",
    "        \"resp\": resp,\n",
    "    }\n",
    "    with AUDIT_LOG.open(\"a\", encoding=\"utf-8\") as f:\n",
    "        f.write(json.dumps(rec) + \"\\n\")\n",
    "    print(f\" audit  {AUDIT_LOG} :: {action}\")\n",
    "def audited_chaos_set(**flags) -> Dict[str, Any]:\n",
    "    resp = _http_post(\"/admin/chaos/set\", flags)\n",
    "    audit_admin(\"chaos.set\", flags, resp)\n",
    "    return resp\n",
    "def audited_rl_update(weights: Dict[str, float]) -> Dict[str, Any]:\n",
    "    resp = _http_post(\"/rl/update\", {\"weights\": weights})\n",
    "    audit_admin(\"rl.update\", {\"weights\": weights}, resp)\n",
    "    return resp\n",
    "def audited_rl_promote(tag: Optional[str] = None) -> Dict[str, Any]:\n",
    "    body = {} if tag is None else {\"tag\": tag}\n",
    "    resp = _http_post(\"/rl/promote\", body)\n",
    "    audit_admin(\"rl.promote\", body, resp)\n",
    "    return resp\n",
    "def audited_rl_rollback(tag: Optional[str] = None) -> Dict[str, Any]:\n",
    "    body = {} if tag is None else {\"tag\": tag}\n",
    "    resp = _http_post(\"/rl/rollback\", body)\n",
    "    audit_admin(\"rl.rollback\", body, resp)\n",
    "    return resp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 636,
   "id": "541898be-9839-4dd7-9142-798293ba9aba",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_desired_state(\n",
    "    *,\n",
    "    chaos: Optional[Dict[str, Any]] = None,\n",
    "    rl_weights: Optional[Dict[str, float]] = None,\n",
    "    namespaces: Optional[List[str]] = None,\n",
    ") -> Path:\n",
    "    \"\"\"\n",
    "    Store what we WANT the gateway to look like.\n",
    "    Then we can diff it later.\n",
    "    \"\"\"\n",
    "    cur = {\n",
    "        \"ts\": int(time.time()),\n",
    "        \"chaos\": chaos or {},\n",
    "        \"rl_weights\": rl_weights or {},\n",
    "        \"namespaces\": namespaces or [],\n",
    "    }\n",
    "    DESIRED_STATE_PATH.write_text(json.dumps(cur, indent=2), encoding=\"utf-8\")\n",
    "    print(f\" desired state  {DESIRED_STATE_PATH}\")\n",
    "    return DESIRED_STATE_PATH\n",
    "def load_desired_state() -> Dict[str, Any]:\n",
    "    if not DESIRED_STATE_PATH.exists():\n",
    "        return {\"chaos\": {}, \"rl_weights\": {}, \"namespaces\": []}\n",
    "    return json.loads(DESIRED_STATE_PATH.read_text(encoding=\"utf-8\"))\n",
    "def fetch_live_state() -> Dict[str, Any]:\n",
    "    chaos = _http_get(\"/admin/chaos/get\")\n",
    "    rl = _http_post(\"/rl/update\", {\"weights\": {}})  \n",
    "    return {\n",
    "        \"ts\": int(time.time()),\n",
    "        \"chaos\": chaos,\n",
    "        \"rl\": rl,\n",
    "    }\n",
    "def detect_drift() -> Dict[str, Any]:\n",
    "    want = load_desired_state()\n",
    "    live = fetch_live_state()\n",
    "    want_chaos = want.get(\"chaos\") or {}\n",
    "    live_chaos = live.get(\"chaos\", {}).get(\"chaos\") or live.get(\"chaos\") or {}\n",
    "    want_rl = want.get(\"rl_weights\") or {}\n",
    "    live_rl = (live.get(\"rl\") or {}).get(\"weights\") or (live.get(\"rl\") or {}).get(\"current\") or {}\n",
    "    drift = {\n",
    "        \"ts\": int(time.time()),\n",
    "        \"chaos_drift\": {},\n",
    "        \"rl_drift\": {},\n",
    "    }\n",
    "    for k, v in want_chaos.items():\n",
    "        if live_chaos.get(k) != v:\n",
    "            drift[\"chaos_drift\"][k] = {\"want\": v, \"have\": live_chaos.get(k)}\n",
    "    for k, v in live_chaos.items():\n",
    "        if k not in want_chaos:\n",
    "            drift[\"chaos_drift\"][k] = {\"want\": None, \"have\": v}\n",
    "    for k, v in want_rl.items():\n",
    "        if live_rl.get(k) != v:\n",
    "            drift[\"rl_drift\"][k] = {\"want\": v, \"have\": live_rl.get(k)}\n",
    "    for k, v in (live_rl or {}).items():\n",
    "        if k not in want_rl:\n",
    "            drift[\"rl_drift\"][k] = {\"want\": None, \"have\": v}\n",
    "    DRIFT_REPORT_PATH.write_text(json.dumps(drift, indent=2), encoding=\"utf-8\")\n",
    "    print(f\" drift report  {DRIFT_REPORT_PATH}\")\n",
    "    if not drift[\"chaos_drift\"] and not drift[\"rl_drift\"]:\n",
    "        print(\" no drift\")\n",
    "    else:\n",
    "        print(\" drift detected\")\n",
    "    return drift\n",
    "def auto_remediate_drift(apply: bool = True) -> Dict[str, Any]:\n",
    "    drift = detect_drift()\n",
    "    want = load_desired_state()\n",
    "    fixes: List[str] = []\n",
    "    if drift[\"chaos_drift\"]:\n",
    "        fixes.append(\"chaos\")\n",
    "    if drift[\"rl_drift\"]:\n",
    "        fixes.append(\"rl\")\n",
    "    results: Dict[str, Any] = {\"applied\": False, \"fixes\": fixes, \"ts\": int(time.time())}\n",
    "    if not fixes:\n",
    "        return results\n",
    "    if not apply:\n",
    "        print(\" would fix drift but apply=False\")\n",
    "        return results\n",
    "    if \"chaos\" in fixes:\n",
    "        print(\" fixing chaos drift  applying desired chaos\")\n",
    "        resp = audited_chaos_set(**(want.get(\"chaos\") or {}))\n",
    "        results[\"chaos_resp\"] = resp\n",
    "    if \"rl\" in fixes and (want.get(\"rl_weights\") or {}):\n",
    "        print(\" fixing rl drift  applying desired rl\")\n",
    "        resp = audited_rl_update(want.get(\"rl_weights\") or {})\n",
    "        results[\"rl_resp\"] = resp\n",
    "    results[\"applied\"] = True\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 637,
   "id": "1337ae85-6046-4648-8986-56be04543f53",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_sla_synthetic(\n",
    "    *,\n",
    "    queries: Optional[List[str]] = None,\n",
    "    n_per_query: int = 5,\n",
    "    max_p95_ms: float = 1800.0,\n",
    "    namespaces: Optional[List[str]] = None,\n",
    ") -> Dict[str, Any]:\n",
    "    \"\"\"\n",
    "    Very small SLA smoke:\n",
    "    - hit a few typical queries\n",
    "    - per-namespace if provided\n",
    "    - record latencies\n",
    "    - fail if any p95 > threshold\n",
    "    \"\"\"\n",
    "    import requests\n",
    "    base = GATEWAY_BASE\n",
    "    if queries is None:\n",
    "        queries = [\n",
    "            \"explain RAG fan-out pipeline\",\n",
    "            \"SELECT id, name FROM customers LIMIT 5\",\n",
    "            \"HIPAA policy for PHI storage\",\n",
    "        ]\n",
    "    if namespaces is None:\n",
    "        namespaces = [\"default\"]\n",
    "    all_samples: List[float] = []\n",
    "    per_ns: Dict[str, List[float]] = {ns: [] for ns in namespaces}\n",
    "    per_query: Dict[str, List[float]] = {q: [] for q in queries}\n",
    "    for ns in namespaces:\n",
    "        for q in queries:\n",
    "            for _ in range(n_per_query):\n",
    "                t0 = time.time()\n",
    "                r = requests.post(\n",
    "                    f\"{base}/answer\",\n",
    "                    headers={\"content-type\": \"application/json\", \"x-namespace\": ns},\n",
    "                    json={\"query\": q, \"top_k\": 3},\n",
    "                    timeout=2.5,\n",
    "                )\n",
    "                dt = (time.time() - t0) * 1000.0\n",
    "                all_samples.append(dt)\n",
    "                per_ns[ns].append(dt)\n",
    "                per_query[q].append(dt)\n",
    "    def _p95(xs: List[float]) -> float:\n",
    "        if not xs:\n",
    "            return float(\"nan\")\n",
    "        xs = sorted(xs)\n",
    "        idx = max(0, int(len(xs) * 0.95) - 1)\n",
    "        return xs[idx]\n",
    "    summary = {\n",
    "        \"ts\": int(time.time()),\n",
    "        \"global\": {\n",
    "            \"n\": len(all_samples),\n",
    "            \"p95_ms\": _p95(all_samples),\n",
    "            \"avg_ms\": (sum(all_samples) / len(all_samples)) if all_samples else None,\n",
    "        },\n",
    "        \"per_ns\": {},\n",
    "        \"per_query\": {},\n",
    "        \"threshold\": max_p95_ms,\n",
    "    }\n",
    "    for ns, xs in per_ns.items():\n",
    "        summary[\"per_ns\"][ns] = {\n",
    "            \"n\": len(xs),\n",
    "            \"p95_ms\": _p95(xs),\n",
    "            \"avg_ms\": (sum(xs) / len(xs)) if xs else None,\n",
    "        }\n",
    "    for q, xs in per_query.items():\n",
    "        summary[\"per_query\"][q] = {\n",
    "            \"n\": len(xs),\n",
    "            \"p95_ms\": _p95(xs),\n",
    "            \"avg_ms\": (sum(xs) / len(xs)) if xs else None,\n",
    "        }\n",
    "    old = []\n",
    "    if SLA_RUNS_PATH.exists():\n",
    "        try:\n",
    "            old = json.loads(SLA_RUNS_PATH.read_text(encoding=\"utf-8\"))\n",
    "        except Exception:\n",
    "            old = []\n",
    "    old.insert(0, summary)\n",
    "    SLA_RUNS_PATH.write_text(json.dumps(old, indent=2), encoding=\"utf-8\")\n",
    "    print(f\" SLA run  {SLA_RUNS_PATH}\")\n",
    "    if summary[\"global\"][\"p95_ms\"] and summary[\"global\"][\"p95_ms\"] > max_p95_ms:\n",
    "        print(f\" SLA FAIL: p95={summary['global']['p95_ms']:.1f} ms > {max_p95_ms} ms\")\n",
    "    else:\n",
    "        print(f\" SLA OK: p95={summary['global']['p95_ms']:.1f} ms  {max_p95_ms} ms\")\n",
    "    return summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 638,
   "id": "b7cc7d66-fd6d-432a-a876-9fc92eb10d68",
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_oncall_md() -> Path:\n",
    "    base = GATEWAY_BASE\n",
    "    lines: List[str] = []\n",
    "    lines.append(\"# Gateway On-Call\")\n",
    "    lines.append(\"\")\n",
    "    lines.append(\"## 0. Ping\")\n",
    "    lines.append(f\"- `curl -s {base}/up | jq`\")\n",
    "    lines.append(f\"- `curl -s {base}/metrics | head -200`\")\n",
    "    lines.append(\"\")\n",
    "    lines.append(\"## 1. Check chaos\")\n",
    "    if ADMIN_TOKEN:\n",
    "        lines.append(f\"- `curl -s {base}/admin/chaos/get -H \\\"x-admin-token: {ADMIN_TOKEN}\\\" | jq`\")\n",
    "    else:\n",
    "        lines.append(f\"- `curl -s {base}/admin/chaos/get | jq`\")\n",
    "    lines.append(\"\")\n",
    "    lines.append(\"## 2. Force BM25 (dense broken)\")\n",
    "    if ADMIN_TOKEN:\n",
    "        lines.append(f\"- `curl -s -X POST {base}/admin/chaos/set -H \\\"x-admin-token: {ADMIN_TOKEN}\\\" -H 'content-type: application/json' -d '{{\\\"force_bm25\\\":true}}' | jq`\")\n",
    "    else:\n",
    "        lines.append(f\"- `curl -s -X POST {base}/admin/chaos/set -H 'content-type: application/json' -d '{{\\\"force_bm25\\\":true}}' | jq`\")\n",
    "    lines.append(\"\")\n",
    "    lines.append(\"## 3. Disable CE (rerank bloating latency)\")\n",
    "    if ADMIN_TOKEN:\n",
    "        lines.append(f\"- `curl -s -X POST {base}/admin/chaos/set -H \\\"x-admin-token: {ADMIN_TOKEN}\\\" -H 'content-type: application/json' -d '{{\\\"disable_ce\\\":true}}' | jq`\")\n",
    "    else:\n",
    "        lines.append(f\"- `curl -s -X POST {base}/admin/chaos/set -H 'content-type: application/json' -d '{{\\\"disable_ce\\\":true}}' | jq`\")\n",
    "    lines.append(\"\")\n",
    "    lines.append(\"## 4. See audit log (who touched it)\")\n",
    "    lines.append(f\"- `cat {AUDIT_LOG}`\")\n",
    "    lines.append(\"\")\n",
    "    lines.append(\"## 5. Re-apply desired state\")\n",
    "    lines.append(\"- open notebook and run `auto_remediate_drift(apply=True)`\")\n",
    "    ONCALL_MD.write_text(\"\\n\".join(lines), encoding=\"utf-8\")\n",
    "    print(f\" on-call sheet  {ONCALL_MD}\")\n",
    "    return ONCALL_MD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 639,
   "id": "c29e4887-9f16-479e-b222-bf42490ae6cc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== demo: drift + sla (non-fatal) ===\n",
      " drift report  \\tmp\\art\\gateway_drift_report.json\n",
      " drift detected\n",
      " on-call sheet  \\tmp\\art\\gateway_oncall.md\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'dict' object has no attribute 'insert'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[639], line 10\u001b[0m\n\u001b[0;32m      8\u001b[0m _ \u001b[38;5;241m=\u001b[39m detect_drift()\n\u001b[0;32m      9\u001b[0m _ \u001b[38;5;241m=\u001b[39m write_oncall_md()\n\u001b[1;32m---> 10\u001b[0m _ \u001b[38;5;241m=\u001b[39m run_sla_synthetic(\n\u001b[0;32m     11\u001b[0m     queries\u001b[38;5;241m=\u001b[39m[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msmoke test\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mexplain dense retriever vs bm25 for RAG\u001b[39m\u001b[38;5;124m\"\u001b[39m],\n\u001b[0;32m     12\u001b[0m     n_per_query\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m,\n\u001b[0;32m     13\u001b[0m     namespaces\u001b[38;5;241m=\u001b[39m[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdefault\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtenant-acme\u001b[39m\u001b[38;5;124m\"\u001b[39m],\n\u001b[0;32m     14\u001b[0m     max_p95_ms\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1800.0\u001b[39m,\n\u001b[0;32m     15\u001b[0m )\n\u001b[0;32m     16\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m continuation (audit+drift+sla) ready.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     17\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTry:\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[1;32mIn[637], line 77\u001b[0m, in \u001b[0;36mrun_sla_synthetic\u001b[1;34m(queries, n_per_query, max_p95_ms, namespaces)\u001b[0m\n\u001b[0;32m     75\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m:\n\u001b[0;32m     76\u001b[0m         old \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m---> 77\u001b[0m old\u001b[38;5;241m.\u001b[39minsert(\u001b[38;5;241m0\u001b[39m, summary)\n\u001b[0;32m     78\u001b[0m SLA_RUNS_PATH\u001b[38;5;241m.\u001b[39mwrite_text(json\u001b[38;5;241m.\u001b[39mdumps(old, indent\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m), encoding\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mutf-8\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     79\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m SLA run  \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mSLA_RUNS_PATH\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'dict' object has no attribute 'insert'"
     ]
    }
   ],
   "source": [
    "print(\"\\n=== demo: drift + sla (non-fatal) ===\")\n",
    "if not DESIRED_STATE_PATH.exists():\n",
    "    save_desired_state(\n",
    "        chaos={\"disable_ce\": False, \"force_bm25\": False, \"verifier_v2\": False},\n",
    "        rl_weights={\"factual\": 1.0, \"sql\": 1.0},\n",
    "        namespaces=[\"default\", \"tenant-acme\"],\n",
    "    )\n",
    "_ = detect_drift()\n",
    "_ = write_oncall_md()\n",
    "_ = run_sla_synthetic(\n",
    "    queries=[\"smoke test\", \"explain dense retriever vs bm25 for RAG\"],\n",
    "    n_per_query=2,\n",
    "    namespaces=[\"default\", \"tenant-acme\"],\n",
    "    max_p95_ms=1800.0,\n",
    ")\n",
    "print(\"\\n continuation (audit+drift+sla) ready.\")\n",
    "print(\"Try:\")\n",
    "print(\"  audited_chaos_set(disable_ce=True)\")\n",
    "print(\"  save_desired_state(chaos={\\\"disable_ce\\\":True})\")\n",
    "print(\"  auto_remediate_drift(apply=True)\")\n",
    "print(\"  run_sla_synthetic()  # quick perf gate\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b7e78bb-205c-4309-a920-e69d80587e30",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import time\n",
    "from pathlib import Path\n",
    "from typing import Any, Dict, List, Optional, Tuple\n",
    "ART_DIR = Path(os.getenv(\"ART_DIR\", \"/tmp/art\"))\n",
    "ART_DIR.mkdir(parents=True, exist_ok=True)\n",
    "print(f\" continuation (alerts + multi-gw + schema) using ART_DIR={ART_DIR}\")\n",
    "GATEWAY_BASE = os.getenv(\"GATEWAY_BASE\", \"\").strip() or \"http://127.0.0.1:9910\"\n",
    "ADMIN_TOKEN = os.getenv(\"GATEWAY_ADMIN_TOKEN\", \"\").strip()\n",
    "ALERT_WEBHOOK = os.getenv(\"GATEWAY_ALERT_WEBHOOK\", \"\").strip()  \n",
    "ARTIFACTS = {\n",
    "    \"agg\": ART_DIR / \"gateway_runs_agg.json\",\n",
    "    \"latest\": ART_DIR / \"gateway_smoke.json\",\n",
    "    \"telemetry\": ART_DIR / \"gateway_telemetry.json\",\n",
    "    \"prom_ts\": ART_DIR / \"gateway_prom_timeseries.json\",\n",
    "    \"drift\": ART_DIR / \"gateway_drift_report.json\",\n",
    "    \"sla\": ART_DIR / \"gateway_sla_runs.json\",\n",
    "}\n",
    "SCHEMA_VERSION = \"1.0.0\" "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1214bc1-2ab2-411b-bcc4-705a8daa3f0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _read_json_safe(p: Path) -> Optional[Dict[str, Any]]:\n",
    "    if not p.exists():\n",
    "        return None\n",
    "    try:\n",
    "        return json.loads(p.read_text(encoding=\"utf-8\"))\n",
    "    except Exception as e:\n",
    "        print(f\" cannot read {p}: {e}\")\n",
    "        return None\n",
    "def _write_json_versioned(p: Path, data: Dict[str, Any]) -> None:\n",
    "    \"\"\"\n",
    "    Always write with schema + ts so we know which version of the notebook produced it.\n",
    "    \"\"\"\n",
    "    data = dict(data)  \n",
    "    if \"_meta\" not in data:\n",
    "        data[\"_meta\"] = {}\n",
    "    data[\"_meta\"][\"schema_version\"] = SCHEMA_VERSION\n",
    "    data[\"_meta\"][\"written_at\"] = int(time.time())\n",
    "    p.write_text(json.dumps(data, indent=2), encoding=\"utf-8\")\n",
    "    print(f\" wrote {p} (schema={SCHEMA_VERSION})\")\n",
    "def _admin_headers(extra: Optional[Dict[str, str]] = None) -> Dict[str, str]:\n",
    "    h = {\"content-type\": \"application/json\"}\n",
    "    if ADMIN_TOKEN:\n",
    "        h[\"x-admin-token\"] = ADMIN_TOKEN\n",
    "    if extra:\n",
    "        h.update(extra)\n",
    "    return h\n",
    "def _http_json(method: str, url: str, *, json_body: Optional[Dict[str, Any]] = None, timeout: float = 2.5) -> Dict[str, Any]:\n",
    "    import requests\n",
    "    try:\n",
    "        if method.upper() == \"GET\":\n",
    "            r = requests.get(url, headers=_admin_headers(), timeout=timeout)\n",
    "        else:\n",
    "            r = requests.post(url, headers=_admin_headers(), json=json_body or {}, timeout=timeout)\n",
    "        try:\n",
    "            js = r.json()\n",
    "        except Exception:\n",
    "            js = {\"text\": r.text}\n",
    "        js[\"_status\"] = r.status_code\n",
    "        js[\"_url\"] = url\n",
    "        return js\n",
    "    except Exception as e:\n",
    "        return {\"ok\": False, \"error\": str(e), \"_url\": url, \"_status\": 599}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e71cb8a4-6838-41ea-96bf-0f2702f5ba3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def send_alert(msg: str, *, level: str = \"warn\", extra: Optional[Dict[str, Any]] = None) -> None:\n",
    "    \"\"\"\n",
    "    Push a message to webhook, or just print if webhook not set.\n",
    "    Payload is slack-compatible (simple).\n",
    "    \"\"\"\n",
    "    payload = {\n",
    "        \"text\": f\"[gateway/{level}] {msg}\",\n",
    "        \"level\": level,\n",
    "        \"extra\": extra or {},\n",
    "    }\n",
    "    if not ALERT_WEBHOOK:\n",
    "        print(f\" (dry-run alert): {json.dumps(payload)}\")\n",
    "        return\n",
    "    import requests\n",
    "    try:\n",
    "        r = requests.post(ALERT_WEBHOOK, json=payload, timeout=2.5)\n",
    "        print(f\" alert sent  {r.status_code}\")\n",
    "    except Exception as e:\n",
    "        print(f\" alert send failed: {e}\")\n",
    "def alert_on_p95(threshold_ms: float = 1800.0) -> Optional[Dict[str, Any]]:\n",
    "    \"\"\"\n",
    "    Read gateway_runs_agg.json and alert if latest p95 > threshold.\n",
    "    \"\"\"\n",
    "    agg = _read_json_safe(ARTIFACTS[\"agg\"])\n",
    "    if not agg:\n",
    "        print(\" no agg  no p95 alert\")\n",
    "        return None\n",
    "    rows = agg.get(\"runs\", [])\n",
    "    if not rows:\n",
    "        print(\" agg has no runs\")\n",
    "        return None\n",
    "    latest = rows[0]\n",
    "    p95 = latest.get(\"p95_ms\")\n",
    "    if isinstance(p95, (int, float)) and p95 > threshold_ms:\n",
    "        msg = f\"p95 regression: {p95:.1f} ms > {threshold_ms} ms (file={latest.get('file')})\"\n",
    "        send_alert(msg, level=\"error\", extra=latest)\n",
    "        return {\"alerted\": True, \"p95\": p95}\n",
    "    print(f\" p95 ok: {p95} ms  {threshold_ms} ms\")\n",
    "    return {\"alerted\": False, \"p95\": p95}\n",
    "def alert_on_dense_errors(dense_limit: int = 0) -> Optional[Dict[str, Any]]:\n",
    "    \"\"\"\n",
    "    Look at gateway_telemetry.json (written by telemetry code) and alert if dense fanout errors > limit\n",
    "    \"\"\"\n",
    "    tel = _read_json_safe(ARTIFACTS[\"telemetry\"])\n",
    "    if not tel:\n",
    "        print(\" no telemetry  skip dense alert\")\n",
    "        return None\n",
    "    fanout = (tel.get(\"suggest\") or {}).get(\"fanout\") or {}\n",
    "    dense_err = fanout.get(\"dense\", 0)\n",
    "    if dense_err > dense_limit:\n",
    "        msg = f\"dense fanout errors = {dense_err} (> {dense_limit})  consider chaos.force_bm25=true\"\n",
    "        send_alert(msg, level=\"warn\", extra=fanout)\n",
    "        return {\"alerted\": True, \"dense_errors\": dense_err}\n",
    "    print(f\" dense fanout ok: {dense_err}  {dense_limit}\")\n",
    "    return {\"alerted\": False, \"dense_errors\": dense_err}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36929bc6-5415-4781-a073-5aeb0dc49920",
   "metadata": {},
   "outputs": [],
   "source": [
    "MULTI_GW_PATH = ART_DIR / \"gateway_multi_drift.json\"\n",
    "def fetch_live_state_for_base(base: str) -> Dict[str, Any]:\n",
    "    chaos = _http_json(\"GET\", f\"{base}/admin/chaos/get\")\n",
    "    rl = _http_json(\"POST\", f\"{base}/rl/update\", json_body={\"weights\": {}})\n",
    "    return {\n",
    "        \"base\": base,\n",
    "        \"chaos\": chaos,\n",
    "        \"rl\": rl,\n",
    "        \"ts\": int(time.time()),\n",
    "    }\n",
    "def compare_state(want: Dict[str, Any], have: Dict[str, Any]) -> Dict[str, Any]:\n",
    "    want_chaos = want.get(\"chaos\") or {}\n",
    "    have_chaos = have.get(\"chaos\", {}).get(\"chaos\") or have.get(\"chaos\") or {}\n",
    "    want_rl = want.get(\"rl_weights\") or {}\n",
    "    have_rl = (have.get(\"rl\") or {}).get(\"weights\") or (have.get(\"rl\") or {}).get(\"current\") or {}\n",
    "    drift_chaos = {}\n",
    "    drift_rl = {}\n",
    "    for k, v in want_chaos.items():\n",
    "        if have_chaos.get(k) != v:\n",
    "            drift_chaos[k] = {\"want\": v, \"have\": have_chaos.get(k)}\n",
    "    for k, v in have_chaos.items():\n",
    "        if k not in want_chaos:\n",
    "            drift_chaos[k] = {\"want\": None, \"have\": v}\n",
    "    for k, v in want_rl.items():\n",
    "        if have_rl.get(k) != v:\n",
    "            drift_rl[k] = {\"want\": v, \"have\": have_rl.get(k)}\n",
    "    for k, v in (have_rl or {}).items():\n",
    "        if k not in want_rl:\n",
    "            drift_rl[k] = {\"want\": None, \"have\": v}\n",
    "    return {\n",
    "        \"chaos_drift\": drift_chaos,\n",
    "        \"rl_drift\": drift_rl,\n",
    "    }\n",
    "def multi_gateway_drift(\n",
    "    bases: Optional[List[str]] = None,\n",
    "    desired_path: Optional[Path] = None,\n",
    "    alert: bool = True,\n",
    ") -> Path:\n",
    "    \"\"\"\n",
    "    Check multiple gateways (prod/stg/eu) against same desired_state.json\n",
    "    \"\"\"\n",
    "    if bases is None:\n",
    "        bases = [\n",
    "            GATEWAY_BASE,\n",
    "            os.getenv(\"GATEWAY_BASE_STAGING\", \"\").strip() or \"http://127.0.0.1:9920\",\n",
    "            os.getenv(\"GATEWAY_BASE_EU\", \"\").strip() or \"http://127.0.0.1:9930\",\n",
    "        ]\n",
    "    if desired_path is None:\n",
    "        desired_path = ART_DIR / \"gateway_desired_state.json\"\n",
    "    desired = _read_json_safe(desired_path) or {\"chaos\": {}, \"rl_weights\": {}}\n",
    "    report: Dict[str, Any] = {\n",
    "        \"ts\": int(time.time()),\n",
    "        \"schema_version\": SCHEMA_VERSION,\n",
    "        \"desired\": desired,\n",
    "        \"gateways\": [],\n",
    "    }\n",
    "    any_drift = False\n",
    "    for base in bases:\n",
    "        live = fetch_live_state_for_base(base)\n",
    "        diff = compare_state(desired, live)\n",
    "        gw_rep = {\n",
    "            \"base\": base,\n",
    "            \"ts\": live[\"ts\"],\n",
    "            \"diff\": diff,\n",
    "        }\n",
    "        report[\"gateways\"].append(gw_rep)\n",
    "        if diff[\"chaos_drift\"] or diff[\"rl_drift\"]:\n",
    "            any_drift = True\n",
    "    _write_json_versioned(MULTI_GW_PATH, report)\n",
    "    if any_drift and alert:\n",
    "        send_alert(\"multi-gateway drift detected  see gateway_multi_drift.json\", level=\"error\", extra=report)\n",
    "    print(f\" multi-gw drift  {MULTI_GW_PATH}\")\n",
    "    return MULTI_GW_PATH"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af739de9-bdbe-41b8-926d-340aeace1a6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _fake_prom_metrics(dense_err: int = 0, rerank: int = 10, rrf: int = 5) -> Dict[str, Any]:\n",
    "    \"\"\"\n",
    "    Create a fake telemetry-like object so we can test the suggestions\n",
    "    without hitting real /metrics\n",
    "    \"\"\"\n",
    "    return {\n",
    "        \"ts\": int(time.time()),\n",
    "        \"route_diag\": {\n",
    "            \"strategies\": {\n",
    "                \"rerank\": rerank,\n",
    "                \"rrf\": rrf,\n",
    "            },\n",
    "            \"fanout_errors\": {\n",
    "                \"dense\": dense_err,\n",
    "            },\n",
    "        },\n",
    "    }\n",
    "def test_metric_logic() -> None:\n",
    "    \"\"\"\n",
    "    Quick assertions  won't crash the notebook if they fail; just prints.\n",
    "    \"\"\"\n",
    "    from pprint import pprint\n",
    "    print(\"\\n=== test: dense err  force_bm25 ===\")\n",
    "    fake = _fake_prom_metrics(dense_err=5, rerank=10, rrf=5)\n",
    "    hints = []\n",
    "    fanout = fake[\"route_diag\"][\"fanout_errors\"]\n",
    "    strat = fake[\"route_diag\"][\"strategies\"]\n",
    "    if fanout.get(\"dense\", 0) > 0:\n",
    "        hints.append(\"dense step is erroring  try POST /admin/chaos/set {\\\"force_bm25\\\": true}\")\n",
    "    if strat.get(\"rerank\", 0) > (strat.get(\"rrf\", 0) * 4 + 5):\n",
    "        hints.append(\"rerank dominates  consider lowering /rl/update factual:0.5 or chaos.disable_ce=true\")\n",
    "    pprint(hints)\n",
    "    if not hints or \"force_bm25\" not in \" \".join(hints):\n",
    "        print(\" expected 'force_bm25' style hint\")\n",
    "    else:\n",
    "        print(\" metric  action logic ok for dense errors\")\n",
    "    print(\"\\n=== test: rerank domination  disable_ce ===\")\n",
    "    fake2 = _fake_prom_metrics(dense_err=0, rerank=50, rrf=5)\n",
    "    hints2 = []\n",
    "    fanout2 = fake2[\"route_diag\"][\"fanout_errors\"]\n",
    "    strat2 = fake2[\"route_diag\"][\"strategies\"]\n",
    "    if fanout2.get(\"dense\", 0) > 0:\n",
    "        hints2.append(\"dense step is erroring  try POST /admin/chaos/set {\\\"force_bm25\\\": true}\")\n",
    "    if strat2.get(\"rerank\", 0) > (strat2.get(\"rrf\", 0) * 4 + 5):\n",
    "        hints2.append(\"rerank dominates  consider lowering /rl/update factual:0.5 or chaos.disable_ce=true\")\n",
    "    pprint(hints2)\n",
    "    if not hints2:\n",
    "        print(\" expected rerank / disable_ce hint\")\n",
    "    else:\n",
    "        print(\" metric  action logic ok for rerank dominance\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 640,
   "id": "5dc166fe-4ac4-4f4d-a24e-95ff1dbdf53d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== alerts check ===\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'alert_on_p95' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[640], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m=== alerts check ===\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m----> 2\u001b[0m alert_on_p95(threshold_ms\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1800.0\u001b[39m)\n\u001b[0;32m      3\u001b[0m alert_on_dense_errors(dense_limit\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m)\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m=== multi-gateway drift check (demo bases) ===\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'alert_on_p95' is not defined"
     ]
    }
   ],
   "source": [
    "print(\"\\n=== alerts check ===\")\n",
    "alert_on_p95(threshold_ms=1800.0)\n",
    "alert_on_dense_errors(dense_limit=0)\n",
    "print(\"\\n=== multi-gateway drift check (demo bases) ===\")\n",
    "_ = multi_gateway_drift()\n",
    "print(\"\\n=== run metric tests ===\")\n",
    "test_metric_logic()\n",
    "print(\"\\n continuation (alerts + multi-gw + schema + tests) loaded.\")\n",
    "print(\"Tip:\")\n",
    "print(\"  send_alert('manual check from notebook')\")\n",
    "print(\"  multi_gateway_drift(['http://gw-prod:9910','http://gw-stg:9910'])\")\n",
    "print(\"  # set GATEWAY_ALERT_WEBHOOK to actually notify Slack/Discord/etc.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 641,
   "id": "f1ff8b24-b15f-4b88-af78-deaf9b04152e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " continuation (rbac + tenants + retention + bundle) using ART_DIR=\\tmp\\art\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "import time\n",
    "import tarfile\n",
    "from pathlib import Path\n",
    "from typing import Any, Dict, List, Optional, Tuple\n",
    "ART_DIR = Path(os.getenv(\"ART_DIR\", \"/tmp/art\"))\n",
    "ART_DIR.mkdir(parents=True, exist_ok=True)\n",
    "print(f\" continuation (rbac + tenants + retention + bundle) using ART_DIR={ART_DIR}\")\n",
    "GATEWAY_BASE = os.getenv(\"GATEWAY_BASE\", \"\").strip() or \"http://127.0.0.1:9910\"\n",
    "ADMIN_TOKEN = os.getenv(\"GATEWAY_ADMIN_TOKEN\", \"\").strip()\n",
    "RBAC_PATH = ART_DIR / \"gateway_rbac.json\"\n",
    "TENANTS_PATH = ART_DIR / \"gateway_tenants.json\"\n",
    "RETENTION_CFG = ART_DIR / \"gateway_retention.json\"\n",
    "FULL_BUNDLE = ART_DIR / \"gateway_full_bundle.tar.gz\"\n",
    "KNOWN_ARTS = [\n",
    "    \"gateway_runs_agg.json\",\n",
    "    \"gateway_smoke.json\",\n",
    "    \"gateway_report.md\",\n",
    "    \"gateway_report.html\",\n",
    "    \"gateway_experiments.json\",\n",
    "    \"gateway_experiments.md\",\n",
    "    \"gateway_experiments.html\",\n",
    "    \"gateway_prometheus.txt\",\n",
    "    \"gateway_prometheus.json\",\n",
    "    \"gateway_route_diag.json\",\n",
    "    \"gateway_latency_hist.json\",\n",
    "    \"gateway_prom_report.md\",\n",
    "    \"gateway_prom_report.html\",\n",
    "    \"gateway_prom_timeseries.json\",\n",
    "    \"gateway_audit.jsonl\",\n",
    "    \"gateway_desired_state.json\",\n",
    "    \"gateway_drift_report.json\",\n",
    "    \"gateway_sla_runs.json\",\n",
    "    \"gateway_ops.md\",\n",
    "    \"gateway_oncall.md\",\n",
    "    \"gateway_bundle.json\",\n",
    "    \"gateway_multi_drift.json\",\n",
    "    \"docker-compose.gateway.yml\",\n",
    "    \"Dockerfile.gateway\",\n",
    "    \"k8s-gateway-deployment.yaml\",\n",
    "    \"k8s-gateway-service.yaml\",\n",
    "    \"gateway-ci.yml\",\n",
    "    \"gateway-values.yaml\",\n",
    "    \"gateway_checklist.md\",\n",
    "]\n",
    "def _now_ts() -> int:\n",
    "    return int(time.time())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 642,
   "id": "f85ad606-4655-4cc0-9ebc-a00889be9a19",
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_default_rbac() -> Path:\n",
    "    \"\"\"\n",
    "    We can't enforce RBAC on the real gateway from here,\n",
    "    but we can write a single source of truth the gateway can read.\n",
    "    \"\"\"\n",
    "    data = {\n",
    "        \"ts\": _now_ts(),\n",
    "        \"schema\": \"rbac/1\",\n",
    "        \"roles\": {\n",
    "            \"admin\": [\n",
    "                \"chaos.set\",\n",
    "                \"chaos.ns.set\",\n",
    "                \"rl.update\",\n",
    "                \"rl.promote\",\n",
    "                \"rl.rollback\",\n",
    "                \"drift.remediate\",\n",
    "                \"ops.write\",\n",
    "            ],\n",
    "            \"ml\": [\n",
    "                \"rl.update\",\n",
    "                \"rl.promote\",\n",
    "                \"rl.rollback\",\n",
    "            ],\n",
    "            \"sre\": [\n",
    "                \"chaos.set\",\n",
    "                \"chaos.ns.set\",\n",
    "                \"drift.remediate\",\n",
    "                \"ops.write\",\n",
    "            ],\n",
    "            \"read-only\": [\n",
    "                \"metrics.read\",\n",
    "                \"runs.read\",\n",
    "                \"ns.smoke\",\n",
    "            ],\n",
    "        },\n",
    "        \"tokens\": {\n",
    "        },\n",
    "        \"actions\": {\n",
    "            \"chaos.set\": \"Set global chaos flags\",\n",
    "            \"chaos.ns.set\": \"Set per-namespace chaos flags\",\n",
    "            \"rl.update\": \"Update router/RL weights\",\n",
    "            \"rl.promote\": \"Promote RL weights live\",\n",
    "            \"rl.rollback\": \"Rollback RL weights\",\n",
    "            \"drift.remediate\": \"Apply desired state to live gateway\",\n",
    "            \"ops.write\": \"Regenerate ops / oncall docs\",\n",
    "            \"metrics.read\": \"Read /metrics, prom, telemetry\",\n",
    "            \"runs.read\": \"Read gateway_runs_agg + smoke\",\n",
    "            \"ns.smoke\": \"Hit /answer with x-namespace for smoke tests\",\n",
    "        },\n",
    "    }\n",
    "    RBAC_PATH.write_text(json.dumps(data, indent=2), encoding=\"utf-8\")\n",
    "    print(f\" RBAC spec  {RBAC_PATH}\")\n",
    "    return RBAC_PATH\n",
    "def load_rbac() -> Dict[str, Any]:\n",
    "    if not RBAC_PATH.exists():\n",
    "        return {\"roles\": {}, \"tokens\": {}, \"actions\": {}}\n",
    "    return json.loads(RBAC_PATH.read_text(encoding=\"utf-8\"))\n",
    "def rbac_can(token: Optional[str], action: str) -> bool:\n",
    "    \"\"\"\n",
    "    Pure notebook check:\n",
    "    - if ADMIN_TOKEN is set and matches  allow everything\n",
    "    - else check token in gateway_rbac.json\n",
    "    \"\"\"\n",
    "    rbac = load_rbac()\n",
    "    if ADMIN_TOKEN and token == ADMIN_TOKEN:\n",
    "        return True\n",
    "    tok_map = rbac.get(\"tokens\") or {}\n",
    "    role = tok_map.get(token or \"\")\n",
    "    if not role:\n",
    "        return False\n",
    "    role_perms = (rbac.get(\"roles\") or {}).get(role, [])\n",
    "    return action in role_perms\n",
    "def demo_rbac() -> None:\n",
    "    print(\"\\n=== RBAC demo ===\")\n",
    "    fake_admin = \"real-prod-token-123\"\n",
    "    fake_ml = \"ml-team-token-456\"\n",
    "    data = load_rbac()\n",
    "    data.setdefault(\"tokens\", {})\n",
    "    data[\"tokens\"][fake_admin] = \"admin\"\n",
    "    data[\"tokens\"][fake_ml] = \"ml\"\n",
    "    RBAC_PATH.write_text(json.dumps(data, indent=2), encoding=\"utf-8\")\n",
    "    print(\" wrote fake tokens for demo\")\n",
    "    print(\"admin can chaos.set? \", rbac_can(fake_admin, \"chaos.set\"))\n",
    "    print(\"ml can chaos.set?    \", rbac_can(fake_ml, \"chaos.set\"))\n",
    "    print(\"ml can rl.update?    \", rbac_can(fake_ml, \"rl.update\"))\n",
    "    print(\"no token can runs.read? \", rbac_can(None, \"runs.read\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 643,
   "id": "3b875a25-5a1a-4b8c-9ce3-8f573f344afe",
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_default_tenants() -> Path:\n",
    "    \"\"\"\n",
    "    define what each namespace SHOULD have.\n",
    "    drift code can later compare this.\n",
    "    \"\"\"\n",
    "    data = {\n",
    "        \"ts\": _now_ts(),\n",
    "        \"schema\": \"tenants/1\",\n",
    "        \"tenants\": {\n",
    "            \"default\": {\n",
    "                \"chaos\": {\n",
    "                    \"disable_ce\": False,\n",
    "                    \"force_bm25\": False,\n",
    "                    \"verifier_v2\": False,\n",
    "                },\n",
    "                \"rl_weights\": {\n",
    "                    \"factual\": 1.0,\n",
    "                    \"sql\": 1.0,\n",
    "                },\n",
    "            },\n",
    "            \"tenant-acme\": {\n",
    "                \"chaos\": {\n",
    "                    \"disable_ce\": True,   \n",
    "                    \"force_bm25\": False,\n",
    "                },\n",
    "                \"rl_weights\": {\n",
    "                    \"factual\": 0.7,\n",
    "                    \"sql\": 1.2,\n",
    "                },\n",
    "            },\n",
    "            \"tenant-beta\": {\n",
    "                \"chaos\": {\n",
    "                    \"disable_ce\": False,\n",
    "                    \"force_bm25\": True,  \n",
    "                },\n",
    "                \"rl_weights\": {\n",
    "                    \"factual\": 1.0,\n",
    "                    \"sql\": 1.0,\n",
    "                },\n",
    "            },\n",
    "        },\n",
    "    }\n",
    "    TENANTS_PATH.write_text(json.dumps(data, indent=2), encoding=\"utf-8\")\n",
    "    print(f\" tenant policy  {TENANTS_PATH}\")\n",
    "    return TENANTS_PATH\n",
    "def load_tenants() -> Dict[str, Any]:\n",
    "    if not TENANTS_PATH.exists():\n",
    "        return {\"tenants\": {}}\n",
    "    return json.loads(TENANTS_PATH.read_text(encoding=\"utf-8\"))\n",
    "def tenant_to_admin_calls(ns: str) -> List[Dict[str, Any]]:\n",
    "    \"\"\"\n",
    "    Convert tenant config  admin/chaos + rl/update calls we would send.\n",
    "    We DON'T actually call the gateway here  we just produce the plan.\n",
    "    \"\"\"\n",
    "    tenants = load_tenants().get(\"tenants\") or {}\n",
    "    cfg = tenants.get(ns)\n",
    "    if not cfg:\n",
    "        print(f\" no tenant cfg for {ns}\")\n",
    "        return []\n",
    "    calls = []\n",
    "    chaos = cfg.get(\"chaos\")\n",
    "    rlw = cfg.get(\"rl_weights\")\n",
    "    if chaos:\n",
    "        calls.append({\n",
    "            \"method\": \"POST\",\n",
    "            \"path\": f\"/admin/chaos/set?ns={ns}\",\n",
    "            \"body\": chaos,\n",
    "        })\n",
    "    if rlw:\n",
    "        calls.append({\n",
    "            \"method\": \"POST\",\n",
    "            \"path\": \"/rl/update\",\n",
    "            \"headers\": {\"x-namespace\": ns},\n",
    "            \"body\": {\"weights\": rlw},\n",
    "        })\n",
    "    return calls\n",
    "def demo_tenant_plan():\n",
    "    print(\"\\n=== tenant plan demo ===\")\n",
    "    for ns in [\"default\", \"tenant-acme\", \"tenant-beta\", \"unknown\"]:\n",
    "        plan = tenant_to_admin_calls(ns)\n",
    "        print(f\"\\n ns={ns}\")\n",
    "        for c in plan:\n",
    "            print(\"  \", c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 644,
   "id": "527d81c7-fd50-419d-b157-a87d3d4df082",
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_default_retention() -> Path:\n",
    "    data = {\n",
    "        \"ts\": _now_ts(),\n",
    "        \"schema\": \"retention/1\",\n",
    "        \"keep_days\": 3,\n",
    "        \"keep_min_files\": 50,\n",
    "        \"extra_globs\": [\n",
    "            \"gateway_prom_runs/metrics_*.txt\",\n",
    "            \"gateway_runs/gateway_smoke_*.json\",\n",
    "        ],\n",
    "    }\n",
    "    RETENTION_CFG.write_text(json.dumps(data, indent=2), encoding=\"utf-8\")\n",
    "    print(f\" retention cfg  {RETENTION_CFG}\")\n",
    "    return RETENTION_CFG\n",
    "def apply_retention(dry_run: bool = True) -> List[Path]:\n",
    "    \"\"\"\n",
    "    super lightweight retention:\n",
    "    - if file older than keep_days  delete\n",
    "    - but keep at least keep_min_files\n",
    "    - also purge prom/runs subdirs\n",
    "    \"\"\"\n",
    "    if not RETENTION_CFG.exists():\n",
    "        write_default_retention()\n",
    "    cfg = json.loads(RETENTION_CFG.read_text(encoding=\"utf-8\"))\n",
    "    keep_days = cfg.get(\"keep_days\", 3)\n",
    "    keep_min = cfg.get(\"keep_min_files\", 50)\n",
    "    extra_globs = cfg.get(\"extra_globs\", [])\n",
    "    now = time.time()\n",
    "    cutoff = now - keep_days * 86400\n",
    "    deleted: List[Path] = []\n",
    "    files = sorted(ART_DIR.glob(\"*\"), key=lambda p: p.stat().st_mtime)\n",
    "    if len(files) > keep_min:\n",
    "        for p in files:\n",
    "            if p.is_dir():\n",
    "                continue\n",
    "            mtime = p.stat().st_mtime\n",
    "            if mtime < cutoff:\n",
    "                if not dry_run:\n",
    "                    try:\n",
    "                        p.unlink()\n",
    "                        deleted.append(p)\n",
    "                    except Exception as e:\n",
    "                        print(f\" cannot delete {p}: {e}\")\n",
    "                else:\n",
    "                    deleted.append(p)\n",
    "    for pattern in extra_globs:\n",
    "        for p in ART_DIR.glob(pattern):\n",
    "            mtime = p.stat().st_mtime\n",
    "            if mtime < cutoff:\n",
    "                if not dry_run:\n",
    "                    try:\n",
    "                        p.unlink()\n",
    "                        deleted.append(p)\n",
    "                    except Exception as e:\n",
    "                        print(f\" cannot delete {p}: {e}\")\n",
    "                else:\n",
    "                    deleted.append(p)\n",
    "    print(f\" retention would delete {len(deleted)} file(s)\" if dry_run else f\" deleted {len(deleted)} file(s)\")\n",
    "    return deleted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 645,
   "id": "af4de584-62cb-4263-8f78-a979a8a28e0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_full_bundle() -> Path:\n",
    "    \"\"\"\n",
    "    tar.gz all the interesting /tmp/art stuff so CI/oncall can grab 1 file.\n",
    "    \"\"\"\n",
    "    with tarfile.open(FULL_BUNDLE, \"w:gz\") as tar:\n",
    "        for name in KNOWN_ARTS:\n",
    "            p = ART_DIR / name\n",
    "            if p.exists():\n",
    "                tar.add(p, arcname=name)\n",
    "        prom_runs = ART_DIR / \"gateway_prom_runs\"\n",
    "        if prom_runs.exists():\n",
    "            for p in prom_runs.glob(\"metrics_*.txt\"):\n",
    "                tar.add(p, arcname=f\"gateway_prom_runs/{p.name}\")\n",
    "        gw_runs = ART_DIR / \"gateway_runs\"\n",
    "        if gw_runs.exists():\n",
    "            for p in gw_runs.glob(\"gateway_smoke_*.json\"):\n",
    "                tar.add(p, arcname=f\"gateway_runs/{p.name}\")\n",
    "    print(f\" full bundle  {FULL_BUNDLE}\")\n",
    "    return FULL_BUNDLE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 646,
   "id": "59d8bea7-c72a-4765-aa2e-89c181547e41",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " RBAC spec  \\tmp\\art\\gateway_rbac.json\n",
      " tenant policy  \\tmp\\art\\gateway_tenants.json\n",
      " retention cfg  \\tmp\\art\\gateway_retention.json\n",
      " full bundle  \\tmp\\art\\gateway_full_bundle.tar.gz\n",
      "\n",
      "=== RBAC demo ===\n",
      " wrote fake tokens for demo\n",
      "admin can chaos.set?  True\n",
      "ml can chaos.set?     False\n",
      "ml can rl.update?     True\n",
      "no token can runs.read?  False\n",
      "\n",
      "=== tenant plan demo ===\n",
      "\n",
      " ns=default\n",
      "   {'method': 'POST', 'path': '/admin/chaos/set?ns=default', 'body': {'disable_ce': False, 'force_bm25': False, 'verifier_v2': False}}\n",
      "   {'method': 'POST', 'path': '/rl/update', 'headers': {'x-namespace': 'default'}, 'body': {'weights': {'factual': 1.0, 'sql': 1.0}}}\n",
      "\n",
      " ns=tenant-acme\n",
      "   {'method': 'POST', 'path': '/admin/chaos/set?ns=tenant-acme', 'body': {'disable_ce': True, 'force_bm25': False}}\n",
      "   {'method': 'POST', 'path': '/rl/update', 'headers': {'x-namespace': 'tenant-acme'}, 'body': {'weights': {'factual': 0.7, 'sql': 1.2}}}\n",
      "\n",
      " ns=tenant-beta\n",
      "   {'method': 'POST', 'path': '/admin/chaos/set?ns=tenant-beta', 'body': {'disable_ce': False, 'force_bm25': True}}\n",
      "   {'method': 'POST', 'path': '/rl/update', 'headers': {'x-namespace': 'tenant-beta'}, 'body': {'weights': {'factual': 1.0, 'sql': 1.0}}}\n",
      " no tenant cfg for unknown\n",
      "\n",
      " ns=unknown\n",
      " retention would delete 33 file(s)\n",
      "\n",
      " continuation (rbac + tenants + retention + bundle) loaded.\n",
      "Tip:\n",
      "  gw_cli('rbac.write')\n",
      "  gw_cli('tenants.demo')\n",
      "  gw_cli('bundle')\n"
     ]
    }
   ],
   "source": [
    "def gw_cli(cmd: str, **kwargs) -> None:\n",
    "    \"\"\"\n",
    "    dumb CLI so you don't have to scroll cells.\n",
    "    \"\"\"\n",
    "    cmd = cmd.strip().lower()\n",
    "    if cmd == \"rbac.write\":\n",
    "        write_default_rbac()\n",
    "    elif cmd == \"rbac.demo\":\n",
    "        demo_rbac()\n",
    "    elif cmd == \"tenants.write\":\n",
    "        write_default_tenants()\n",
    "    elif cmd == \"tenants.demo\":\n",
    "        demo_tenant_plan()\n",
    "    elif cmd == \"retention.dry\":\n",
    "        apply_retention(dry_run=True)\n",
    "    elif cmd == \"retention.apply\":\n",
    "        apply_retention(dry_run=False)\n",
    "    elif cmd == \"bundle\":\n",
    "        build_full_bundle()\n",
    "    else:\n",
    "        print(\" unknown cmd. try:\")\n",
    "        print(\"  gw_cli('rbac.write')\")\n",
    "        print(\"  gw_cli('rbac.demo')\")\n",
    "        print(\"  gw_cli('tenants.write')\")\n",
    "        print(\"  gw_cli('tenants.demo')\")\n",
    "        print(\"  gw_cli('retention.dry')\")\n",
    "        print(\"  gw_cli('retention.apply')\")\n",
    "        print(\"  gw_cli('bundle')\")\n",
    "write_default_rbac()\n",
    "write_default_tenants()\n",
    "write_default_retention()\n",
    "build_full_bundle()\n",
    "demo_rbac()\n",
    "demo_tenant_plan()\n",
    "apply_retention(dry_run=True)\n",
    "print(\"\\n continuation (rbac + tenants + retention + bundle) loaded.\")\n",
    "print(\"Tip:\")\n",
    "print(\"  gw_cli('rbac.write')\")\n",
    "print(\"  gw_cli('tenants.demo')\")\n",
    "print(\"  gw_cli('bundle')\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 647,
   "id": "cc83b660-418b-44f2-b1e2-55eb75964f3a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " continuation (cooldowns + sync + grafana + cli) using ART_DIR=\\tmp\\art\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "import time\n",
    "from pathlib import Path\n",
    "from typing import Any, Dict, List, Optional, Tuple\n",
    "ART_DIR = Path(os.getenv(\"ART_DIR\", \"/tmp/art\"))\n",
    "ART_DIR.mkdir(parents=True, exist_ok=True)\n",
    "print(f\" continuation (cooldowns + sync + grafana + cli) using ART_DIR={ART_DIR}\")\n",
    "GATEWAY_BASE = os.getenv(\"GATEWAY_BASE\", \"\").strip() or \"http://127.0.0.1:9910\"\n",
    "ADMIN_TOKEN = os.getenv(\"GATEWAY_ADMIN_TOKEN\", \"\").strip()\n",
    "ALERT_WEBHOOK = os.getenv(\"GATEWAY_ALERT_WEBHOOK\", \"\").strip()\n",
    "DESIRED_STATE_PATH = ART_DIR / \"gateway_desired_state.json\"\n",
    "MULTI_GW_PATH = ART_DIR / \"gateway_multi_drift.json\"\n",
    "ALERT_STATE_PATH = ART_DIR / \"gateway_alert_state.json\"\n",
    "GRAFANA_DASH_PATH = ART_DIR / \"gateway_grafana_dashboard.json\"\n",
    "DOCS_OPS_PATH = ART_DIR / \"gateway_ops_docs.md\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 648,
   "id": "c025b05a-df79-4789-802e-13dd4307266e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _admin_headers(extra: Optional[Dict[str, str]] = None) -> Dict[str, str]:\n",
    "    h = {\"content-type\": \"application/json\"}\n",
    "    if ADMIN_TOKEN:\n",
    "        h[\"x-admin-token\"] = ADMIN_TOKEN\n",
    "    if extra:\n",
    "        h.update(extra)\n",
    "    return h\n",
    "def _http_json(method: str, url: str, *, json_body: Optional[Dict[str, Any]] = None, timeout: float = 2.5) -> Dict[str, Any]:\n",
    "    import requests\n",
    "    try:\n",
    "        if method.upper() == \"GET\":\n",
    "            r = requests.get(url, headers=_admin_headers(), timeout=timeout)\n",
    "        else:\n",
    "            r = requests.post(url, headers=_admin_headers(), json=json_body or {}, timeout=timeout)\n",
    "        try:\n",
    "            js = r.json()\n",
    "        except Exception:\n",
    "            js = {\"text\": r.text}\n",
    "        js[\"_status\"] = r.status_code\n",
    "        js[\"_url\"] = url\n",
    "        return js\n",
    "    except Exception as e:\n",
    "        return {\"ok\": False, \"error\": str(e), \"_url\": url, \"_status\": 599}\n",
    "def _read_json_safe(p: Path) -> Any:\n",
    "    if not p.exists():\n",
    "        return None\n",
    "    try:\n",
    "        return json.loads(p.read_text(encoding=\"utf-8\"))\n",
    "    except Exception as e:\n",
    "        print(f\" failed to read {p}: {e}\")\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 649,
   "id": "9984e8dc-1fb5-4992-b8e7-a1391efd7e19",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _load_alert_state() -> Dict[str, Any]:\n",
    "    if ALERT_STATE_PATH.exists():\n",
    "        try:\n",
    "            return json.loads(ALERT_STATE_PATH.read_text(encoding=\"utf-8\"))\n",
    "        except Exception:\n",
    "            return {}\n",
    "    return {}\n",
    "def _save_alert_state(state: Dict[str, Any]) -> None:\n",
    "    ALERT_STATE_PATH.write_text(json.dumps(state, indent=2), encoding=\"utf-8\")\n",
    "def send_alert_cooldown(\n",
    "    key: str,\n",
    "    msg: str,\n",
    "    *,\n",
    "    level: str = \"warn\",\n",
    "    min_interval_s: int = 600,\n",
    "    extra: Optional[Dict[str, Any]] = None,\n",
    ") -> bool:\n",
    "    \"\"\"\n",
    "    send alert only if last alert for `key` older than min_interval_s\n",
    "    returns True if actually sent\n",
    "    \"\"\"\n",
    "    state = _load_alert_state()\n",
    "    now = int(time.time())\n",
    "    last = state.get(key, 0)\n",
    "    if now - last < min_interval_s:\n",
    "        print(f\" cooldown active for {key} ({now - last}s < {min_interval_s}s)\")\n",
    "        return False\n",
    "    payload = {\n",
    "        \"text\": f\"[gateway/{level}] {msg}\",\n",
    "        \"level\": level,\n",
    "        \"extra\": extra or {},\n",
    "    }\n",
    "    if not ALERT_WEBHOOK:\n",
    "        print(f\" (dry-run, cooldown ok): {json.dumps(payload)}\")\n",
    "        state[key] = now\n",
    "        _save_alert_state(state)\n",
    "        return True\n",
    "    import requests\n",
    "    try:\n",
    "        r = requests.post(ALERT_WEBHOOK, json=payload, timeout=2.5)\n",
    "        print(f\" alert sent (cooldown)  {r.status_code}\")\n",
    "        state[key] = now\n",
    "        _save_alert_state(state)\n",
    "        return True\n",
    "    except Exception as e:\n",
    "        print(f\" alert send failed: {e}\")\n",
    "        return False\n",
    "def alert_on_agg_with_cooldown(threshold_ms: float = 1800.0, cooldown_s: int = 900) -> None:\n",
    "    agg = _read_json_safe(ART_DIR / \"gateway_runs_agg.json\") or {}\n",
    "    runs = agg.get(\"runs\", [])\n",
    "    if not runs:\n",
    "        print(\" no runs in agg, skipping alert\")\n",
    "        return\n",
    "    latest = runs[0]\n",
    "    p95 = latest.get(\"p95_ms\")\n",
    "    if isinstance(p95, (int, float)) and p95 > threshold_ms:\n",
    "        send_alert_cooldown(\n",
    "            key=\"p95_regression\",\n",
    "            msg=f\"p95 regression: {p95:.1f} ms > {threshold_ms} ms\",\n",
    "            level=\"error\",\n",
    "            min_interval_s=cooldown_s,\n",
    "            extra=latest,\n",
    "        )\n",
    "    else:\n",
    "        print(f\" p95 ok: {p95} ms  {threshold_ms} ms\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 650,
   "id": "750c3722-1c0d-4332-b31d-b8eb0ca93692",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fetch_live_state_for_base(base: str) -> Dict[str, Any]:\n",
    "    chaos = _http_json(\"GET\", f\"{base}/admin/chaos/get\")\n",
    "    rl = _http_json(\"POST\", f\"{base}/rl/update\", json_body={\"weights\": {}})\n",
    "    return {\n",
    "        \"base\": base,\n",
    "        \"chaos\": chaos,\n",
    "        \"rl\": rl,\n",
    "        \"ts\": int(time.time()),\n",
    "    }\n",
    "def apply_state_to_base(base: str, chaos: Dict[str, Any], rl_weights: Dict[str, float]) -> Dict[str, Any]:\n",
    "    \"\"\"\n",
    "    push chaos + rl to target gateway\n",
    "    \"\"\"\n",
    "    res: Dict[str, Any] = {\"base\": base}\n",
    "    ch = _http_json(\"POST\", f\"{base}/admin/chaos/set\", json_body=chaos)\n",
    "    res[\"chaos_resp\"] = ch\n",
    "    rl = _http_json(\"POST\", f\"{base}/rl/update\", json_body={\"weights\": rl_weights})\n",
    "    res[\"rl_resp\"] = rl\n",
    "    return res\n",
    "def sync_gateways_from_source(\n",
    "    source_base: str,\n",
    "    targets: Optional[List[str]] = None,\n",
    "    *,\n",
    "    alert_on_drift: bool = True,\n",
    ") -> Dict[str, Any]:\n",
    "    \"\"\"\n",
    "    - read chaos/RL from source_base\n",
    "    - push to all targets\n",
    "    - useful to keep stg/eu in line with prod\n",
    "    \"\"\"\n",
    "    if targets is None:\n",
    "        targets = [\n",
    "            os.getenv(\"GATEWAY_BASE_STAGING\", \"\").strip() or \"http://127.0.0.1:9920\",\n",
    "            os.getenv(\"GATEWAY_BASE_EU\", \"\").strip() or \"http://127.0.0.1:9930\",\n",
    "        ]\n",
    "    src = fetch_live_state_for_base(source_base)\n",
    "    src_chaos = src.get(\"chaos\", {}).get(\"chaos\") or src.get(\"chaos\") or {}\n",
    "    src_rl = (src.get(\"rl\") or {}).get(\"weights\") or (src.get(\"rl\") or {}).get(\"current\") or {}\n",
    "    out = {\n",
    "        \"ts\": int(time.time()),\n",
    "        \"source\": source_base,\n",
    "        \"applied\": [],\n",
    "    }\n",
    "    for tgt in targets:\n",
    "        if not tgt or tgt == source_base:\n",
    "            continue\n",
    "        res = apply_state_to_base(tgt, src_chaos, src_rl)\n",
    "        out[\"applied\"].append(res)\n",
    "        if alert_on_drift and (res[\"chaos_resp\"].get(\"_status\") not in (200, 201, 204)):\n",
    "            send_alert_cooldown(\n",
    "                key=f\"drift_{tgt}\",\n",
    "                msg=f\"could not sync chaos to {tgt} (status={res['chaos_resp'].get('_status')})\",\n",
    "                level=\"warn\",\n",
    "                extra=res,\n",
    "            )\n",
    "    sync_path = ART_DIR / \"gateway_multi_sync.json\"\n",
    "    sync_path.write_text(json.dumps(out, indent=2), encoding=\"utf-8\")\n",
    "    print(f\" multi-gateway sync  {sync_path}\")\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 651,
   "id": "9b8216f9-240b-4485-9ba8-2ed072857cfd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _mtime(p: Path) -> float:\n",
    "    try:\n",
    "        return p.stat().st_mtime\n",
    "    except FileNotFoundError:\n",
    "        return 0.0\n",
    "def notebook_hot_reload_loop(\n",
    "    *,\n",
    "    interval_s: float = 5.0,\n",
    "    apply_drift: bool = True,\n",
    "    max_loops: int = 50,\n",
    ") -> None:\n",
    "    \"\"\"\n",
    "    Poll a small set of files and if they changed  run appropriate fixer.\n",
    "    This is for local notebook/dev only.\n",
    "    \"\"\"\n",
    "    watch_files = {\n",
    "        \"desired\": DESIRED_STATE_PATH,\n",
    "        \"tenants\": ART_DIR / \"gateway_tenants.json\",\n",
    "        \"rbac\": ART_DIR / \"gateway_rbac.json\",\n",
    "    }\n",
    "    last_mtime = {k: _mtime(p) for k, p in watch_files.items()}\n",
    "    print(f\" hot-reload loop start (max_loops={max_loops})\")\n",
    "    loops = 0\n",
    "    while loops < max_loops:\n",
    "        time.sleep(interval_s)\n",
    "        loops += 1\n",
    "        changed = []\n",
    "        for name, path in watch_files.items():\n",
    "            mt = _mtime(path)\n",
    "            if mt > last_mtime[name]:\n",
    "                changed.append((name, path))\n",
    "                last_mtime[name] = mt\n",
    "        if not changed:\n",
    "            continue\n",
    "        print(f\" detected changes: {changed}\")\n",
    "        if apply_drift and \"auto_remediate_drift\" in globals():\n",
    "            auto_remediate_drift(apply=True)\n",
    "        else:\n",
    "            print(\" drift remediation not in scope, just detected changes\")\n",
    "    print(\" hot-reload loop done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 652,
   "id": "bf28b68b-e813-4470-bd96-52a7bdd498cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_grafana_dashboard(\n",
    "    title: str = \"RAG Gateway\",\n",
    "    prom_ds: str = \"Prometheus\",\n",
    ") -> Path:\n",
    "    \"\"\"\n",
    "    emit a single-dashboard JSON so SRE can import\n",
    "    \"\"\"\n",
    "    dash = {\n",
    "        \"annotations\": {\"list\": []},\n",
    "        \"editable\": True,\n",
    "        \"gnetId\": None,\n",
    "        \"graphTooltip\": 0,\n",
    "        \"id\": None,\n",
    "        \"links\": [],\n",
    "        \"panels\": [\n",
    "            {\n",
    "                \"type\": \"timeseries\",\n",
    "                \"title\": \"Gateway latency p95 (ms)\",\n",
    "                \"datasource\": {\"type\": \"prometheus\", \"uid\": prom_ds},\n",
    "                \"targets\": [\n",
    "                    {\n",
    "                        \"expr\": 'histogram_quantile(0.95, sum(rate(gateway_answer_latency_ms_bucket[5m])) by (le))',\n",
    "                        \"legendFormat\": \"p95\",\n",
    "                        \"refId\": \"A\",\n",
    "                    }\n",
    "                ],\n",
    "                \"gridPos\": {\"h\": 9, \"w\": 12, \"x\": 0, \"y\": 0},\n",
    "            },\n",
    "            {\n",
    "                \"type\": \"timeseries\",\n",
    "                \"title\": \"Fanout errors by step\",\n",
    "                \"datasource\": {\"type\": \"prometheus\", \"uid\": prom_ds},\n",
    "                \"targets\": [\n",
    "                    {\n",
    "                        \"expr\": 'sum(rate(gateway_fanout_errors_total[5m])) by (step)',\n",
    "                        \"legendFormat\": \"{{step}}\",\n",
    "                        \"refId\": \"A\",\n",
    "                    }\n",
    "                ],\n",
    "                \"gridPos\": {\"h\": 9, \"w\": 12, \"x\": 12, \"y\": 0},\n",
    "            },\n",
    "            {\n",
    "                \"type\": \"timeseries\",\n",
    "                \"title\": \"Strategy mix\",\n",
    "                \"datasource\": {\"type\": \"prometheus\", \"uid\": prom_ds},\n",
    "                \"targets\": [\n",
    "                    {\n",
    "                        \"expr\": 'sum(rate(gateway_strategy_selected_total[5m])) by (strategy)',\n",
    "                        \"legendFormat\": \"{{strategy}}\",\n",
    "                        \"refId\": \"A\",\n",
    "                    }\n",
    "                ],\n",
    "                \"gridPos\": {\"h\": 8, \"w\": 24, \"x\": 0, \"y\": 9},\n",
    "            },\n",
    "        ],\n",
    "        \"refresh\": \"30s\",\n",
    "        \"schemaVersion\": 36,\n",
    "        \"style\": \"dark\",\n",
    "        \"tags\": [\"rag\", \"gateway\"],\n",
    "        \"templating\": {\"list\": []},\n",
    "        \"time\": {\"from\": \"now-1h\", \"to\": \"now\"},\n",
    "        \"timepicker\": {},\n",
    "        \"timezone\": \"\",\n",
    "        \"title\": title,\n",
    "        \"uid\": \"rag-gateway\",\n",
    "        \"version\": 1,\n",
    "    }\n",
    "    GRAFANA_DASH_PATH.write_text(json.dumps(dash, indent=2), encoding=\"utf-8\")\n",
    "    print(f\" grafana dashboard  {GRAFANA_DASH_PATH}\")\n",
    "    return GRAFANA_DASH_PATH"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 653,
   "id": "d0d85b2a-19f5-41a1-93e4-f79105db6988",
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_ops_docs() -> Path:\n",
    "    lines: List[str] = []\n",
    "    lines.append(\"# Gateway Ops (Notebook Edition)\")\n",
    "    lines.append(\"\")\n",
    "    lines.append(\"## 1. Alert checks (cooldown)\")\n",
    "    lines.append(\"```python\")\n",
    "    lines.append(\"alert_on_agg_with_cooldown(1800.0, 900)\")\n",
    "    lines.append(\"```\")\n",
    "    lines.append(\"\")\n",
    "    lines.append(\"## 2. Sync staging/EU from prod\")\n",
    "    lines.append(\"```python\")\n",
    "    lines.append(\"sync_gateways_from_source('http://127.0.0.1:9910')\")\n",
    "    lines.append(\"```\")\n",
    "    lines.append(\"\")\n",
    "    lines.append(\"## 3. Hot-reload\")\n",
    "    lines.append(\"```python\")\n",
    "    lines.append(\"notebook_hot_reload_loop(interval_s=5.0, max_loops=100)\")\n",
    "    lines.append(\"```\")\n",
    "    lines.append(\"\")\n",
    "    lines.append(\"## 4. Grafana\")\n",
    "    lines.append(\"```python\")\n",
    "    lines.append(\"write_grafana_dashboard()\")\n",
    "    lines.append(\"```\")\n",
    "    DOCS_OPS_PATH.write_text(\"\\n\".join(lines), encoding=\"utf-8\")\n",
    "    print(f\" ops docs  {DOCS_OPS_PATH}\")\n",
    "    return DOCS_OPS_PATH"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 654,
   "id": "c25e7219-6d6e-4764-bd71-2d7e67af319e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " grafana dashboard  \\tmp\\art\\gateway_grafana_dashboard.json\n",
      " ops docs  \\tmp\\art\\gateway_ops_docs.md\n",
      " p95 ok: None ms  1800.0 ms\n",
      "\n",
      " continuation (cooldowns + multi-gw sync + grafana + cli) loaded.\n",
      "Try:\n",
      "  gw_cli2('alert.p95')\n",
      "  gw_cli2('sync.from.prod', source='http://127.0.0.1:9910')\n",
      "  gw_cli2('grafana')\n",
      "  gw_cli2('hot')\n"
     ]
    }
   ],
   "source": [
    "def gw_cli2(cmd: str, **kwargs) -> None:\n",
    "    cmd = cmd.strip().lower()\n",
    "    if cmd == \"alert.p95\":\n",
    "        alert_on_agg_with_cooldown()\n",
    "    elif cmd == \"sync.from.prod\":\n",
    "        src = kwargs.get(\"source\", GATEWAY_BASE)\n",
    "        sync_gateways_from_source(src)\n",
    "    elif cmd == \"hot\":\n",
    "        notebook_hot_reload_loop()\n",
    "    elif cmd == \"grafana\":\n",
    "        write_grafana_dashboard()\n",
    "    elif cmd == \"docs\":\n",
    "        write_ops_docs()\n",
    "    else:\n",
    "        print(\" unknown cmd. try:\")\n",
    "        print(\"  gw_cli2('alert.p95')\")\n",
    "        print(\"  gw_cli2('sync.from.prod')\")\n",
    "        print(\"  gw_cli2('hot')\")\n",
    "        print(\"  gw_cli2('grafana')\")\n",
    "        print(\"  gw_cli2('docs')\")\n",
    "write_grafana_dashboard()\n",
    "write_ops_docs()\n",
    "alert_on_agg_with_cooldown()\n",
    "print(\"\\n continuation (cooldowns + multi-gw sync + grafana + cli) loaded.\")\n",
    "print(\"Try:\")\n",
    "print(\"  gw_cli2('alert.p95')\")\n",
    "print(\"  gw_cli2('sync.from.prod', source='http://127.0.0.1:9910')\")\n",
    "print(\"  gw_cli2('grafana')\")\n",
    "print(\"  gw_cli2('hot')\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 655,
   "id": "ed845a1f-b793-4466-9b46-39490565377e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " continuation (schema-validate + runner + http-export) using ART_DIR=\\tmp\\art\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "import time\n",
    "import http.server\n",
    "import socketserver\n",
    "from pathlib import Path\n",
    "from typing import Any, Dict, List, Optional, Tuple\n",
    "ART_DIR = Path(os.getenv(\"ART_DIR\", \"/tmp/art\"))\n",
    "ART_DIR.mkdir(parents=True, exist_ok=True)\n",
    "print(f\" continuation (schema-validate + runner + http-export) using ART_DIR={ART_DIR}\")\n",
    "KNOWN_ARTS = {\n",
    "    \"runs_agg\": \"gateway_runs_agg.json\",\n",
    "    \"latest\": \"gateway_smoke.json\",\n",
    "    \"telemetry\": \"gateway_telemetry.json\",\n",
    "    \"prom_ts\": \"gateway_prom_timeseries.json\",\n",
    "    \"drift\": \"gateway_drift_report.json\",\n",
    "    \"sla\": \"gateway_sla_runs.json\",\n",
    "    \"ops\": \"gateway_ops.md\",\n",
    "    \"oncall\": \"gateway_oncall.md\",\n",
    "    \"rbac\": \"gateway_rbac.json\",\n",
    "    \"tenants\": \"gateway_tenants.json\",\n",
    "    \"desired\": \"gateway_desired_state.json\",\n",
    "    \"multi_drift\": \"gateway_multi_drift.json\",\n",
    "    \"full_bundle\": \"gateway_full_bundle.tar.gz\",\n",
    "    \"grafana\": \"gateway_grafana_dashboard.json\",\n",
    "    \"ops_docs\": \"gateway_ops_docs.md\",\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 656,
   "id": "76f8427a-778e-4df2-a74b-130d3eb6e043",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " make-like docs  \\tmp\\art\\gateway_make.md\n",
      "\n",
      " continuation (schema-validate + runner + http-export) loaded.\n",
      "Try:\n",
      "  gw_run('daily')\n",
      "  gw_run('oncall')\n",
      "  # serve_artifacts(9919)   # local only\n"
     ]
    }
   ],
   "source": [
    "EXPECTED_SCHEMA = \"1.0.0\"\n",
    "def _read_json(path: Path) -> Optional[Dict[str, Any]]:\n",
    "    if not path.exists():\n",
    "        return None\n",
    "    try:\n",
    "        return json.loads(path.read_text(encoding=\"utf-8\"))\n",
    "    except Exception as e:\n",
    "        print(f\" failed to read {path}: {e}\")\n",
    "        return None\n",
    "def validate_schema_versions() -> Dict[str, Any]:\n",
    "    \"\"\"\n",
    "    walk known json artifacts and assert _meta.schema_version == EXPECTED_SCHEMA\n",
    "    \"\"\"\n",
    "    problems: List[Dict[str, Any]] = []\n",
    "    checked: List[Dict[str, Any]] = []\n",
    "    for name, rel in KNOWN_ARTS.items():\n",
    "        if not rel.endswith(\".json\"):\n",
    "            continue \n",
    "        p = ART_DIR / rel\n",
    "        if not p.exists():\n",
    "            problems.append({\"file\": rel, \"error\": \"missing\"})\n",
    "            continue\n",
    "        js = _read_json(p)\n",
    "        if not js:\n",
    "            problems.append({\"file\": rel, \"error\": \"unreadable\"})\n",
    "            continue\n",
    "        meta = js.get(\"_meta\") or {}\n",
    "        got = meta.get(\"schema_version\")\n",
    "        if got is None:\n",
    "            problems.append({\"file\": rel, \"error\": \"no-schema\", \"have\": None, \"want\": EXPECTED_SCHEMA})\n",
    "        elif got != EXPECTED_SCHEMA:\n",
    "            problems.append({\"file\": rel, \"error\": \"schema-mismatch\", \"have\": got, \"want\": EXPECTED_SCHEMA})\n",
    "        checked.append({\"file\": rel, \"schema\": got})\n",
    "    out = {\n",
    "        \"ts\": int(time.time()),\n",
    "        \"expected\": EXPECTED_SCHEMA,\n",
    "        \"checked\": checked,\n",
    "        \"problems\": problems,\n",
    "    }\n",
    "    report_path = ART_DIR / \"gateway_schema_report.json\"\n",
    "    report_path.write_text(json.dumps(out, indent=2), encoding=\"utf-8\")\n",
    "    if problems:\n",
    "        print(f\" schema problems  {report_path}\")\n",
    "        for pr in problems:\n",
    "            print(\"  -\", pr)\n",
    "    else:\n",
    "        print(f\" schema ok  {report_path}\")\n",
    "    return out\n",
    "def _has(func_name: str) -> bool:\n",
    "    \"\"\"\n",
    "    helper: see if a function from previous cells is in scope\n",
    "    \"\"\"\n",
    "    return func_name in globals()\n",
    "def gw_run(profile: str = \"daily\") -> Dict[str, Any]:\n",
    "    \"\"\"\n",
    "    1. validate schemas\n",
    "    2. refresh telemetry (if funcs exist)\n",
    "    3. alert on p95 (with cooldown)\n",
    "    4. detect drift + optionally remediate\n",
    "    5. sync other gateways from primary\n",
    "    6. re-write grafana + ops docs\n",
    "    \"\"\"\n",
    "    print(f\"\\n gw_run(profile={profile!r})\")\n",
    "    ts = int(time.time())\n",
    "    result: Dict[str, Any] = {\"ts\": ts, \"profile\": profile, \"steps\": []}\n",
    "    schema_res = validate_schema_versions()\n",
    "    result[\"steps\"].append({\"step\": \"schema\", \"res\": schema_res})\n",
    "    if _has(\"dump_all_telemetry\"):\n",
    "        tel_path = dump_all_telemetry()\n",
    "        result[\"steps\"].append({\"step\": \"telemetry\", \"res\": str(tel_path)})\n",
    "    else:\n",
    "        result[\"steps\"].append({\"step\": \"telemetry\", \"res\": \"not-in-scope\"})\n",
    "    if _has(\"alert_on_agg_with_cooldown\"):\n",
    "        alert_on_agg_with_cooldown()\n",
    "        result[\"steps\"].append({\"step\": \"alert\", \"res\": \"p95-checked\"})\n",
    "    elif _has(\"alert_on_p95\"):\n",
    "        alert_on_p95()\n",
    "        result[\"steps\"].append({\"step\": \"alert\", \"res\": \"p95-checked-basic\"})\n",
    "    else:\n",
    "        result[\"steps\"].append({\"step\": \"alert\", \"res\": \"not-in-scope\"})\n",
    "    if profile in (\"daily\", \"oncall\", \"full\") and _has(\"auto_remediate_drift\"):\n",
    "        drift_res = auto_remediate_drift(apply=True)\n",
    "        result[\"steps\"].append({\"step\": \"drift\", \"res\": drift_res})\n",
    "    elif _has(\"detect_drift\"):\n",
    "        drift_res = detect_drift()\n",
    "        result[\"steps\"].append({\"step\": \"drift\", \"res\": drift_res})\n",
    "    else:\n",
    "        result[\"steps\"].append({\"step\": \"drift\", \"res\": \"not-in-scope\"})\n",
    "    if profile in (\"daily\", \"full\") and _has(\"sync_gateways_from_source\"):\n",
    "        sync_res = sync_gateways_from_source(os.getenv(\"GATEWAY_BASE\", \"http://127.0.0.1:9910\"))\n",
    "        result[\"steps\"].append({\"step\": \"sync\", \"res\": sync_res})\n",
    "    else:\n",
    "        result[\"steps\"].append({\"step\": \"sync\", \"res\": \"skipped\"})\n",
    "    if _has(\"write_grafana_dashboard\"):\n",
    "        g = write_grafana_dashboard()\n",
    "        result[\"steps\"].append({\"step\": \"grafana\", \"res\": str(g)})\n",
    "    if _has(\"write_ops_docs\"):\n",
    "        d = write_ops_docs()\n",
    "        result[\"steps\"].append({\"step\": \"ops-docs\", \"res\": str(d)})\n",
    "    if _has(\"run_sla_synthetic\") and profile in (\"daily\", \"full\"):\n",
    "        sla = run_sla_synthetic(max_p95_ms=1800.0)\n",
    "        result[\"steps\"].append({\"step\": \"sla\", \"res\": sla})\n",
    "    out_path = ART_DIR / f\"gateway_run_{ts}.json\"\n",
    "    out_path.write_text(json.dumps(result, indent=2), encoding=\"utf-8\")\n",
    "    print(f\" gw_run report  {out_path}\")\n",
    "    return result\n",
    "def serve_artifacts(port: int = 9919) -> None:\n",
    "    \"\"\"\n",
    "    tiny HTTP server to expose /tmp/art for humans/CI\n",
    "    run only in local/dev\n",
    "    \"\"\"\n",
    "    os.chdir(str(ART_DIR))\n",
    "    handler = http.server.SimpleHTTPRequestHandler\n",
    "    with socketserver.TCPServer((\"0.0.0.0\", port), handler) as httpd:\n",
    "        print(f\" serving {ART_DIR} at http://0.0.0.0:{port}/  (Ctrl+C to stop)\")\n",
    "        try:\n",
    "            httpd.serve_forever()\n",
    "        except KeyboardInterrupt:\n",
    "            print(\" server stopped\")\n",
    "def write_make_docs() -> Path:\n",
    "    \"\"\"\n",
    "    not a real Makefile, but a markdown with the 'main' commands\n",
    "    \"\"\"\n",
    "    lines = []\n",
    "    lines.append(\"# Gateway Notebook Targets\")\n",
    "    lines.append(\"\")\n",
    "    lines.append(\"```bash\")\n",
    "    lines.append(\"# full daily run\")\n",
    "    lines.append(\"python -c \\\"from notebook import gw_run; gw_run('daily')\\\"\")\n",
    "    lines.append(\"\")\n",
    "    lines.append(\"# refresh telemetry only\")\n",
    "    lines.append(\"python -c \\\"from notebook import dump_all_telemetry; dump_all_telemetry()\\\"\")\n",
    "    lines.append(\"\")\n",
    "    lines.append(\"# expose artifacts locally\")\n",
    "    lines.append(\"python -c \\\"from notebook import serve_artifacts; serve_artifacts(9919)\\\"\")\n",
    "    lines.append(\"```\")\n",
    "    mk_path = ART_DIR / \"gateway_make.md\"\n",
    "    mk_path.write_text(\"\\n\".join(lines), encoding=\"utf-8\")\n",
    "    print(f\" make-like docs  {mk_path}\")\n",
    "    return mk_path\n",
    "write_make_docs()\n",
    "print(\"\\n continuation (schema-validate + runner + http-export) loaded.\")\n",
    "print(\"Try:\")\n",
    "print(\"  gw_run('daily')\")\n",
    "print(\"  gw_run('oncall')\")\n",
    "print(\"  # serve_artifacts(9919)   # local only\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 657,
   "id": "d789322a-4648-4a17-9b26-efad8a2106b9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " continuation (config-bundle + k8s + agent-loop + cli) using ART_DIR=\\tmp\\art\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "import time\n",
    "import random\n",
    "import argparse\n",
    "from pathlib import Path\n",
    "from typing import Any, Dict, List, Optional, Tuple\n",
    "ART_DIR = Path(os.getenv(\"ART_DIR\", \"/tmp/art\"))\n",
    "ART_DIR.mkdir(parents=True, exist_ok=True)\n",
    "print(f\" continuation (config-bundle + k8s + agent-loop + cli) using ART_DIR={ART_DIR}\")\n",
    "RBAC_PATH = ART_DIR / \"gateway_rbac.json\"\n",
    "TENANTS_PATH = ART_DIR / \"gateway_tenants.json\"\n",
    "DESIRED_STATE_PATH = ART_DIR / \"gateway_desired_state.json\"\n",
    "PROM_DASH_PATH = ART_DIR / \"gateway_grafana_dashboard.json\"\n",
    "OPS_DOCS_PATH = ART_DIR / \"gateway_ops_docs.md\"\n",
    "FULL_BUNDLE = ART_DIR / \"gateway_full_bundle.tar.gz\"\n",
    "SCHEMA_REPORT = ART_DIR / \"gateway_schema_report.json\"\n",
    "GATEWAY_CONFIG_PATH = ART_DIR / \"gateway_config.json\"\n",
    "K8S_CONFIGMAP_PATH = ART_DIR / \"k8s-gateway-configmap.yaml\"\n",
    "K8S_SECRET_PATH = ART_DIR / \"k8s-gateway-secret.yaml\"\n",
    "AGENT_SCRIPT_PATH = ART_DIR / \"gateway_agent.sh\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 658,
   "id": "25190981-ed30-4fb0-9a78-2a8a6591900e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _read_json_safe(p: Path) -> Any:\n",
    "    if not p.exists():\n",
    "        return None\n",
    "    try:\n",
    "        return json.loads(p.read_text(encoding=\"utf-8\"))\n",
    "    except Exception as e:\n",
    "        print(f\" failed to read {p}: {e}\")\n",
    "        return None\n",
    "def _now() -> int:\n",
    "    return int(time.time())\n",
    "def build_gateway_config() -> Path:\n",
    "    \"\"\"\n",
    "    Flatten the notebook-produced files into a single config blob the real gateway\n",
    "    (or a sidecar) can read.\n",
    "    \"\"\"\n",
    "    cfg: Dict[str, Any] = {\n",
    "        \"ts\": _now(),\n",
    "        \"version\": \"1.0.0\",\n",
    "        \"note\": \"auto-generated from notebook (cooldowns + sync + grafana + schema)\",\n",
    "        \"rbac\": _read_json_safe(RBAC_PATH) or {},\n",
    "        \"tenants\": _read_json_safe(TENANTS_PATH) or {},\n",
    "        \"desired_state\": _read_json_safe(DESIRED_STATE_PATH) or {},\n",
    "        \"schema_report\": _read_json_safe(SCHEMA_REPORT) or {},\n",
    "        \"grafana_dashboard\": _read_json_safe(PROM_DASH_PATH) or {},\n",
    "    }\n",
    "    GATEWAY_CONFIG_PATH.write_text(json.dumps(cfg, indent=2), encoding=\"utf-8\")\n",
    "    print(f\" gateway config  {GATEWAY_CONFIG_PATH}\")\n",
    "    return GATEWAY_CONFIG_PATH"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 659,
   "id": "80d05994-855a-45ab-85b1-630addcb175a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_k8s_configmap(\n",
    "    name: str = \"rag-gateway-config\",\n",
    "    namespace: str = \"default\",\n",
    ") -> Path:\n",
    "    \"\"\"\n",
    "    ConfigMap with all the control-plane JSONs.\n",
    "    Mount this at /etc/rag-gateway in the pod.\n",
    "    \"\"\"\n",
    "    data: Dict[str, str] = {}\n",
    "    for path, key in [\n",
    "        (GATEWAY_CONFIG_PATH, \"gateway_config.json\"),\n",
    "        (RBAC_PATH, \"gateway_rbac.json\"),\n",
    "        (TENANTS_PATH, \"gateway_tenants.json\"),\n",
    "        (DESIRED_STATE_PATH, \"gateway_desired_state.json\"),\n",
    "        (PROM_DASH_PATH, \"gateway_grafana_dashboard.json\"),\n",
    "        (OPS_DOCS_PATH, \"gateway_ops_docs.md\"),\n",
    "    ]:\n",
    "        if path.exists():\n",
    "            data[key] = path.read_text(encoding=\"utf-8\")\n",
    "    cm_lines = []\n",
    "    cm_lines.append(\"apiVersion: v1\")\n",
    "    cm_lines.append(\"kind: ConfigMap\")\n",
    "    cm_lines.append(f\"metadata:\\n  name: {name}\\n  namespace: {namespace}\")\n",
    "    cm_lines.append(\"data:\")\n",
    "    for k, v in data.items():\n",
    "        cm_lines.append(f\"  {k}: |\")\n",
    "        for line in v.splitlines():\n",
    "            cm_lines.append(f\"    {line}\")\n",
    "    K8S_CONFIGMAP_PATH.write_text(\"\\n\".join(cm_lines), encoding=\"utf-8\")\n",
    "    print(f\" k8s configmap  {K8S_CONFIGMAP_PATH}\")\n",
    "    return K8S_CONFIGMAP_PATH\n",
    "def write_k8s_secret(\n",
    "    name: str = \"rag-gateway-secrets\",\n",
    "    namespace: str = \"default\",\n",
    ") -> Path:\n",
    "    \"\"\"\n",
    "    Secret for admin token / alert webhook  we dont want these in ConfigMap.\n",
    "    Stores as stringData (for dev); in prod youd template/base64.\n",
    "    \"\"\"\n",
    "    admin_token = os.getenv(\"GATEWAY_ADMIN_TOKEN\", \"\")\n",
    "    alert_wh = os.getenv(\"GATEWAY_ALERT_WEBHOOK\", \"\")\n",
    "    lines = []\n",
    "    lines.append(\"apiVersion: v1\")\n",
    "    lines.append(\"kind: Secret\")\n",
    "    lines.append(f\"metadata:\\n  name: {name}\\n  namespace: {namespace}\")\n",
    "    lines.append(\"type: Opaque\")\n",
    "    lines.append(\"stringData:\")\n",
    "    lines.append(f\"  GATEWAY_ADMIN_TOKEN: \\\"{admin_token}\\\"\")\n",
    "    lines.append(f\"  GATEWAY_ALERT_WEBHOOK: \\\"{alert_wh}\\\"\")\n",
    "    K8S_SECRET_PATH.write_text(\"\\n\".join(lines), encoding=\"utf-8\")\n",
    "    print(f\" k8s secret  {K8S_SECRET_PATH}\")\n",
    "    return K8S_SECRET_PATH"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 660,
   "id": "5d623c74-e6dc-414d-aae3-8423bec53427",
   "metadata": {},
   "outputs": [],
   "source": [
    "def gateway_agent_once(profile: str = \"daily\") -> Dict[str, Any]:\n",
    "    \"\"\"\n",
    "    One-shot agent run.\n",
    "    Uses gw_run(...) from previous cells if available.\n",
    "    \"\"\"\n",
    "    out: Dict[str, Any] = {\"ts\": _now(), \"profile\": profile}\n",
    "    cfg = build_gateway_config()\n",
    "    out[\"config\"] = str(cfg)\n",
    "    if \"gw_run\" in globals():\n",
    "        res = gw_run(profile=profile)\n",
    "        out[\"gw_run\"] = res\n",
    "    else:\n",
    "        out[\"gw_run\"] = {\"ok\": False, \"reason\": \"gw_run not in scope\"}\n",
    "    return out\n",
    "def gateway_agent_loop(\n",
    "    *,\n",
    "    profile: str = \"daily\",\n",
    "    interval_s: int = 300,\n",
    "    jitter_s: int = 30,\n",
    "    max_loops: int = 0,\n",
    ") -> None:\n",
    "    \"\"\"\n",
    "    Run in a pod/cron-style container.\n",
    "    interval_s = base interval\n",
    "    jitter_s   = max random added delay\n",
    "    max_loops=0  run forever\n",
    "    \"\"\"\n",
    "    print(f\" gateway agent loop start (profile={profile}, interval={interval_s}s, jitter{jitter_s}s)\")\n",
    "    loops = 0\n",
    "    while True:\n",
    "        loops += 1\n",
    "        print(f\"\\n agent iteration {loops}\")\n",
    "        res = gateway_agent_once(profile=profile)\n",
    "        print(\"   result keys:\", list(res.keys()))\n",
    "        if max_loops and loops >= max_loops:\n",
    "            print(\" agent loop done (max_loops reached)\")\n",
    "            break\n",
    "        sleep_for = interval_s + random.randint(0, jitter_s)\n",
    "        print(f\" sleeping {sleep_for}s \")\n",
    "        time.sleep(sleep_for)\n",
    "def write_agent_shell() -> Path:\n",
    "    \"\"\"\n",
    "    So SRE can just: sh /tmp/art/gateway_agent.sh\n",
    "    \"\"\"\n",
    "    content = \"\"\"#!/usr/bin/env bash\n",
    "set -euo pipefail\n",
    "echo \"[agent] starting gateway agent (notebook export)\"\n",
    "python - << 'PY'\n",
    "from pathlib import Path\n",
    "import os\n",
    "# assume we are running inside the same env\n",
    "from time import sleep\n",
    "try:\n",
    "    from __main__ import gateway_agent_loop\n",
    "except ImportError:\n",
    "    # if we ran this as a plain file\n",
    "    from gateway_notebook import gateway_agent_loop  # adjust if needed\n",
    "gateway_agent_loop(profile=\"daily\", interval_s=300, jitter_s=30, max_loops=0)\n",
    "PY\n",
    "\"\"\"\n",
    "    AGENT_SCRIPT_PATH.write_text(content, encoding=\"utf-8\")\n",
    "    AGENT_SCRIPT_PATH.chmod(0o755)\n",
    "    print(f\" agent shell  {AGENT_SCRIPT_PATH}\")\n",
    "    return AGENT_SCRIPT_PATH"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 661,
   "id": "634ca224-a423-4ec8-8578-9150451fd13e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_self_check() -> Dict[str, Any]:\n",
    "    \"\"\"\n",
    "    very small smoke so we know we didnt break old artifacts\n",
    "    \"\"\"\n",
    "    problems: List[str] = []\n",
    "    for p in [RBAC_PATH, TENANTS_PATH, DESIRED_STATE_PATH]:\n",
    "        if not p.exists():\n",
    "            problems.append(f\"missing {p.name}\")\n",
    "    if not GATEWAY_CONFIG_PATH.exists():\n",
    "        build_gateway_config()\n",
    "    out = {\n",
    "        \"ts\": _now(),\n",
    "        \"problems\": problems,\n",
    "        \"config_exists\": GATEWAY_CONFIG_PATH.exists(),\n",
    "    }\n",
    "    check_path = ART_DIR / \"gateway_selfcheck.json\"\n",
    "    check_path.write_text(json.dumps(out, indent=2), encoding=\"utf-8\")\n",
    "    print(f\" self-check  {check_path}\")\n",
    "    if problems:\n",
    "        print(\" self-check problems:\", problems)\n",
    "    else:\n",
    "        print(\" self-check OK\")\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 662,
   "id": "b2372fdd-e48c-4a1a-bba9-324d9f321b3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def main_cli():\n",
    "    \"\"\"\n",
    "    python this_file.py --run daily\n",
    "    python this_file.py --agent\n",
    "    python this_file.py --k8s\n",
    "    \"\"\"\n",
    "    parser = argparse.ArgumentParser(description=\"Gateway notebook control CLI\")\n",
    "    parser.add_argument(\"--run\", dest=\"run_profile\", help=\"run gw_run(profile=...) once\")\n",
    "    parser.add_argument(\"--agent\", action=\"store_true\", help=\"start agent loop (infinite)\")\n",
    "    parser.add_argument(\"--k8s\", action=\"store_true\", help=\"write k8s ConfigMap + Secret\")\n",
    "    parser.add_argument(\"--selfcheck\", action=\"store_true\", help=\"run self-check\")\n",
    "    parser.add_argument(\"--loops\", type=int, default=0, help=\"max loops for agent (0 = infinite)\")\n",
    "    args = parser.parse_args()\n",
    "    if args.k8s:\n",
    "        build_gateway_config()\n",
    "        write_k8s_configmap()\n",
    "        write_k8s_secret()\n",
    "        return\n",
    "    if args.selfcheck:\n",
    "        run_self_check()\n",
    "        return\n",
    "    if args.run_profile:\n",
    "        gateway_agent_once(profile=args.run_profile)\n",
    "        return\n",
    "    if args.agent:\n",
    "        gateway_agent_loop(profile=\"daily\", max_loops=args.loops)\n",
    "        return\n",
    "    build_gateway_config()\n",
    "    print(\" done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 663,
   "id": "44b5c66a-6806-44c0-a84e-fbba9060fffc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " gateway config  \\tmp\\art\\gateway_config.json\n",
      " k8s configmap  \\tmp\\art\\k8s-gateway-configmap.yaml\n",
      " k8s secret  \\tmp\\art\\k8s-gateway-secret.yaml\n",
      " agent shell  \\tmp\\art\\gateway_agent.sh\n",
      " self-check  \\tmp\\art\\gateway_selfcheck.json\n",
      " self-check OK\n",
      "\n",
      " continuation (config-bundle + k8s + agent + cli) loaded.\n",
      "Try:\n",
      "  # 1) one-shot daily\n",
      "  python this_notebook_export.py --run daily\n",
      "  # 2) k8s artifacts\n",
      "  python this_notebook_export.py --k8s\n",
      "  # 3) long-running agent\n",
      "  python this_notebook_export.py --agent --loops=3\n"
     ]
    }
   ],
   "source": [
    "build_gateway_config()\n",
    "write_k8s_configmap()\n",
    "write_k8s_secret()\n",
    "write_agent_shell()\n",
    "run_self_check()\n",
    "print(\"\\n continuation (config-bundle + k8s + agent + cli) loaded.\")\n",
    "print(\"Try:\")\n",
    "print(\"  # 1) one-shot daily\")\n",
    "print(\"  python this_notebook_export.py --run daily\")\n",
    "print(\"  # 2) k8s artifacts\")\n",
    "print(\"  python this_notebook_export.py --k8s\")\n",
    "print(\"  # 3) long-running agent\")\n",
    "print(\"  python this_notebook_export.py --agent --loops=3\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 664,
   "id": "26fd0f43-c1c5-4982-86a5-ac35ba03dfa5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " continuation (migrate + rust-export + ci-check) using ART_DIR=\\tmp\\art\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "import time\n",
    "from pathlib import Path\n",
    "from typing import Any, Dict, List, Optional, Tuple, Callable\n",
    "ART_DIR = Path(os.getenv(\"ART_DIR\", \"/tmp/art\"))\n",
    "ART_DIR.mkdir(parents=True, exist_ok=True)\n",
    "print(f\" continuation (migrate + rust-export + ci-check) using ART_DIR={ART_DIR}\")\n",
    "ARTS: Dict[str, Path] = {\n",
    "    \"config\": ART_DIR / \"gateway_config.json\",\n",
    "    \"desired\": ART_DIR / \"gateway_desired_state.json\",\n",
    "    \"rbac\": ART_DIR / \"gateway_rbac.json\",\n",
    "    \"tenants\": ART_DIR / \"gateway_tenants.json\",\n",
    "    \"schema_report\": ART_DIR / \"gateway_schema_report.json\",\n",
    "    \"multi_drift\": ART_DIR / \"gateway_multi_drift.json\",\n",
    "    \"sla\": ART_DIR / \"gateway_sla_runs.json\",\n",
    "    \"agg\": ART_DIR / \"gateway_runs_agg.json\",\n",
    "    \"prom_ts\": ART_DIR / \"gateway_prom_timeseries.json\",\n",
    "    \"full_bundle\": ART_DIR / \"gateway_full_bundle.tar.gz\",\n",
    "}\n",
    "MIG_DIR = ART_DIR / \"migrations\"\n",
    "MIG_DIR.mkdir(parents=True, exist_ok=True)\n",
    "CURRENT_SCHEMA = \"1.1.0\"\n",
    "SCHEMA_COMPAT = {\"1.0.0\", \"1.0.1\", \"1.1.0\"}\n",
    "MIGRATIONS: Dict[Tuple[str, str], Callable[[Dict[str, Any]], Dict[str, Any]]] = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 665,
   "id": "a921390b-6b9b-4d80-9f87-eb5e96cbe55a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _read_json_safe(p: Path) -> Optional[Any]:\n",
    "    if not p.exists():\n",
    "        return None\n",
    "    try:\n",
    "        return json.loads(p.read_text(encoding=\"utf-8\"))\n",
    "    except Exception as e:\n",
    "        print(f\" cannot read {p}: {e}\")\n",
    "        return None\n",
    "def _write_json_versioned(p: Path, data: Dict[str, Any], *, schema: Optional[str] = None) -> None:\n",
    "    data = dict(data)\n",
    "    meta = data.get(\"_meta\") or {}\n",
    "    meta[\"schema_version\"] = schema or CURRENT_SCHEMA\n",
    "    meta[\"written_at\"] = int(time.time())\n",
    "    data[\"_meta\"] = meta\n",
    "    p.write_text(json.dumps(data, indent=2), encoding=\"utf-8\")\n",
    "    print(f\" wrote {p} (schema={meta['schema_version']})\")\n",
    "def register_migration(src: str, dst: str):\n",
    "    def _wrap(fn: Callable[[Dict[str, Any]], Dict[str, Any]]):\n",
    "        MIGRATIONS[(src, dst)] = fn\n",
    "        return fn\n",
    "    return _wrap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 666,
   "id": "7c95df9f-77e1-4447-b973-390ac69ab432",
   "metadata": {},
   "outputs": [],
   "source": [
    "@register_migration(\"1.0.0\", \"1.1.0\")\n",
    "def _mig_100_to_110(doc: Dict[str, Any]) -> Dict[str, Any]:\n",
    "    \"\"\"\n",
    "    Make as FEW assumptions as possible.\n",
    "    - ensure _meta\n",
    "    - add ops_profile\n",
    "    - if tenants is a dict, ensure each tenant is a dict, then add ratelimits\n",
    "    \"\"\"\n",
    "    out: Dict[str, Any] = dict(doc)\n",
    "    meta = out.get(\"_meta\") or {}\n",
    "    meta.setdefault(\"source\", \"notebook/gateway\")\n",
    "    meta[\"schema_version\"] = \"1.1.0\"\n",
    "    out[\"_meta\"] = meta\n",
    "    if \"ops_profile\" not in out:\n",
    "        out[\"ops_profile\"] = \"daily\"\n",
    "    tenants = out.get(\"tenants\")\n",
    "    if isinstance(tenants, dict):\n",
    "        for ns, cfg in tenants.items():\n",
    "            if not isinstance(cfg, dict):\n",
    "                continue\n",
    "            cfg.setdefault(\n",
    "                \"ratelimits\",\n",
    "                {\n",
    "                    \"qps\": 20,\n",
    "                    \"burst\": 40,\n",
    "                },\n",
    "            )\n",
    "    return out\n",
    "@register_migration(\"1.0.1\", \"1.1.0\")\n",
    "def _mig_101_to_110(doc: Dict[str, Any]) -> Dict[str, Any]:\n",
    "    return _mig_100_to_110(doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 667,
   "id": "fc2979ef-44fe-4ec8-8b22-79798b530549",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _find_migration_path(src: str, dst: str) -> List[Tuple[str, str]]:\n",
    "    \"\"\"\n",
    "    Right now we only support:\n",
    "      1.0.0  1.1.0\n",
    "      1.0.1  1.1.0\n",
    "    Extend here if you add more bumps.\n",
    "    \"\"\"\n",
    "    if (src, dst) in MIGRATIONS:\n",
    "        return [(src, dst)]\n",
    "    return []\n",
    "def migrate_doc(doc: Any) -> Any:\n",
    "    \"\"\"\n",
    "    Accept ANY json (dict, list, int). If it's not a dict  return as-is.\n",
    "    If it's a dict but has no _meta  treat as 1.0.0.\n",
    "    \"\"\"\n",
    "    if not isinstance(doc, dict):\n",
    "        return doc\n",
    "    meta = doc.get(\"_meta\") or {}\n",
    "    have = meta.get(\"schema_version\", \"1.0.0\")\n",
    "    if have == CURRENT_SCHEMA:\n",
    "        return doc\n",
    "    path = _find_migration_path(have, CURRENT_SCHEMA)\n",
    "    if not path:\n",
    "        new_doc = dict(doc)\n",
    "        new_meta = new_doc.get(\"_meta\") or {}\n",
    "        new_meta[\"schema_version\"] = CURRENT_SCHEMA\n",
    "        new_meta.setdefault(\"source\", \"notebook/gateway\")\n",
    "        new_meta[\"written_at\"] = int(time.time())\n",
    "        new_doc[\"_meta\"] = new_meta\n",
    "        return new_doc\n",
    "    out = dict(doc)\n",
    "    for (src, dst) in path:\n",
    "        fn = MIGRATIONS.get((src, dst))\n",
    "        if not fn:\n",
    "            print(f\" missing migration impl for {src}{dst}, stopping on this doc\")\n",
    "            break\n",
    "        out = fn(out)\n",
    "    return out\n",
    "def migrate_all_known_artifacts() -> Dict[str, Any]:\n",
    "    \"\"\"\n",
    "    Walk all known JSON files. If something is missing or not a dict, we just skip it.\n",
    "    Never crash.\n",
    "    \"\"\"\n",
    "    res: Dict[str, Any] = {\n",
    "        \"ts\": int(time.time()),\n",
    "        \"migrated\": [],\n",
    "        \"skipped\": [],\n",
    "    }\n",
    "    for name, path in ARTS.items():\n",
    "        if not path.exists():\n",
    "            res[\"skipped\"].append({\"name\": name, \"reason\": \"missing\"})\n",
    "            continue\n",
    "        raw = _read_json_safe(path)\n",
    "        if raw is None:\n",
    "            res[\"skipped\"].append({\"name\": name, \"reason\": \"unreadable\"})\n",
    "            continue\n",
    "        if isinstance(raw, dict):\n",
    "            before = (raw.get(\"_meta\") or {}).get(\"schema_version\")\n",
    "            after = migrate_doc(raw)\n",
    "            _write_json_versioned(path, after, schema=CURRENT_SCHEMA)\n",
    "            res[\"migrated\"].append({\"name\": name, \"from\": before, \"to\": CURRENT_SCHEMA})\n",
    "        else:\n",
    "            wrapped = {\n",
    "                \"data\": raw,\n",
    "                \"_meta\": {\n",
    "                    \"schema_version\": CURRENT_SCHEMA,\n",
    "                    \"wrapped\": True,\n",
    "                    \"written_at\": int(time.time()),\n",
    "                    \"source\": \"notebook/gateway\",\n",
    "                },\n",
    "            }\n",
    "            _write_json_versioned(path, wrapped, schema=CURRENT_SCHEMA)\n",
    "            res[\"migrated\"].append({\"name\": name, \"from\": \"non-dict\", \"to\": CURRENT_SCHEMA})\n",
    "    mig_report = ART_DIR / \"gateway_migration_report.json\"\n",
    "    mig_report.write_text(json.dumps(res, indent=2), encoding=\"utf-8\")\n",
    "    print(f\" migration report  {mig_report}\")\n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 668,
   "id": "7b61378d-4b93-4aae-bfd9-a2bfb7eefe09",
   "metadata": {},
   "outputs": [],
   "source": [
    "RUST_CFG_PATH = ART_DIR / \"gateway_config.rs\"\n",
    "def write_rust_config_from_json(json_path: Path = ARTS[\"config\"]) -> Path:\n",
    "    js = _read_json_safe(json_path) or {}\n",
    "    js_str = json.dumps(js)\n",
    "    content = f\"\"\"// auto-generated from notebook\n",
    "// ts = {int(time.time())}\n",
    "pub const GATEWAY_CONFIG_JSON: &str = r#\"{js_str}\"#;\n",
    "\"\"\"\n",
    "    RUST_CFG_PATH.write_text(content, encoding=\"utf-8\")\n",
    "    print(f\" rust config  {RUST_CFG_PATH}\")\n",
    "    return RUST_CFG_PATH"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 669,
   "id": "2fbfa700-069e-4f6e-9d03-f8d382799778",
   "metadata": {},
   "outputs": [],
   "source": [
    "SHIP_MANIFEST = ART_DIR / \"gateway_ship.json\"\n",
    "def build_ship_manifest(extra_files: Optional[List[Path]] = None) -> Path:\n",
    "    files: List[Dict[str, Any]] = []\n",
    "    for name, path in ARTS.items():\n",
    "        files.append(\n",
    "            {\n",
    "                \"name\": name,\n",
    "                \"path\": str(path),\n",
    "                \"exists\": path.exists(),\n",
    "                \"size\": path.stat().st_size if path.exists() else 0,\n",
    "            }\n",
    "        )\n",
    "    if extra_files:\n",
    "        for p in extra_files:\n",
    "            files.append(\n",
    "                {\n",
    "                    \"name\": p.name,\n",
    "                    \"path\": str(p),\n",
    "                    \"exists\": p.exists(),\n",
    "                    \"size\": p.stat().st_size if p.exists() else 0,\n",
    "                }\n",
    "            )\n",
    "    out = {\n",
    "        \"ts\": int(time.time()),\n",
    "        \"schema\": CURRENT_SCHEMA,\n",
    "        \"files\": files,\n",
    "        \"note\": \"CI: upload these somewhere (S3/GCS/artifacts)\",\n",
    "    }\n",
    "    SHIP_MANIFEST.write_text(json.dumps(out, indent=2), encoding=\"utf-8\")\n",
    "    print(f\" ship manifest  {SHIP_MANIFEST}\")\n",
    "    return SHIP_MANIFEST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 670,
   "id": "1f8f828a-006a-4de4-a365-6acc9415ecb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "CI_CHECK_PATH = ART_DIR / \"gateway_ci_check.py\"\n",
    "def write_ci_check_script() -> Path:\n",
    "    src = f'''#!/usr/bin/env python3\n",
    "import json, sys, argparse\n",
    "from pathlib import Path\n",
    "ART_DIR = Path(\"{ART_DIR}\")\n",
    "agg_path = ART_DIR / \"gateway_runs_agg.json\"\n",
    "schema_path = ART_DIR / \"gateway_schema_report.json\"\n",
    "drift_path = ART_DIR / \"gateway_drift_report.json\"\n",
    "def main():\n",
    "    ap = argparse.ArgumentParser()\n",
    "    ap.add_argument(\"--max-p95\", type=float, default=1800.0)\n",
    "    args = ap.parse_args()\n",
    "    # 1) schema must not have problems (if file exists)\n",
    "    if schema_path.exists():\n",
    "        rep = json.loads(schema_path.read_text())\n",
    "        probs = rep.get(\"problems\") or []\n",
    "        if probs:\n",
    "            print(\"[ci]  schema problems:\", probs)\n",
    "            sys.exit(1)\n",
    "    else:\n",
    "        print(\"[ci]  no schema report, continuing\")\n",
    "    # 2) p95 must be under threshold\n",
    "    if agg_path.exists():\n",
    "        agg = json.loads(agg_path.read_text())\n",
    "        runs = agg.get(\"runs\") or []\n",
    "        if runs:\n",
    "            latest = runs[0]\n",
    "            p95 = latest.get(\"p95_ms\")\n",
    "            if isinstance(p95, (int, float)) and p95 > args.max_p95:\n",
    "                print(f\"[ci]  p95 too high: {{p95}} ms > {{args.max_p95}} ms\")\n",
    "                sys.exit(1)\n",
    "            else:\n",
    "                print(f\"[ci]  p95 ok: {{p95}} ms\")\n",
    "        else:\n",
    "            print(\"[ci]  agg has no runs, continuing\")\n",
    "    else:\n",
    "        print(\"[ci]  no agg file, continuing\")\n",
    "    # 3) drift detection is non-fatal\n",
    "    if drift_path.exists():\n",
    "        d = json.loads(drift_path.read_text())\n",
    "        if d.get(\"chaos_drift\") or d.get(\"rl_drift\"):\n",
    "            print(\"[ci]  drift detected (non-fatal):\", d)\n",
    "        else:\n",
    "            print(\"[ci]  no drift\")\n",
    "    print(\"[ci]  all checks passed\")\n",
    "    sys.exit(0)\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n",
    "'''\n",
    "    CI_CHECK_PATH.write_text(src, encoding=\"utf-8\")\n",
    "    CI_CHECK_PATH.chmod(0o755)\n",
    "    print(f\" CI check script  {CI_CHECK_PATH}\")\n",
    "    return CI_CHECK_PATH"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 671,
   "id": "173ef7eb-819d-4091-ace9-94cc40afaf5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "CONTROL_PATH = ART_DIR / \"gateway_control.json\"\n",
    "def write_control_plane_snapshot() -> Path:\n",
    "    desired = _read_json_safe(ARTS[\"desired\"]) or {}\n",
    "    tenants = _read_json_safe(ARTS[\"tenants\"]) or {}\n",
    "    if not isinstance(desired, dict):\n",
    "        desired = {}\n",
    "    if not isinstance(tenants, dict):\n",
    "        tenants = {\"tenants\": {}}\n",
    "    ctrl = {\n",
    "        \"ts\": int(time.time()),\n",
    "        \"schema\": CURRENT_SCHEMA,\n",
    "        \"chaos\": desired.get(\"chaos\") or {},\n",
    "        \"rl_weights\": desired.get(\"rl_weights\") or {},\n",
    "        \"tenants\": tenants.get(\"tenants\") or {},\n",
    "    }\n",
    "    CONTROL_PATH.write_text(json.dumps(ctrl, indent=2), encoding=\"utf-8\")\n",
    "    print(f\" control-plane snapshot  {CONTROL_PATH}\")\n",
    "    return CONTROL_PATH"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 672,
   "id": "cf380732-f36c-4878-8d55-d61f45478a75",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " wrote \\tmp\\art\\gateway_config.json (schema=1.1.0)\n",
      " wrote \\tmp\\art\\gateway_desired_state.json (schema=1.1.0)\n",
      " wrote \\tmp\\art\\gateway_rbac.json (schema=1.1.0)\n",
      " wrote \\tmp\\art\\gateway_tenants.json (schema=1.1.0)\n",
      " wrote \\tmp\\art\\gateway_multi_drift.json (schema=1.1.0)\n",
      " wrote \\tmp\\art\\gateway_sla_runs.json (schema=1.1.0)\n",
      " wrote \\tmp\\art\\gateway_runs_agg.json (schema=1.1.0)\n",
      " wrote \\tmp\\art\\gateway_prom_timeseries.json (schema=1.1.0)\n",
      " cannot read \\tmp\\art\\gateway_full_bundle.tar.gz: 'utf-8' codec can't decode byte 0x8b in position 1: invalid start byte\n",
      " migration report  \\tmp\\art\\gateway_migration_report.json\n",
      " rust config  \\tmp\\art\\gateway_config.rs\n",
      " ship manifest  \\tmp\\art\\gateway_ship.json\n",
      " CI check script  \\tmp\\art\\gateway_ci_check.py\n",
      " control-plane snapshot  \\tmp\\art\\gateway_control.json\n",
      "\n",
      " continuation (migrate + rust-export + ci-check + control-plane) done.\n",
      "Try:\n",
      "  python \\tmp\\art\\gateway_ci_check.py --max-p95 1800\n",
      "  # gateway can poll: \\tmp\\art\\gateway_control.json\n",
      "  # rust include: \\tmp\\art\\gateway_config.rs\n"
     ]
    }
   ],
   "source": [
    "mig_res = migrate_all_known_artifacts()\n",
    "rust_cfg = write_rust_config_from_json()\n",
    "ship = build_ship_manifest()\n",
    "ci_script = write_ci_check_script()\n",
    "ctrl = write_control_plane_snapshot()\n",
    "print(\"\\n continuation (migrate + rust-export + ci-check + control-plane) done.\")\n",
    "print(f\"Try:\")\n",
    "print(f\"  python {CI_CHECK_PATH} --max-p95 1800\")\n",
    "print(f\"  # gateway can poll: {CONTROL_PATH}\")\n",
    "print(f\"  # rust include: {RUST_CFG_PATH}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 673,
   "id": "94ca3512-e4c4-4865-8943-ac3585138c6c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " continuation (json-schema + rust-crate + ci-export + upload-stubs) using ART_DIR=\\tmp\\art\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "import time\n",
    "from pathlib import Path\n",
    "from typing import Any, Dict, List, Optional\n",
    "ART_DIR = Path(os.getenv(\"ART_DIR\", \"/tmp/art\"))\n",
    "ART_DIR.mkdir(parents=True, exist_ok=True)\n",
    "print(f\" continuation (json-schema + rust-crate + ci-export + upload-stubs) using ART_DIR={ART_DIR}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 674,
   "id": "b205ff8d-001c-460f-a404-23e4555cfe4f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " schema  \\tmp\\art\\schemas\\gateway_control_plane.schema.json\n",
      " schema  \\tmp\\art\\schemas\\gateway_desired_state.schema.json\n",
      " schema  \\tmp\\art\\schemas\\gateway_rbac.schema.json\n",
      " schema  \\tmp\\art\\schemas\\gateway_tenants.schema.json\n"
     ]
    }
   ],
   "source": [
    "SCHEMA_DIR = ART_DIR / \"schemas\"\n",
    "SCHEMA_DIR.mkdir(parents=True, exist_ok=True)\n",
    "def _write_schema(name: str, obj: Dict[str, Any]) -> Path:\n",
    "    p = SCHEMA_DIR / f\"{name}.schema.json\"\n",
    "    p.write_text(json.dumps(obj, indent=2), encoding=\"utf-8\")\n",
    "    print(f\" schema  {p}\")\n",
    "    return p\n",
    "def write_control_plane_schema() -> Path:\n",
    "    schema = {\n",
    "        \"$schema\": \"http://json-schema.org/draft-07/schema#\",\n",
    "        \"title\": \"GatewayControlPlane\",\n",
    "        \"type\": \"object\",\n",
    "        \"required\": [\"ts\", \"schema\", \"chaos\", \"rl_weights\", \"tenants\"],\n",
    "        \"properties\": {\n",
    "            \"ts\": {\"type\": \"integer\"},\n",
    "            \"schema\": {\"type\": \"string\"},\n",
    "            \"chaos\": {\n",
    "                \"type\": \"object\",\n",
    "                \"additionalProperties\": {\"type\": [\"boolean\", \"number\", \"string\", \"null\"]},\n",
    "            },\n",
    "            \"rl_weights\": {\n",
    "                \"type\": \"object\",\n",
    "                \"additionalProperties\": {\"type\": \"number\"},\n",
    "            },\n",
    "            \"tenants\": {\n",
    "                \"type\": \"object\",\n",
    "                \"additionalProperties\": {\n",
    "                    \"type\": \"object\",\n",
    "                    \"properties\": {\n",
    "                        \"chaos\": {\"type\": \"object\"},\n",
    "                        \"rl_weights\": {\"type\": \"object\"},\n",
    "                        \"ratelimits\": {\n",
    "                            \"type\": \"object\",\n",
    "                            \"properties\": {\n",
    "                                \"qps\": {\"type\": \"number\"},\n",
    "                                \"burst\": {\"type\": \"number\"},\n",
    "                            },\n",
    "                            \"required\": [\"qps\", \"burst\"],\n",
    "                        },\n",
    "                    },\n",
    "                },\n",
    "            },\n",
    "            \"_meta\": {\"type\": \"object\"},\n",
    "        },\n",
    "        \"additionalProperties\": True,\n",
    "    }\n",
    "    return _write_schema(\"gateway_control_plane\", schema)\n",
    "def write_desired_state_schema() -> Path:\n",
    "    schema = {\n",
    "        \"$schema\": \"http://json-schema.org/draft-07/schema#\",\n",
    "        \"title\": \"GatewayDesiredState\",\n",
    "        \"type\": \"object\",\n",
    "        \"required\": [\"ts\", \"chaos\", \"rl_weights\"],\n",
    "        \"properties\": {\n",
    "            \"ts\": {\"type\": \"integer\"},\n",
    "            \"chaos\": {\"type\": \"object\"},\n",
    "            \"rl_weights\": {\"type\": \"object\"},\n",
    "            \"namespaces\": {\n",
    "                \"type\": \"array\",\n",
    "                \"items\": {\"type\": \"string\"},\n",
    "            },\n",
    "            \"_meta\": {\"type\": \"object\"},\n",
    "        },\n",
    "        \"additionalProperties\": True,\n",
    "    }\n",
    "    return _write_schema(\"gateway_desired_state\", schema)\n",
    "def write_rbac_schema() -> Path:\n",
    "    schema = {\n",
    "        \"$schema\": \"http://json-schema.org/draft-07/schema#\",\n",
    "        \"title\": \"GatewayRBAC\",\n",
    "        \"type\": \"object\",\n",
    "        \"required\": [\"roles\", \"actions\"],\n",
    "        \"properties\": {\n",
    "            \"ts\": {\"type\": \"integer\"},\n",
    "            \"schema\": {\"type\": \"string\"},\n",
    "            \"roles\": {\n",
    "                \"type\": \"object\",\n",
    "                \"additionalProperties\": {\n",
    "                    \"type\": \"array\",\n",
    "                    \"items\": {\"type\": \"string\"},\n",
    "                },\n",
    "            },\n",
    "            \"tokens\": {\n",
    "                \"type\": \"object\",\n",
    "                \"additionalProperties\": {\"type\": \"string\"},\n",
    "            },\n",
    "            \"actions\": {\n",
    "                \"type\": \"object\",\n",
    "                \"additionalProperties\": {\"type\": \"string\"},\n",
    "            },\n",
    "        },\n",
    "        \"additionalProperties\": True,\n",
    "    }\n",
    "    return _write_schema(\"gateway_rbac\", schema)\n",
    "def write_tenants_schema() -> Path:\n",
    "    schema = {\n",
    "        \"$schema\": \"http://json-schema.org/draft-07/schema#\",\n",
    "        \"title\": \"GatewayTenants\",\n",
    "        \"type\": \"object\",\n",
    "        \"required\": [\"tenants\"],\n",
    "        \"properties\": {\n",
    "            \"ts\": {\"type\": \"integer\"},\n",
    "            \"schema\": {\"type\": \"string\"},\n",
    "            \"tenants\": {\n",
    "                \"type\": \"object\",\n",
    "                \"additionalProperties\": {\n",
    "                    \"type\": \"object\",\n",
    "                    \"properties\": {\n",
    "                        \"chaos\": {\"type\": \"object\"},\n",
    "                        \"rl_weights\": {\"type\": \"object\"},\n",
    "                        \"ratelimits\": {\"type\": \"object\"},\n",
    "                    },\n",
    "                },\n",
    "            },\n",
    "        },\n",
    "        \"additionalProperties\": True,\n",
    "    }\n",
    "    return _write_schema(\"gateway_tenants\", schema)\n",
    "schema_cp = write_control_plane_schema()\n",
    "schema_desired = write_desired_state_schema()\n",
    "schema_rbac = write_rbac_schema()\n",
    "schema_tenants = write_tenants_schema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 675,
   "id": "2e00e4f2-2990-4ec2-879f-d9d70c1adda3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " rust crate  \\tmp\\art\\rust-gateway-config\n"
     ]
    }
   ],
   "source": [
    "RUST_CRATE_DIR = ART_DIR / \"rust-gateway-config\"\n",
    "(RUST_CRATE_DIR / \"src\").mkdir(parents=True, exist_ok=True)\n",
    "def render_rust_crate(\n",
    "    crate_name: str = \"gateway_config_embed\",\n",
    "    json_path: Path = ART_DIR / \"gateway_config.json\",\n",
    ") -> Path:\n",
    "    js = {}\n",
    "    if json_path.exists():\n",
    "        try:\n",
    "            js = json.loads(json_path.read_text(encoding=\"utf-8\"))\n",
    "        except Exception as e:\n",
    "            print(f\" could not read {json_path}: {e}\")\n",
    "    js_str = json.dumps(js)\n",
    "    cargo = f\"\"\"[package]\n",
    "name = \"{crate_name}\"\n",
    "version = \"0.1.0\"\n",
    "edition = \"2021\"\n",
    "[lib]\n",
    "name = \"{crate_name}\"\n",
    "path = \"src/lib.rs\"\n",
    "[dependencies]\n",
    "serde = {{ version = \"1\", features = [\"derive\"] }}\n",
    "serde_json = \"1\"\n",
    "\"\"\"\n",
    "    lib_rs = f\"\"\"//! auto-generated from notebook\n",
    "//! holds the latest notebook-produced gateway config\n",
    "use serde::{{Deserialize, Serialize}};\n",
    "pub const GATEWAY_CONFIG_JSON: &str = r#\"{js_str}\"#;\n",
    "#[derive(Debug, Clone, Serialize, Deserialize)]\n",
    "pub struct GatewayConfig {{\n",
    "    #[serde(default)]\n",
    "    pub ts: Option<u64>,\n",
    "    #[serde(default)]\n",
    "    pub version: Option<String>,\n",
    "    #[serde(default)]\n",
    "    pub note: Option<String>,\n",
    "    #[serde(default)]\n",
    "    pub rbac: serde_json::Value,\n",
    "    #[serde(default)]\n",
    "    pub tenants: serde_json::Value,\n",
    "    #[serde(default)]\n",
    "    pub desired_state: serde_json::Value,\n",
    "    #[serde(default)]\n",
    "    pub schema_report: serde_json::Value,\n",
    "    #[serde(default)]\n",
    "    pub grafana_dashboard: serde_json::Value,\n",
    "}}\n",
    "\n",
    "impl GatewayConfig {{\n",
    "    pub fn from_embedded() -> serde_json::Result<Self> {{\n",
    "        serde_json::from_str(GATEWAY_CONFIG_JSON)\n",
    "    }}\n",
    "}}\n",
    "\"\"\"\n",
    "    (RUST_CRATE_DIR / \"Cargo.toml\").write_text(cargo, encoding=\"utf-8\")\n",
    "    (RUST_CRATE_DIR / \"src\" / \"lib.rs\").write_text(lib_rs, encoding=\"utf-8\")\n",
    "    print(f\" rust crate  {RUST_CRATE_DIR}\")\n",
    "    return RUST_CRATE_DIR\n",
    "rust_crate = render_rust_crate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 676,
   "id": "55236487-1440-46d2-8cfe-5e71d8945534",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " GH workflow  \\tmp\\art\\.github\\workflows\\gateway-notebook-ci.yml\n"
     ]
    }
   ],
   "source": [
    "GH_DIR = ART_DIR / \".github\" / \"workflows\"\n",
    "GH_DIR.mkdir(parents=True, exist_ok=True)\n",
    "CI_WF_PATH = GH_DIR / \"gateway-notebook-ci.yml\"\n",
    "def write_notebook_ci_workflow() -> Path:\n",
    "    content = f\"\"\"name: gateway-notebook-ci\n",
    "on:\n",
    "  push:\n",
    "    paths:\n",
    "      - \"gateway/**\"\n",
    "      - \".github/workflows/gateway-notebook-ci.yml\"\n",
    "      - \"notebooks/**\"\n",
    "  workflow_dispatch: {{}}\n",
    "jobs:\n",
    "  notebook-ci:\n",
    "    runs-on: ubuntu-latest\n",
    "    steps:\n",
    "      - name: Checkout\n",
    "        uses: actions/checkout@v4\n",
    "      - name: Set up Python\n",
    "        uses: actions/setup-python@v5\n",
    "        with:\n",
    "          python-version: \"3.11\"\n",
    "      - name: Install deps\n",
    "        run: |\n",
    "          python -m pip install --upgrade pip\n",
    "          # add jsonschema if you want to validate schemas\n",
    "          pip install jsonschema\n",
    "      - name: Run CI check script from artifacts\n",
    "        run: |\n",
    "          if [ -f /tmp/art/gateway_ci_check.py ]; then\n",
    "            python /tmp/art/gateway_ci_check.py --max-p95 1800\n",
    "          else\n",
    "            echo \"no /tmp/art/gateway_ci_check.py, skipping\"\n",
    "          fi\n",
    "\"\"\"\n",
    "    CI_WF_PATH.write_text(content, encoding=\"utf-8\")\n",
    "    print(f\" GH workflow  {CI_WF_PATH}\")\n",
    "    return CI_WF_PATH\n",
    "wf = write_notebook_ci_workflow()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 677,
   "id": "0a622002-7994-4580-b911-71456e6dff39",
   "metadata": {},
   "outputs": [],
   "source": [
    "UPLOADER_PATH = ART_DIR / \"gateway_upload_report.json\"\n",
    "def upload_artifacts(\n",
    "    targets: Optional[List[Path]] = None,\n",
    "    method: str = \"dry-run\",\n",
    "    endpoint: Optional[str] = None,\n",
    ") -> Path:\n",
    "    \"\"\"\n",
    "    Super simple uploader stub.\n",
    "    method = \"dry-run\" | \"http-post\"\n",
    "    \"\"\"\n",
    "    if targets is None:\n",
    "        targets = [\n",
    "            ART_DIR / \"gateway_config.json\",\n",
    "            ART_DIR / \"gateway_full_bundle.tar.gz\",\n",
    "            ART_DIR / \"gateway_control.json\",\n",
    "            ART_DIR / \"gateway_schema_report.json\",\n",
    "        ]\n",
    "    sent: List[Dict[str, Any]] = []\n",
    "    for t in targets:\n",
    "        info = {\"file\": str(t), \"exists\": t.exists(), \"size\": t.stat().st_size if t.exists() else 0}\n",
    "        if not t.exists():\n",
    "            info[\"status\"] = \"missing\"\n",
    "            sent.append(info)\n",
    "            continue\n",
    "        if method == \"dry-run\":\n",
    "            print(f\" (dry-run) would upload  {t}\")\n",
    "            info[\"status\"] = \"dry-run\"\n",
    "        elif method == \"http-post\" and endpoint:\n",
    "            import requests\n",
    "            try:\n",
    "                r = requests.post(endpoint, files={\"file\": t.open(\"rb\")}, timeout=3.0)\n",
    "                info[\"status\"] = f\"http {r.status_code}\"\n",
    "            except Exception as e:\n",
    "                info[\"status\"] = f\"error: {e}\"\n",
    "        sent.append(info)\n",
    "    rep = {\n",
    "        \"ts\": int(time.time()),\n",
    "        \"method\": method,\n",
    "        \"endpoint\": endpoint,\n",
    "        \"files\": sent,\n",
    "    }\n",
    "    UPLOADER_PATH.write_text(json.dumps(rep, indent=2), encoding=\"utf-8\")\n",
    "    print(f\" upload report  {UPLOADER_PATH}\")\n",
    "    return UPLOADER_PATH"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 678,
   "id": "05f7fdc0-09ac-4682-b6f0-427032dad980",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " (dry-run) would upload  \\tmp\\art\\gateway_config.json\n",
      " (dry-run) would upload  \\tmp\\art\\gateway_full_bundle.tar.gz\n",
      " (dry-run) would upload  \\tmp\\art\\gateway_control.json\n",
      " upload report  \\tmp\\art\\gateway_upload_report.json\n",
      "\n",
      " continuation (json-schema + rust-crate + ci-export + upload-stubs) done.\n",
      "Try:\n",
      "  # validate schemas exist\n",
      "  ls \\tmp\\art\\schemas\n",
      "  # build rust crate\n",
      "  cd \\tmp\\art\\rust-gateway-config && cargo build  # in real env\n",
      "  # run uploader\n",
      "  upload_artifacts(method='http-post', endpoint='http://localhost:9000/upload')\n"
     ]
    }
   ],
   "source": [
    "upload_artifacts(method=\"dry-run\")\n",
    "print(\"\\n continuation (json-schema + rust-crate + ci-export + upload-stubs) done.\")\n",
    "print(\"Try:\")\n",
    "print(\"  # validate schemas exist\")\n",
    "print(f\"  ls {SCHEMA_DIR}\")\n",
    "print(\"  # build rust crate\")\n",
    "print(f\"  cd {RUST_CRATE_DIR} && cargo build  # in real env\")\n",
    "print(\"  # run uploader\")\n",
    "print(\"  upload_artifacts(method='http-post', endpoint='http://localhost:9000/upload')\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 679,
   "id": "121c38b0-e369-4d47-8c8d-66646431029d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " continuation (rust-handlers + sidecar-poller + prom-rules + k8s-cron) using ART_DIR=\\tmp\\art\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "import time\n",
    "from pathlib import Path\n",
    "from typing import Any, Dict, Optional, List\n",
    "ART_DIR = Path(os.getenv(\"ART_DIR\", \"/tmp/art\"))\n",
    "ART_DIR.mkdir(parents=True, exist_ok=True)\n",
    "print(f\" continuation (rust-handlers + sidecar-poller + prom-rules + k8s-cron) using ART_DIR={ART_DIR}\")\n",
    "GATEWAY_BASE = os.getenv(\"GATEWAY_BASE\", \"\").strip() or \"http://127.0.0.1:9910\"\n",
    "ADMIN_TOKEN = os.getenv(\"GATEWAY_ADMIN_TOKEN\", \"\").strip()\n",
    "RUST_STATE_PATH = ART_DIR / \"gateway_state.rs\"\n",
    "RUST_ADMIN_PATH = ART_DIR / \"gateway_admin.rs\"\n",
    "RUST_LOADER_PATH = ART_DIR / \"gateway_loader.rs\"\n",
    "SIDECAR_PATH = ART_DIR / \"gateway_sidecar_poller.py\"\n",
    "PROM_RULES_PATH = ART_DIR / \"prometheus-gateway-rules.yml\"\n",
    "K8S_CRON_PATH = ART_DIR / \"k8s-gateway-agent-cronjob.yaml\"\n",
    "CONTROL_PLANE_JSON = ART_DIR / \"gateway_control.json\"     \n",
    "DESIRED_STATE_JSON = ART_DIR / \"gateway_desired_state.json\"\n",
    "TENANTS_JSON = ART_DIR / \"gateway_tenants.json\"\n",
    "RBAC_JSON = ART_DIR / \"gateway_rbac.json\"\n",
    "def _read_json_safe(p: Path) -> Any:\n",
    "    if not p.exists():\n",
    "        return None\n",
    "    try:\n",
    "        return json.loads(p.read_text(encoding=\"utf-8\"))\n",
    "    except Exception as e:\n",
    "        print(f\" cannot read {p}: {e}\")\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 680,
   "id": "dd6490f5-9c4e-434d-8c6b-017329fcfc44",
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_rust_state_module() -> Path:\n",
    "    \"\"\"\n",
    "    minimal state shared by handlers; gateway can call reload_from_json(...)\n",
    "    \"\"\"\n",
    "    src = \"\"\"// auto-generated by notebook\n",
    "use std::sync::{Arc, RwLock};\n",
    "use serde::{Deserialize, Serialize};\n",
    "use std::collections::HashMap;\n",
    "#[derive(Debug, Clone, Serialize, Deserialize, Default)]\n",
    "pub struct ChaosConfig {\n",
    "    #[serde(flatten)]\n",
    "    pub flags: HashMap<String, serde_json::Value>,\n",
    "}\n",
    "#[derive(Debug, Clone, Serialize, Deserialize, Default)]\n",
    "pub struct RlWeights {\n",
    "    #[serde(flatten)]\n",
    "    pub weights: HashMap<String, f64>,\n",
    "}\n",
    "#[derive(Debug, Clone, Serialize, Deserialize, Default)]\n",
    "pub struct TenantConfig {\n",
    "    #[serde(default)]\n",
    "    pub chaos: ChaosConfig,\n",
    "    #[serde(default)]\n",
    "    pub rl_weights: RlWeights,\n",
    "    #[serde(default)]\n",
    "    pub ratelimits: Option<serde_json::Value>,\n",
    "}\n",
    "#[derive(Debug, Clone, Serialize, Deserialize, Default)]\n",
    "pub struct GatewayControlPlane {\n",
    "    pub ts: u64,\n",
    "    pub schema: String,\n",
    "    #[serde(default)]\n",
    "    pub chaos: ChaosConfig,\n",
    "    #[serde(default)]\n",
    "    pub rl_weights: RlWeights,\n",
    "    #[serde(default)]\n",
    "    pub tenants: HashMap<String, TenantConfig>,\n",
    "    #[serde(default)]\n",
    "    pub _meta: Option<serde_json::Value>,\n",
    "}\n",
    "#[derive(Clone)]\n",
    "pub struct AppState {\n",
    "    inner: Arc<RwLock<GatewayControlPlane>>,\n",
    "    pub admin_token: Option<String>,\n",
    "}\n",
    "impl AppState {\n",
    "    pub fn new(initial: GatewayControlPlane, admin_token: Option<String>) -> Self {\n",
    "        Self {\n",
    "            inner: Arc::new(RwLock::new(initial)),\n",
    "            admin_token,\n",
    "        }\n",
    "    }\n",
    "    pub fn read(&self) -> GatewayControlPlane {\n",
    "        self.inner.read().unwrap().clone()\n",
    "    }\n",
    "    pub fn update(&self, new_val: GatewayControlPlane) {\n",
    "        *self.inner.write().unwrap() = new_val;\n",
    "    }\n",
    "    pub fn check_admin(&self, tok: Option<&str>) -> bool {\n",
    "        match (&self.admin_token, tok) {\n",
    "            (Some(required), Some(got)) => got == required,\n",
    "            (Some(_), None) => false,\n",
    "            (None, _) => true,\n",
    "        }\n",
    "    }\n",
    "    pub fn merge_effective_chaos(&self, ns: Option<&str>) -> ChaosConfig {\n",
    "        let s = self.inner.read().unwrap();\n",
    "        let mut out = s.chaos.clone();\n",
    "        if let Some(ns) = ns {\n",
    "            if let Some(tcfg) = s.tenants.get(ns) {\n",
    "                for (k, v) in &tcfg.chaos.flags {\n",
    "                    out.flags.insert(k.clone(), v.clone());\n",
    "                }\n",
    "            }\n",
    "        }\n",
    "        out\n",
    "    }\n",
    "    pub fn merge_effective_rl(&self, ns: Option<&str>) -> RlWeights {\n",
    "        let s = self.inner.read().unwrap();\n",
    "        let mut out = s.rl_weights.clone();\n",
    "        if let Some(ns) = ns {\n",
    "            if let Some(tcfg) = s.tenants.get(ns) {\n",
    "                for (k, v) in &tcfg.rl_weights.weights {\n",
    "                    out.weights.insert(k.clone(), *v);\n",
    "                }\n",
    "            }\n",
    "        }\n",
    "        out\n",
    "    }\n",
    "}\n",
    "\"\"\"\n",
    "    RUST_STATE_PATH.write_text(src, encoding=\"utf-8\")\n",
    "    print(f\" rust state module  {RUST_STATE_PATH}\")\n",
    "    return RUST_STATE_PATH"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 681,
   "id": "e45bb13b-a229-4435-9503-edab67b5927a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_rust_admin_module() -> Path:\n",
    "    \"\"\"\n",
    "    Axum handlers matching the notebook endpoints:\n",
    "    - GET  /admin/chaos/get\n",
    "    - POST /admin/chaos/set\n",
    "    - POST /rl/update\n",
    "    - GET  /config/current\n",
    "    \"\"\"\n",
    "    src = \"\"\"// auto-generated by notebook\n",
    "use axum::{extract::State, Json, http::StatusCode, response::IntoResponse};\n",
    "use serde_json::json;\n",
    "use crate::gateway_state::{AppState, GatewayControlPlane, ChaosConfig, RlWeights};\n",
    "fn admin_token_from_header(headers: &axum::http::HeaderMap) -> Option<String> {\n",
    "    headers\n",
    "        .get(\"x-admin-token\")\n",
    "        .and_then(|v| v.to_str().ok())\n",
    "        .map(|s| s.to_string())\n",
    "}\n",
    "pub async fn get_chaos(\n",
    "    State(app): State<AppState>,\n",
    "    headers: axum::http::HeaderMap,\n",
    ") -> impl IntoResponse {\n",
    "    if !app.check_admin(admin_token_from_header(&headers).as_deref()) {\n",
    "        return (StatusCode::UNAUTHORIZED, Json(json!({\"error\":\"unauthorized\"})));\n",
    "    }\n",
    "    let cur = app.read();\n",
    "    (StatusCode::OK, Json(json!({ \"chaos\": cur.chaos.flags })))\n",
    "}\n",
    "#[derive(Debug, serde::Deserialize)]\n",
    "pub struct ChaosPayload {\n",
    "    #[serde(flatten)]\n",
    "    pub flags: serde_json::Map<String, serde_json::Value>,\n",
    "    #[serde(default)]\n",
    "    pub ns: Option<String>,\n",
    "}\n",
    "pub async fn set_chaos(\n",
    "    State(app): State<AppState>,\n",
    "    headers: axum::http::HeaderMap,\n",
    "    Json(payload): Json<ChaosPayload>,\n",
    ") -> impl IntoResponse {\n",
    "    if !app.check_admin(admin_token_from_header(&headers).as_deref()) {\n",
    "        return (StatusCode::UNAUTHORIZED, Json(json!({\"error\":\"unauthorized\"})));\n",
    "    }\n",
    "    let mut cur = app.read();\n",
    "    if let Some(ns) = &payload.ns {\n",
    "        if let Some(tcfg) = cur.tenants.get_mut(ns) {\n",
    "            for (k, v) in payload.flags.iter() {\n",
    "                tcfg.chaos.flags.insert(k.clone(), v.clone());\n",
    "            }\n",
    "        } else {\n",
    "            // create tenant on the fly\n",
    "            use std::collections::HashMap;\n",
    "            let mut tcfg = crate::gateway_state::TenantConfig::default();\n",
    "            for (k, v) in payload.flags.iter() {\n",
    "                tcfg.chaos.flags.insert(k.clone(), v.clone());\n",
    "            }\n",
    "            cur.tenants.insert(ns.clone(), tcfg);\n",
    "        }\n",
    "    } else {\n",
    "        for (k, v) in payload.flags.iter() {\n",
    "            cur.chaos.flags.insert(k.clone(), v.clone());\n",
    "        }\n",
    "    }\n",
    "    app.update(cur.clone());\n",
    "    (StatusCode::OK, Json(json!({\"ok\": true, \"chaos\": cur.chaos.flags})))\n",
    "}\n",
    "#[derive(Debug, serde::Deserialize)]\n",
    "pub struct RlPayload {\n",
    "    pub weights: serde_json::Map<String, serde_json::Value>,\n",
    "    #[serde(default)]\n",
    "    pub ns: Option<String>,\n",
    "}\n",
    "pub async fn rl_update(\n",
    "    State(app): State<AppState>,\n",
    "    headers: axum::http::HeaderMap,\n",
    "    Json(payload): Json<RlPayload>,\n",
    ") -> impl IntoResponse {\n",
    "    if !app.check_admin(admin_token_from_header(&headers).as_deref()) {\n",
    "        return (StatusCode::UNAUTHORIZED, Json(json!({\"error\":\"unauthorized\"})));\n",
    "    }\n",
    "    let mut cur = app.read();\n",
    "    if let Some(ns) = &payload.ns {\n",
    "        if let Some(tcfg) = cur.tenants.get_mut(ns) {\n",
    "            for (k, v) in payload.weights.iter() {\n",
    "                if let Some(num) = v.as_f64() {\n",
    "                    tcfg.rl_weights.weights.insert(k.clone(), num);\n",
    "                }\n",
    "            }\n",
    "        }\n",
    "    } else {\n",
    "        for (k, v) in payload.weights.iter() {\n",
    "            if let Some(num) = v.as_f64() {\n",
    "                cur.rl_weights.weights.insert(k.clone(), num);\n",
    "            }\n",
    "        }\n",
    "    }\n",
    "    app.update(cur.clone());\n",
    "    (StatusCode::OK, Json(json!({\"ok\": true})))\n",
    "}\n",
    "pub async fn get_config(State(app): State<AppState>) -> impl IntoResponse {\n",
    "    let cur = app.read();\n",
    "    (StatusCode::OK, Json(cur))\n",
    "}\n",
    "\"\"\"\n",
    "    RUST_ADMIN_PATH.write_text(src, encoding=\"utf-8\")\n",
    "    print(f\" rust admin module  {RUST_ADMIN_PATH}\")\n",
    "    return RUST_ADMIN_PATH"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 682,
   "id": "721dbb40-8951-4024-8dbf-20f6adf0b2ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_rust_loader_module() -> Path:\n",
    "    src = f\"\"\"// auto-generated by notebook\n",
    "use std::fs;\n",
    "use std::path::Path;\n",
    "use crate::gateway_state::GatewayControlPlane;\n",
    "pub fn load_control_plane<P: AsRef<Path>>(p: P) -> GatewayControlPlane {{\n",
    "    let path = p.as_ref();\n",
    "    match fs::read_to_string(path) {{\n",
    "        Ok(txt) => {{\n",
    "            match serde_json::from_str::<GatewayControlPlane>(&txt) {{\n",
    "                Ok(cfg) => cfg,\n",
    "                Err(e) => {{\n",
    "                    eprintln!(\"failed to parse control plane {{}}: {{}}\", path.display(), e);\n",
    "                    GatewayControlPlane::default()\n",
    "                }}\n",
    "            }}\n",
    "        }}\n",
    "        Err(e) => {{\n",
    "            eprintln!(\"control plane file not found ({{}}): {{}}\", path.display(), e);\n",
    "            GatewayControlPlane::default()\n",
    "        }}\n",
    "    }}\n",
    "}}\n",
    "pub fn default_path() -> &'static str {{\n",
    "    \"{CONTROL_PLANE_JSON}\"\n",
    "}}\n",
    "\"\"\"\n",
    "    RUST_LOADER_PATH.write_text(src, encoding=\"utf-8\")\n",
    "    print(f\" rust loader module  {RUST_LOADER_PATH}\")\n",
    "    return RUST_LOADER_PATH"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 683,
   "id": "814aa192-3c25-4f45-82d3-cad46512bcc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_sidecar_poller() -> Path:\n",
    "    src = f\"\"\"#!/usr/bin/env python3\n",
    "import os, json, time, requests\n",
    "from pathlib import Path\n",
    "ART_DIR = Path(os.getenv(\"ART_DIR\", \"{ART_DIR}\"))\n",
    "CONTROL = ART_DIR / \"gateway_control.json\"\n",
    "GATEWAY_BASE = os.getenv(\"GATEWAY_BASE\", \"{GATEWAY_BASE}\")\n",
    "ADMIN_TOKEN = os.getenv(\"GATEWAY_ADMIN_TOKEN\", \"\").strip()\n",
    "INTERVAL = float(os.getenv(\"GW_SIDECAR_INTERVAL\", \"5\"))\n",
    "\n",
    "def _headers():\n",
    "    h = {{\"content-type\": \"application/json\"}}\n",
    "    if ADMIN_TOKEN:\n",
    "        h[\"x-admin-token\"] = ADMIN_TOKEN\n",
    "    return h\n",
    "def main():\n",
    "    last_seen = None\n",
    "    print(f\"[sidecar] watching {{CONTROL}} every {{INTERVAL}}s  {{GATEWAY_BASE}}\")\n",
    "    while True:\n",
    "        try:\n",
    "            if CONTROL.exists():\n",
    "                txt = CONTROL.read_text(encoding=\"utf-8\")\n",
    "                if txt != last_seen:\n",
    "                    # push global chaos\n",
    "                    doc = json.loads(txt)\n",
    "                    chaos = doc.get(\"chaos\") or {{}}\n",
    "                    rl = doc.get(\"rl_weights\") or {{}}\n",
    "                    print(\"[sidecar] detected change  pushing chaos+rl\")\n",
    "                    r1 = requests.post(f\"{{GATEWAY_BASE}}/admin/chaos/set\", headers=_headers(), json=chaos, timeout=2.5)\n",
    "                    r2 = requests.post(f\"{{GATEWAY_BASE}}/rl/update\", headers=_headers(), json={{\"weights\": rl}}, timeout=2.5)\n",
    "                    print(\"[sidecar] chaos:\", r1.status_code, \"rl:\", r2.status_code)\n",
    "                    last_seen = txt\n",
    "            else:\n",
    "                print(\"[sidecar] control-file missing, skipping\")\n",
    "        except Exception as e:\n",
    "            print(\"[sidecar] error:\", e)\n",
    "        time.sleep(INTERVAL)\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n",
    "\"\"\"\n",
    "    SIDECAR_PATH.write_text(src, encoding=\"utf-8\")\n",
    "    SIDECAR_PATH.chmod(0o755)\n",
    "    print(f\" sidecar poller  {SIDECAR_PATH}\")\n",
    "    return SIDECAR_PATH"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 684,
   "id": "06878165-d705-4d80-8f3c-69baa3eb956e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_prom_rules() -> Path:\n",
    "    content = \"\"\"groups:\n",
    "  - name: rag-gateway\n",
    "    rules:\n",
    "      - alert: GatewayLatencyHighP95\n",
    "        expr: histogram_quantile(0.95, sum(rate(gateway_answer_latency_ms_bucket[5m])) by (le)) > 1.8\n",
    "        for: 2m\n",
    "        labels:\n",
    "          severity: critical\n",
    "        annotations:\n",
    "          summary: \"Gateway p95 is high\"\n",
    "          description: \"Gateway p95 latency > 1.8s for 2m\"\n",
    "      - alert: GatewayDenseErrors\n",
    "        expr: sum(rate(gateway_fanout_errors_total{{step=\"dense\"}}[5m])) > 0\n",
    "        for: 2m\n",
    "        labels:\n",
    "          severity: warning\n",
    "        annotations:\n",
    "          summary: \"Gateway dense fanout errors\"\n",
    "          description: \"dense retriever is erroring  consider force_bm25\"\n",
    "      - alert: GatewayNoStrategyMix\n",
    "        expr: sum(rate(gateway_strategy_selected_total[5m])) == 0\n",
    "        for: 5m\n",
    "        labels:\n",
    "          severity: warning\n",
    "        annotations:\n",
    "          summary: \"Gateway not selecting strategies\"\n",
    "          description: \"RAG strategy mix is 0  gateway might be stuck or misconfigured\"\n",
    "\"\"\"\n",
    "    PROM_RULES_PATH.write_text(content, encoding=\"utf-8\")\n",
    "    print(f\" prom rules  {PROM_RULES_PATH}\")\n",
    "    return PROM_RULES_PATH"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 685,
   "id": "37c5b11e-c675-45b1-aa6f-12ef82ae89e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_k8s_cronjob() -> Path:\n",
    "    content = f\"\"\"apiVersion: batch/v1\n",
    "kind: CronJob\n",
    "metadata:\n",
    "  name: rag-gateway-agent\n",
    "spec:\n",
    "  schedule: \"*/5 * * * *\"  # every 5 minutes\n",
    "  jobTemplate:\n",
    "    spec:\n",
    "      template:\n",
    "        spec:\n",
    "          restartPolicy: OnFailure\n",
    "          containers:\n",
    "            - name: gateway-agent\n",
    "              image: python:3.11-slim\n",
    "              env:\n",
    "                - name: ART_DIR\n",
    "                  value: \"/tmp/art\"\n",
    "                - name: GATEWAY_BASE\n",
    "                  value: \"{GATEWAY_BASE}\"\n",
    "                - name: GATEWAY_ADMIN_TOKEN\n",
    "                  valueFrom:\n",
    "                    secretKeyRef:\n",
    "                      name: rag-gateway-secrets\n",
    "                      key: GATEWAY_ADMIN_TOKEN\n",
    "              volumeMounts:\n",
    "                - name: artdir\n",
    "                  mountPath: /tmp/art\n",
    "              command: [\"python\", \"-c\"]\n",
    "              args:\n",
    "                - |\n",
    "                  import json, time, os, requests\n",
    "                  from pathlib import Path\n",
    "                  ART_DIR = Path(os.getenv(\"ART_DIR\", \"/tmp/art\"))\n",
    "                  ctrl = ART_DIR / \"gateway_control.json\"\n",
    "                  base = os.getenv(\"GATEWAY_BASE\", \"{GATEWAY_BASE}\")\n",
    "                  tok = os.getenv(\"GATEWAY_ADMIN_TOKEN\", \"\")\n",
    "                  h = {{\"content-type\": \"application/json\"}}\n",
    "                  if tok:\n",
    "                      h[\"x-admin-token\"] = tok\n",
    "                  if ctrl.exists():\n",
    "                      doc = json.loads(ctrl.read_text())\n",
    "                      chaos = doc.get(\"chaos\") or {{}}\n",
    "                      rl = doc.get(\"rl_weights\") or {{}}\n",
    "                      requests.post(f\"{{base}}/admin/chaos/set\", headers=h, json=chaos, timeout=3)\n",
    "                      requests.post(f\"{{base}}/rl/update\", headers=h, json={{\"weights\": rl}}, timeout=3)\n",
    "                      print(\"pushed control  gateway\")\n",
    "                  else:\n",
    "                      print(\"no control file, skipping\")\n",
    "          volumes:\n",
    "            - name: artdir\n",
    "              persistentVolumeClaim:\n",
    "                claimName: rag-gateway-art\n",
    "\"\"\"\n",
    "    K8S_CRON_PATH.write_text(content, encoding=\"utf-8\")\n",
    "    print(f\" k8s cronjob  {K8S_CRON_PATH}\")\n",
    "    return K8S_CRON_PATH"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 686,
   "id": "b841a82d-e285-475a-9f30-2485484a58ca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " rust state module  \\tmp\\art\\gateway_state.rs\n",
      " rust admin module  \\tmp\\art\\gateway_admin.rs\n",
      " rust loader module  \\tmp\\art\\gateway_loader.rs\n",
      " sidecar poller  \\tmp\\art\\gateway_sidecar_poller.py\n",
      " prom rules  \\tmp\\art\\prometheus-gateway-rules.yml\n",
      " k8s cronjob  \\tmp\\art\\k8s-gateway-agent-cronjob.yaml\n",
      "\n",
      " continuation (rust-handlers + sidecar + prom-rules + k8s-cron) done.\n",
      "Try:\n",
      "  cat \\tmp\\art\\gateway_state.rs\n",
      "  cat \\tmp\\art\\gateway_admin.rs\n",
      "  python \\tmp\\art\\gateway_sidecar_poller.py   # local only\n",
      "  # kubectl apply -f \\tmp\\art\\k8s-gateway-agent-cronjob.yaml\n"
     ]
    }
   ],
   "source": [
    "_ = write_rust_state_module()\n",
    "_ = write_rust_admin_module()\n",
    "_ = write_rust_loader_module()\n",
    "_ = write_sidecar_poller()\n",
    "_ = write_prom_rules()\n",
    "_ = write_k8s_cronjob()\n",
    "print(\"\\n continuation (rust-handlers + sidecar + prom-rules + k8s-cron) done.\")\n",
    "print(\"Try:\")\n",
    "print(f\"  cat {RUST_STATE_PATH}\")\n",
    "print(f\"  cat {RUST_ADMIN_PATH}\")\n",
    "print(f\"  python {SIDECAR_PATH}   # local only\")\n",
    "print(f\"  # kubectl apply -f {K8S_CRON_PATH}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 687,
   "id": "f8cbf085-d962-48d5-8d92-17a68cd15136",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " continuation (safety + pending-ops + rollout + k8s-sidecar-deploy + janitor) using ART_DIR=\\tmp\\art\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "import time\n",
    "import random\n",
    "from pathlib import Path\n",
    "from typing import Any, Dict, List, Optional, Tuple\n",
    "ART_DIR = Path(os.getenv(\"ART_DIR\", \"/tmp/art\"))\n",
    "ART_DIR.mkdir(parents=True, exist_ok=True)\n",
    "print(f\" continuation (safety + pending-ops + rollout + k8s-sidecar-deploy + janitor) using ART_DIR={ART_DIR}\")\n",
    "GATEWAY_BASE = os.getenv(\"GATEWAY_BASE\", \"\").strip() or \"http://127.0.0.1:9910\"\n",
    "ADMIN_TOKEN = os.getenv(\"GATEWAY_ADMIN_TOKEN\", \"\").strip()\n",
    "CURRENT_SCHEMA = \"1.1.0\"  \n",
    "CONTROL_PATH = ART_DIR / \"gateway_control.json\"\n",
    "CONTROL_HISTORY_DIR = ART_DIR / \"control-history\"\n",
    "CONTROL_HISTORY_DIR.mkdir(parents=True, exist_ok=True)\n",
    "PENDING_OPS_PATH = ART_DIR / \"gateway_pending_ops.jsonl\"\n",
    "ADMIN_LOG_PATH = ART_DIR / \"gateway_admin_ops.log\"\n",
    "JANITOR_REPORT = ART_DIR / \"gateway_janitor.json\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 688,
   "id": "31b403bc-74c6-4467-a2cd-ad28509e0cf1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _admin_headers(extra: Optional[Dict[str, str]] = None) -> Dict[str, str]:\n",
    "    h = {\"content-type\": \"application/json\"}\n",
    "    if ADMIN_TOKEN:\n",
    "        h[\"x-admin-token\"] = ADMIN_TOKEN\n",
    "    if extra:\n",
    "        h.update(extra)\n",
    "    return h\n",
    "def _http_req(method: str, url: str, *, json_body: Optional[Dict[str, Any]] = None, timeout: float = 2.5) -> Dict[str, Any]:\n",
    "    import requests\n",
    "    try:\n",
    "        if method.upper() == \"GET\":\n",
    "            r = requests.get(url, headers=_admin_headers(), timeout=timeout)\n",
    "        else:\n",
    "            r = requests.post(url, headers=_admin_headers(), json=json_body or {}, timeout=timeout)\n",
    "        try:\n",
    "            js = r.json()\n",
    "        except Exception:\n",
    "            js = {\"text\": r.text}\n",
    "        js[\"_status\"] = r.status_code\n",
    "        js[\"_url\"] = url\n",
    "        return js\n",
    "    except Exception as e:\n",
    "        return {\"ok\": False, \"error\": str(e), \"_url\": url, \"_status\": 599}\n",
    "def _log_admin_event(kind: str, payload: Dict[str, Any]) -> None:\n",
    "    \"\"\"\n",
    "    mirror of the earlier audit but flat text so Rust/k8s logs can tail it\n",
    "    \"\"\"\n",
    "    rec = {\n",
    "        \"ts\": int(time.time()),\n",
    "        \"kind\": kind,\n",
    "        \"payload\": payload,\n",
    "    }\n",
    "    with ADMIN_LOG_PATH.open(\"a\", encoding=\"utf-8\") as f:\n",
    "        f.write(json.dumps(rec) + \"\\n\")\n",
    "    print(f\" admin-op: {kind} {payload}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 689,
   "id": "01085870-487e-4d01-adb1-cb4cb9049927",
   "metadata": {},
   "outputs": [],
   "source": [
    "def enqueue_pending_op(op: Dict[str, Any]) -> None:\n",
    "    \"\"\"\n",
    "    Store an intent to change gateway. sidecar/cron can replay.\n",
    "    \"\"\"\n",
    "    op = dict(op)\n",
    "    op.setdefault(\"ts\", int(time.time()))\n",
    "    with PENDING_OPS_PATH.open(\"a\", encoding=\"utf-8\") as f:\n",
    "        f.write(json.dumps(op) + \"\\n\")\n",
    "    print(f\" queued pending op  {PENDING_OPS_PATH}\")\n",
    "def iter_pending_ops(limit: int = 200) -> List[Dict[str, Any]]:\n",
    "    if not PENDING_OPS_PATH.exists():\n",
    "        return []\n",
    "    out: List[Dict[str, Any]] = []\n",
    "    with PENDING_OPS_PATH.open(\"r\", encoding=\"utf-8\") as f:\n",
    "        for i, line in enumerate(f):\n",
    "            if not line.strip():\n",
    "                continue\n",
    "            try:\n",
    "                js = json.loads(line)\n",
    "                out.append(js)\n",
    "            except Exception:\n",
    "                pass\n",
    "            if i + 1 >= limit:\n",
    "                break\n",
    "    return out\n",
    "def rewrite_pending_ops(ops: List[Dict[str, Any]]) -> None:\n",
    "    with PENDING_OPS_PATH.open(\"w\", encoding=\"utf-8\") as f:\n",
    "        for op in ops:\n",
    "            f.write(json.dumps(op) + \"\\n\")\n",
    "def safe_admin_post(path: str, body: Dict[str, Any], *, retries: int = 2, backoff_s: float = 0.4) -> Dict[str, Any]:\n",
    "    url = f\"{GATEWAY_BASE}{path}\"\n",
    "    last_err: Optional[Dict[str, Any]] = None\n",
    "    for attempt in range(retries + 1):\n",
    "        res = _http_req(\"POST\", url, json_body=body)\n",
    "        code = res.get(\"_status\", 599)\n",
    "        if 200 <= code < 300:\n",
    "            _log_admin_event(\"admin.post.ok\", {\"path\": path, \"body\": body, \"status\": code})\n",
    "            return res\n",
    "        last_err = res\n",
    "        print(f\" admin POST failed (try {attempt+1}/{retries+1})  {code}\")\n",
    "        time.sleep(backoff_s * (attempt + 1))\n",
    "    enqueue_pending_op({\"method\": \"POST\", \"path\": path, \"body\": body})\n",
    "    _log_admin_event(\"admin.post.enqueued\", {\"path\": path, \"body\": body, \"error\": last_err})\n",
    "    return last_err or {\"ok\": False, \"_status\": 599, \"error\": \"admin post failed + enqueued\"}\n",
    "def replay_pending_ops(max_ops: int = 100) -> Dict[str, Any]:\n",
    "    \"\"\"\n",
    "    Try to push previously failed ops.\n",
    "    \"\"\"\n",
    "    ops = iter_pending_ops(limit=max_ops)\n",
    "    if not ops:\n",
    "        print(\" no pending ops\")\n",
    "        return {\"replayed\": 0, \"left\": 0}\n",
    "    kept: List[Dict[str, Any]] = []\n",
    "    done = 0\n",
    "    for op in ops:\n",
    "        method = op.get(\"method\", \"POST\")\n",
    "        path = op.get(\"path\", \"\")\n",
    "        body = op.get(\"body\") or {}\n",
    "        if not path:\n",
    "            continue\n",
    "        res = _http_req(method, f\"{GATEWAY_BASE}{path}\", json_body=body)\n",
    "        code = res.get(\"_status\", 599)\n",
    "        if 200 <= code < 300:\n",
    "            done += 1\n",
    "            _log_admin_event(\"admin.replay.ok\", {\"path\": path, \"body\": body})\n",
    "        else:\n",
    "            kept.append(op)\n",
    "            _log_admin_event(\"admin.replay.fail\", {\"path\": path, \"body\": body, \"status\": code})\n",
    "    rewrite_pending_ops(kept)\n",
    "    print(f\" replayed={done}, still_pending={len(kept)}\")\n",
    "    return {\"replayed\": done, \"left\": len(kept)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 690,
   "id": "1ab709d3-ceae-44d0-8395-1deb31bf5c34",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_control_plane() -> Dict[str, Any]:\n",
    "    if not CONTROL_PATH.exists():\n",
    "        return {}\n",
    "    try:\n",
    "        return json.loads(CONTROL_PATH.read_text(encoding=\"utf-8\"))\n",
    "    except Exception as e:\n",
    "        print(f\" cannot read control-plane: {e}\")\n",
    "        return {}\n",
    "def version_gate(control: Optional[Dict[str, Any]] = None, *, allow_if_missing: bool = True) -> bool:\n",
    "    \"\"\"\n",
    "    returns True if we are allowed to apply drift/remediation.\n",
    "    \"\"\"\n",
    "    if control is None:\n",
    "        control = load_control_plane()\n",
    "    sch = control.get(\"schema\") or (control.get(\"_meta\") or {}).get(\"schema_version\")\n",
    "    if not sch:\n",
    "        return allow_if_missing\n",
    "    if sch not in {CURRENT_SCHEMA}:\n",
    "        print(f\" version gate: control schema={sch} != notebook schema={CURRENT_SCHEMA}  block\")\n",
    "        return False\n",
    "    return True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 691,
   "id": "821e3255-f1b8-47cc-8405-e842afc4d175",
   "metadata": {},
   "outputs": [],
   "source": [
    "def rotate_control_plane(max_keep: int = 5) -> None:\n",
    "    \"\"\"\n",
    "    keep last N snapshots in /tmp/art/control-history\n",
    "    \"\"\"\n",
    "    if not CONTROL_PATH.exists():\n",
    "        return\n",
    "    ts = int(time.time())\n",
    "    tgt = CONTROL_HISTORY_DIR / f\"control-{ts}.json\"\n",
    "    tgt.write_text(CONTROL_PATH.read_text(encoding=\"utf-8\"), encoding=\"utf-8\")\n",
    "    all_hist = sorted(CONTROL_HISTORY_DIR.glob(\"control-*.json\"), key=lambda p: p.stat().st_mtime, reverse=True)\n",
    "    for extra in all_hist[max_keep:]:\n",
    "        try:\n",
    "            extra.unlink()\n",
    "        except Exception:\n",
    "            pass\n",
    "    print(f\" control-plane rotated  {tgt}\")\n",
    "def list_control_history() -> List[Path]:\n",
    "    return sorted(CONTROL_HISTORY_DIR.glob(\"control-*.json\"), key=lambda p: p.stat().st_mtime, reverse=True)\n",
    "def rollback_control_plane(idx: int = 1) -> Optional[Path]:\n",
    "    \"\"\"\n",
    "    idx=0  latest (basically current), idx=1  previous, ...\n",
    "    \"\"\"\n",
    "    hist = list_control_history()\n",
    "    if not hist:\n",
    "        print(\" no control history to rollback to\")\n",
    "        return None\n",
    "    if idx >= len(hist):\n",
    "        print(f\" idx {idx} out of range, have {len(hist)}\")\n",
    "        return None\n",
    "    src = hist[idx]\n",
    "    CONTROL_PATH.write_text(src.read_text(encoding=\"utf-8\"), encoding=\"utf-8\")\n",
    "    print(f\" rolled back control-plane  {src}\")\n",
    "    return src\n",
    "def push_current_control_plane() -> None:\n",
    "    \"\"\"\n",
    "    push current control-plane to gateway (protected by version gate + safe admin)\n",
    "    \"\"\"\n",
    "    ctrl = load_control_plane()\n",
    "    if not ctrl:\n",
    "        print(\" no control-plane to push\")\n",
    "        return\n",
    "    if not version_gate(ctrl):\n",
    "        print(\" version gate failed  not pushing\")\n",
    "        return\n",
    "    chaos = ctrl.get(\"chaos\") or {}\n",
    "    rl = ctrl.get(\"rl_weights\") or {}\n",
    "    safe_admin_post(\"/admin/chaos/set\", chaos)\n",
    "    safe_admin_post(\"/rl/update\", {\"weights\": rl})\n",
    "    print(\" control-plane pushed (safe-admin)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 692,
   "id": "88346edf-15fa-4ff9-ab57-e7bfb7e2f94f",
   "metadata": {},
   "outputs": [],
   "source": [
    "K8S_SIDECAR_DEPLOY = ART_DIR / \"k8s-gateway-sidecar.yaml\"\n",
    "def write_k8s_sidecar_deploy(\n",
    "    name: str = \"rag-gateway-sidecar\",\n",
    "    namespace: str = \"default\",\n",
    "    image: str = \"python:3.11-slim\",\n",
    "    interval_s: int = 5,\n",
    ") -> Path:\n",
    "    \"\"\"\n",
    "    deployment that runs the sidecar poller (the one we wrote earlier)\n",
    "    and mounts /tmp/art from a PVC so notebook + sidecar share state.\n",
    "    \"\"\"\n",
    "    content = f\"\"\"apiVersion: apps/v1\n",
    "kind: Deployment\n",
    "metadata:\n",
    "  name: {name}\n",
    "  namespace: {namespace}\n",
    "spec:\n",
    "  replicas: 1\n",
    "  selector:\n",
    "    matchLabels:\n",
    "      app: {name}\n",
    "  template:\n",
    "    metadata:\n",
    "      labels:\n",
    "        app: {name}\n",
    "    spec:\n",
    "      containers:\n",
    "        - name: sidecar\n",
    "          image: {image}\n",
    "          env:\n",
    "            - name: ART_DIR\n",
    "              value: \"/tmp/art\"\n",
    "            - name: GATEWAY_BASE\n",
    "              value: \"{GATEWAY_BASE}\"\n",
    "            - name: GATEWAY_ADMIN_TOKEN\n",
    "              valueFrom:\n",
    "                secretKeyRef:\n",
    "                  name: rag-gateway-secrets\n",
    "                  key: GATEWAY_ADMIN_TOKEN\n",
    "            - name: GW_SIDECAR_INTERVAL\n",
    "              value: \"{interval_s}\"\n",
    "          volumeMounts:\n",
    "            - name: artdir\n",
    "              mountPath: /tmp/art\n",
    "          command: [\"python\", \"/tmp/art/gateway_sidecar_poller.py\"]\n",
    "      volumes:\n",
    "        - name: artdir\n",
    "          persistentVolumeClaim:\n",
    "            claimName: rag-gateway-art\n",
    "\"\"\"\n",
    "    K8S_SIDECAR_DEPLOY.write_text(content, encoding=\"utf-8\")\n",
    "    print(f\" k8s sidecar deploy  {K8S_SIDECAR_DEPLOY}\")\n",
    "    return K8S_SIDECAR_DEPLOY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 693,
   "id": "8022c334-a080-4f7b-b606-3d06cc7f7d03",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_janitor(\n",
    "    max_art_mb: int = 500,\n",
    "    keep_bundles: int = 3,\n",
    ") -> Dict[str, Any]:\n",
    "    \"\"\"\n",
    "    - warn if /tmp/art > max_art_mb\n",
    "    - keep only last N gateway_full_bundle.tar.gz\n",
    "    \"\"\"\n",
    "    total_bytes = 0\n",
    "    files: List[Path] = []\n",
    "    for p in ART_DIR.glob(\"**/*\"):\n",
    "        if p.is_file():\n",
    "            total_bytes += p.stat().st_size\n",
    "            files.append(p)\n",
    "    mb = total_bytes / (1024 * 1024)\n",
    "    warn = mb > max_art_mb\n",
    "    bundles = sorted(ART_DIR.glob(\"gateway_full_bundle*.tar.gz\"), key=lambda p: p.stat().st_mtime, reverse=True)\n",
    "    removed = []\n",
    "    for extra in bundles[keep_bundles:]:\n",
    "        try:\n",
    "            extra.unlink()\n",
    "            removed.append(str(extra))\n",
    "        except Exception:\n",
    "            pass\n",
    "    rep = {\n",
    "        \"ts\": int(time.time()),\n",
    "        \"total_mb\": round(mb, 2),\n",
    "        \"warn\": warn,\n",
    "        \"removed\": removed,\n",
    "    }\n",
    "    JANITOR_REPORT.write_text(json.dumps(rep, indent=2), encoding=\"utf-8\")\n",
    "    if warn:\n",
    "        print(f\" /tmp/art is fat: {mb:.1f} MB (> {max_art_mb} MB)\")\n",
    "    else:\n",
    "        print(f\" janitor: {mb:.1f} MB\")\n",
    "    if removed:\n",
    "        print(f\" removed old bundles: {removed}\")\n",
    "    return rep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 694,
   "id": "2c5c5b70-888f-48e1-bd79-cf358f1b5d2c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " k8s sidecar deploy  \\tmp\\art\\k8s-gateway-sidecar.yaml\n",
      " control-plane rotated  \\tmp\\art\\control-history\\control-1762114724.json\n",
      " no pending ops\n",
      " janitor: 0.3 MB\n",
      " safety check  \\tmp\\art\\gateway_safety_check.json\n",
      "\n",
      " continuation (safety + pending-ops + rollout + k8s-sidecar-deploy + janitor) ready.\n",
      "Try:\n",
      "  push_current_control_plane()\n",
      "  replay_pending_ops()\n",
      "  rollback_control_plane(1)\n",
      "  # kubectl apply -f \\tmp\\art\\k8s-gateway-sidecar.yaml\n"
     ]
    }
   ],
   "source": [
    "def selfcheck_safety() -> Dict[str, Any]:\n",
    "    if CONTROL_PATH.exists():\n",
    "        rotate_control_plane()\n",
    "    replay_res = replay_pending_ops()\n",
    "    jan = run_janitor()\n",
    "    out = {\n",
    "        \"replay\": replay_res,\n",
    "        \"janitor\": jan,\n",
    "    }\n",
    "    chk_path = ART_DIR / \"gateway_safety_check.json\"\n",
    "    chk_path.write_text(json.dumps(out, indent=2), encoding=\"utf-8\")\n",
    "    print(f\" safety check  {chk_path}\")\n",
    "    return out\n",
    "_ = write_k8s_sidecar_deploy()\n",
    "_ = selfcheck_safety()\n",
    "print(\"\\n continuation (safety + pending-ops + rollout + k8s-sidecar-deploy + janitor) ready.\")\n",
    "print(\"Try:\")\n",
    "print(\"  push_current_control_plane()\")\n",
    "print(\"  replay_pending_ops()\")\n",
    "print(\"  rollback_control_plane(1)\")\n",
    "print(f\"  # kubectl apply -f {K8S_SIDECAR_DEPLOY}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 695,
   "id": "49671cdb-5c20-430a-bfea-6e8db8a613ea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " continuation (e2e probes + disk-alerts + admin-cli + scenario-sim + export) using ART_DIR=\\tmp\\art\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "import time\n",
    "import random\n",
    "from pathlib import Path\n",
    "from typing import Any, Dict, List, Optional, Tuple, Callable\n",
    "ART_DIR = Path(os.getenv(\"ART_DIR\", \"/tmp/art\"))\n",
    "ART_DIR.mkdir(parents=True, exist_ok=True)\n",
    "print(f\" continuation (e2e probes + disk-alerts + admin-cli + scenario-sim + export) using ART_DIR={ART_DIR}\")\n",
    "GATEWAY_BASE = os.getenv(\"GATEWAY_BASE\", \"\").strip() or \"http://127.0.0.1:9910\"\n",
    "ADMIN_TOKEN = os.getenv(\"GATEWAY_ADMIN_TOKEN\", \"\").strip()\n",
    "ALERT_WEBHOOK = os.getenv(\"GATEWAY_ALERT_WEBHOOK\", \"\").strip()\n",
    "CURRENT_SCHEMA = \"1.1.0\"\n",
    "E2E_REPORT = ART_DIR / \"gateway_e2e_report.json\"\n",
    "DISK_ALERT = ART_DIR / \"gateway_disk_alert.json\"\n",
    "ADMIN_TAIL = ART_DIR / \"gateway_admin_ops.log\"                \n",
    "PENDING_OPS = ART_DIR / \"gateway_pending_ops.jsonl\"\n",
    "CONTROL_PATH = ART_DIR / \"gateway_control.json\"\n",
    "EXPORT_ENTRYPOINT = ART_DIR / \"gateway_entrypoint.py\"\n",
    "SCENARIOS_REPORT = ART_DIR / \"gateway_scenarios.json\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 696,
   "id": "d290e4d1-40ce-4961-b898-6a1ec7fa7a20",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _admin_headers(extra: Optional[Dict[str, str]] = None) -> Dict[str, str]:\n",
    "    h = {\"content-type\": \"application/json\"}\n",
    "    if ADMIN_TOKEN:\n",
    "        h[\"x-admin-token\"] = ADMIN_TOKEN\n",
    "    if extra:\n",
    "        h.update(extra)\n",
    "    return h\n",
    "def _http_req(method: str, url: str, *, json_body: Optional[Dict[str, Any]] = None, timeout: float = 2.5) -> Dict[str, Any]:\n",
    "    import requests\n",
    "    try:\n",
    "        if method.upper() == \"GET\":\n",
    "            r = requests.get(url, headers=_admin_headers(), timeout=timeout)\n",
    "        else:\n",
    "            r = requests.post(url, headers=_admin_headers(), json=json_body or {}, timeout=timeout)\n",
    "        try:\n",
    "            js = r.json()\n",
    "        except Exception:\n",
    "            js = {\"text\": r.text}\n",
    "        js[\"_status\"] = r.status_code\n",
    "        js[\"_url\"] = url\n",
    "        return js\n",
    "    except Exception as e:\n",
    "        return {\"ok\": False, \"error\": str(e), \"_url\": url, \"_status\": 599}\n",
    "def _send_alert(msg: str, *, level: str = \"warn\", extra: Optional[Dict[str, Any]] = None) -> None:\n",
    "    payload = {\n",
    "        \"text\": f\"[gateway/{level}] {msg}\",\n",
    "        \"level\": level,\n",
    "        \"extra\": extra or {},\n",
    "    }\n",
    "    if not ALERT_WEBHOOK:\n",
    "        print(f\" (dry-run): {json.dumps(payload)}\")\n",
    "        return\n",
    "    import requests\n",
    "    try:\n",
    "        r = requests.post(ALERT_WEBHOOK, json=payload, timeout=2.5)\n",
    "        print(f\" alert sent  {r.status_code}\")\n",
    "    except Exception as e:\n",
    "        print(f\" alert failed: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 697,
   "id": "cbabdcbb-7c10-4ecc-84e9-2051209407d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def e2e_probe(\n",
    "    *,\n",
    "    namespaces: Optional[List[str]] = None,\n",
    "    queries: Optional[List[str]] = None,\n",
    "    top_k: int = 2,\n",
    "    max_latency_ms: float = 1800.0,\n",
    ") -> Dict[str, Any]:\n",
    "    \"\"\"\n",
    "    Full black-box check:\n",
    "      - /up\n",
    "      - /metrics\n",
    "      - /admin/chaos/get (non-fatal)\n",
    "      - /answer across namespaces\n",
    "    \"\"\"\n",
    "    print(\"\\n=== e2e probe ===\")\n",
    "    if namespaces is None:\n",
    "        namespaces = [\"default\", \"tenant-acme\", \"tenant-beta\"]\n",
    "    if queries is None:\n",
    "        queries = [\n",
    "            \"smoke test\",\n",
    "            \"explain RAG fan-out pipeline\",\n",
    "        ]\n",
    "    up = _http_req(\"GET\", f\"{GATEWAY_BASE}/up\")\n",
    "    metrics = _http_req(\"GET\", f\"{GATEWAY_BASE}/metrics\")\n",
    "    chaos = _http_req(\"GET\", f\"{GATEWAY_BASE}/admin/chaos/get\")\n",
    "    results: List[Dict[str, Any]] = []\n",
    "    worst_ms = 0.0\n",
    "    for ns in namespaces:\n",
    "        for q in queries:\n",
    "            t0 = time.time()\n",
    "            ans = _http_req(\n",
    "                \"POST\",\n",
    "                f\"{GATEWAY_BASE}/answer\",\n",
    "                json_body={\"query\": q, \"top_k\": top_k},\n",
    "            )\n",
    "            dt_ms = (time.time() - t0) * 1000.0\n",
    "            worst_ms = max(worst_ms, dt_ms)\n",
    "            results.append(\n",
    "                {\n",
    "                    \"ns\": ns,\n",
    "                    \"query\": q,\n",
    "                    \"status\": ans.get(\"_status\"),\n",
    "                    \"latency_ms\": round(dt_ms, 1),\n",
    "                    \"ok\": 200 <= (ans.get(\"_status\") or 0) < 300,\n",
    "                }\n",
    "            )\n",
    "    report = {\n",
    "        \"ts\": int(time.time()),\n",
    "        \"base\": GATEWAY_BASE,\n",
    "        \"up\": up.get(\"_status\"),\n",
    "        \"metrics\": metrics.get(\"_status\"),\n",
    "        \"chaos\": chaos.get(\"_status\"),\n",
    "        \"probes\": results,\n",
    "        \"worst_latency_ms\": round(worst_ms, 1),\n",
    "        \"sla_ok\": (worst_ms <= max_latency_ms),\n",
    "    }\n",
    "    E2E_REPORT.write_text(json.dumps(report, indent=2), encoding=\"utf-8\")\n",
    "    print(f\" e2e report  {E2E_REPORT}\")\n",
    "    if not report[\"sla_ok\"]:\n",
    "        _send_alert(\n",
    "            f\"e2e probe latency exceeded: {worst_ms:.1f} ms > {max_latency_ms} ms\",\n",
    "            level=\"error\",\n",
    "            extra=report,\n",
    "        )\n",
    "    return report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 698,
   "id": "7653f2c8-47f9-4de7-a4b4-5462774308dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def disk_alert(\n",
    "    *,\n",
    "    max_mb: int = 500,\n",
    "    alert: bool = True,\n",
    ") -> Dict[str, Any]:\n",
    "    total_bytes = 0\n",
    "    for p in ART_DIR.glob(\"**/*\"):\n",
    "        if p.is_file():\n",
    "            total_bytes += p.stat().st_size\n",
    "    mb = total_bytes / (1024 * 1024)\n",
    "    rep = {\n",
    "        \"ts\": int(time.time()),\n",
    "        \"total_mb\": round(mb, 2),\n",
    "        \"threshold_mb\": max_mb,\n",
    "        \"warn\": mb > max_mb,\n",
    "    }\n",
    "    DISK_ALERT.write_text(json.dumps(rep, indent=2), encoding=\"utf-8\")\n",
    "    if rep[\"warn\"] and alert:\n",
    "        _send_alert(f\"/tmp/art disk={mb:.1f} MB > {max_mb} MB\", level=\"warn\", extra=rep)\n",
    "    else:\n",
    "        print(f\" disk ok: {mb:.1f} MB  {max_mb} MB\")\n",
    "    return rep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 699,
   "id": "67f31340-e990-4995-bc06-9dd20078bbfc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tail_admin_ops(n: int = 40) -> List[Dict[str, Any]]:\n",
    "    \"\"\"\n",
    "    Read the last N lines from gateway_admin_ops.log (if exists)\n",
    "    \"\"\"\n",
    "    if not ADMIN_TAIL.exists():\n",
    "        print(\" no admin ops log yet\")\n",
    "        return []\n",
    "    lines = ADMIN_TAIL.read_text(encoding=\"utf-8\").splitlines()\n",
    "    sliced = lines[-n:]\n",
    "    out: List[Dict[str, Any]] = []\n",
    "    for ln in sliced:\n",
    "        try:\n",
    "            out.append(json.loads(ln))\n",
    "        except Exception:\n",
    "            out.append({\"raw\": ln})\n",
    "    print(f\" last {len(out)} admin ops:\")\n",
    "    for r in out:\n",
    "        print(\" -\", r)\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 700,
   "id": "09982478-66a9-4ac4-a1ef-81022cfd743c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_scenarios() -> Dict[str, Any]:\n",
    "    \"\"\"\n",
    "    high-level simulation over your existing functions:\n",
    "      - dense broken  set force_bm25\n",
    "      - latency spike  enqueue pending ops\n",
    "      - tenant override  per-ns chaos\n",
    "    this is to prove that safety/pending-ops actually work.\n",
    "    \"\"\"\n",
    "    scenarios: List[Dict[str, Any]] = []\n",
    "    print(\"\\n scenario: dense broken\")\n",
    "    dense_payload = {\"force_bm25\": True}\n",
    "    res1 = _http_req(\"POST\", f\"{GATEWAY_BASE}/admin/chaos/set\", json_body=dense_payload)\n",
    "    if not (200 <= res1.get(\"_status\", 599) < 300):\n",
    "        from datetime import datetime\n",
    "        pending = {\n",
    "            \"method\": \"POST\",\n",
    "            \"path\": \"/admin/chaos/set\",\n",
    "            \"body\": dense_payload,\n",
    "            \"reason\": \"scenario-dense\",\n",
    "            \"ts\": int(time.time()),\n",
    "            \"at\": datetime.utcnow().isoformat() + \"Z\",\n",
    "        }\n",
    "        with PENDING_OPS.open(\"a\", encoding=\"utf-8\") as f:\n",
    "            f.write(json.dumps(pending) + \"\\n\")\n",
    "    scenarios.append({\"name\": \"dense-broken\", \"result\": res1})\n",
    "    print(\"\\n scenario: latency spike\")\n",
    "    fake_probe = {\n",
    "        \"ts\": int(time.time()),\n",
    "        \"base\": GATEWAY_BASE,\n",
    "        \"worst_latency_ms\": 2100.0,\n",
    "    }\n",
    "    _send_alert(\"synthetic latency spike > 2s\", level=\"error\", extra=fake_probe)\n",
    "    scenarios.append({\"name\": \"latency-spike\", \"result\": fake_probe})\n",
    "    print(\"\\n scenario: tenant override\")\n",
    "    tenant_payload = {\"disable_ce\": True}\n",
    "    res3 = _http_req(\"POST\", f\"{GATEWAY_BASE}/admin/chaos/set?ns=tenant-acme\", json_body=tenant_payload)\n",
    "    scenarios.append({\"name\": \"tenant-override\", \"result\": res3})\n",
    "    out = {\n",
    "        \"ts\": int(time.time()),\n",
    "        \"scenarios\": scenarios,\n",
    "    }\n",
    "    SCENARIOS_REPORT.write_text(json.dumps(out, indent=2), encoding=\"utf-8\")\n",
    "    print(f\" scenarios  {SCENARIOS_REPORT}\")\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 701,
   "id": "9d725cfd-12b7-47f7-a651-c165118f3cbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def gw_admin_cli(cmd: str, **kwargs) -> None:\n",
    "    cmd = cmd.strip().lower()\n",
    "    if cmd == \"e2e\":\n",
    "        e2e_probe()\n",
    "    elif cmd == \"disk\":\n",
    "        disk_alert()\n",
    "    elif cmd == \"tail\":\n",
    "        tail_admin_ops(kwargs.get(\"n\", 40))\n",
    "    elif cmd == \"scenarios\":\n",
    "        run_scenarios()\n",
    "    elif cmd == \"push\":\n",
    "        if CONTROL_PATH.exists():\n",
    "            from datetime import datetime\n",
    "            ctrl = json.loads(CONTROL_PATH.read_text(encoding=\"utf-8\"))\n",
    "            sch = ctrl.get(\"schema\") or (ctrl.get(\"_meta\") or {}).get(\"schema_version\")\n",
    "            if sch != CURRENT_SCHEMA:\n",
    "                print(f\" control-plane schema {sch} != {CURRENT_SCHEMA}, NOT pushing\")\n",
    "                return\n",
    "            res1 = _http_req(\"POST\", f\"{GATEWAY_BASE}/admin/chaos/set\", json_body=ctrl.get(\"chaos\") or {})\n",
    "            res2 = _http_req(\"POST\", f\"{GATEWAY_BASE}/rl/update\", json_body={\"weights\": ctrl.get(\"rl_weights\") or {}})\n",
    "            print(\"push-res chaos:\", res1.get(\"_status\"), \"rl:\", res2.get(\"_status\"))\n",
    "            log = {\n",
    "                \"ts\": int(time.time()),\n",
    "                \"kind\": \"admin.cli.push\",\n",
    "                \"ctrl_ts\": ctrl.get(\"ts\"),\n",
    "                \"at\": datetime.utcnow().isoformat() + \"Z\",\n",
    "            }\n",
    "            with ADMIN_TAIL.open(\"a\", encoding=\"utf-8\") as f:\n",
    "                f.write(json.dumps(log) + \"\\n\")\n",
    "        else:\n",
    "            print(\" no control-plane to push\")\n",
    "    else:\n",
    "        print(\" unknown cmd. try:\")\n",
    "        print(\"  gw_admin_cli('e2e')\")\n",
    "        print(\"  gw_admin_cli('disk')\")\n",
    "        print(\"  gw_admin_cli('tail', n=50)\")\n",
    "        print(\"  gw_admin_cli('scenarios')\")\n",
    "        print(\"  gw_admin_cli('push')\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 702,
   "id": "02acbd11-d13f-4c25-b7d6-fa346e4cdacc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " entrypoint  \\tmp\\art\\gateway_entrypoint.py\n"
     ]
    }
   ],
   "source": [
    "def write_entrypoint() -> Path:\n",
    "    content = f\"\"\"#!/usr/bin/env python3\n",
    "# auto-generated: notebook entrypoint\n",
    "import json, os, sys, time\n",
    "from pathlib import Path\n",
    "ART_DIR = Path(\"{ART_DIR}\")\n",
    "E2E = ART_DIR / \"gateway_e2e_report.json\"\n",
    "DISK = ART_DIR / \"gateway_disk_alert.json\"\n",
    "SCEN = ART_DIR / \"gateway_scenarios.json\"\n",
    "def main():\n",
    "    cmd = sys.argv[1] if len(sys.argv) > 1 else \"e2e\"\n",
    "    if cmd == \"e2e\":\n",
    "        from __main__ import e2e_probe\n",
    "        e2e_probe()\n",
    "    elif cmd == \"disk\":\n",
    "        from __main__ import disk_alert\n",
    "        disk_alert()\n",
    "    elif cmd == \"scenarios\":\n",
    "        from __main__ import run_scenarios\n",
    "        run_scenarios()\n",
    "    else:\n",
    "        print(\"unknown cmd:\", cmd)\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n",
    "\"\"\"\n",
    "    EXPORT_ENTRYPOINT.write_text(content, encoding=\"utf-8\")\n",
    "    EXPORT_ENTRYPOINT.chmod(0o755)\n",
    "    print(f\" entrypoint  {EXPORT_ENTRYPOINT}\")\n",
    "    return EXPORT_ENTRYPOINT\n",
    "_ = write_entrypoint()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 703,
   "id": "d262cf13-341e-4a6e-9731-d44b5b3f8e7d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== e2e probe ===\n",
      " e2e report  \\tmp\\art\\gateway_e2e_report.json\n",
      " disk ok: 0.3 MB  500 MB\n",
      "\n",
      " continuation (e2e probes + disk-alerts + admin-cli + scenario-sim + export) ready.\n",
      "Try:\n",
      "  gw_admin_cli('e2e')\n",
      "  gw_admin_cli('disk')\n",
      "  gw_admin_cli('tail', n=30)\n",
      "  gw_admin_cli('scenarios')\n",
      "  python \\tmp\\art\\gateway_entrypoint.py e2e  # SRE-friendly\n"
     ]
    }
   ],
   "source": [
    "_ = e2e_probe()\n",
    "_ = disk_alert()\n",
    "print(\"\\n continuation (e2e probes + disk-alerts + admin-cli + scenario-sim + export) ready.\")\n",
    "print(\"Try:\")\n",
    "print(\"  gw_admin_cli('e2e')\")\n",
    "print(\"  gw_admin_cli('disk')\")\n",
    "print(\"  gw_admin_cli('tail', n=30)\")\n",
    "print(\"  gw_admin_cli('scenarios')\")\n",
    "print(f\"  python {EXPORT_ENTRYPOINT} e2e  # SRE-friendly\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 704,
   "id": "3f6d019d-391e-4fa2-abb5-8617dbc066fa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " continuation (live-diff + safemode + sre-bundle + docker-export) using ART_DIR=\\tmp\\art\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "import time\n",
    "import tarfile\n",
    "from pathlib import Path\n",
    "from typing import Any, Dict, List, Optional, Tuple\n",
    "ART_DIR = Path(os.getenv(\"ART_DIR\", \"/tmp/art\"))\n",
    "ART_DIR.mkdir(parents=True, exist_ok=True)\n",
    "print(f\" continuation (live-diff + safemode + sre-bundle + docker-export) using ART_DIR={ART_DIR}\")\n",
    "GATEWAY_BASE = os.getenv(\"GATEWAY_BASE\", \"\").strip() or \"http://127.0.0.1:9910\"\n",
    "ADMIN_TOKEN = os.getenv(\"GATEWAY_ADMIN_TOKEN\", \"\").strip()\n",
    "CURRENT_SCHEMA = \"1.1.0\"\n",
    "CONTROL_PATH = ART_DIR / \"gateway_control.json\"\n",
    "LIVE_SNAPSHOT_PATH = ART_DIR / \"gateway_live_snapshot.json\"\n",
    "DIFF_REPORT_PATH = ART_DIR / \"gateway_control_vs_live.json\"\n",
    "SRE_BUNDLE_PATH = ART_DIR / \"gateway_sre_bundle.tar.gz\"\n",
    "DOCKERFILE_PATH = ART_DIR / \"Dockerfile.gateway-notebook\"\n",
    "COMPOSE_PATH = ART_DIR / \"docker-compose.gateway-notebook.yml\"\n",
    "RUNBOOK_PATH = ART_DIR / \"gateway_runbook.md\"\n",
    "INCIDENT_LOG_PATH = ART_DIR / \"gateway_incidents.jsonl\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 705,
   "id": "144105e7-5d2b-443d-9e5b-b8fac8ab8f32",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _admin_headers(extra: Optional[Dict[str, str]] = None) -> Dict[str, str]:\n",
    "    h = {\"content-type\": \"application/json\"}\n",
    "    if ADMIN_TOKEN:\n",
    "        h[\"x-admin-token\"] = ADMIN_TOKEN\n",
    "    if extra:\n",
    "        h.update(extra)\n",
    "    return h\n",
    "def _http_req(method: str, url: str, *, json_body: Optional[Dict[str, Any]] = None, timeout: float = 2.5) -> Dict[str, Any]:\n",
    "    import requests\n",
    "    try:\n",
    "        if method.upper() == \"GET\":\n",
    "            r = requests.get(url, headers=_admin_headers(), timeout=timeout)\n",
    "        else:\n",
    "            r = requests.post(url, headers=_admin_headers(), json=json_body or {}, timeout=timeout)\n",
    "        try:\n",
    "            js = r.json()\n",
    "        except Exception:\n",
    "            js = {\"text\": r.text}\n",
    "        js[\"_status\"] = r.status_code\n",
    "        js[\"_url\"] = url\n",
    "        return js\n",
    "    except Exception as e:\n",
    "        return {\"ok\": False, \"error\": str(e), \"_url\": url, \"_status\": 599}\n",
    "def load_control() -> Dict[str, Any]:\n",
    "    if not CONTROL_PATH.exists():\n",
    "        return {}\n",
    "    try:\n",
    "        return json.loads(CONTROL_PATH.read_text(encoding=\"utf-8\"))\n",
    "    except Exception as e:\n",
    "        print(f\" cannot read control: {e}\")\n",
    "        return {}\n",
    "def fetch_live_snapshot() -> Dict[str, Any]:\n",
    "    \"\"\"\n",
    "    Try to read current chaos/RL from live gateway.\n",
    "    If gateway is down  still write a snapshot with the error.\n",
    "    \"\"\"\n",
    "    chaos = _http_req(\"GET\", f\"{GATEWAY_BASE}/admin/chaos/get\")\n",
    "    rl = _http_req(\"POST\", f\"{GATEWAY_BASE}/rl/update\", json_body={\"weights\": {}})\n",
    "    snap = {\n",
    "        \"ts\": int(time.time()),\n",
    "        \"base\": GATEWAY_BASE,\n",
    "        \"chaos\": (chaos.get(\"chaos\") if \"chaos\" in chaos else chaos),\n",
    "        \"rl\": (rl.get(\"weights\") or rl.get(\"current\") or rl),\n",
    "    }\n",
    "    LIVE_SNAPSHOT_PATH.write_text(json.dumps(snap, indent=2), encoding=\"utf-8\")\n",
    "    print(f\" live snapshot  {LIVE_SNAPSHOT_PATH}\")\n",
    "    return snap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 706,
   "id": "88d94a10-9272-474f-8437-8990e92c23f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def diff_control_vs_live(\n",
    "    control: Optional[Dict[str, Any]] = None,\n",
    "    live: Optional[Dict[str, Any]] = None,\n",
    ") -> Dict[str, Any]:\n",
    "    \"\"\"\n",
    "    tight diff: chaos and rl_weights\n",
    "    \"\"\"\n",
    "    if control is None:\n",
    "        control = load_control()\n",
    "    if live is None:\n",
    "        live = fetch_live_snapshot()\n",
    "    want_chaos = control.get(\"chaos\") or {}\n",
    "    want_rl = control.get(\"rl_weights\") or {}\n",
    "    have_chaos = live.get(\"chaos\") or {}\n",
    "    have_rl = live.get(\"rl\") or {}\n",
    "    chaos_drift: Dict[str, Any] = {}\n",
    "    rl_drift: Dict[str, Any] = {}\n",
    "    for k, v in want_chaos.items():\n",
    "        if have_chaos.get(k) != v:\n",
    "            chaos_drift[k] = {\"want\": v, \"have\": have_chaos.get(k)}\n",
    "    for k, v in have_chaos.items():\n",
    "        if k not in want_chaos:\n",
    "            chaos_drift[k] = {\"want\": None, \"have\": v}\n",
    "    for k, v in want_rl.items():\n",
    "        if have_rl.get(k) != v:\n",
    "            rl_drift[k] = {\"want\": v, \"have\": have_rl.get(k)}\n",
    "    for k, v in have_rl.items():\n",
    "        if k not in want_rl:\n",
    "            rl_drift[k] = {\"want\": None, \"have\": v}\n",
    "    out = {\n",
    "        \"ts\": int(time.time()),\n",
    "        \"schema\": CURRENT_SCHEMA,\n",
    "        \"base\": GATEWAY_BASE,\n",
    "        \"chaos_drift\": chaos_drift,\n",
    "        \"rl_drift\": rl_drift,\n",
    "        \"control_ts\": control.get(\"ts\"),\n",
    "    }\n",
    "    DIFF_REPORT_PATH.write_text(json.dumps(out, indent=2), encoding=\"utf-8\")\n",
    "    print(f\" control vs live diff  {DIFF_REPORT_PATH}\")\n",
    "    return out\n",
    "def write_incident(\n",
    "    *,\n",
    "    title: str,\n",
    "    severity: str = \"warn\",\n",
    "    details: Optional[Dict[str, Any]] = None,\n",
    ") -> None:\n",
    "    rec = {\n",
    "        \"ts\": int(time.time()),\n",
    "        \"title\": title,\n",
    "        \"severity\": severity,\n",
    "        \"details\": details or {},\n",
    "    }\n",
    "    with INCIDENT_LOG_PATH.open(\"a\", encoding=\"utf-8\") as f:\n",
    "        f.write(json.dumps(rec) + \"\\n\")\n",
    "    print(f\" incident logged: {title} ({severity})\")\n",
    "def apply_safemode(reason: str = \"manual\", ns: Optional[str] = None) -> Dict[str, Any]:\n",
    "    \"\"\"\n",
    "    Safemode = make gateway fall back to cheap/simple search.\n",
    "    We do it WITHOUT losing the desired state: we just POST a chaos override.\n",
    "    If gateway is down, we append to pending ops (previous cell handles it).\n",
    "    \"\"\"\n",
    "    body = {\n",
    "        \"force_bm25\": True,\n",
    "        \"disable_ce\": True,\n",
    "        \"verifier_v2\": False,\n",
    "    }\n",
    "    if ns:\n",
    "        res = _http_req(\"POST\", f\"{GATEWAY_BASE}/admin/chaos/set?ns={ns}\", json_body=body)\n",
    "    else:\n",
    "        res = _http_req(\"POST\", f\"{GATEWAY_BASE}/admin/chaos/set\", json_body=body)\n",
    "    if not (200 <= res.get(\"_status\", 599) < 300):\n",
    "        pend = {\n",
    "            \"method\": \"POST\",\n",
    "            \"path\": \"/admin/chaos/set\" + (f\"?ns={ns}\" if ns else \"\"),\n",
    "            \"body\": body,\n",
    "            \"reason\": f\"safemode-{reason}\",\n",
    "            \"ts\": int(time.time()),\n",
    "        }\n",
    "        (ART_DIR / \"gateway_pending_ops.jsonl\").open(\"a\", encoding=\"utf-8\").write(json.dumps(pend) + \"\\n\")\n",
    "        write_incident(title=\"safemode-enqueued\", severity=\"error\", details=pend)\n",
    "        return {\"ok\": False, \"enqueued\": True, \"res\": res}\n",
    "    write_incident(title=\"safemode-applied\", severity=\"warn\", details={\"ns\": ns, \"reason\": reason})\n",
    "    print(\" safemode applied\")\n",
    "    return {\"ok\": True, \"res\": res}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 707,
   "id": "f8c026ca-abf3-45a2-82dd-d5ea9b1bf643",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clear_safemode() -> Dict[str, Any]:\n",
    "    \"\"\"\n",
    "    best-effort: reload control  push its chaos\n",
    "    \"\"\"\n",
    "    ctrl = load_control()\n",
    "    chaos = ctrl.get(\"chaos\") or {}\n",
    "    res = _http_req(\"POST\", f\"{GATEWAY_BASE}/admin/chaos/set\", json_body=chaos)\n",
    "    if 200 <= res.get(\"_status\", 599) < 300:\n",
    "        write_incident(title=\"safemode-cleared\", severity=\"info\", details={\"chaos\": chaos})\n",
    "        print(\" safemode cleared  control chaos restored\")\n",
    "    else:\n",
    "        write_incident(title=\"safemode-clear-failed\", severity=\"error\", details={\"res\": res})\n",
    "    return res\n",
    "def write_runbook() -> Path:\n",
    "    lines: List[str] = []\n",
    "    lines.append(\"# RAG Gateway Runbook (auto-generated)\")\n",
    "    lines.append(\"\")\n",
    "    lines.append(\"## 1. Check live vs control\")\n",
    "    lines.append(\"```python\")\n",
    "    lines.append(\"diff_control_vs_live()\")\n",
    "    lines.append(\"```\")\n",
    "    lines.append(\"\")\n",
    "    lines.append(\"## 2. Force safemode\")\n",
    "    lines.append(\"```python\")\n",
    "    lines.append(\"apply_safemode(reason='latency-spike')\")\n",
    "    lines.append(\"```\")\n",
    "    lines.append(\"\")\n",
    "    lines.append(\"## 3. Clear safemode (back to desired)\")\n",
    "    lines.append(\"```python\")\n",
    "    lines.append(\"clear_safemode()\")\n",
    "    lines.append(\"```\")\n",
    "    lines.append(\"\")\n",
    "    lines.append(\"## 4. Replay failed ops\")\n",
    "    lines.append(\"```python\")\n",
    "    lines.append(\"from pathlib import Path\")\n",
    "    lines.append(\"# previous cell defined replay_pending_ops()\")\n",
    "    lines.append(\"replay_pending_ops()  # best-effort\")\n",
    "    lines.append(\"```\")\n",
    "    lines.append(\"\")\n",
    "    lines.append(\"## 5. Rollback control-plane\")\n",
    "    lines.append(\"```python\")\n",
    "    lines.append(\"rollback_control_plane(1)\")\n",
    "    lines.append(\"push_current_control_plane()\")\n",
    "    lines.append(\"```\")\n",
    "    RUNBOOK_PATH.write_text(\"\\n\".join(lines), encoding=\"utf-8\")\n",
    "    print(f\" runbook  {RUNBOOK_PATH}\")\n",
    "    return RUNBOOK_PATH\n",
    "def write_dockerfile() -> Path:\n",
    "    content = f\"\"\"# auto-generated gateway notebook Dockerfile\n",
    "FROM python:3.11-slim\n",
    "WORKDIR /app\n",
    "ENV ART_DIR=/tmp/art \\\\\n",
    "    GATEWAY_BASE={GATEWAY_BASE}\n",
    "RUN mkdir -p /tmp/art\n",
    "COPY . /app\n",
    "# you can add: pip install requests jsonschema ...\n",
    "CMD [\"python\", \"-c\", \"print('run the generated entrypoint from /tmp/art if mounted')\"]\n",
    "\"\"\"\n",
    "    DOCKERFILE_PATH.write_text(content, encoding=\"utf-8\")\n",
    "    print(f\" dockerfile  {DOCKERFILE_PATH}\")\n",
    "    return DOCKERFILE_PATH\n",
    "def write_docker_compose() -> Path:\n",
    "    content = f\"\"\"version: \"3.9\"\n",
    "services:\n",
    "  gateway-sidecar:\n",
    "    image: python:3.11-slim\n",
    "    environment:\n",
    "      - ART_DIR=/tmp/art\n",
    "      - GATEWAY_BASE={GATEWAY_BASE}\n",
    "      - GATEWAY_ADMIN_TOKEN={ADMIN_TOKEN}\n",
    "    volumes:\n",
    "      - ./art:/tmp/art\n",
    "    command: [\"python\", \"/tmp/art/gateway_sidecar_poller.py\"]\n",
    "  gateway-admin:\n",
    "    image: python:3.11-slim\n",
    "    environment:\n",
    "      - ART_DIR=/tmp/art\n",
    "      - GATEWAY_BASE={GATEWAY_BASE}\n",
    "      - GATEWAY_ADMIN_TOKEN={ADMIN_TOKEN}\n",
    "    volumes:\n",
    "      - ./art:/tmp/art\n",
    "    command: [\"python\", \"/tmp/art/gateway_entrypoint.py\", \"e2e\"]\n",
    "\"\"\"\n",
    "    COMPOSE_PATH.write_text(content, encoding=\"utf-8\")\n",
    "    print(f\" docker-compose  {COMPOSE_PATH}\")\n",
    "    return COMPOSE_PATH\n",
    "def build_sre_bundle() -> Path:\n",
    "    \"\"\"\n",
    "    Throw all high-signal files into 1 tar.gz for oncall.\n",
    "    \"\"\"\n",
    "    wanted = [\n",
    "        \"gateway_control.json\",\n",
    "        \"gateway_live_snapshot.json\",\n",
    "        \"gateway_control_vs_live.json\",\n",
    "        \"gateway_e2e_report.json\",\n",
    "        \"gateway_disk_alert.json\",\n",
    "        \"gateway_janitor.json\",\n",
    "        \"gateway_admin_ops.log\",\n",
    "        \"gateway_scenarios.json\",\n",
    "        \"gateway_runbook.md\",\n",
    "        \"k8s-gateway-sidecar.yaml\",\n",
    "        \"k8s-gateway-agent-cronjob.yaml\",\n",
    "        \"k8s-gateway-configmap.yaml\",\n",
    "        \"k8s-gateway-secret.yaml\",\n",
    "        \"prometheus-gateway-rules.yml\",\n",
    "    ]\n",
    "    with tarfile.open(SRE_BUNDLE_PATH, \"w:gz\") as tar:\n",
    "        for name in wanted:\n",
    "            p = ART_DIR / name\n",
    "            if p.exists():\n",
    "                tar.add(p, arcname=name)\n",
    "    print(f\" SRE bundle  {SRE_BUNDLE_PATH}\")\n",
    "    return SRE_BUNDLE_PATH\n",
    "def gw_cli3(cmd: str, **kwargs) -> None:\n",
    "    cmd = cmd.strip().lower()\n",
    "    if cmd == \"live\":\n",
    "        fetch_live_snapshot()\n",
    "    elif cmd == \"diff\":\n",
    "        diff_control_vs_live()\n",
    "    elif cmd == \"safemode\":\n",
    "        apply_safemode(reason=kwargs.get(\"reason\", \"manual\"), ns=kwargs.get(\"ns\"))\n",
    "    elif cmd == \"clear\":\n",
    "        clear_safemode()\n",
    "    elif cmd == \"runbook\":\n",
    "        write_runbook()\n",
    "    elif cmd == \"bundle\":\n",
    "        build_sre_bundle()\n",
    "    elif cmd == \"docker\":\n",
    "        write_dockerfile()\n",
    "        write_docker_compose()\n",
    "    else:\n",
    "        print(\" unknown cmd. try:\")\n",
    "        print(\"  gw_cli3('live')\")\n",
    "        print(\"  gw_cli3('diff')\")\n",
    "        print(\"  gw_cli3('safemode')\")\n",
    "        print(\"  gw_cli3('clear')\")\n",
    "        print(\"  gw_cli3('runbook')\")\n",
    "        print(\"  gw_cli3('bundle')\")\n",
    "        print(\"  gw_cli3('docker')\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 708,
   "id": "836497d7-6239-4221-b14c-972a23ff0d89",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " runbook  \\tmp\\art\\gateway_runbook.md\n",
      " live snapshot  \\tmp\\art\\gateway_live_snapshot.json\n",
      " control vs live diff  \\tmp\\art\\gateway_control_vs_live.json\n",
      " SRE bundle  \\tmp\\art\\gateway_sre_bundle.tar.gz\n",
      " dockerfile  \\tmp\\art\\Dockerfile.gateway-notebook\n",
      " docker-compose  \\tmp\\art\\docker-compose.gateway-notebook.yml\n",
      "\n",
      " continuation (live-diff + safemode + sre-bundle + docker-export) loaded.\n",
      "Try:\n",
      "  gw_cli3('diff')\n",
      "  gw_cli3('safemode', reason='latency-spike')\n",
      "  gw_cli3('clear')\n",
      "  gw_cli3('bundle')\n",
      "  gw_cli3('docker')\n"
     ]
    }
   ],
   "source": [
    "_ = write_runbook()\n",
    "_ = diff_control_vs_live()\n",
    "_ = build_sre_bundle()\n",
    "_ = write_dockerfile()\n",
    "_ = write_docker_compose()\n",
    "print(\"\\n continuation (live-diff + safemode + sre-bundle + docker-export) loaded.\")\n",
    "print(\"Try:\")\n",
    "print(\"  gw_cli3('diff')\")\n",
    "print(\"  gw_cli3('safemode', reason='latency-spike')\")\n",
    "print(\"  gw_cli3('clear')\")\n",
    "print(\"  gw_cli3('bundle')\")\n",
    "print(\"  gw_cli3('docker')\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 709,
   "id": "bc1d6159-a866-4fb9-b2a8-159e3b095f68",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " continuation (idempotent + dedupe + blue/green + rbac-tenant + rotate + offline) using ART_DIR=\\tmp\\art\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "import time\n",
    "import socket\n",
    "from pathlib import Path\n",
    "from typing import Any, Dict, List, Optional, Tuple\n",
    "ART_DIR = Path(os.getenv(\"ART_DIR\", \"/tmp/art\"))\n",
    "ART_DIR.mkdir(parents=True, exist_ok=True)\n",
    "print(f\" continuation (idempotent + dedupe + blue/green + rbac-tenant + rotate + offline) using ART_DIR={ART_DIR}\")\n",
    "CONTROL_PATH = ART_DIR / \"gateway_control.json\"\n",
    "CONTROL_HISTORY_DIR = ART_DIR / \"control-history\"\n",
    "PENDING_OPS_PATH = ART_DIR / \"gateway_pending_ops.jsonl\"\n",
    "ADMIN_LOG_PATH = ART_DIR / \"gateway_admin_ops.log\"\n",
    "INCIDENT_LOG_PATH = ART_DIR / \"gateway_incidents.jsonl\"\n",
    "RBAC_PATH = ART_DIR / \"gateway_rbac.json\"\n",
    "TENANTS_PATH = ART_DIR / \"gateway_tenants.json\"\n",
    "CONTROL_BLUE = ART_DIR / \"gateway_control.blue.json\"\n",
    "CONTROL_GREEN = ART_DIR / \"gateway_control.green.json\"\n",
    "CONTROL_ACTIVE = ART_DIR / \"gateway_control.active\"   \n",
    "OFFLINE_CHANGES = ART_DIR / \"gateway_offline_changes.jsonl\"\n",
    "GATEWAY_BASE = os.getenv(\"GATEWAY_BASE\", \"\").strip() or \"http://127.0.0.1:9910\"\n",
    "ADMIN_TOKEN = os.getenv(\"GATEWAY_ADMIN_TOKEN\", \"\").strip()\n",
    "CURRENT_SCHEMA = \"1.1.0\"\n",
    "REDACT_KEYS = {\"token\", \"secret\", \"webhook\", \"x-admin-token\", \"authorization\"}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 710,
   "id": "bc860945-3d1d-4114-88ad-8a2b645c3e8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_if_changed(path: Path, data: Dict[str, Any], *, schema: Optional[str] = None) -> bool:\n",
    "    \"\"\"\n",
    "    Write JSON only if content changed (ignoring written_at).\n",
    "    Returns True if wrote.\n",
    "    \"\"\"\n",
    "    new_obj = dict(data)\n",
    "    meta = new_obj.get(\"_meta\") or {}\n",
    "    if schema:\n",
    "        meta[\"schema_version\"] = schema\n",
    "    meta[\"written_at\"] = int(time.time())\n",
    "    new_obj[\"_meta\"] = meta\n",
    "    new_txt = json.dumps(new_obj, indent=2, sort_keys=True)\n",
    "    if path.exists():\n",
    "        old_txt = path.read_text(encoding=\"utf-8\")\n",
    "        try:\n",
    "            old_js = json.loads(old_txt)\n",
    "            old_js.get(\"_meta\", {}).pop(\"written_at\", None)\n",
    "            tmp_new = json.loads(new_txt)\n",
    "            tmp_new.get(\"_meta\", {}).pop(\"written_at\", None)\n",
    "            if old_js == tmp_new:\n",
    "                print(f\" no change  {path.name} (idempotent)\")\n",
    "                return False\n",
    "        except Exception:\n",
    "            pass\n",
    "    path.write_text(new_txt, encoding=\"utf-8\")\n",
    "    print(f\" wrote  {path} (idempotent)\")\n",
    "    return True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 711,
   "id": "33c13707-fd2e-423a-ae76-35ae3c3ee959",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _iter_jsonl(path: Path, limit: int = 500) -> List[Dict[str, Any]]:\n",
    "    if not path.exists():\n",
    "        return []\n",
    "    out: List[Dict[str, Any]] = []\n",
    "    with path.open(\"r\", encoding=\"utf-8\") as f:\n",
    "        for i, line in enumerate(f):\n",
    "            if not line.strip():\n",
    "                continue\n",
    "            try:\n",
    "                out.append(json.loads(line))\n",
    "            except Exception:\n",
    "                pass\n",
    "            if i + 1 >= limit:\n",
    "                break\n",
    "    return out\n",
    "def _write_jsonl(path: Path, rows: List[Dict[str, Any]]) -> None:\n",
    "    with path.open(\"w\", encoding=\"utf-8\") as f:\n",
    "        for r in rows:\n",
    "            f.write(json.dumps(r) + \"\\n\")\n",
    "def dedupe_pending_ops(max_keep: int = 300) -> Dict[str, Any]:\n",
    "    \"\"\"\n",
    "    Remove exact-duplicate admin ops (same method+path+body).\n",
    "    Keeps newest.\n",
    "    \"\"\"\n",
    "    rows = _iter_jsonl(PENDING_OPS_PATH, limit=2000)\n",
    "    seen = set()\n",
    "    deduped: List[Dict[str, Any]] = []\n",
    "    for r in reversed(rows):  \n",
    "        sig = (r.get(\"method\", \"POST\"), r.get(\"path\", \"\"), json.dumps(r.get(\"body\") or {}, sort_keys=True))\n",
    "        if sig in seen:\n",
    "            continue\n",
    "        seen.add(sig)\n",
    "        deduped.append(r)\n",
    "    deduped = list(reversed(deduped))\n",
    "    if len(deduped) > max_keep:\n",
    "        deduped = deduped[-max_keep:]\n",
    "    _write_jsonl(PENDING_OPS_PATH, deduped)\n",
    "    print(f\" pending-ops deduped  {len(rows)}  {len(deduped)}\")\n",
    "    return {\"before\": len(rows), \"after\": len(deduped)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 712,
   "id": "33bc82ba-f3db-4479-be81-1367e7760da9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _control_exists() -> bool:\n",
    "    return CONTROL_PATH.exists() or CONTROL_BLUE.exists() or CONTROL_GREEN.exists()\n",
    "def write_control_variant(name: str, control: Dict[str, Any]) -> Path:\n",
    "    \"\"\"\n",
    "    name = 'blue' or 'green'\n",
    "    \"\"\"\n",
    "    target = CONTROL_BLUE if name == \"blue\" else CONTROL_GREEN\n",
    "    write_if_changed(target, control, schema=CURRENT_SCHEMA)\n",
    "    return target\n",
    "def set_active_control(name: str) -> None:\n",
    "    if name not in (\"blue\", \"green\"):\n",
    "        raise ValueError(\"control name must be 'blue' or 'green'\")\n",
    "    CONTROL_ACTIVE.write_text(name, encoding=\"utf-8\")\n",
    "    print(f\" active control  {name}\")\n",
    "def get_active_control_name() -> str:\n",
    "    if CONTROL_ACTIVE.exists():\n",
    "        return CONTROL_ACTIVE.read_text(encoding=\"utf-8\").strip() or \"blue\"\n",
    "    if CONTROL_BLUE.exists():\n",
    "        return \"blue\"\n",
    "    if CONTROL_GREEN.exists():\n",
    "        return \"green\"\n",
    "    return \"blue\"\n",
    "def load_active_control() -> Dict[str, Any]:\n",
    "    name = get_active_control_name()\n",
    "    path = CONTROL_BLUE if name == \"blue\" else CONTROL_GREEN\n",
    "    if path.exists():\n",
    "        try:\n",
    "            return json.loads(path.read_text(encoding=\"utf-8\"))\n",
    "        except Exception:\n",
    "            pass\n",
    "    if CONTROL_PATH.exists():\n",
    "        return json.loads(CONTROL_PATH.read_text(encoding=\"utf-8\"))\n",
    "    return {}\n",
    "def switch_control() -> str:\n",
    "    \"\"\"\n",
    "    swap blue <-> green, create empty if missing\n",
    "    \"\"\"\n",
    "    cur = get_active_control_name()\n",
    "    nxt = \"green\" if cur == \"blue\" else \"blue\"\n",
    "    if not (CONTROL_BLUE if nxt == \"blue\" else CONTROL_GREEN).exists():\n",
    "        data = load_active_control() or {\"ts\": int(time.time()), \"chaos\": {}, \"rl_weights\": {}}\n",
    "        write_control_variant(nxt, data)\n",
    "    set_active_control(nxt)\n",
    "    return nxt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 713,
   "id": "4b6c6fed-01df-422c-83a4-21cead171f2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_rbac() -> Dict[str, Any]:\n",
    "    if not RBAC_PATH.exists():\n",
    "        return {\"roles\": {}, \"tokens\": {}, \"actions\": {}}\n",
    "    try:\n",
    "        return json.loads(RBAC_PATH.read_text(encoding=\"utf-8\"))\n",
    "    except Exception:\n",
    "        return {\"roles\": {}, \"tokens\": {}, \"actions\": {}}\n",
    "def load_tenants() -> Dict[str, Any]:\n",
    "    if not TENANTS_PATH.exists():\n",
    "        return {\"tenants\": {}}\n",
    "    try:\n",
    "        return json.loads(TENANTS_PATH.read_text(encoding=\"utf-8\"))\n",
    "    except Exception:\n",
    "        return {\"tenants\": {}}\n",
    "def rbac_can_tenant(token: Optional[str], action: str, ns: Optional[str]) -> bool:\n",
    "    \"\"\"\n",
    "    Extend previous rbac_can(...) idea:\n",
    "    - if token has role with 'tenants:*'  can touch all tenants\n",
    "    - else if ns is None  need global perm\n",
    "    - else need 'tenant:{ns}:{action}' OR global action\n",
    "    \"\"\"\n",
    "    rbac = load_rbac()\n",
    "    if ADMIN_TOKEN and token == ADMIN_TOKEN:\n",
    "        return True\n",
    "    tok_map = rbac.get(\"tokens\") or {}\n",
    "    role = tok_map.get(token or \"\")\n",
    "    if not role:\n",
    "        return False\n",
    "    perms = (rbac.get(\"roles\") or {}).get(role, [])\n",
    "    if \"tenants:*\" in perms:\n",
    "        return True\n",
    "    if ns is None:\n",
    "        return action in perms\n",
    "    want = f\"tenant:{ns}:{action}\"\n",
    "    if want in perms:\n",
    "        return True\n",
    "    return action in perms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 714,
   "id": "55308161-b9bf-476c-a3d1-13f26342c550",
   "metadata": {},
   "outputs": [],
   "source": [
    "def rotate_file(path: Path, *, max_size_mb: int = 10, keep: int = 5) -> None:\n",
    "    if not path.exists():\n",
    "        return\n",
    "    size_mb = path.stat().st_size / (1024 * 1024)\n",
    "    if size_mb < max_size_mb:\n",
    "        return\n",
    "    ts = int(time.time())\n",
    "    rotated = path.with_name(f\"{path.name}.{ts}\")\n",
    "    path.rename(rotated)\n",
    "    siblings = sorted(path.parent.glob(path.name + \".*\"), key=lambda p: p.stat().st_mtime, reverse=True)\n",
    "    for extra in siblings[keep:]:\n",
    "        try:\n",
    "            extra.unlink()\n",
    "        except Exception:\n",
    "            pass\n",
    "    print(f\" rotated {path.name} ({size_mb:.1f} MB)\")\n",
    "def rotate_logs() -> None:\n",
    "    rotate_file(ADMIN_LOG_PATH, max_size_mb=20, keep=7)\n",
    "    rotate_file(INCIDENT_LOG_PATH, max_size_mb=20, keep=7)\n",
    "    rotate_file(PENDING_OPS_PATH, max_size_mb=10, keep=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 715,
   "id": "2675a0d0-bea4-4edb-9cf0-a2941e58f312",
   "metadata": {},
   "outputs": [],
   "source": [
    "def redact_dict(d: Any) -> Any:\n",
    "    if isinstance(d, dict):\n",
    "        out = {}\n",
    "        for k, v in d.items():\n",
    "            if k.lower() in REDACT_KEYS:\n",
    "                out[k] = \"***REDACTED***\"\n",
    "            else:\n",
    "                out[k] = redact_dict(v)\n",
    "        return out\n",
    "    if isinstance(d, list):\n",
    "        return [redact_dict(x) for x in d]\n",
    "    return d\n",
    "def add_offline_change(kind: str, payload: Dict[str, Any]) -> None:\n",
    "    \"\"\"\n",
    "    Store edits when gateway is unreachable.\n",
    "    \"\"\"\n",
    "    rec = {\n",
    "        \"ts\": int(time.time()),\n",
    "        \"kind\": kind,\n",
    "        \"payload\": payload,\n",
    "        \"host\": socket.gethostname(),\n",
    "    }\n",
    "    with OFFLINE_CHANGES.open(\"a\", encoding=\"utf-8\") as f:\n",
    "        f.write(json.dumps(rec) + \"\\n\")\n",
    "    print(f\" offline change queued: {kind}\")\n",
    "def iter_offline_changes(limit: int = 200) -> List[Dict[str, Any]]:\n",
    "    return _iter_jsonl(OFFLINE_CHANGES, limit=limit)\n",
    "def apply_offline_changes() -> Dict[str, Any]:\n",
    "    \"\"\"\n",
    "    Merge offline changes into the *active* control-plane (blue/green)\n",
    "    without losing schema.\n",
    "    \"\"\"\n",
    "    changes = iter_offline_changes(limit=500)\n",
    "    if not changes:\n",
    "        print(\" no offline changes\")\n",
    "        return {\"applied\": 0}\n",
    "    ctrl = load_active_control() or {\"ts\": int(time.time()), \"chaos\": {}, \"rl_weights\": {}}\n",
    "    for ch in changes:\n",
    "        kind = ch.get(\"kind\")\n",
    "        payload = ch.get(\"payload\") or {}\n",
    "        if kind == \"set-chaos\":\n",
    "            ctrl.setdefault(\"chaos\", {}).update(payload)\n",
    "        elif kind == \"set-rl\":\n",
    "            ctrl.setdefault(\"rl_weights\", {}).update(payload)\n",
    "        else:\n",
    "            print(f\" unknown offline change kind={kind}, skipping\")\n",
    "    ctrl[\"ts\"] = int(time.time())\n",
    "    active = get_active_control_name()\n",
    "    write_control_variant(active, ctrl)\n",
    "    OFFLINE_CHANGES.write_text(\"\", encoding=\"utf-8\")\n",
    "    print(f\" applied {len(changes)} offline change(s)  control ({active})\")\n",
    "    return {\"applied\": len(changes), \"active\": active}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 716,
   "id": "8873b34d-75e1-4836-b5a9-35b5ab9a6686",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " pending-ops deduped  0  0\n",
      " no offline changes\n",
      " live snapshot  \\tmp\\art\\gateway_live_snapshot.json\n",
      " control vs live diff  \\tmp\\art\\gateway_control_vs_live.json\n",
      " wrote  \\tmp\\art\\gateway_maintenance.json (idempotent)\n",
      " controller maintenance  \\tmp\\art\\gateway_maintenance.json\n",
      "\n",
      " continuation (idempotent + dedupe + blue/green + rbac-tenant + rotate + offline) loaded.\n",
      "Try:\n",
      "  switch_control()\n",
      "  add_offline_change('set-chaos', {'disable_ce': True})\n",
      "  apply_offline_changes()\n",
      "  dedupe_pending_ops()\n"
     ]
    }
   ],
   "source": [
    "def controller_maintenance() -> Dict[str, Any]:\n",
    "    \"\"\"\n",
    "    One-shot \"fix my notebook state\" helper.\n",
    "    \"\"\"\n",
    "    rotate_logs()\n",
    "    ded = dedupe_pending_ops()\n",
    "    off = apply_offline_changes()\n",
    "    diff_res = None\n",
    "    if \"diff_control_vs_live\" in globals():\n",
    "        try:\n",
    "            diff_res = diff_control_vs_live()\n",
    "        except Exception as e:\n",
    "            diff_res = {\"error\": str(e)}\n",
    "    rep = {\n",
    "        \"ts\": int(time.time()),\n",
    "        \"dedupe\": ded,\n",
    "        \"offline\": off,\n",
    "        \"diff\": diff_res,\n",
    "    }\n",
    "    rep_path = ART_DIR / \"gateway_maintenance.json\"\n",
    "    write_if_changed(rep_path, rep, schema=CURRENT_SCHEMA)\n",
    "    print(f\" controller maintenance  {rep_path}\")\n",
    "    return rep\n",
    "controller_maintenance()\n",
    "print(\"\\n continuation (idempotent + dedupe + blue/green + rbac-tenant + rotate + offline) loaded.\")\n",
    "print(\"Try:\")\n",
    "print(\"  switch_control()\")\n",
    "print(\"  add_offline_change('set-chaos', {'disable_ce': True})\")\n",
    "print(\"  apply_offline_changes()\")\n",
    "print(\"  dedupe_pending_ops()\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 717,
   "id": "aa96a9b5-1651-42bc-8ddb-27ede279232a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " continuation (tenant-SLO + SLO-eval + prom-export + helm-gen + sre-kit) using ART_DIR=\\tmp\\art\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "import time\n",
    "import socket\n",
    "import http.server\n",
    "import socketserver\n",
    "import tarfile\n",
    "from pathlib import Path\n",
    "from typing import Any, Dict, List, Optional, Tuple\n",
    "ART_DIR = Path(os.getenv(\"ART_DIR\", \"/tmp/art\"))\n",
    "ART_DIR.mkdir(parents=True, exist_ok=True)\n",
    "print(f\" continuation (tenant-SLO + SLO-eval + prom-export + helm-gen + sre-kit) using ART_DIR={ART_DIR}\")\n",
    "E2E_REPORT = ART_DIR / \"gateway_e2e_report.json\"\n",
    "PROM_TS = ART_DIR / \"gateway_prom_timeseries.json\"\n",
    "SLA_RUNS = ART_DIR / \"gateway_sla_runs.json\"\n",
    "CONTROL_PATH = ART_DIR / \"gateway_control.json\"\n",
    "TENANTS_PATH = ART_DIR / \"gateway_tenants.json\"\n",
    "SRE_BUNDLE = ART_DIR / \"gateway_sre_bundle.tar.gz\"  \n",
    "GATEWAY_BASE = os.getenv(\"GATEWAY_BASE\", \"\").strip() or \"http://127.0.0.1:9910\"\n",
    "ADMIN_TOKEN = os.getenv(\"GATEWAY_ADMIN_TOKEN\", \"\").strip()\n",
    "CURRENT_SCHEMA = \"1.1.0\"\n",
    "SLO_DEF_PATH = ART_DIR / \"gateway_slo.json\"\n",
    "SLO_EVAL_PATH = ART_DIR / \"gateway_slo_eval.json\"\n",
    "PROM_EXPORT_STATE = ART_DIR / \"gateway_prom_state.json\"\n",
    "HELM_VALUES_PATH = ART_DIR / \"helm-gateway-values.yaml\"\n",
    "SRE_KIT_PATH = ART_DIR / \"gateway_sre_kit.tar.gz\"\n",
    "NOTEBOOK_METRICS = ART_DIR / \"gateway_controller_metrics.json\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 718,
   "id": "ad0bd1e6-371c-4b50-a68c-b5a580d95d0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_default_slos() -> Path:\n",
    "    \"\"\"\n",
    "    Define SLOs per-tenant AND global.\n",
    "    p95 is ms.\n",
    "    errors is % (0-1).\n",
    "    \"\"\"\n",
    "    slo = {\n",
    "        \"ts\": int(time.time()),\n",
    "        \"schema\": CURRENT_SCHEMA,\n",
    "        \"global\": {\n",
    "            \"p95_ms\": 1800.0,\n",
    "            \"error_rate\": 0.05,\n",
    "        },\n",
    "        \"tenants\": {\n",
    "            \"default\": {\n",
    "                \"p95_ms\": 1800.0,\n",
    "                \"error_rate\": 0.05,\n",
    "            },\n",
    "            \"tenant-acme\": {\n",
    "                \"p95_ms\": 1200.0,\n",
    "                \"error_rate\": 0.03,\n",
    "            },\n",
    "            \"tenant-beta\": {\n",
    "                \"p95_ms\": 1500.0,\n",
    "                \"error_rate\": 0.04,\n",
    "            },\n",
    "        },\n",
    "    }\n",
    "    SLO_DEF_PATH.write_text(json.dumps(slo, indent=2), encoding=\"utf-8\")\n",
    "    print(f\" tenant SLOs  {SLO_DEF_PATH}\")\n",
    "    return SLO_DEF_PATH\n",
    "def load_slos() -> Dict[str, Any]:\n",
    "    if not SLO_DEF_PATH.exists():\n",
    "        return {\"global\": {\"p95_ms\": 1800.0, \"error_rate\": 0.05}, \"tenants\": {}}\n",
    "    try:\n",
    "        return json.loads(SLO_DEF_PATH.read_text(encoding=\"utf-8\"))\n",
    "    except Exception:\n",
    "        return {\"global\": {\"p95_ms\": 1800.0, \"error_rate\": 0.05}, \"tenants\": {}}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 719,
   "id": "e5d5fd23-4755-4dfc-919d-91018464f652",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _load_json_safe(p: Path) -> Any:\n",
    "    if not p.exists():\n",
    "        return None\n",
    "    try:\n",
    "        return json.loads(p.read_text(encoding=\"utf-8\"))\n",
    "    except Exception:\n",
    "        return None\n",
    "def eval_slos() -> Path:\n",
    "    slo = load_slos()\n",
    "    e2e = _load_json_safe(E2E_REPORT) or {}\n",
    "    sla = _load_json_safe(SLA_RUNS) or {}\n",
    "    prom = _load_json_safe(PROM_TS) or {}\n",
    "    probes = e2e.get(\"probes\") or []\n",
    "    per_tenant: Dict[str, Dict[str, Any]] = {}\n",
    "    for pr in probes:\n",
    "        ns = pr.get(\"ns\") or \"default\"\n",
    "        lat = pr.get(\"latency_ms\")\n",
    "        ok = pr.get(\"ok\", False)\n",
    "        st = per_tenant.setdefault(ns, {\"latencies\": [], \"errors\": 0, \"total\": 0})\n",
    "        st[\"total\"] += 1\n",
    "        if isinstance(lat, (int, float)):\n",
    "            st[\"latencies\"].append(lat)\n",
    "        if not ok:\n",
    "            st[\"errors\"] += 1\n",
    "    breaches: List[Dict[str, Any]] = []\n",
    "    for ns, st in per_tenant.items():\n",
    "        slo_ns = slo.get(\"tenants\", {}).get(ns) or slo.get(\"global\") or {}\n",
    "        target_p95 = slo_ns.get(\"p95_ms\", 1800.0)\n",
    "        target_err = slo_ns.get(\"error_rate\", 0.05)\n",
    "        lats = sorted(st[\"latencies\"])\n",
    "        p95 = None\n",
    "        if lats:\n",
    "            idx = int(len(lats) * 0.95) - 1\n",
    "            idx = max(0, min(idx, len(lats) - 1))\n",
    "            p95 = lats[idx]\n",
    "        err_rate = (st[\"errors\"] / st[\"total\"]) if st[\"total\"] else 0.0\n",
    "        ns_res = {\n",
    "            \"tenant\": ns,\n",
    "            \"target_p95_ms\": target_p95,\n",
    "            \"actual_p95_ms\": p95,\n",
    "            \"target_error_rate\": target_err,\n",
    "            \"actual_error_rate\": round(err_rate, 3),\n",
    "            \"total_probes\": st[\"total\"],\n",
    "            \"breach_p95\": bool(p95 and p95 > target_p95),\n",
    "            \"breach_error\": err_rate > target_err,\n",
    "        }\n",
    "        if ns_res[\"breach_p95\"] or ns_res[\"breach_error\"]:\n",
    "            breaches.append(ns_res)\n",
    "        per_tenant[ns] = ns_res  \n",
    "    out = {\n",
    "        \"ts\": int(time.time()),\n",
    "        \"schema\": CURRENT_SCHEMA,\n",
    "        \"global\": slo.get(\"global\", {}),\n",
    "        \"tenants\": per_tenant,\n",
    "        \"breaches\": breaches,\n",
    "        \"sources\": {\n",
    "            \"e2e\": E2E_REPORT.name if E2E_REPORT.exists() else None,\n",
    "            \"sla\": SLA_RUNS.name if SLA_RUNS.exists() else None,\n",
    "            \"prom_ts\": PROM_TS.name if PROM_TS.exists() else None,\n",
    "        },\n",
    "    }\n",
    "    SLO_EVAL_PATH.write_text(json.dumps(out, indent=2), encoding=\"utf-8\")\n",
    "    if breaches:\n",
    "        print(f\" SLO breaches: {len(breaches)}  {SLO_EVAL_PATH}\")\n",
    "    else:\n",
    "        print(f\" SLOs ok  {SLO_EVAL_PATH}\")\n",
    "    return SLO_EVAL_PATH"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 720,
   "id": "2cb66b4c-97c7-41c2-bcd5-1d3710f35135",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _compute_disk_mb(root: Path) -> float:\n",
    "    total = 0\n",
    "    for p in root.glob(\"**/*\"):\n",
    "        if p.is_file():\n",
    "            total += p.stat().st_size\n",
    "    return round(total / (1024 * 1024), 2)\n",
    "def write_controller_metrics() -> Path:\n",
    "    slo_eval = _load_json_safe(SLO_EVAL_PATH) or {}\n",
    "    disk_mb = _compute_disk_mb(ART_DIR)\n",
    "    metrics = {\n",
    "        \"ts\": int(time.time()),\n",
    "        \"schema\": CURRENT_SCHEMA,\n",
    "        \"disk_mb\": disk_mb,\n",
    "        \"slo_breaches\": len(slo_eval.get(\"breaches\") or []),\n",
    "    }\n",
    "    NOTEBOOK_METRICS.write_text(json.dumps(metrics, indent=2), encoding=\"utf-8\")\n",
    "    print(f\" controller metrics  {NOTEBOOK_METRICS}\")\n",
    "    return NOTEBOOK_METRICS\n",
    "class PromHandler(http.server.BaseHTTPRequestHandler):\n",
    "    def do_GET(self):\n",
    "        if self.path not in (\"/metrics\", \"/\"):\n",
    "            self.send_response(404)\n",
    "            self.end_headers()\n",
    "            return\n",
    "        slo_eval = _load_json_safe(SLO_EVAL_PATH) or {}\n",
    "        breaches = slo_eval.get(\"breaches\") or []\n",
    "        disk_mb = _compute_disk_mb(ART_DIR)\n",
    "        lines = []\n",
    "        lines.append(\"# HELP gateway_controller_disk_mb Disk usage of /tmp/art\")\n",
    "        lines.append(\"# TYPE gateway_controller_disk_mb gauge\")\n",
    "        lines.append(f\"gateway_controller_disk_mb {disk_mb}\")\n",
    "        lines.append(\"# HELP gateway_controller_slo_breaches Number of current SLO breaches\")\n",
    "        lines.append(\"# TYPE gateway_controller_slo_breaches gauge\")\n",
    "        lines.append(f\"gateway_controller_slo_breaches {len(breaches)}\")\n",
    "        tenants = slo_eval.get(\"tenants\") or {}\n",
    "        for ns, data in tenants.items():\n",
    "            ap = data.get(\"actual_p95_ms\") or 0\n",
    "            tp = data.get(\"target_p95_ms\") or 0\n",
    "            lines.append(f'gateway_controller_tenant_p95_ms{{tenant=\"{ns}\"}} {ap}')\n",
    "            lines.append(f'gateway_controller_tenant_target_p95_ms{{tenant=\"{ns}\"}} {tp}')\n",
    "        out = \"\\n\".join(lines) + \"\\n\"\n",
    "        out_bytes = out.encode(\"utf-8\")\n",
    "        self.send_response(200)\n",
    "        self.send_header(\"Content-Type\", \"text/plain; version=0.0.4\")\n",
    "        self.send_header(\"Content-Length\", str(len(out_bytes)))\n",
    "        self.end_headers()\n",
    "        self.wfile.write(out_bytes)\n",
    "def serve_prom_exporter(port: int = 9925) -> None:\n",
    "    \"\"\"\n",
    "    Local-only prometheus exporter. Run only when you want to scrape notebook state.\n",
    "    \"\"\"\n",
    "    with socketserver.TCPServer((\"0.0.0.0\", port), PromHandler) as httpd:\n",
    "        print(f\" prom exporter at http://0.0.0.0:{port}/metrics\")\n",
    "        try:\n",
    "            httpd.serve_forever()\n",
    "        except KeyboardInterrupt:\n",
    "            print(\" prom exporter stopped\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 721,
   "id": "712b7c16-9720-45af-9739-ccd2f5ecbeac",
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_helm_values(\n",
    "    name: str = \"rag-gateway\",\n",
    "    image: str = \"python:3.11-slim\",\n",
    "    sidecar_image: str = \"python:3.11-slim\",\n",
    ") -> Path:\n",
    "    vals = f\"\"\"# auto-generated from notebook\n",
    "fullnameOverride: {name}\n",
    "image:\n",
    "  repository: {image}\n",
    "  tag: latest\n",
    "  pullPolicy: IfNotPresent\n",
    "gateway:\n",
    "  baseUrl: \"{GATEWAY_BASE}\"\n",
    "  adminToken: \"{ADMIN_TOKEN}\"\n",
    "sidecar:\n",
    "  enabled: true\n",
    "  image: {sidecar_image}\n",
    "  artDir: /tmp/art\n",
    "  pollIntervalSeconds: 5\n",
    "persistence:\n",
    "  enabled: true\n",
    "  existingClaim: rag-gateway-art\n",
    "\"\"\"\n",
    "    HELM_VALUES_PATH.write_text(vals, encoding=\"utf-8\")\n",
    "    print(f\" helm values  {HELM_VALUES_PATH}\")\n",
    "    return HELM_VALUES_PATH"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 722,
   "id": "7abf44d8-dab4-4ee1-9be8-f9135d0a4116",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_sre_kit() -> Path:\n",
    "    \"\"\"\n",
    "    Smaller than the big SRE bundle: only the operationally useful files.\n",
    "    \"\"\"\n",
    "    wanted = [\n",
    "        \"gateway_entrypoint.py\",\n",
    "        \"gateway_sidecar_poller.py\",\n",
    "        \"gateway_runbook.md\",\n",
    "        \"gateway_slo.json\",\n",
    "        \"gateway_slo_eval.json\",\n",
    "        \"prometheus-gateway-rules.yml\",\n",
    "        \"k8s-gateway-sidecar.yaml\",\n",
    "        \"k8s-gateway-agent-cronjob.yaml\",\n",
    "        \"helm-gateway-values.yaml\",\n",
    "    ]\n",
    "    with tarfile.open(SRE_KIT_PATH, \"w:gz\") as tar:\n",
    "        for name in wanted:\n",
    "            p = ART_DIR / name\n",
    "            if p.exists():\n",
    "                tar.add(p, arcname=name)\n",
    "    print(f\" SRE kit  {SRE_KIT_PATH}\")\n",
    "    return SRE_KIT_PATH"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 723,
   "id": "cbaead87-3668-41d0-ba8d-d31d2eb74148",
   "metadata": {},
   "outputs": [],
   "source": [
    "def gw_cli4(cmd: str, **kwargs) -> None:\n",
    "    cmd = cmd.strip().lower()\n",
    "    if cmd == \"slo.write\":\n",
    "        write_default_slos()\n",
    "    elif cmd == \"slo.eval\":\n",
    "        eval_slos()\n",
    "    elif cmd == \"prom.serve\":\n",
    "        port = int(kwargs.get(\"port\", 9925))\n",
    "        serve_prom_exporter(port=port)\n",
    "    elif cmd == \"helm\":\n",
    "        write_helm_values()\n",
    "    elif cmd == \"sre.kit\":\n",
    "        build_sre_kit()\n",
    "    elif cmd == \"metrics\":\n",
    "        write_controller_metrics()\n",
    "    else:\n",
    "        print(\" unknown cmd. try:\")\n",
    "        print(\"  gw_cli4('slo.write')\")\n",
    "        print(\"  gw_cli4('slo.eval')\")\n",
    "        print(\"  gw_cli4('prom.serve', port=9925)\")\n",
    "        print(\"  gw_cli4('helm')\")\n",
    "        print(\"  gw_cli4('sre.kit')\")\n",
    "        print(\"  gw_cli4('metrics')\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 724,
   "id": "d232b3d0-3a96-45ca-b12f-9e97563e38c2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " tenant SLOs  \\tmp\\art\\gateway_slo.json\n",
      " SLO breaches: 3  \\tmp\\art\\gateway_slo_eval.json\n",
      " controller metrics  \\tmp\\art\\gateway_controller_metrics.json\n",
      " helm values  \\tmp\\art\\helm-gateway-values.yaml\n",
      " SRE kit  \\tmp\\art\\gateway_sre_kit.tar.gz\n",
      "\n",
      " continuation (tenant-SLO + SLO-eval + prom-export + helm-gen + sre-kit) loaded.\n",
      "Try:\n",
      "  gw_cli4('slo.write')\n",
      "  gw_cli4('slo.eval')\n",
      "  gw_cli4('prom.serve', port=9925)\n",
      "  gw_cli4('helm')\n",
      "  gw_cli4('sre.kit')\n"
     ]
    }
   ],
   "source": [
    "write_default_slos()\n",
    "eval_slos()\n",
    "write_controller_metrics()\n",
    "write_helm_values()\n",
    "build_sre_kit()\n",
    "print(\"\\n continuation (tenant-SLO + SLO-eval + prom-export + helm-gen + sre-kit) loaded.\")\n",
    "print(\"Try:\")\n",
    "print(\"  gw_cli4('slo.write')\")\n",
    "print(\"  gw_cli4('slo.eval')\")\n",
    "print(\"  gw_cli4('prom.serve', port=9925)\")\n",
    "print(\"  gw_cli4('helm')\")\n",
    "print(\"  gw_cli4('sre.kit')\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 725,
   "id": "09d0e5e0-3c0e-403b-add5-e38b8038e318",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " continuation (provenance + http-admin + pkg-cli + tests) using ART_DIR=\\tmp\\art\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "import time\n",
    "import socket\n",
    "import argparse\n",
    "import http.server\n",
    "import socketserver\n",
    "from pathlib import Path\n",
    "from typing import Any, Dict, List, Optional, Tuple, Callable\n",
    "ART_DIR = Path(os.getenv(\"ART_DIR\", \"/tmp/art\"))\n",
    "ART_DIR.mkdir(parents=True, exist_ok=True)\n",
    "print(f\" continuation (provenance + http-admin + pkg-cli + tests) using ART_DIR={ART_DIR}\")\n",
    "CONTROL_PATH = ART_DIR / \"gateway_control.json\"\n",
    "CONTROL_BLUE = ART_DIR / \"gateway_control.blue.json\"\n",
    "CONTROL_GREEN = ART_DIR / \"gateway_control.green.json\"\n",
    "CONTROL_ACTIVE = ART_DIR / \"gateway_control.active\"\n",
    "OFFLINE_CHANGES = ART_DIR / \"gateway_offline_changes.jsonl\"\n",
    "SLO_DEF_PATH = ART_DIR / \"gateway_slo.json\"\n",
    "SLO_EVAL_PATH = ART_DIR / \"gateway_slo_eval.json\"\n",
    "MAINT_PATH = ART_DIR / \"gateway_maintenance.json\"\n",
    "AUDIT_PATH = ART_DIR / \"gateway_audit.jsonl\"\n",
    "PKG_DIR = ART_DIR / \"gateway_controller\"\n",
    "TESTS_DIR = ART_DIR / \"gateway_controller_tests\"\n",
    "GATEWAY_BASE = os.getenv(\"GATEWAY_BASE\", \"\").strip() or \"http://127.0.0.1:9910\"\n",
    "ADMIN_TOKEN = os.getenv(\"GATEWAY_ADMIN_TOKEN\", \"\").strip()\n",
    "CURRENT_SCHEMA = \"1.1.0\"\n",
    "MIN_SUPPORTED_SCHEMA = \"1.0.0\"\n",
    "CONTROLLER_VERSION = \"0.2.0-notebook\"\n",
    "CONTROLLER_INFO_PATH = ART_DIR / \"gateway_controller_info.json\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 726,
   "id": "69173934-effe-4f1a-980d-43fa5135f51e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _read_json(path: Path, default: Any = None) -> Any:\n",
    "    if not path.exists():\n",
    "        return default\n",
    "    try:\n",
    "        return json.loads(path.read_text(encoding=\"utf-8\"))\n",
    "    except Exception:\n",
    "        return default\n",
    "def _write_json(path: Path, data: Dict[str, Any]) -> None:\n",
    "    path.write_text(json.dumps(data, indent=2), encoding=\"utf-8\")\n",
    "def _load_active_control_fallback() -> Dict[str, Any]:\n",
    "    if \"load_active_control\" in globals():\n",
    "        try:\n",
    "            return globals()[\"load_active_control\"]()\n",
    "        except Exception:\n",
    "            pass\n",
    "    if CONTROL_PATH.exists():\n",
    "        return _read_json(CONTROL_PATH, {})\n",
    "    return {\"ts\": int(time.time()), \"chaos\": {}, \"rl_weights\": {}}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 727,
   "id": "ec97b40c-6569-4f7d-9e85-2f08e893cb2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _get_git_info() -> Dict[str, Any]:\n",
    "    cwd = Path.cwd()\n",
    "    head = cwd / \".git\" / \"HEAD\"\n",
    "    if head.exists():\n",
    "        txt = head.read_text(encoding=\"utf-8\").strip()\n",
    "        if txt.startswith(\"ref:\"):\n",
    "            ref_path = cwd / \".git\" / txt.split(\" \", 1)[1]\n",
    "            if ref_path.exists():\n",
    "                return {\n",
    "                    \"head\": txt,\n",
    "                    \"ref_commit\": ref_path.read_text(encoding=\"utf-8\").strip(),\n",
    "                }\n",
    "        return {\"head\": txt}\n",
    "    git_sha = os.getenv(\"GIT_SHA\") or os.getenv(\"GITHUB_SHA\")\n",
    "    if git_sha:\n",
    "        return {\"env_sha\": git_sha}\n",
    "    return {}\n",
    "def build_provenance(kind: str, extra: Optional[Dict[str, Any]] = None) -> Dict[str, Any]:\n",
    "    prov = {\n",
    "        \"ts\": int(time.time()),\n",
    "        \"kind\": kind,\n",
    "        \"actor\": os.getenv(\"GATEWAY_ACTOR\", \"notebook\"),\n",
    "        \"host\": socket.gethostname(),\n",
    "        \"controller_version\": CONTROLLER_VERSION,\n",
    "        \"schema\": CURRENT_SCHEMA,\n",
    "        \"min_supported_schema\": MIN_SUPPORTED_SCHEMA,\n",
    "        \"git\": _get_git_info(),\n",
    "    }\n",
    "    if extra:\n",
    "        prov[\"extra\"] = extra\n",
    "    return prov\n",
    "def audit_log(kind: str, extra: Optional[Dict[str, Any]] = None) -> None:\n",
    "    rec = build_provenance(kind, extra)\n",
    "    with AUDIT_PATH.open(\"a\", encoding=\"utf-8\") as f:\n",
    "        f.write(json.dumps(rec) + \"\\n\")\n",
    "    if AUDIT_PATH.stat().st_size > 5 * 1024 * 1024: \n",
    "        ts = int(time.time())\n",
    "        AUDIT_PATH.rename(AUDIT_PATH.with_name(f\"gateway_audit.jsonl.{ts}\"))\n",
    "    print(f\" audit: {kind}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 728,
   "id": "a426cdec-6b52-46a3-b78b-072bf9f9a82f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def iter_offline_changes(limit: int = 500) -> List[Dict[str, Any]]:\n",
    "    if not OFFLINE_CHANGES.exists():\n",
    "        return []\n",
    "    out: List[Dict[str, Any]] = []\n",
    "    with OFFLINE_CHANGES.open(\"r\", encoding=\"utf-8\") as f:\n",
    "        for i, line in enumerate(f):\n",
    "            if not line.strip():\n",
    "                continue\n",
    "            try:\n",
    "                out.append(json.loads(line))\n",
    "            except Exception:\n",
    "                pass\n",
    "            if i + 1 >= limit:\n",
    "                break\n",
    "    return out\n",
    "def dedupe_offline_changes(max_keep: int = 200) -> Dict[str, Any]:\n",
    "    rows = iter_offline_changes(limit=2000)\n",
    "    seen = set()\n",
    "    deduped: List[Dict[str, Any]] = []\n",
    "    for r in reversed(rows):  \n",
    "        sig = (r.get(\"kind\"), json.dumps(r.get(\"payload\") or {}, sort_keys=True))\n",
    "        if sig in seen:\n",
    "            continue\n",
    "        seen.add(sig)\n",
    "        deduped.append(r)\n",
    "    deduped = list(reversed(deduped))\n",
    "    if len(deduped) > max_keep:\n",
    "        deduped = deduped[-max_keep:]\n",
    "    with OFFLINE_CHANGES.open(\"w\", encoding=\"utf-8\") as f:\n",
    "        for r in deduped:\n",
    "            f.write(json.dumps(r) + \"\\n\")\n",
    "    print(f\" offline-changes deduped  {len(rows)}  {len(deduped)}\")\n",
    "    return {\"before\": len(rows), \"after\": len(deduped)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 729,
   "id": "c57a4443-ae23-4b91-82af-de1361d7a4be",
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_controller_info() -> Path:\n",
    "    info = {\n",
    "        \"ts\": int(time.time()),\n",
    "        \"controller_version\": CONTROLLER_VERSION,\n",
    "        \"schema\": CURRENT_SCHEMA,\n",
    "        \"min_supported_schema\": MIN_SUPPORTED_SCHEMA,\n",
    "        \"gateway_base\": GATEWAY_BASE,\n",
    "        \"host\": socket.gethostname(),\n",
    "        \"git\": _get_git_info(),\n",
    "    }\n",
    "    _write_json(CONTROLLER_INFO_PATH, info)\n",
    "    print(f\" controller-info  {CONTROLLER_INFO_PATH}\")\n",
    "    return CONTROLLER_INFO_PATH"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 730,
   "id": "7f4b4d04-e022-4b9a-ab89-b7f40dcff08a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ControllerHandler(http.server.BaseHTTPRequestHandler):\n",
    "    def _json(self, code: int, payload: Dict[str, Any]) -> None:\n",
    "        body = json.dumps(payload).encode(\"utf-8\")\n",
    "        self.send_response(code)\n",
    "        self.send_header(\"Content-Type\", \"application/json\")\n",
    "        self.send_header(\"Content-Length\", str(len(body)))\n",
    "        self.end_headers()\n",
    "        self.wfile.write(body)\n",
    "    def do_GET(self):\n",
    "        if self.path in (\"/\", \"/healthz\"):\n",
    "            self._json(200, {\"ok\": True, \"ts\": int(time.time())})\n",
    "            return\n",
    "        if self.path == \"/version\":\n",
    "            self._json(200, _read_json(CONTROLLER_INFO_PATH, {}))\n",
    "            return\n",
    "        if self.path == \"/control/active\":\n",
    "            name = \"blue\"\n",
    "            if CONTROL_ACTIVE.exists():\n",
    "                name = CONTROL_ACTIVE.read_text(encoding=\"utf-8\").strip() or \"blue\"\n",
    "            ctrl = _load_active_control_fallback()\n",
    "            self._json(200, {\"active\": name, \"control\": ctrl})\n",
    "            return\n",
    "        if self.path == \"/slo/eval\":\n",
    "            self._json(200, _read_json(SLO_EVAL_PATH, {}))\n",
    "            return\n",
    "        if self.path == \"/audit/tail\":\n",
    "            if AUDIT_PATH.exists():\n",
    "                lines = AUDIT_PATH.read_text(encoding=\"utf-8\").splitlines()[-80:]\n",
    "                out = []\n",
    "                for ln in lines:\n",
    "                    try:\n",
    "                        out.append(json.loads(ln))\n",
    "                    except Exception:\n",
    "                        out.append({\"raw\": ln})\n",
    "                self._json(200, {\"events\": out})\n",
    "            else:\n",
    "                self._json(200, {\"events\": []})\n",
    "            return\n",
    "        self._json(404, {\"error\": \"not-found\"})\n",
    "    def do_POST(self):\n",
    "        length = int(self.headers.get(\"content-length\", \"0\") or \"0\")\n",
    "        raw = self.rfile.read(length) if length else b\"{}\"\n",
    "        try:\n",
    "            data = json.loads(raw.decode(\"utf-8\") or \"{}\")\n",
    "        except Exception:\n",
    "            data = {}\n",
    "        path = self.path\n",
    "        sent_tok = self.headers.get(\"x-admin-token\") or data.get(\"admin_token\")\n",
    "        if ADMIN_TOKEN and sent_tok != ADMIN_TOKEN and path not in (\"/maintenance/run\", \"/healthz\"):\n",
    "            self._json(401, {\"error\": \"unauthorized\"})\n",
    "            return\n",
    "        if path == \"/maintenance/run\":\n",
    "            if \"controller_maintenance\" in globals():\n",
    "                rep = globals()[\"controller_maintenance\"]()\n",
    "            else:\n",
    "                rep = {\"ts\": int(time.time()), \"note\": \"controller_maintenance not in scope\"}\n",
    "            audit_log(\"maintenance.run\", rep)\n",
    "            self._json(200, rep)\n",
    "            return\n",
    "        if path == \"/control/switch\":\n",
    "            if \"switch_control\" in globals():\n",
    "                nxt = globals()[\"switch_control\"]()\n",
    "                audit_log(\"control.switch\", {\"active\": nxt})\n",
    "                self._json(200, {\"active\": nxt})\n",
    "            else:\n",
    "                self._json(500, {\"error\": \"switch_control not in scope\"})\n",
    "            return\n",
    "        if path == \"/safemode/apply\":\n",
    "            if \"apply_safemode\" in globals():\n",
    "                ns = data.get(\"ns\")\n",
    "                reason = data.get(\"reason\", \"manual-http\")\n",
    "                res = globals()[\"apply_safemode\"](reason=reason, ns=ns)\n",
    "                audit_log(\"safemode.apply\", {\"ns\": ns, \"reason\": reason, \"res\": res})\n",
    "                self._json(200, res)\n",
    "            else:\n",
    "                self._json(500, {\"error\": \"apply_safemode not in scope\"})\n",
    "            return\n",
    "        if path == \"/safemode/clear\":\n",
    "            if \"clear_safemode\" in globals():\n",
    "                res = globals()[\"clear_safemode\"]()\n",
    "                audit_log(\"safemode.clear\", {\"res\": res})\n",
    "                self._json(200, {\"ok\": True})\n",
    "            else:\n",
    "                self._json(500, {\"error\": \"clear_safemode not in scope\"})\n",
    "            return\n",
    "        if path == \"/slo/eval\":\n",
    "            if \"eval_slos\" in globals():\n",
    "                res = globals()[\"eval_slos\"]()\n",
    "                audit_log(\"slo.eval.http\", {\"path\": str(res)})\n",
    "                self._json(200, _read_json(res, {}))\n",
    "            else:\n",
    "                self._json(500, {\"error\": \"eval_slos not in scope\"})\n",
    "            return\n",
    "        self._json(404, {\"error\": \"not-found\"})\n",
    "def serve_controller_http(port: int = 9926) -> None:\n",
    "    with socketserver.TCPServer((\"0.0.0.0\", port), ControllerHandler) as httpd:\n",
    "        print(f\" controller HTTP at http://0.0.0.0:{port}\")\n",
    "        try:\n",
    "            httpd.serve_forever()\n",
    "        except KeyboardInterrupt:\n",
    "            print(\" controller HTTP stopped\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 731,
   "id": "684e6ffc-e1ff-410d-b84c-bb3833483769",
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_controller_package() -> Path:\n",
    "    PKG_DIR.mkdir(parents=True, exist_ok=True)\n",
    "    (PKG_DIR / \"__init__.py\").write_text(\n",
    "        f\"\"\"# auto-generated\n",
    "CONTROLLER_VERSION = \"{CONTROLLER_VERSION}\"\n",
    "CURRENT_SCHEMA = \"{CURRENT_SCHEMA}\"\n",
    "MIN_SUPPORTED_SCHEMA = \"{MIN_SUPPORTED_SCHEMA}\"\n",
    "ART_DIR = r\"{ART_DIR}\"\n",
    "\"\"\",\n",
    "        encoding=\"utf-8\",\n",
    "    )\n",
    "    cli_py = f\"\"\"# auto-generated CLI\n",
    "import sys, json, os, time\n",
    "from pathlib import Path\n",
    "\n",
    "ART_DIR = Path(r\"{ART_DIR}\")\n",
    "sys.path.insert(0, str(ART_DIR))\n",
    "def main():\n",
    "    cmd = sys.argv[1] if len(sys.argv) > 1 else \"version\"\n",
    "    if cmd == \"version\":\n",
    "        info_path = ART_DIR / \"gateway_controller_info.json\"\n",
    "        if info_path.exists():\n",
    "            print(info_path.read_text())\n",
    "        else:\n",
    "            print(json.dumps({{\"error\":\"no controller info\"}}, indent=2))\n",
    "    elif cmd == \"maintenance\":\n",
    "        # will be no-op if not in scope\n",
    "        try:\n",
    "            from __main__ import controller_maintenance\n",
    "            res = controller_maintenance()\n",
    "            print(json.dumps(res, indent=2))\n",
    "        except Exception as e:\n",
    "            print(json.dumps({{\"error\": str(e)}}))\n",
    "    elif cmd == \"slo.eval\":\n",
    "        try:\n",
    "            from __main__ import eval_slos\n",
    "            p = eval_slos()\n",
    "            print(Path(p).read_text())\n",
    "        except Exception as e:\n",
    "            print(json.dumps({{\"error\": str(e)}}))\n",
    "    else:\n",
    "        print(\"unknown cmd:\", cmd)\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n",
    "\"\"\"\n",
    "    (PKG_DIR / \"cli.py\").write_text(cli_py, encoding=\"utf-8\")\n",
    "    print(f\" controller package  {PKG_DIR}\")\n",
    "    return PKG_DIR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 732,
   "id": "a5722eb5-44dd-4d2e-8ddd-3c097edcbeba",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_notebook_smoke_tests() -> Dict[str, Any]:\n",
    "    \"\"\"\n",
    "    quick assertions so CI / humans know the generated stuff is coherent\n",
    "    \"\"\"\n",
    "    errs: List[str] = []\n",
    "    ctrl = _load_active_control_fallback()\n",
    "    if not isinstance(ctrl, dict):\n",
    "        errs.append(\"control not dict\")\n",
    "    slo_eval = _read_json(SLO_EVAL_PATH, {})\n",
    "    if not slo_eval:\n",
    "        errs.append(\"no SLO eval\")\n",
    "    if not CONTROL_BLUE.exists() and not CONTROL_GREEN.exists():\n",
    "        blu = ctrl or {\"ts\": int(time.time()), \"chaos\": {}, \"rl_weights\": {}}\n",
    "        CONTROL_BLUE.write_text(json.dumps(blu, indent=2), encoding=\"utf-8\")\n",
    "    if not CONTROLLER_INFO_PATH.exists():\n",
    "        write_controller_info()\n",
    "    out = {\n",
    "        \"ts\": int(time.time()),\n",
    "        \"errors\": errs,\n",
    "        \"ok\": not bool(errs),\n",
    "    }\n",
    "    test_path = ART_DIR / \"gateway_tests_report.json\"\n",
    "    _write_json(test_path, out)\n",
    "    print(f\" notebook smoke-tests  {test_path}\")\n",
    "    if errs:\n",
    "        print(\" tests failed:\", errs)\n",
    "    else:\n",
    "        print(\" tests passed\")\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 733,
   "id": "f3797712-5da2-4799-9179-8f95b71a0195",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " controller-info  \\tmp\\art\\gateway_controller_info.json\n",
      " offline-changes deduped  0  0\n",
      " controller package  \\tmp\\art\\gateway_controller\n",
      " notebook smoke-tests  \\tmp\\art\\gateway_tests_report.json\n",
      " tests passed\n",
      "\n",
      " continuation (provenance + http-admin + pkg-cli + tests) loaded.\n",
      "Try:\n",
      "  # start HTTP admin (local only)\n",
      "  # serve_controller_http(9926)\n",
      "  # CLI-ish run:\n",
      "  python -m gateway_controller.cli version\n",
      "  python -m gateway_controller.cli maintenance\n"
     ]
    }
   ],
   "source": [
    "write_controller_info()\n",
    "dedupe_offline_changes()\n",
    "write_controller_package()\n",
    "run_notebook_smoke_tests()\n",
    "print(\"\\n continuation (provenance + http-admin + pkg-cli + tests) loaded.\")\n",
    "print(\"Try:\")\n",
    "print(\"  # start HTTP admin (local only)\")\n",
    "print(\"  # serve_controller_http(9926)\")\n",
    "print(\"  # CLI-ish run:\")\n",
    "print(\"  python -m gateway_controller.cli version\")\n",
    "print(\"  python -m gateway_controller.cli maintenance\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 734,
   "id": "852bffaf-5aca-40b8-a582-46abd520ed69",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " continuation (env + chaos-policy + control import/export + bootstrap + http-rl) using ART_DIR=\\tmp\\art\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "import time\n",
    "import socket\n",
    "from pathlib import Path\n",
    "from typing import Any, Dict, List, Optional, Tuple, Callable\n",
    "ART_DIR = Path(os.getenv(\"ART_DIR\", \"/tmp/art\"))\n",
    "ART_DIR.mkdir(parents=True, exist_ok=True)\n",
    "print(f\" continuation (env + chaos-policy + control import/export + bootstrap + http-rl) using ART_DIR={ART_DIR}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 735,
   "id": "1bf6c732-0ceb-4f18-88bb-102aef68e721",
   "metadata": {},
   "outputs": [],
   "source": [
    "ENV_PATH = ART_DIR / \"gateway_env.json\"\n",
    "DEFAULT_ENV = {\n",
    "    \"env\": os.getenv(\"GATEWAY_ENV\", \"dev\"),\n",
    "    \"base\": os.getenv(\"GATEWAY_BASE\", \"http://127.0.0.1:9910\"),\n",
    "    \"writable\": True,\n",
    "    \"allow_prod_push\": False,\n",
    "    \"ts\": int(time.time()),\n",
    "}\n",
    "def load_env() -> Dict[str, Any]:\n",
    "    if ENV_PATH.exists():\n",
    "        try:\n",
    "            return json.loads(ENV_PATH.read_text(encoding=\"utf-8\"))\n",
    "        except Exception as e:\n",
    "            print(f\" failed to read {ENV_PATH}: {e}\")\n",
    "    ENV_PATH.write_text(json.dumps(DEFAULT_ENV, indent=2), encoding=\"utf-8\")\n",
    "    return DEFAULT_ENV\n",
    "def write_env(env: Dict[str, Any]) -> Path:\n",
    "    env = dict(env)\n",
    "    env.setdefault(\"ts\", int(time.time()))\n",
    "    ENV_PATH.write_text(json.dumps(env, indent=2), encoding=\"utf-8\")\n",
    "    print(f\" env  {ENV_PATH}\")\n",
    "    return ENV_PATH\n",
    "ENV = load_env()\n",
    "GATEWAY_BASE = ENV.get(\"base\") or os.getenv(\"GATEWAY_BASE\", \"http://127.0.0.1:9910\")\n",
    "ADMIN_TOKEN = os.getenv(\"GATEWAY_ADMIN_TOKEN\", \"\").strip()\n",
    "CURRENT_SCHEMA = \"1.1.0\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 736,
   "id": "4f8b258a-0eb9-43f6-b403-5d219f58bb38",
   "metadata": {},
   "outputs": [],
   "source": [
    "SCHEMA_DIR = ART_DIR / \"schemas\"\n",
    "SCHEMA_DIR.mkdir(parents=True, exist_ok=True)\n",
    "def _jsonschema_validate(doc: Dict[str, Any], schema_name: str) -> bool:\n",
    "    \"\"\"\n",
    "    Best-effort jsonschema. If jsonschema not installed  warn but continue.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        import jsonschema\n",
    "    except Exception:\n",
    "        print(\" jsonschema not installed, skipping strict validation\")\n",
    "        return True\n",
    "    schema_path = SCHEMA_DIR / f\"{schema_name}.schema.json\"\n",
    "    if not schema_path.exists():\n",
    "        print(f\" no schema for {schema_name}, skipping\")\n",
    "        return True\n",
    "    try:\n",
    "        schema = json.loads(schema_path.read_text(encoding=\"utf-8\"))\n",
    "        jsonschema.validate(doc, schema)\n",
    "        return True\n",
    "    except Exception as e:\n",
    "        print(f\" schema validation failed for {schema_name}: {e}\")\n",
    "        return False\n",
    "def safe_write_json(path: Path, data: Dict[str, Any], *, schema_name: Optional[str] = None) -> bool:\n",
    "    \"\"\"\n",
    "    validate  write\n",
    "    \"\"\"\n",
    "    if schema_name:\n",
    "        ok = _jsonschema_validate(data, schema_name)\n",
    "        if not ok:\n",
    "            print(f\" NOT writing {path.name} due to schema error\")\n",
    "            return False\n",
    "    meta = data.get(\"_meta\") or {}\n",
    "    meta[\"schema_version\"] = CURRENT_SCHEMA\n",
    "    meta[\"written_at\"] = int(time.time())\n",
    "    data[\"_meta\"] = meta\n",
    "    path.write_text(json.dumps(data, indent=2), encoding=\"utf-8\")\n",
    "    print(f\" wrote (schema-enforced)  {path}\")\n",
    "    return True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 737,
   "id": "ca08e079-9f35-4206-b3ed-d61069df6bec",
   "metadata": {},
   "outputs": [],
   "source": [
    "POLICY_PATH = ART_DIR / \"gateway_chaos_policy.json\"\n",
    "DEFAULT_POLICY = {\n",
    "    \"ts\": int(time.time()),\n",
    "    \"allowed_flags\": [\n",
    "        \"force_bm25\",\n",
    "        \"disable_ce\",\n",
    "        \"verifier_v2\",\n",
    "        \"dense_weight\",\n",
    "        \"rerank_top_k\",\n",
    "    ],\n",
    "    \"blocked_flags\": [\n",
    "        \"disable_payments\",\n",
    "        \"wipe_index\",\n",
    "        \"drop_tenants\",\n",
    "    ],\n",
    "    \"max_ttl_s\": 30 * 60,\n",
    "}\n",
    "def load_policy() -> Dict[str, Any]:\n",
    "    if POLICY_PATH.exists():\n",
    "        try:\n",
    "            return json.loads(POLICY_PATH.read_text(encoding=\"utf-8\"))\n",
    "        except Exception as e:\n",
    "            print(f\" failed to read policy: {e}\")\n",
    "    POLICY_PATH.write_text(json.dumps(DEFAULT_POLICY, indent=2), encoding=\"utf-8\")\n",
    "    return DEFAULT_POLICY\n",
    "CHAOS_POLICY = load_policy()\n",
    "def chaos_allowed(flags: Dict[str, Any], source: str = \"notebook\") -> Tuple[Dict[str, Any], List[str]]:\n",
    "    \"\"\"\n",
    "    returns (filtered_flags, rejected_list)\n",
    "    \"\"\"\n",
    "    allowed = set(CHAOS_POLICY.get(\"allowed_flags\") or [])\n",
    "    blocked = set(CHAOS_POLICY.get(\"blocked_flags\") or [])\n",
    "    out: Dict[str, Any] = {}\n",
    "    rejected: List[str] = []\n",
    "    for k, v in (flags or {}).items():\n",
    "        if k in blocked:\n",
    "            rejected.append(k)\n",
    "            continue\n",
    "        if allowed and k not in allowed:\n",
    "            rejected.append(k)\n",
    "            continue\n",
    "        out[k] = v\n",
    "    return out, rejected\n",
    "def apply_chaos_with_policy(flags: Dict[str, Any], *, ns: Optional[str] = None, reason: str = \"manual\") -> Dict[str, Any]:\n",
    "    \"\"\"\n",
    "    wrapper over /admin/chaos/set that enforces policy\n",
    "    \"\"\"\n",
    "    good, bad = chaos_allowed(flags)\n",
    "    payload = dict(good)\n",
    "    payload[\"_reason\"] = reason\n",
    "    payload[\"_ts\"] = int(time.time())\n",
    "    if ns:\n",
    "        target = f\"{GATEWAY_BASE}/admin/chaos/set?ns={ns}\"\n",
    "    else:\n",
    "        target = f\"{GATEWAY_BASE}/admin/chaos/set\"\n",
    "    import requests\n",
    "    h = {\"content-type\": \"application/json\"}\n",
    "    if ADMIN_TOKEN:\n",
    "        h[\"x-admin-token\"] = ADMIN_TOKEN\n",
    "    try:\n",
    "        r = requests.post(target, json=good, headers=h, timeout=2.5)\n",
    "        res = {\n",
    "            \"ok\": 200 <= r.status_code < 300,\n",
    "            \"status\": r.status_code,\n",
    "            \"filtered_out\": bad,\n",
    "            \"applied\": good,\n",
    "        }\n",
    "    except Exception as e:\n",
    "        res = {\n",
    "            \"ok\": False,\n",
    "            \"error\": str(e),\n",
    "            \"filtered_out\": bad,\n",
    "            \"applied\": good,\n",
    "        }\n",
    "    audit = {\n",
    "        \"ts\": int(time.time()),\n",
    "        \"kind\": \"chaos.apply\",\n",
    "        \"reason\": reason,\n",
    "        \"ns\": ns,\n",
    "        \"applied\": good,\n",
    "        \"rejected\": bad,\n",
    "    }\n",
    "    (ART_DIR / \"gateway_chaos_audit.jsonl\").open(\"a\", encoding=\"utf-8\").write(json.dumps(audit) + \"\\n\")\n",
    "    print(f\" chaos-policy  applied {len(good)} flag(s), rejected {bad}\")\n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 738,
   "id": "ede604d3-6f5f-4ee6-9e7b-217c1c71bbaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "CONTROL_PATH = ART_DIR / \"gateway_control.json\"\n",
    "CONTROL_BLUE = ART_DIR / \"gateway_control.blue.json\"\n",
    "CONTROL_GREEN = ART_DIR / \"gateway_control.green.json\"\n",
    "def export_active_control(path: Optional[Path] = None) -> Path:\n",
    "    ctrl = {}\n",
    "    if \"load_active_control\" in globals():\n",
    "        ctrl = globals()[\"load_active_control\"]() or {}\n",
    "    elif CONTROL_PATH.exists():\n",
    "        ctrl = json.loads(CONTROL_PATH.read_text(encoding=\"utf-8\"))\n",
    "    else:\n",
    "        ctrl = {\"ts\": int(time.time()), \"chaos\": {}, \"rl_weights\": {}}\n",
    "    if path is None:\n",
    "        path = ART_DIR / f\"gateway_control_export_{int(time.time())}.json\"\n",
    "    safe_write_json(path, ctrl, schema_name=\"gateway_control_plane\")\n",
    "    print(f\" exported active control  {path}\")\n",
    "    return path\n",
    "def import_control_from_file(path: Path, *, target: str = \"blue\", switch: bool = True) -> Path:\n",
    "    raw = json.loads(path.read_text(encoding=\"utf-8\"))\n",
    "    ok = _jsonschema_validate(raw, \"gateway_control_plane\")\n",
    "    if not ok:\n",
    "        raise RuntimeError(f\"invalid control file: {path}\")\n",
    "    tgt = CONTROL_BLUE if target == \"blue\" else CONTROL_GREEN\n",
    "    safe_write_json(tgt, raw, schema_name=\"gateway_control_plane\")\n",
    "    print(f\" imported control  {tgt}\")\n",
    "    if switch and \"set_active_control\" in globals():\n",
    "        globals()[\"set_active_control\"](target)\n",
    "    return tgt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 739,
   "id": "f7be7874-f09e-44dd-9a81-9e112ece5455",
   "metadata": {},
   "outputs": [],
   "source": [
    "def notebook_bootstrap(profile: str = \"dev\") -> Dict[str, Any]:\n",
    "    \"\"\"\n",
    "    create minimal set of artifacts so SRE/dev can start without running 19 cells\n",
    "    \"\"\"\n",
    "    out: Dict[str, Any] = {\"ts\": int(time.time()), \"profile\": profile, \"steps\": []}\n",
    "    if not ENV_PATH.exists():\n",
    "        write_env(DEFAULT_ENV)\n",
    "        out[\"steps\"].append(\"env\")\n",
    "    if not CONTROL_PATH.exists():\n",
    "        base_ctrl = {\n",
    "            \"ts\": int(time.time()),\n",
    "            \"schema\": CURRENT_SCHEMA,\n",
    "            \"chaos\": {},\n",
    "            \"rl_weights\": {},\n",
    "            \"tenants\": {},\n",
    "        }\n",
    "        safe_write_json(CONTROL_PATH, base_ctrl, schema_name=\"gateway_control_plane\")\n",
    "        out[\"steps\"].append(\"control\")\n",
    "    if \"write_default_slos\" in globals():\n",
    "        p = globals()[\"write_default_slos\"]()\n",
    "        out[\"steps\"].append(f\"slo:{p}\")\n",
    "    if \"write_helm_values\" in globals():\n",
    "        p = globals()[\"write_helm_values\"]()\n",
    "        out[\"steps\"].append(f\"helm:{p}\")\n",
    "    if (ART_DIR / \"prometheus-gateway-rules.yml\").exists():\n",
    "        out[\"steps\"].append(\"prom-rules:exists\")\n",
    "    out_path = ART_DIR / \"gateway_bootstrap.json\"\n",
    "    out_path.write_text(json.dumps(out, indent=2), encoding=\"utf-8\")\n",
    "    print(f\" bootstrap  {out_path}\")\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 740,
   "id": "1d7752b3-06af-4865-9394-8d92459fd2bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "RATE_STATE: Dict[str, List[float]] = {}\n",
    "RATE_LIMIT_RPS = 3.0       \n",
    "RATE_LIMIT_BURST = 10      \n",
    "RATE_WINDOW_S = 5.0\n",
    "def _rate_ok(key: str) -> bool:\n",
    "    now = time.time()\n",
    "    hist = RATE_STATE.setdefault(key, [])\n",
    "    hist = [t for t in hist if now - t < RATE_WINDOW_S]\n",
    "    RATE_STATE[key] = hist\n",
    "    if len(hist) >= RATE_LIMIT_BURST:\n",
    "        return False\n",
    "    hist.append(now)\n",
    "    RATE_STATE[key] = hist\n",
    "    return True\n",
    "def wrap_controller_handler(handler_cls):\n",
    "    \"\"\"\n",
    "    monkey-patch style: if you already defined ControllerHandler earlier,\n",
    "    you can re-serve with this wrapped version\n",
    "    \"\"\"\n",
    "    class RLHandler(handler_cls):\n",
    "        def do_GET(self_inner):\n",
    "            ip = self_inner.client_address[0] if self_inner.client_address else \"unknown\"\n",
    "            if not _rate_ok(f\"GET:{ip}\"):\n",
    "                self_inner.send_response(429)\n",
    "                self_inner.end_headers()\n",
    "                self_inner.wfile.write(b'{\"error\":\"rate-limited\"}')\n",
    "                return\n",
    "            return super(RLHandler, self_inner).do_GET()\n",
    "        def do_POST(self_inner):\n",
    "            ip = self_inner.client_address[0] if self_inner.client_address else \"unknown\"\n",
    "            if not _rate_ok(f\"POST:{ip}\"):\n",
    "                self_inner.send_response(429)\n",
    "                self_inner.end_headers()\n",
    "                self_inner.wfile.write(b'{\"error\":\"rate-limited\"}')\n",
    "                return\n",
    "            return super(RLHandler, self_inner).do_POST()\n",
    "    return RLHandler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 741,
   "id": "76428f67-dbad-4049-9335-cfd48e205753",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " continuation (env + chaos-policy + control import/export + bootstrap + http-rl) loaded.\n",
      "Try:\n",
      "  gw_cli5('env.show')\n",
      "  gw_cli5('bootstrap')\n",
      "  gw_cli5('control.export')\n",
      "  gw_cli5('chaos.apply', flags={'force_bm25': True})\n",
      "  # if you want rate-limited HTTP admin:\n",
      "  # from __main__ import ControllerHandler; serve_controller_http(9926, handler=wrap_controller_handler(ControllerHandler))  # adapt to your runner\n"
     ]
    }
   ],
   "source": [
    "def gw_cli5(cmd: str, **kwargs) -> None:\n",
    "    cmd = cmd.strip().lower()\n",
    "    if cmd == \"env.show\":\n",
    "        print(json.dumps(load_env(), indent=2))\n",
    "    elif cmd == \"env.write\":\n",
    "        e = kwargs.get(\"env\") or {}\n",
    "        write_env(e)\n",
    "    elif cmd == \"control.export\":\n",
    "        export_active_control()\n",
    "    elif cmd == \"control.import\":\n",
    "        src = kwargs.get(\"path\")\n",
    "        if not src:\n",
    "            print(\" need path=/path/to/control.json\")\n",
    "            return\n",
    "        import_control_from_file(Path(src), target=kwargs.get(\"target\", \"blue\"), switch=kwargs.get(\"switch\", True))\n",
    "    elif cmd == \"bootstrap\":\n",
    "        notebook_bootstrap()\n",
    "    elif cmd == \"chaos.apply\":\n",
    "        flags = kwargs.get(\"flags\") or {\"force_bm25\": True}\n",
    "        ns = kwargs.get(\"ns\")\n",
    "        apply_chaos_with_policy(flags, ns=ns, reason=\"cli\")\n",
    "    else:\n",
    "        print(\" unknown cmd. try:\")\n",
    "        print(\"  gw_cli5('env.show')\")\n",
    "        print(\"  gw_cli5('control.export')\")\n",
    "        print(\"  gw_cli5('control.import', path='/tmp/art/foo.json')\")\n",
    "        print(\"  gw_cli5('bootstrap')\")\n",
    "        print(\"  gw_cli5('chaos.apply', flags={'force_bm25': True})\")\n",
    "if ENV.get(\"env\") == \"dev\" and not (ART_DIR / \"gateway_bootstrap.json\").exists():\n",
    "    notebook_bootstrap(\"dev\")\n",
    "print(\"\\n continuation (env + chaos-policy + control import/export + bootstrap + http-rl) loaded.\")\n",
    "print(\"Try:\")\n",
    "print(\"  gw_cli5('env.show')\")\n",
    "print(\"  gw_cli5('bootstrap')\")\n",
    "print(\"  gw_cli5('control.export')\")\n",
    "print(\"  gw_cli5('chaos.apply', flags={'force_bm25': True})\")\n",
    "print(\"  # if you want rate-limited HTTP admin:\")\n",
    "print(\"  # from __main__ import ControllerHandler; serve_controller_http(9926, handler=wrap_controller_handler(ControllerHandler))  # adapt to your runner\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 742,
   "id": "42cf634a-afb6-4d86-88e1-4b4394bfd1be",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " continuation (burn-rate + fixtures + secrets + gitops + replay + quickstart) using ART_DIR=\\tmp\\art\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "import time\n",
    "import secrets\n",
    "from pathlib import Path\n",
    "from typing import Any, Dict, List, Optional, Tuple\n",
    "ART_DIR = Path(os.getenv(\"ART_DIR\", \"/tmp/art\"))\n",
    "ART_DIR.mkdir(parents=True, exist_ok=True)\n",
    "print(f\" continuation (burn-rate + fixtures + secrets + gitops + replay + quickstart) using ART_DIR={ART_DIR}\")\n",
    "CURRENT_SCHEMA = \"1.1.0\"\n",
    "GATEWAY_BASE = os.getenv(\"GATEWAY_BASE\", \"http://127.0.0.1:9910\")\n",
    "ADMIN_TOKEN = os.getenv(\"GATEWAY_ADMIN_TOKEN\", \"\").strip()\n",
    "E2E_REPORT = ART_DIR / \"gateway_e2e_report.json\"\n",
    "SLA_RUNS = ART_DIR / \"gateway_sla_runs.json\"\n",
    "PROM_TS = ART_DIR / \"gateway_prom_timeseries.json\"\n",
    "SLO_EVAL = ART_DIR / \"gateway_slo_eval.json\"\n",
    "SLO_BURN = ART_DIR / \"gateway_slo_burn.json\"\n",
    "FIXTURES_DIR = ART_DIR / \"fixtures\"\n",
    "FIXTURES_DIR.mkdir(parents=True, exist_ok=True)\n",
    "SECRET_VER_PATH = ART_DIR / \"gateway_secret_version.json\"\n",
    "K8S_DIR = ART_DIR / \"deploy\"\n",
    "K8S_DIR.mkdir(parents=True, exist_ok=True)\n",
    "REPLAY_SRC_LOG = ART_DIR / \"gateway_admin_ops.log\"\n",
    "REPLAY_SCRIPT = ART_DIR / \"gateway_query_replay.py\"\n",
    "QUICKSTART_PATH = ART_DIR / \"gateway_quickstart.py\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 743,
   "id": "d4591194-bab7-449b-a107-bdc817c718f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _load_json_safe(p: Path) -> Any:\n",
    "    if not p.exists():\n",
    "        return None\n",
    "    try:\n",
    "        return json.loads(p.read_text(encoding=\"utf-8\"))\n",
    "    except Exception:\n",
    "        return None\n",
    "def compute_burn_rates(\n",
    "    error_budget: float = 0.05,\n",
    "    fast_window_s: int = 5 * 60,\n",
    "    slow_window_s: int = 30 * 60,\n",
    ") -> Path:\n",
    "    \"\"\"\n",
    "    Approximate burn-rate calculator.\n",
    "    We look at:\n",
    "      - gateway_sla_runs.json  preferred\n",
    "      - else gateway_prom_timeseries.json\n",
    "      - else e2e_report as fallback (degenerate)\n",
    "    Output follows SRE-ish shape:\n",
    "      {\n",
    "        \"fast\": {\"window_s\": 300, \"errors\": X, \"requests\": Y, \"burn_rate\": ...},\n",
    "        \"slow\": {...},\n",
    "        \"ts\": ...\n",
    "      }\n",
    "    \"\"\"\n",
    "    now = int(time.time())\n",
    "    sla = _load_json_safe(SLA_RUNS) or {}\n",
    "    prom = _load_json_safe(PROM_TS) or {}\n",
    "    e2e = _load_json_safe(E2E_REPORT) or {}\n",
    "    def _calc_from_sla(win_s: int) -> Tuple[int, int]:\n",
    "        runs = sla.get(\"runs\") or []\n",
    "        cutoff = now - win_s\n",
    "        errs = 0\n",
    "        total = 0\n",
    "        for r in runs:\n",
    "            ts = r.get(\"ts\") or 0\n",
    "            if ts < cutoff:\n",
    "                continue\n",
    "            total += r.get(\"count\", 0)\n",
    "            errs += r.get(\"errors\", 0)\n",
    "        return errs, total\n",
    "    def _calc_from_e2e(win_s: int) -> Tuple[int, int]:\n",
    "        probes = e2e.get(\"probes\") or []\n",
    "        errs = sum(1 for p in probes if not p.get(\"ok\"))\n",
    "        total = len(probes)\n",
    "        return errs, total\n",
    "    def _calc(win_s: int) -> Tuple[int, int]:\n",
    "        if sla.get(\"runs\"):\n",
    "            return _calc_from_sla(win_s)\n",
    "        return _calc_from_e2e(win_s)\n",
    "    fast_err, fast_total = _calc(fast_window_s)\n",
    "    slow_err, slow_total = _calc(slow_window_s)\n",
    "    def _burn(err: int, tot: int) -> float:\n",
    "        if tot == 0:\n",
    "            return 0.0\n",
    "        return round((err / tot) / error_budget, 3)\n",
    "    out = {\n",
    "        \"ts\": now,\n",
    "        \"schema\": CURRENT_SCHEMA,\n",
    "        \"error_budget\": error_budget,\n",
    "        \"fast\": {\n",
    "            \"window_s\": fast_window_s,\n",
    "            \"errors\": fast_err,\n",
    "            \"total\": fast_total,\n",
    "            \"burn_rate\": _burn(fast_err, fast_total),\n",
    "        },\n",
    "        \"slow\": {\n",
    "            \"window_s\": slow_window_s,\n",
    "            \"errors\": slow_err,\n",
    "            \"total\": slow_total,\n",
    "            \"burn_rate\": _burn(slow_err, slow_total),\n",
    "        },\n",
    "    }\n",
    "    SLO_BURN.write_text(json.dumps(out, indent=2), encoding=\"utf-8\")\n",
    "    print(f\" SLO burn  {SLO_BURN}\")\n",
    "    return SLO_BURN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 744,
   "id": "14714780-81a6-4b69-a234-9259b3d7a098",
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_fixtures() -> Dict[str, str]:\n",
    "    \"\"\"\n",
    "    Drop synthetic but valid fixtures so CI can run without real gateway.\n",
    "    \"\"\"\n",
    "    created: Dict[str, str] = {}\n",
    "    f_control = FIXTURES_DIR / \"control.json\"\n",
    "    if not f_control.exists():\n",
    "        ctrl = {\n",
    "            \"ts\": int(time.time()),\n",
    "            \"schema\": CURRENT_SCHEMA,\n",
    "            \"chaos\": {\"force_bm25\": True},\n",
    "            \"rl_weights\": {\"dense\": 0.6, \"bm25\": 0.4},\n",
    "            \"tenants\": {\n",
    "                \"tenant-acme\": {\n",
    "                    \"chaos\": {\"disable_ce\": True},\n",
    "                    \"rl_weights\": {\"dense\": 0.4, \"bm25\": 0.6},\n",
    "                    \"ratelimits\": {\"qps\": 10, \"burst\": 20},\n",
    "                }\n",
    "            },\n",
    "            \"_meta\": {\"source\": \"fixtures\"},\n",
    "        }\n",
    "        f_control.write_text(json.dumps(ctrl, indent=2), encoding=\"utf-8\")\n",
    "        created[\"control\"] = str(f_control)\n",
    "    f_runs = FIXTURES_DIR / \"runs_agg.json\"\n",
    "    if not f_runs.exists():\n",
    "        runs = {\n",
    "            \"runs\": [\n",
    "                {\"ts\": int(time.time()) - 60, \"p95_ms\": 850, \"count\": 120, \"errors\": 1},\n",
    "                {\"ts\": int(time.time()) - 120, \"p95_ms\": 1100, \"count\": 140, \"errors\": 4},\n",
    "            ]\n",
    "        }\n",
    "        f_runs.write_text(json.dumps(runs, indent=2), encoding=\"utf-8\")\n",
    "        created[\"runs_agg\"] = str(f_runs)\n",
    "    f_schema = FIXTURES_DIR / \"schema_report.json\"\n",
    "    if not f_schema.exists():\n",
    "        f_schema.write_text(json.dumps({\"problems\": []}, indent=2), encoding=\"utf-8\")\n",
    "        created[\"schema\"] = str(f_schema)\n",
    "    f_prom = FIXTURES_DIR / \"prom_ts.json\"\n",
    "    if not f_prom.exists():\n",
    "        f_prom.write_text(\n",
    "            json.dumps(\n",
    "                {\n",
    "                    \"metrics\": [\n",
    "                        {\"name\": \"gateway_answer_latency_ms\", \"ts\": int(time.time()), \"p95\": 1.1}\n",
    "                    ]\n",
    "                },\n",
    "                indent=2,\n",
    "            ),\n",
    "            encoding=\"utf-8\",\n",
    "        )\n",
    "        created[\"prom_ts\"] = str(f_prom)\n",
    "    print(f\" fixtures  {created}\")\n",
    "    return created"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 745,
   "id": "d8b0c55f-94ff-4911-b55f-cd6eb89df23d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_secret_version() -> Dict[str, Any]:\n",
    "    if SECRET_VER_PATH.exists():\n",
    "        try:\n",
    "            return json.loads(SECRET_VER_PATH.read_text(encoding=\"utf-8\"))\n",
    "        except Exception:\n",
    "            pass\n",
    "    base = {\n",
    "        \"ts\": int(time.time()),\n",
    "        \"version\": 1,\n",
    "        \"token\": secrets.token_hex(16),\n",
    "        \"rotated_by\": \"notebook\",\n",
    "    }\n",
    "    SECRET_VER_PATH.write_text(json.dumps(base, indent=2), encoding=\"utf-8\")\n",
    "    print(f\" secret-version init  {SECRET_VER_PATH}\")\n",
    "    return base\n",
    "def rotate_secret(reason: str = \"manual\") -> Dict[str, Any]:\n",
    "    cur = load_secret_version()\n",
    "    nxt = {\n",
    "        \"ts\": int(time.time()),\n",
    "        \"version\": int(cur.get(\"version\", 1)) + 1,\n",
    "        \"token\": secrets.token_hex(16),\n",
    "        \"rotated_by\": os.getenv(\"USER\") or \"notebook\",\n",
    "        \"reason\": reason,\n",
    "    }\n",
    "    SECRET_VER_PATH.write_text(json.dumps(nxt, indent=2), encoding=\"utf-8\")\n",
    "    print(f\" secret rotated  v{nxt['version']}\")\n",
    "    return nxt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 746,
   "id": "611ca921-2597-4754-b85e-5cf33d4bc020",
   "metadata": {},
   "outputs": [],
   "source": [
    "def collect_gitops() -> Path:\n",
    "    \"\"\"\n",
    "    Copy or re-write the already generated K8s / Helm / prom files\n",
    "    into a stable 'deploy' folder.\n",
    "    \"\"\"\n",
    "    wanted = [\n",
    "        ART_DIR / \"k8s-gateway-sidecar.yaml\",\n",
    "        ART_DIR / \"k8s-gateway-agent-cronjob.yaml\",\n",
    "        ART_DIR / \"prometheus-gateway-rules.yml\",\n",
    "        ART_DIR / \"helm-gateway-values.yaml\",\n",
    "        ART_DIR / \"gateway_runbook.md\",\n",
    "    ]\n",
    "    readme = K8S_DIR / \"README.md\"\n",
    "    for src in wanted:\n",
    "        if src.exists():\n",
    "            dst = K8S_DIR / src.name\n",
    "            dst.write_text(src.read_text(encoding=\"utf-8\"), encoding=\"utf-8\")\n",
    "    readme.write_text(\n",
    "        \"\"\"# Gateway GitOps bundle (auto-generated)\n",
    "apply in this order:\n",
    "1. prometheus-gateway-rules.yml (if you manage Prom separately, skip)\n",
    "2. k8s-gateway-sidecar.yaml\n",
    "3. k8s-gateway-agent-cronjob.yaml\n",
    "4. helm-gateway-values.yaml (for Helm-based deploys)\n",
    "\"\"\",\n",
    "        encoding=\"utf-8\",\n",
    "    )\n",
    "    print(f\" gitops  {K8S_DIR}\")\n",
    "    return K8S_DIR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 747,
   "id": "cbac59f9-171c-4fbc-86fd-049a7018f625",
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_replay_script() -> Path:\n",
    "    \"\"\"\n",
    "    Drop a runnable script that replays last N /answer-like admin ops\n",
    "    or just pushes 3 synthetic queries to the gateway.\n",
    "    \"\"\"\n",
    "    src = f\"\"\"#!/usr/bin/env python3\n",
    "import os, json, time, requests\n",
    "from pathlib import Path\n",
    "ART_DIR = Path(os.getenv(\"ART_DIR\", \"{ART_DIR}\"))\n",
    "LOG = ART_DIR / \"gateway_admin_ops.log\"\n",
    "BASE = os.getenv(\"GATEWAY_BASE\", \"{GATEWAY_BASE}\")\n",
    "TOK = os.getenv(\"GATEWAY_ADMIN_TOKEN\", \"\").strip()\n",
    "def _headers():\n",
    "    h = {{\"content-type\": \"application/json\"}}\n",
    "    if TOK:\n",
    "        h[\"x-admin-token\"] = TOK\n",
    "    return h\n",
    "def replay(n=20):\n",
    "    if LOG.exists():\n",
    "        lines = LOG.read_text(encoding=\"utf-8\").splitlines()[-n:]\n",
    "        print(f\"[replay] found {{len(lines)}} admin events, but this script will send synthetic /answer probes instead\")\n",
    "    qs = [\n",
    "        \"what is the gateway topology?\",\n",
    "        \"show me the latency profile\",\n",
    "        \"explain RAG fan-out policies\",\n",
    "    ]\n",
    "    for q in qs:\n",
    "        try:\n",
    "            r = requests.post(f\"{{BASE}}/answer\", json={{\"query\": q}}, headers=_headers(), timeout=3.0)\n",
    "            print(\"[replay]\", q, \"\", r.status_code)\n",
    "        except Exception as e:\n",
    "            print(\"[replay] error:\", e)\n",
    "if __name__ == \"__main__\":\n",
    "    replay()\n",
    "\"\"\"\n",
    "    REPLAY_SCRIPT.write_text(src, encoding=\"utf-8\")\n",
    "    REPLAY_SCRIPT.chmod(0o755)\n",
    "    print(f\" replay script  {REPLAY_SCRIPT}\")\n",
    "    return REPLAY_SCRIPT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 748,
   "id": "5a496fe2-2e72-4a4e-8017-641beebe4735",
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_quickstart() -> Path:\n",
    "    \"\"\"\n",
    "    single-entry script to:\n",
    "      - bootstrap notebook artifacts\n",
    "      - write SLOs\n",
    "      - compute burn\n",
    "      - collect gitops\n",
    "    \"\"\"\n",
    "    src = f\"\"\"#!/usr/bin/env python3\n",
    "import json, os, time\n",
    "from pathlib import Path\n",
    "ART_DIR = Path(os.getenv(\"ART_DIR\", \"{ART_DIR}\"))\n",
    "\n",
    "def main():\n",
    "    # this imports the notebook-namespace funcs if run inside same env\n",
    "    try:\n",
    "        from __main__ import notebook_bootstrap\n",
    "    except Exception:\n",
    "        notebook_bootstrap = None\n",
    "    steps = []\n",
    "    if notebook_bootstrap:\n",
    "        steps.append(\"bootstrap\")\n",
    "        notebook_bootstrap(\"dev\")\n",
    "    # SLOs\n",
    "    try:\n",
    "        from __main__ import write_default_slos, eval_slos\n",
    "        write_default_slos()\n",
    "        eval_slos()\n",
    "        steps.append(\"slo\")\n",
    "    except Exception:\n",
    "        pass\n",
    "    # burn\n",
    "    try:\n",
    "        from __main__ import compute_burn_rates\n",
    "        compute_burn_rates()\n",
    "        steps.append(\"burn\")\n",
    "    except Exception:\n",
    "        pass\n",
    "    # gitops\n",
    "    try:\n",
    "        from __main__ import collect_gitops\n",
    "        collect_gitops()\n",
    "        steps.append(\"gitops\")\n",
    "    except Exception:\n",
    "        pass\n",
    "    out = {{\n",
    "        \"ts\": int(time.time()),\n",
    "        \"steps\": steps,\n",
    "        \"art_dir\": str(ART_DIR),\n",
    "    }}\n",
    "    (ART_DIR / \"gateway_quickstart_report.json\").write_text(json.dumps(out, indent=2), encoding=\"utf-8\")\n",
    "    print(\"[quickstart] done:\", steps)\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n",
    "\"\"\"\n",
    "    QUICKSTART_PATH.write_text(src, encoding=\"utf-8\")\n",
    "    QUICKSTART_PATH.chmod(0o755)\n",
    "    print(f\" quickstart  {QUICKSTART_PATH}\")\n",
    "    return QUICKSTART_PATH"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 749,
   "id": "cc174f18-00e8-4b1f-901d-fe67049a514d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def gw_cli6(cmd: str, **kwargs) -> None:\n",
    "    cmd = cmd.strip().lower()\n",
    "    if cmd == \"slo.burn\":\n",
    "        compute_burn_rates()\n",
    "    elif cmd == \"fixtures.write\":\n",
    "        write_fixtures()\n",
    "    elif cmd == \"secret.rotate\":\n",
    "        reason = kwargs.get(\"reason\", \"manual\")\n",
    "        rotate_secret(reason=reason)\n",
    "    elif cmd == \"gitops\":\n",
    "        collect_gitops()\n",
    "    elif cmd == \"replay.write\":\n",
    "        write_replay_script()\n",
    "    elif cmd == \"quickstart.write\":\n",
    "        write_quickstart()\n",
    "    else:\n",
    "        print(\" unknown cmd. try:\")\n",
    "        print(\"  gw_cli6('slo.burn')\")\n",
    "        print(\"  gw_cli6('fixtures.write')\")\n",
    "        print(\"  gw_cli6('secret.rotate', reason='cron')\")\n",
    "        print(\"  gw_cli6('gitops')\")\n",
    "        print(\"  gw_cli6('replay.write')\")\n",
    "        print(\"  gw_cli6('quickstart.write')\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 750,
   "id": "50c7dd18-ca3d-422d-a49e-7f644e8f222a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " SLO burn  \\tmp\\art\\gateway_slo_burn.json\n",
      " fixtures  {}\n",
      " gitops  \\tmp\\art\\deploy\n",
      " replay script  \\tmp\\art\\gateway_query_replay.py\n",
      " quickstart  \\tmp\\art\\gateway_quickstart.py\n",
      "\n",
      " continuation (burn-rate + fixtures + secrets + gitops + replay + quickstart) loaded.\n",
      "Try:\n",
      "  gw_cli6('slo.burn')\n",
      "  gw_cli6('fixtures.write')\n",
      "  gw_cli6('secret.rotate', reason='rotation')\n",
      "  gw_cli6('gitops')\n",
      "  gw_cli6('replay.write')\n",
      "  gw_cli6('quickstart.write')\n"
     ]
    }
   ],
   "source": [
    "compute_burn_rates()\n",
    "write_fixtures()\n",
    "load_secret_version()\n",
    "collect_gitops()\n",
    "write_replay_script()\n",
    "write_quickstart()\n",
    "print(\"\\n continuation (burn-rate + fixtures + secrets + gitops + replay + quickstart) loaded.\")\n",
    "print(\"Try:\")\n",
    "print(\"  gw_cli6('slo.burn')\")\n",
    "print(\"  gw_cli6('fixtures.write')\")\n",
    "print(\"  gw_cli6('secret.rotate', reason='rotation')\")\n",
    "print(\"  gw_cli6('gitops')\")\n",
    "print(\"  gw_cli6('replay.write')\")\n",
    "print(\"  gw_cli6('quickstart.write')\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 751,
   "id": "c80c696e-8715-40e9-bc52-92ea73e91ca6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " continuation (artifact-index + state-lint + backup/restore + scheduler + docs + remote-report) using ART_DIR=\\tmp\\art\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "import time\n",
    "import tarfile\n",
    "import socket\n",
    "from pathlib import Path\n",
    "from typing import Any, Dict, List, Optional, Tuple, Callable\n",
    "ART_DIR = Path(os.getenv(\"ART_DIR\", \"/tmp/art\"))\n",
    "ART_DIR.mkdir(parents=True, exist_ok=True)\n",
    "print(f\" continuation (artifact-index + state-lint + backup/restore + scheduler + docs + remote-report) using ART_DIR={ART_DIR}\")\n",
    "CURRENT_SCHEMA = \"1.1.0\"\n",
    "HOSTNAME = socket.gethostname()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 752,
   "id": "bd48d13d-8694-4ea8-ba54-1c838560346f",
   "metadata": {},
   "outputs": [],
   "source": [
    "ART_INDEX_PATH = ART_DIR / \"gateway_artifact_index.json\"\n",
    "KNOWN_JSON = {\n",
    "    \"gateway_control.json\": \"gateway_control_plane\",\n",
    "    \"gateway_desired_state.json\": \"gateway_desired_state\",\n",
    "    \"gateway_tenants.json\": \"gateway_tenants\",\n",
    "    \"gateway_rbac.json\": \"gateway_rbac\",\n",
    "    \"gateway_control.blue.json\": \"gateway_control_plane\",\n",
    "    \"gateway_control.green.json\": \"gateway_control_plane\",\n",
    "    \"gateway_slo.json\": None,\n",
    "    \"gateway_slo_eval.json\": None,\n",
    "    \"gateway_slo_burn.json\": None,\n",
    "    \"gateway_e2e_report.json\": None,\n",
    "    \"gateway_disk_alert.json\": None,\n",
    "    \"gateway_janitor.json\": None,\n",
    "    \"gateway_maintenance.json\": None,\n",
    "    \"gateway_env.json\": None,\n",
    "    \"gateway_bootstrap.json\": None,\n",
    "    \"gateway_quickstart_report.json\": None,\n",
    "}\n",
    "SCHEMA_DIR = ART_DIR / \"schemas\"\n",
    "def _jsonschema_validate(doc: Dict[str, Any], schema_name: str) -> bool:\n",
    "    try:\n",
    "        import jsonschema\n",
    "    except Exception:\n",
    "        return True\n",
    "    schema_path = SCHEMA_DIR / f\"{schema_name}.schema.json\"\n",
    "    if not schema_path.exists():\n",
    "        return True\n",
    "    try:\n",
    "        schema = json.loads(schema_path.read_text(encoding=\"utf-8\"))\n",
    "        jsonschema.validate(doc, schema)\n",
    "        return True\n",
    "    except Exception as e:\n",
    "        print(f\" validation failed for {schema_name}: {e}\")\n",
    "        return False\n",
    "def build_artifact_index() -> Path:\n",
    "    \"\"\"\n",
    "    Scan ART_DIR and build a structured index of files, types, sizes, mtimes,\n",
    "    and schema-validation result (for known JSONs).\n",
    "    \"\"\"\n",
    "    items: List[Dict[str, Any]] = []\n",
    "    now = int(time.time())\n",
    "    for p in sorted(ART_DIR.glob(\"**/*\")):\n",
    "        if not p.is_file():\n",
    "            continue\n",
    "        rel = p.relative_to(ART_DIR).as_posix()\n",
    "        ext = p.suffix.lower()\n",
    "        sz = p.stat().st_size\n",
    "        mt = int(p.stat().st_mtime)\n",
    "        entry: Dict[str, Any] = {\n",
    "            \"path\": rel,\n",
    "            \"size\": sz,\n",
    "            \"mtime\": mt,\n",
    "            \"age_s\": now - mt,\n",
    "            \"ext\": ext,\n",
    "        }\n",
    "        if rel in KNOWN_JSON and ext == \".json\":\n",
    "            try:\n",
    "                js = json.loads(p.read_text(encoding=\"utf-8\"))\n",
    "                schema_name = KNOWN_JSON[rel]\n",
    "                if schema_name:\n",
    "                    ok = _jsonschema_validate(js, schema_name)\n",
    "                    entry[\"schema\"] = schema_name\n",
    "                    entry[\"schema_ok\"] = ok\n",
    "                else:\n",
    "                    entry[\"schema\"] = None\n",
    "                    entry[\"schema_ok\"] = True\n",
    "            except Exception as e:\n",
    "                entry[\"schema_ok\"] = False\n",
    "                entry[\"error\"] = str(e)\n",
    "        items.append(entry)\n",
    "    idx = {\n",
    "        \"ts\": now,\n",
    "        \"host\": HOSTNAME,\n",
    "        \"schema\": CURRENT_SCHEMA,\n",
    "        \"count\": len(items),\n",
    "        \"items\": items,\n",
    "    }\n",
    "    ART_INDEX_PATH.write_text(json.dumps(idx, indent=2), encoding=\"utf-8\")\n",
    "    print(f\" artifact index  {ART_INDEX_PATH}\")\n",
    "    return ART_INDEX_PATH"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 753,
   "id": "95069e00-1ffe-4b38-b068-e90660b0ee08",
   "metadata": {},
   "outputs": [],
   "source": [
    "LINT_PATH = ART_DIR / \"gateway_state_lint.json\"\n",
    "def lint_gateway_state() -> Path:\n",
    "    \"\"\"\n",
    "    Run a few consistency checks across the generated files.\n",
    "    - control must exist (or blue/green)\n",
    "    - SLO must exist\n",
    "    - env must exist\n",
    "    - drift file present  ok\n",
    "    \"\"\"\n",
    "    problems: List[str] = []\n",
    "    control_like = [\n",
    "        ART_DIR / \"gateway_control.json\",\n",
    "        ART_DIR / \"gateway_control.blue.json\",\n",
    "        ART_DIR / \"gateway_control.green.json\",\n",
    "    ]\n",
    "    if not any(p.exists() for p in control_like):\n",
    "        problems.append(\"no control-plane file present\")\n",
    "    if not (ART_DIR / \"gateway_env.json\").exists():\n",
    "        problems.append(\"no gateway_env.json present\")\n",
    "    if not (ART_DIR / \"gateway_slo.json\").exists():\n",
    "        problems.append(\"no gateway_slo.json present (run gw_cli4('slo.write'))\")\n",
    "    pend = ART_DIR / \"gateway_pending_ops.jsonl\"\n",
    "    if pend.exists() and pend.stat().st_size > 2 * 1024 * 1024:\n",
    "        problems.append(\"gateway_pending_ops.jsonl > 2MB  run dedupe / replay\")\n",
    "    disk_alert = ART_DIR / \"gateway_disk_alert.json\"\n",
    "    if disk_alert.exists():\n",
    "        try:\n",
    "            da = json.loads(disk_alert.read_text(encoding=\"utf-8\"))\n",
    "            if da.get(\"warn\"):\n",
    "                problems.append(f\"disk usage over threshold: {da.get('total_mb')} MB\")\n",
    "        except Exception:\n",
    "            pass\n",
    "    out = {\n",
    "        \"ts\": int(time.time()),\n",
    "        \"schema\": CURRENT_SCHEMA,\n",
    "        \"problems\": problems,\n",
    "        \"ok\": not problems,\n",
    "    }\n",
    "    LINT_PATH.write_text(json.dumps(out, indent=2), encoding=\"utf-8\")\n",
    "    if problems:\n",
    "        print(f\" lint: {len(problems)} issue(s)  {LINT_PATH}\")\n",
    "    else:\n",
    "        print(f\" lint ok  {LINT_PATH}\")\n",
    "    return LINT_PATH"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 754,
   "id": "5f31161a-2ef1-4608-805a-245f2e730b28",
   "metadata": {},
   "outputs": [],
   "source": [
    "BACKUP_DIR = ART_DIR / \"backups\"\n",
    "BACKUP_DIR.mkdir(parents=True, exist_ok=True)\n",
    "def create_backup(max_keep: int = 5) -> Path:\n",
    "    ts = int(time.time())\n",
    "    tgt = BACKUP_DIR / f\"art-backup-{ts}.tar.gz\"\n",
    "    with tarfile.open(tgt, \"w:gz\") as tar:\n",
    "        for p in ART_DIR.iterdir():\n",
    "            if p.name in (\"backups\",):\n",
    "                continue\n",
    "            tar.add(p, arcname=p.name)\n",
    "    all_bk = sorted(BACKUP_DIR.glob(\"art-backup-*.tar.gz\"), key=lambda p: p.stat().st_mtime, reverse=True)\n",
    "    for extra in all_bk[max_keep:]:\n",
    "        try:\n",
    "            extra.unlink()\n",
    "        except Exception:\n",
    "            pass\n",
    "    print(f\" backup  {tgt}\")\n",
    "    return tgt\n",
    "def restore_backup(path: Path) -> None:\n",
    "    if not path.exists():\n",
    "        raise FileNotFoundError(str(path))\n",
    "    with tarfile.open(path, \"r:gz\") as tar:\n",
    "        tar.extractall(path=ART_DIR)\n",
    "    print(f\" restored backup from {path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 755,
   "id": "3e35abed-5e49-4f3b-b57e-bce245000caa",
   "metadata": {},
   "outputs": [],
   "source": [
    "SCHEDULE_PATH = ART_DIR / \"gateway_schedule.json\"\n",
    "DEFAULT_SCHEDULE = {\n",
    "    \"ts\": int(time.time()),\n",
    "    \"jobs\": [\n",
    "        {\"name\": \"artifact-index\", \"every_s\": 300, \"last_ts\": 0, \"fn\": \"build_artifact_index\"},\n",
    "        {\"name\": \"lint\", \"every_s\": 600, \"last_ts\": 0, \"fn\": \"lint_gateway_state\"},\n",
    "        {\"name\": \"slo-burn\", \"every_s\": 600, \"last_ts\": 0, \"fn\": \"compute_burn_rates\"},\n",
    "    ],\n",
    "}\n",
    "def load_schedule() -> Dict[str, Any]:\n",
    "    if not SCHEDULE_PATH.exists():\n",
    "        SCHEDULE_PATH.write_text(json.dumps(DEFAULT_SCHEDULE, indent=2), encoding=\"utf-8\")\n",
    "        return DEFAULT_SCHEDULE\n",
    "    try:\n",
    "        return json.loads(SCHEDULE_PATH.read_text(encoding=\"utf-8\"))\n",
    "    except Exception:\n",
    "        return DEFAULT_SCHEDULE\n",
    "def save_schedule(sch: Dict[str, Any]) -> None:\n",
    "    sch[\"ts\"] = int(time.time())\n",
    "    SCHEDULE_PATH.write_text(json.dumps(sch, indent=2), encoding=\"utf-8\")\n",
    "def run_scheduler_once() -> Dict[str, Any]:\n",
    "    sch = load_schedule()\n",
    "    now = int(time.time())\n",
    "    ran: List[str] = []\n",
    "    for job in sch.get(\"jobs\", []):\n",
    "        name = job.get(\"name\")\n",
    "        every = int(job.get(\"every_s\", 60))\n",
    "        last = int(job.get(\"last_ts\", 0))\n",
    "        fn_name = job.get(\"fn\")\n",
    "        if now - last >= every:\n",
    "            if fn_name in globals():\n",
    "                try:\n",
    "                    globals()[fn_name]()  \n",
    "                    job[\"last_ts\"] = now\n",
    "                    ran.append(name)\n",
    "                except Exception as e:\n",
    "                    print(f\" scheduler: job {name} failed: {e}\")\n",
    "            else:\n",
    "                print(f\" scheduler: fn {fn_name} not in globals()\")\n",
    "    save_schedule(sch)\n",
    "    rep = {\"ts\": now, \"ran\": ran}\n",
    "    print(f\" scheduler tick  {rep}\")\n",
    "    return rep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 756,
   "id": "1a8762c6-f1be-45df-9f58-5a29595e4a5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "DOC_PATH = ART_DIR / \"gateway_docs.md\"\n",
    "def write_docs() -> Path:\n",
    "    idx = _load_json_if_exists(ART_INDEX_PATH) or {}\n",
    "    lint = _load_json_if_exists(LINT_PATH) or {}\n",
    "    slo = _load_json_if_exists(ART_DIR / \"gateway_slo.json\") or {}\n",
    "    slo_eval = _load_json_if_exists(ART_DIR / \"gateway_slo_eval.json\") or {}\n",
    "    lines: List[str] = []\n",
    "    lines.append(\"# Gateway Notebook Docs (auto-generated)\")\n",
    "    lines.append(\"\")\n",
    "    lines.append(f\"- Generated at: {time.ctime()}\")\n",
    "    lines.append(f\"- Host: `{HOSTNAME}`\")\n",
    "    lines.append(\"\")\n",
    "    lines.append(\"## Artifacts\")\n",
    "    lines.append(f\"- Total artifacts: **{idx.get('count', 'n/a')}**\")\n",
    "    for it in idx.get(\"items\", [])[:40]:  # cap\n",
    "        lines.append(f\"  - `{it['path']}` ({it['size']} bytes)\")\n",
    "    lines.append(\"\")\n",
    "    lines.append(\"## Lint\")\n",
    "    if lint.get(\"ok\"):\n",
    "        lines.append(\"-  State lint: OK\")\n",
    "    else:\n",
    "        lines.append(\"-  State lint: issues detected\")\n",
    "        for pr in lint.get(\"problems\", []):\n",
    "            lines.append(f\"  - {pr}\")\n",
    "    lines.append(\"\")\n",
    "    lines.append(\"## SLOs\")\n",
    "    if slo:\n",
    "        lines.append(\"### Defined\")\n",
    "        lines.append(\"```json\")\n",
    "        lines.append(json.dumps(slo, indent=2))\n",
    "        lines.append(\"```\")\n",
    "    if slo_eval:\n",
    "        lines.append(\"### Last Evaluation\")\n",
    "        lines.append(\"```json\")\n",
    "        lines.append(json.dumps(slo_eval, indent=2))\n",
    "        lines.append(\"```\")\n",
    "    DOC_PATH.write_text(\"\\n\".join(lines), encoding=\"utf-8\")\n",
    "    print(f\" docs  {DOC_PATH}\")\n",
    "    return DOC_PATH\n",
    "def _load_json_if_exists(p: Path) -> Optional[Dict[str, Any]]:\n",
    "    if not p.exists():\n",
    "        return None\n",
    "    try:\n",
    "        return json.loads(p.read_text(encoding=\"utf-8\"))\n",
    "    except Exception:\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 757,
   "id": "12218198-71a0-49ec-8c9e-cd030364a39a",
   "metadata": {},
   "outputs": [],
   "source": [
    "REMOTE_REPORTER = os.getenv(\"GATEWAY_REPORT_WEBHOOK\", \"\").strip()\n",
    "REPORT_PATH = ART_DIR / \"gateway_remote_report.json\"\n",
    "def push_remote_report() -> Dict[str, Any]:\n",
    "    \"\"\"\n",
    "    Send small payload to remote webhook with lint + burn + disk.\n",
    "    If no webhook  just write local file.\n",
    "    \"\"\"\n",
    "    disk_mb = 0.0\n",
    "    for p in ART_DIR.glob(\"**/*\"):\n",
    "        if p.is_file():\n",
    "            disk_mb += p.stat().st_size\n",
    "    disk_mb = round(disk_mb / (1024 * 1024), 2)\n",
    "    lint = _load_json_if_exists(LINT_PATH) or {}\n",
    "    burn = _load_json_if_exists(ART_DIR / \"gateway_slo_burn.json\") or {}\n",
    "    payload = {\n",
    "        \"ts\": int(time.time()),\n",
    "        \"host\": HOSTNAME,\n",
    "        \"disk_mb\": disk_mb,\n",
    "        \"lint_ok\": lint.get(\"ok\", True),\n",
    "        \"lint_problems\": lint.get(\"problems\") or [],\n",
    "        \"burn\": burn,\n",
    "    }\n",
    "    if not REMOTE_REPORTER:\n",
    "        REPORT_PATH.write_text(json.dumps(payload, indent=2), encoding=\"utf-8\")\n",
    "        print(f\" remote-report (local only)  {REPORT_PATH}\")\n",
    "        return payload\n",
    "    try:\n",
    "        import requests\n",
    "        r = requests.post(REMOTE_REPORTER, json=payload, timeout=2.5)\n",
    "        payload[\"status\"] = r.status_code\n",
    "        REPORT_PATH.write_text(json.dumps(payload, indent=2), encoding=\"utf-8\")\n",
    "        print(f\" remote-report  {r.status_code}\")\n",
    "    except Exception as e:\n",
    "        payload[\"error\"] = str(e)\n",
    "        REPORT_PATH.write_text(json.dumps(payload, indent=2), encoding=\"utf-8\")\n",
    "        print(f\" remote-report failed: {e}\")\n",
    "    return payload"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 758,
   "id": "c9e9e8e4-500d-4690-8469-10444294b924",
   "metadata": {},
   "outputs": [],
   "source": [
    "def gw_cli7(cmd: str, **kwargs) -> None:\n",
    "    cmd = cmd.strip().lower()\n",
    "    if cmd == \"index\":\n",
    "        build_artifact_index()\n",
    "    elif cmd == \"lint\":\n",
    "        lint_gateway_state()\n",
    "    elif cmd == \"backup\":\n",
    "        create_backup()\n",
    "    elif cmd == \"restore\":\n",
    "        src = kwargs.get(\"path\")\n",
    "        if not src:\n",
    "            print(\" need path=/tmp/art/backups/....tar.gz\")\n",
    "            return\n",
    "        restore_backup(Path(src))\n",
    "    elif cmd == \"schedule.tick\":\n",
    "        run_scheduler_once()\n",
    "    elif cmd == \"docs\":\n",
    "        write_docs()\n",
    "    elif cmd == \"report\":\n",
    "        push_remote_report()\n",
    "    else:\n",
    "        print(\" unknown cmd. try:\")\n",
    "        print(\"  gw_cli7('index')\")\n",
    "        print(\"  gw_cli7('lint')\")\n",
    "        print(\"  gw_cli7('backup')\")\n",
    "        print(\"  gw_cli7('restore', path='/tmp/art/backups/art-backup-123.tar.gz')\")\n",
    "        print(\"  gw_cli7('schedule.tick')\")\n",
    "        print(\"  gw_cli7('docs')\")\n",
    "        print(\"  gw_cli7('report')\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 759,
   "id": "58fce293-3b96-4ceb-81a3-dfa4bf14d76d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " artifact index  \\tmp\\art\\gateway_artifact_index.json\n",
      " lint ok  \\tmp\\art\\gateway_state_lint.json\n",
      " backup  \\tmp\\art\\backups\\art-backup-1762114755.tar.gz\n",
      " docs  \\tmp\\art\\gateway_docs.md\n",
      " remote-report (local only)  \\tmp\\art\\gateway_remote_report.json\n",
      "\n",
      " continuation (artifact-index + state-lint + backup/restore + scheduler + docs + remote-report) loaded.\n",
      "Try:\n",
      "  gw_cli7('index')\n",
      "  gw_cli7('lint')\n",
      "  gw_cli7('backup')\n",
      "  gw_cli7('schedule.tick')\n",
      "  gw_cli7('docs')\n",
      "  gw_cli7('report')\n"
     ]
    }
   ],
   "source": [
    "build_artifact_index()\n",
    "lint_gateway_state()\n",
    "create_backup(max_keep=3)\n",
    "write_docs()\n",
    "push_remote_report()\n",
    "print(\"\\n continuation (artifact-index + state-lint + backup/restore + scheduler + docs + remote-report) loaded.\")\n",
    "print(\"Try:\")\n",
    "print(\"  gw_cli7('index')\")\n",
    "print(\"  gw_cli7('lint')\")\n",
    "print(\"  gw_cli7('backup')\")\n",
    "print(\"  gw_cli7('schedule.tick')\")\n",
    "print(\"  gw_cli7('docs')\")\n",
    "print(\"  gw_cli7('report')\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 760,
   "id": "23988c0e-667f-49eb-a738-dfa0366407c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " continuation (remote-sync + schema-migrate + tenant-quotas + oncall-inbox + grafana + manifest) using ART_DIR=\\tmp\\art\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "import time\n",
    "import socket\n",
    "from pathlib import Path\n",
    "from typing import Any, Dict, List, Optional, Tuple\n",
    "ART_DIR = Path(os.getenv(\"ART_DIR\", \"/tmp/art\"))\n",
    "ART_DIR.mkdir(parents=True, exist_ok=True)\n",
    "print(f\" continuation (remote-sync + schema-migrate + tenant-quotas + oncall-inbox + grafana + manifest) using ART_DIR={ART_DIR}\")\n",
    "CURRENT_SCHEMA = \"1.1.0\"\n",
    "NEXT_SCHEMA = \"1.2.0\"  \n",
    "HOSTNAME = socket.gethostname()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 761,
   "id": "44af3ecf-7bb3-4eb4-a598-3084c266b003",
   "metadata": {},
   "outputs": [],
   "source": [
    "REMOTE_CONTROL_URL = os.getenv(\"GATEWAY_REMOTE_CONTROL_URL\", \"\").strip()\n",
    "REMOTE_STATE_URL = os.getenv(\"GATEWAY_REMOTE_STATE_URL\", \"\").strip()\n",
    "REMOTE_PUSH_URL = os.getenv(\"GATEWAY_REMOTE_PUSH_URL\", \"\").strip()\n",
    "CONTROL_PATH = ART_DIR / \"gateway_control.json\"\n",
    "CONTROL_BLUE = ART_DIR / \"gateway_control.blue.json\"\n",
    "CONTROL_GREEN = ART_DIR / \"gateway_control.green.json\"\n",
    "ENV_PATH = ART_DIR / \"gateway_env.json\"\n",
    "MANIFEST_PATH = ART_DIR / \"gateway_manifest.json\"\n",
    "GRAFANA_PATH = ART_DIR / \"grafana-gateway-dashboard.json\"\n",
    "ONCALL_INBOX = ART_DIR / \"gateway_oncall_inbox.jsonl\"\n",
    "TENANTS_PATH = ART_DIR / \"gateway_tenants.json\"\n",
    "TENANT_QUOTAS_PATH = ART_DIR / \"gateway_tenant_quotas.json\"\n",
    "def _load_json_safe(p: Path, default: Any = None) -> Any:\n",
    "    if not p.exists():\n",
    "        return default\n",
    "    try:\n",
    "        return json.loads(p.read_text(encoding=\"utf-8\"))\n",
    "    except Exception:\n",
    "        return default\n",
    "def _http_json(url: str, *, method: str = \"GET\", payload: Optional[Dict[str, Any]] = None, timeout: float = 3.0) -> Dict[str, Any]:\n",
    "    import requests\n",
    "    try:\n",
    "        if method.upper() == \"GET\":\n",
    "            r = requests.get(url, timeout=timeout)\n",
    "        else:\n",
    "            r = requests.post(url, json=payload or {}, timeout=timeout)\n",
    "        try:\n",
    "            js = r.json()\n",
    "        except Exception:\n",
    "            js = {\"text\": r.text}\n",
    "        js[\"_status\"] = r.status_code\n",
    "        js[\"_url\"] = url\n",
    "        return js\n",
    "    except Exception as e:\n",
    "        return {\"ok\": False, \"error\": str(e), \"_url\": url, \"_status\": 599}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 762,
   "id": "3496ff4a-82ec-47df-a72c-70e3eabdb76e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def migrate_control_to_1_2_0(ctrl: Dict[str, Any]) -> Dict[str, Any]:\n",
    "    \"\"\"\n",
    "    soft migration:\n",
    "      - ensure top-level 'schema' == 1.2.0\n",
    "      - normalize tenants -> namespaces (but keep tenants for backward compat)\n",
    "      - ensure chaos has 'ttl_s' if policy enforces it\n",
    "    \"\"\"\n",
    "    ctrl = dict(ctrl or {})\n",
    "    meta = ctrl.get(\"_meta\") or {}\n",
    "    meta[\"migrated_at\"] = int(time.time())\n",
    "    meta[\"from_schema\"] = ctrl.get(\"schema\") or CURRENT_SCHEMA\n",
    "    tenants = ctrl.get(\"tenants\") or {}\n",
    "    namespaces = ctrl.get(\"namespaces\") or {}\n",
    "    if tenants and not namespaces:\n",
    "        namespaces = {}\n",
    "        for name, spec in tenants.items():\n",
    "            namespaces[name] = spec\n",
    "    chaos = ctrl.get(\"chaos\") or {}\n",
    "    if chaos and \"_ttl_s\" not in chaos:\n",
    "        chaos[\"_ttl_s\"] = 30 * 60  \n",
    "    ctrl[\"namespaces\"] = namespaces\n",
    "    ctrl[\"tenants\"] = tenants  \n",
    "    ctrl[\"chaos\"] = chaos\n",
    "    ctrl[\"_meta\"] = meta\n",
    "    ctrl[\"schema\"] = NEXT_SCHEMA\n",
    "    return ctrl\n",
    "def migrate_all_known_controls() -> Dict[str, Any]:\n",
    "    touched = []\n",
    "    for p in (CONTROL_PATH, CONTROL_BLUE, CONTROL_GREEN):\n",
    "        if p.exists():\n",
    "            raw = _load_json_safe(p, {})\n",
    "            if not raw:\n",
    "                continue\n",
    "            old_schema = raw.get(\"schema\") or raw.get(\"_meta\", {}).get(\"schema_version\")\n",
    "            if old_schema == NEXT_SCHEMA:\n",
    "                continue\n",
    "            new = migrate_control_to_1_2_0(raw)\n",
    "            p.write_text(json.dumps(new, indent=2), encoding=\"utf-8\")\n",
    "            touched.append(p.name)\n",
    "    out = {\"ts\": int(time.time()), \"migrated\": touched, \"target\": NEXT_SCHEMA}\n",
    "    print(f\" schema-migrate  {out}\")\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 763,
   "id": "4e354aac-8c91-44de-be6a-841b5b4e4d3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def pull_remote_control(url: Optional[str] = None, *, target: str = \"blue\", switch: bool = False) -> Dict[str, Any]:\n",
    "    url = url or REMOTE_CONTROL_URL\n",
    "    if not url:\n",
    "        print(\" no REMOTE_CONTROL_URL set  skipping\")\n",
    "        return {\"ok\": False, \"error\": \"no-url\"}\n",
    "    res = _http_json(url, method=\"GET\")\n",
    "    code = res.get(\"_status\", 0)\n",
    "    if not (200 <= code < 300):\n",
    "        print(f\" remote pull failed  {code}\")\n",
    "        return {\"ok\": False, \"status\": code, \"error\": res.get(\"error\")}\n",
    "    raw = dict(res)\n",
    "    for k in (\"_status\", \"_url\"):\n",
    "        raw.pop(k, None)\n",
    "    migrated = migrate_control_to_1_2_0(raw)\n",
    "    tgt = CONTROL_BLUE if target == \"blue\" else CONTROL_GREEN\n",
    "    tgt.write_text(json.dumps(migrated, indent=2), encoding=\"utf-8\")\n",
    "    print(f\" pulled remote control  {tgt}\")\n",
    "    if switch and \"set_active_control\" in globals():\n",
    "        globals()[\"set_active_control\"](target)\n",
    "    return {\"ok\": True, \"path\": str(tgt), \"switched\": switch}\n",
    "def push_state_bundle(url: Optional[str] = None) -> Dict[str, Any]:\n",
    "    url = url or REMOTE_PUSH_URL\n",
    "    bundle = {\n",
    "        \"ts\": int(time.time()),\n",
    "        \"host\": HOSTNAME,\n",
    "        \"env\": _load_json_safe(ENV_PATH, {}),\n",
    "        \"control\": _load_json_safe(CONTROL_PATH, {}),\n",
    "        \"slo_eval\": _load_json_safe(ART_DIR / \"gateway_slo_eval.json\", {}),\n",
    "        \"lint\": _load_json_safe(ART_DIR / \"gateway_state_lint.json\", {}),\n",
    "    }\n",
    "    if not url:\n",
    "        (ART_DIR / \"gateway_state_bundle.json\").write_text(json.dumps(bundle, indent=2), encoding=\"utf-8\")\n",
    "        print(\" state bundle (local-only)  gateway_state_bundle.json\")\n",
    "        return {\"ok\": True, \"local\": True}\n",
    "    res = _http_json(url, method=\"POST\", payload=bundle)\n",
    "    print(f\" pushed state bundle  {res.get('_status')}\")\n",
    "    return {\"ok\": 200 <= res.get(\"_status\", 0) < 300, \"status\": res.get(\"_status\"), \"resp\": res}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 764,
   "id": "83306dc0-2893-4a49-8eab-196165872017",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_tenants() -> Dict[str, Any]:\n",
    "    return _load_json_safe(TENANTS_PATH, {\"tenants\": {}})\n",
    "def build_tenant_quotas() -> Path:\n",
    "    tenants = load_tenants().get(\"tenants\") or {}\n",
    "    quotas: Dict[str, Dict[str, Any]] = {}\n",
    "    for name, spec in tenants.items():\n",
    "        rl = (spec.get(\"ratelimits\") or {})\n",
    "        qps = rl.get(\"qps\", 5)\n",
    "        burst = rl.get(\"burst\", 10)\n",
    "        quotas[name] = {\n",
    "            \"qps\": qps,\n",
    "            \"burst\": burst,\n",
    "            \"updated_at\": int(time.time()),\n",
    "        }\n",
    "    out = {\n",
    "        \"ts\": int(time.time()),\n",
    "        \"schema\": NEXT_SCHEMA,\n",
    "        \"quotas\": quotas,\n",
    "    }\n",
    "    TENANT_QUOTAS_PATH.write_text(json.dumps(out, indent=2), encoding=\"utf-8\")\n",
    "    print(f\" tenant quotas  {TENANT_QUOTAS_PATH}\")\n",
    "    return TENANT_QUOTAS_PATH\n",
    "def push_tenant_quotas() -> Dict[str, Any]:\n",
    "    quotas = _load_json_safe(TENANT_QUOTAS_PATH, {})\n",
    "    if not quotas:\n",
    "        print(\" no tenant quotas to push\")\n",
    "        return {\"ok\": False, \"error\": \"no-quotas\"}\n",
    "    base = os.getenv(\"GATEWAY_BASE\", \"http://127.0.0.1:9910\")\n",
    "    import requests\n",
    "    url = f\"{base}/admin/tenants/quotas/set\"\n",
    "    h = {\"content-type\": \"application/json\"}\n",
    "    tok = os.getenv(\"GATEWAY_ADMIN_TOKEN\", \"\").strip()\n",
    "    if tok:\n",
    "        h[\"x-admin-token\"] = tok\n",
    "    try:\n",
    "        r = requests.post(url, json=quotas, headers=h, timeout=2.5)\n",
    "        print(f\" pushed tenant quotas  {r.status_code}\")\n",
    "        return {\"ok\": 200 <= r.status_code < 300, \"status\": r.status_code}\n",
    "    except Exception as e:\n",
    "        print(f\" push tenant quotas failed: {e}\")\n",
    "        return {\"ok\": False, \"error\": str(e)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 765,
   "id": "2e6892b0-b4b3-4e3e-8c24-ca526b8b9edb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def oncall_add(summary: str, *, severity: str = \"warn\", data: Optional[Dict[str, Any]] = None) -> None:\n",
    "    rec = {\n",
    "        \"ts\": int(time.time()),\n",
    "        \"summary\": summary,\n",
    "        \"severity\": severity,\n",
    "        \"data\": data or {},\n",
    "        \"host\": HOSTNAME,\n",
    "        \"acked\": False,\n",
    "    }\n",
    "    with ONCALL_INBOX.open(\"a\", encoding=\"utf-8\") as f:\n",
    "        f.write(json.dumps(rec) + \"\\n\")\n",
    "    print(f\" oncall item  {summary}\")\n",
    "def oncall_list(limit: int = 50) -> List[Dict[str, Any]]:\n",
    "    if not ONCALL_INBOX.exists():\n",
    "        return []\n",
    "    out: List[Dict[str, Any]] = []\n",
    "    with ONCALL_INBOX.open(\"r\", encoding=\"utf-8\") as f:\n",
    "        for i, line in enumerate(f):\n",
    "            if not line.strip():\n",
    "                continue\n",
    "            try:\n",
    "                js = json.loads(line)\n",
    "                out.append(js)\n",
    "            except Exception:\n",
    "                pass\n",
    "            if i + 1 >= limit:\n",
    "                break\n",
    "    return out\n",
    "def oncall_ack(idx: int = 0) -> None:\n",
    "    rows = oncall_list(limit=500)\n",
    "    if idx >= len(rows):\n",
    "        print(f\" oncall ack: idx {idx} out of range\")\n",
    "        return\n",
    "    rows[idx][\"acked\"] = True\n",
    "    with ONCALL_INBOX.open(\"w\", encoding=\"utf-8\") as f:\n",
    "        for r in rows:\n",
    "            f.write(json.dumps(r) + \"\\n\")\n",
    "    print(f\" oncall acked idx={idx}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 766,
   "id": "500ad9f5-a1da-4039-ace2-eceb456853f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_grafana_dashboard() -> Path:\n",
    "    \"\"\"\n",
    "    emit a minimal grafana dashboard JSON so SRE can import without hand-crafting it\n",
    "    \"\"\"\n",
    "    dash = {\n",
    "        \"title\": \"Gateway Control / SLO / Controller\",\n",
    "        \"uid\": \"gateway-auto\",\n",
    "        \"schemaVersion\": 37,\n",
    "        \"version\": 1,\n",
    "        \"panels\": [\n",
    "            {\n",
    "                \"type\": \"timeseries\",\n",
    "                \"title\": \"Controller Disk MB\",\n",
    "                \"targets\": [{\"expr\": \"gateway_controller_disk_mb\"}],\n",
    "                \"id\": 1,\n",
    "            },\n",
    "            {\n",
    "                \"type\": \"timeseries\",\n",
    "                \"title\": \"SLO Breaches\",\n",
    "                \"targets\": [{\"expr\": \"gateway_controller_slo_breaches\"}],\n",
    "                \"id\": 2,\n",
    "            },\n",
    "            {\n",
    "                \"type\": \"timeseries\",\n",
    "                \"title\": \"Tenant p95 (ms)\",\n",
    "                \"targets\": [{\"expr\": 'gateway_controller_tenant_p95_ms{tenant!=\"\"}'}],\n",
    "                \"id\": 3,\n",
    "            },\n",
    "            {\n",
    "                \"type\": \"timeseries\",\n",
    "                \"title\": \"Pending Ops File Size (MB)\",\n",
    "                \"targets\": [{\"expr\": \"gateway_pending_ops_size_mb\"}],  \n",
    "                \"id\": 4,\n",
    "            },\n",
    "        ],\n",
    "        \"time\": {\"from\": \"now-6h\", \"to\": \"now\"},\n",
    "    }\n",
    "    GRAFANA_PATH.write_text(json.dumps(dash, indent=2), encoding=\"utf-8\")\n",
    "    print(f\" grafana dashboard  {GRAFANA_PATH}\")\n",
    "    return GRAFANA_PATH"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 767,
   "id": "bd03b44a-5d4d-47a0-ae9e-f5d7366cdfa3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_gateway_manifest() -> Path:\n",
    "    \"\"\"\n",
    "    One place that lists:\n",
    "      - schema versions\n",
    "      - available CLI commands\n",
    "      - generated files\n",
    "    \"\"\"\n",
    "    files = [p.name for p in ART_DIR.iterdir() if p.is_file()]\n",
    "    manifest = {\n",
    "        \"ts\": int(time.time()),\n",
    "        \"host\": HOSTNAME,\n",
    "        \"current_schema\": CURRENT_SCHEMA,\n",
    "        \"next_schema\": NEXT_SCHEMA,\n",
    "        \"generated_files\": files,\n",
    "        \"cli\": [\n",
    "            \"gw_admin_cli(...)\",\n",
    "            \"gw_cli3(...)\",\n",
    "            \"gw_cli4(...)\",\n",
    "            \"gw_cli5(...)\",\n",
    "            \"gw_cli6(...)\",\n",
    "            \"gw_cli7(...)\",\n",
    "            \"gw_cli8(...)\",  \n",
    "        ],\n",
    "        \"remote_control_url\": REMOTE_CONTROL_URL or None,\n",
    "        \"remote_push_url\": REMOTE_PUSH_URL or None,\n",
    "    }\n",
    "    MANIFEST_PATH.write_text(json.dumps(manifest, indent=2), encoding=\"utf-8\")\n",
    "    print(f\" manifest  {MANIFEST_PATH}\")\n",
    "    return MANIFEST_PATH"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 768,
   "id": "9c504ba7-c7f4-4b41-9a91-5128c57065f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_post_scheduler_jobs() -> None:\n",
    "    \"\"\"\n",
    "    can be called from existing scheduler to also do remote report/sync/migrate\n",
    "    \"\"\"\n",
    "    migrate_all_known_controls()\n",
    "    build_tenant_quotas()\n",
    "    write_grafana_dashboard()\n",
    "    write_gateway_manifest()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 769,
   "id": "f96617a9-214b-4369-ac00-baf5b5eec077",
   "metadata": {},
   "outputs": [],
   "source": [
    "def gw_cli8(cmd: str, **kwargs) -> None:\n",
    "    cmd = cmd.strip().lower()\n",
    "    if cmd == \"remote.pull\":\n",
    "        url = kwargs.get(\"url\")\n",
    "        pull_remote_control(url=url, target=kwargs.get(\"target\", \"blue\"), switch=kwargs.get(\"switch\", False))\n",
    "    elif cmd == \"remote.push\":\n",
    "        url = kwargs.get(\"url\")\n",
    "        push_state_bundle(url=url)\n",
    "    elif cmd == \"migrate\":\n",
    "        migrate_all_known_controls()\n",
    "    elif cmd == \"tenant.quotas\":\n",
    "        build_tenant_quotas()\n",
    "        if kwargs.get(\"push\", False):\n",
    "            push_tenant_quotas()\n",
    "    elif cmd == \"oncall.add\":\n",
    "        summary = kwargs.get(\"summary\", \"manual oncall item\")\n",
    "        oncall_add(summary, severity=kwargs.get(\"severity\", \"warn\"), data=kwargs.get(\"data\") or {})\n",
    "    elif cmd == \"oncall.list\":\n",
    "        items = oncall_list()\n",
    "        print(json.dumps(items, indent=2))\n",
    "    elif cmd == \"oncall.ack\":\n",
    "        oncall_ack(kwargs.get(\"idx\", 0))\n",
    "    elif cmd == \"grafana\":\n",
    "        write_grafana_dashboard()\n",
    "    elif cmd == \"manifest\":\n",
    "        write_gateway_manifest()\n",
    "    else:\n",
    "        print(\" unknown cmd. try:\")\n",
    "        print(\"  gw_cli8('remote.pull', url='https://.../control.json', target='blue', switch=True)\")\n",
    "        print(\"  gw_cli8('remote.push', url='https://.../ingest')\")\n",
    "        print(\"  gw_cli8('migrate')\")\n",
    "        print(\"  gw_cli8('tenant.quotas', push=True)\")\n",
    "        print(\"  gw_cli8('oncall.add', summary='latency spike', severity='error')\")\n",
    "        print(\"  gw_cli8('oncall.list')\")\n",
    "        print(\"  gw_cli8('oncall.ack', idx=0)\")\n",
    "        print(\"  gw_cli8('grafana')\")\n",
    "        print(\"  gw_cli8('manifest')\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 770,
   "id": "5b00c668-892c-45a8-a867-ce0bda39a4bd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " schema-migrate  {'ts': 1762114762, 'migrated': ['gateway_control.json'], 'target': '1.2.0'}\n",
      " tenant quotas  \\tmp\\art\\gateway_tenant_quotas.json\n",
      " grafana dashboard  \\tmp\\art\\grafana-gateway-dashboard.json\n",
      " manifest  \\tmp\\art\\gateway_manifest.json\n",
      "\n",
      " continuation (remote-sync + schema-migrate + tenant-quotas + oncall-inbox + grafana + manifest) loaded.\n",
      "Try:\n",
      "  gw_cli8('remote.pull', url='https://example.com/gw/control.json', switch=True)\n",
      "  gw_cli8('remote.push', url='https://example.com/gw/state')\n",
      "  gw_cli8('migrate')\n",
      "  gw_cli8('tenant.quotas', push=True)\n",
      "  gw_cli8('oncall.add', summary='p95 > 1800ms', severity='error')\n",
      "  gw_cli8('grafana')\n",
      "  gw_cli8('manifest')\n"
     ]
    }
   ],
   "source": [
    "migrate_all_known_controls()\n",
    "build_tenant_quotas()\n",
    "write_grafana_dashboard()\n",
    "write_gateway_manifest()\n",
    "print(\"\\n continuation (remote-sync + schema-migrate + tenant-quotas + oncall-inbox + grafana + manifest) loaded.\")\n",
    "print(\"Try:\")\n",
    "print(\"  gw_cli8('remote.pull', url='https://example.com/gw/control.json', switch=True)\")\n",
    "print(\"  gw_cli8('remote.push', url='https://example.com/gw/state')\")\n",
    "print(\"  gw_cli8('migrate')\")\n",
    "print(\"  gw_cli8('tenant.quotas', push=True)\")\n",
    "print(\"  gw_cli8('oncall.add', summary='p95 > 1800ms', severity='error')\")\n",
    "print(\"  gw_cli8('grafana')\")\n",
    "print(\"  gw_cli8('manifest')\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 771,
   "id": "1e5b35a8-4761-4a83-96bf-e026724988be",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " continuation (freeze + promo-gates + canary + retention + dlq + otel + sbom + k6 + n8n-http) using ART_DIR=\\tmp\\art\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "import time\n",
    "import hashlib\n",
    "import socket\n",
    "from pathlib import Path\n",
    "from typing import Any, Dict, List, Optional, Tuple, Callable\n",
    "ART_DIR = Path(os.getenv(\"ART_DIR\", \"/tmp/art\"))\n",
    "ART_DIR.mkdir(parents=True, exist_ok=True)\n",
    "print(f\" continuation (freeze + promo-gates + canary + retention + dlq + otel + sbom + k6 + n8n-http) using ART_DIR={ART_DIR}\")\n",
    "HOSTNAME = socket.gethostname()\n",
    "CURRENT_SCHEMA = \"1.2.0\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 772,
   "id": "fa2c6c1b-3f2b-4ba1-be8e-b9318cb72051",
   "metadata": {},
   "outputs": [],
   "source": [
    "FREEZE_PATH = ART_DIR / \"gateway_freeze.json\"\n",
    "DLQ_PATH = ART_DIR / \"n8n-dlq.jsonl\"\n",
    "PROMOTION_POLICY_PATH = ART_DIR / \"gateway_promotion_policy.json\"\n",
    "CANARY_SLO_PATH = ART_DIR / \"gateway_slo_eval.canary.json\"\n",
    "BASELINE_SLO_PATH = ART_DIR / \"gateway_slo_eval.baseline.json\"\n",
    "OTEL_SPANS_PATH = ART_DIR / \"otel_spans.jsonl\"\n",
    "SBOM_PATH = ART_DIR / \"gateway_sbom.json\"\n",
    "SBOM_SIG_PATH = ART_DIR / \"gateway_sbom.sig\"\n",
    "K6_SCENARIO_PATH = ART_DIR / \"k6-gateway-smoke.js\"\n",
    "GOLDEN_QUERIES_PATH = ART_DIR / \"gateway_golden_queries.json\"\n",
    "GOLDEN_RESULTS_PATH = ART_DIR / \"gateway_golden_results.json\"\n",
    "GOLDEN_DIFF_PATH = ART_DIR / \"gateway_golden_diff.json\"\n",
    "DEFAULT_PROMOTION_POLICY = {\n",
    "    \"ts\": int(time.time()),\n",
    "    \"schema\": CURRENT_SCHEMA,\n",
    "    \"required\": {\n",
    "        \"p95_ms\": 1500.0,\n",
    "        \"error_rate\": 0.03,\n",
    "        \"burn_fast\": 1.0,\n",
    "        \"burn_slow\": 1.0,\n",
    "    },\n",
    "    \"canary_rel_p95\": 1.10,  \n",
    "    \"canary_rel_err\": 1.25,  \n",
    "}\n",
    "def load_freeze() -> Dict[str, Any]:\n",
    "    if not FREEZE_PATH.exists():\n",
    "        return {\"frozen\": False}\n",
    "    try:\n",
    "        return json.loads(FREEZE_PATH.read_text(encoding=\"utf-8\"))\n",
    "    except Exception:\n",
    "        return {\"frozen\": False}\n",
    "def is_frozen() -> bool:\n",
    "    data = load_freeze()\n",
    "    if not data.get(\"frozen\"):\n",
    "        return False\n",
    "    until = data.get(\"until_ts\")\n",
    "    if until and time.time() > until:\n",
    "        try:\n",
    "            FREEZE_PATH.unlink()\n",
    "        except Exception:\n",
    "            pass\n",
    "        return False\n",
    "    return True\n",
    "def set_freeze(reason: str = \"manual\", ttl_s: int = 3600) -> Path:\n",
    "    data = {\n",
    "        \"ts\": int(time.time()),\n",
    "        \"frozen\": True,\n",
    "        \"reason\": reason,\n",
    "        \"until_ts\": int(time.time()) + int(ttl_s),\n",
    "        \"host\": HOSTNAME,\n",
    "    }\n",
    "    FREEZE_PATH.write_text(json.dumps(data, indent=2), encoding=\"utf-8\")\n",
    "    print(f\" gateway frozen  {FREEZE_PATH}\")\n",
    "    return FREEZE_PATH\n",
    "def clear_freeze() -> None:\n",
    "    if FREEZE_PATH.exists():\n",
    "        FREEZE_PATH.unlink()\n",
    "    print(\" freeze cleared\")\n",
    "def dlq_append(kind: str, payload: Dict[str, Any]) -> None:\n",
    "    rec = {\n",
    "        \"ts\": int(time.time()),\n",
    "        \"kind\": kind,\n",
    "        \"payload\": payload,\n",
    "        \"host\": HOSTNAME,\n",
    "    }\n",
    "    with DLQ_PATH.open(\"a\", encoding=\"utf-8\") as f:\n",
    "        f.write(json.dumps(rec) + \"\\n\")\n",
    "    print(f\" DLQ append  {DLQ_PATH} ({kind})\")\n",
    "def dlq_iter(limit: int = 200) -> List[Dict[str, Any]]:\n",
    "    if not DLQ_PATH.exists():\n",
    "        return []\n",
    "    out: List[Dict[str, Any]] = []\n",
    "    with DLQ_PATH.open(\"r\", encoding=\"utf-8\") as f:\n",
    "        for i, line in enumerate(f):\n",
    "            line = line.strip()\n",
    "            if not line:\n",
    "                continue\n",
    "            try:\n",
    "                out.append(json.loads(line))\n",
    "            except Exception:\n",
    "                pass\n",
    "            if i + 1 >= limit:\n",
    "                break\n",
    "    return out\n",
    "def guard_mutating_op(op_name: str, payload: Optional[Dict[str, Any]] = None) -> Tuple[bool, Dict[str, Any]]:\n",
    "    \"\"\"\n",
    "    returns (allowed, err_payload)\n",
    "    \"\"\"\n",
    "    if not is_frozen():\n",
    "        return True, {}\n",
    "    err = {\n",
    "        \"ok\": False,\n",
    "        \"error\": \"gateway is frozen\",\n",
    "        \"op\": op_name,\n",
    "        \"ts\": int(time.time()),\n",
    "    }\n",
    "    dlq_append(f\"frozen.{op_name}\", payload or {})\n",
    "    return False, err"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 773,
   "id": "edfa2a6d-c9b3-4483-ab32-782e31614b6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_promotion_policy() -> Dict[str, Any]:\n",
    "    if PROMOTION_POLICY_PATH.exists():\n",
    "        try:\n",
    "            return json.loads(PROMOTION_POLICY_PATH.read_text(encoding=\"utf-8\"))\n",
    "        except Exception:\n",
    "            pass\n",
    "    PROMOTION_POLICY_PATH.write_text(json.dumps(DEFAULT_PROMOTION_POLICY, indent=2), encoding=\"utf-8\")\n",
    "    return DEFAULT_PROMOTION_POLICY\n",
    "def _load_json(p: Path) -> Optional[Dict[str, Any]]:\n",
    "    if not p.exists():\n",
    "        return None\n",
    "    try:\n",
    "        return json.loads(p.read_text(encoding=\"utf-8\"))\n",
    "    except Exception:\n",
    "        return None\n",
    "def check_promotion_gates(\n",
    "    slo_eval_path: Path = ART_DIR / \"gateway_slo_eval.json\",\n",
    "    burn_path: Path = ART_DIR / \"gateway_slo_burn.json\",\n",
    ") -> Dict[str, Any]:\n",
    "    pol = load_promotion_policy()\n",
    "    req = pol.get(\"required\") or {}\n",
    "    slo = _load_json(slo_eval_path) or {}\n",
    "    burn = _load_json(burn_path) or {}\n",
    "    tenants = slo.get(\"tenants\") or {}\n",
    "    ns_key = \"default\"\n",
    "    if ns_key not in tenants and tenants:\n",
    "        ns_key = list(tenants.keys())[0]\n",
    "    slo_ns = tenants.get(ns_key) or {}\n",
    "    actual_p95 = slo_ns.get(\"actual_p95_ms\") or 999999\n",
    "    actual_err = slo_ns.get(\"actual_error_rate\") or 1.0\n",
    "    fast_br = (burn.get(\"fast\") or {}).get(\"burn_rate\", 0.0)\n",
    "    slow_br = (burn.get(\"slow\") or {}).get(\"burn_rate\", 0.0)\n",
    "    ok = True\n",
    "    reasons: List[str] = []\n",
    "    if actual_p95 > req.get(\"p95_ms\", 1500.0):\n",
    "        ok = False\n",
    "        reasons.append(f\"p95 {actual_p95} > {req.get('p95_ms')}\")\n",
    "    if actual_err > req.get(\"error_rate\", 0.03):\n",
    "        ok = False\n",
    "        reasons.append(f\"error_rate {actual_err} > {req.get('error_rate')}\")\n",
    "    if fast_br > req.get(\"burn_fast\", 1.0):\n",
    "        ok = False\n",
    "        reasons.append(f\"fast burn {fast_br} > {req.get('burn_fast')}\")\n",
    "    if slow_br > req.get(\"burn_slow\", 1.0):\n",
    "        ok = False\n",
    "        reasons.append(f\"slow burn {slow_br} > {req.get('burn_slow')}\")\n",
    "    res = {\n",
    "        \"ts\": int(time.time()),\n",
    "        \"ok\": ok,\n",
    "        \"reasons\": reasons,\n",
    "        \"slo_ns\": slo_ns,\n",
    "        \"burn\": burn,\n",
    "    }\n",
    "    (ART_DIR / \"gateway_promotion_gate.json\").write_text(json.dumps(res, indent=2), encoding=\"utf-8\")\n",
    "    if not ok and \"oncall_add\" in globals():\n",
    "        globals()[\"oncall_add\"](\"promotion blocked: \" + \"; \".join(reasons), severity=\"error\", data=res)\n",
    "    print(f\" promotion-gate  {res}\")\n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 774,
   "id": "d2ad6761-40c6-46aa-a189-c921d5f9a773",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_as_baseline(from_path: Path = ART_DIR / \"gateway_slo_eval.json\") -> Path:\n",
    "    if not from_path.exists():\n",
    "        raise FileNotFoundError(str(from_path))\n",
    "    BASELINE_SLO_PATH.write_text(from_path.read_text(encoding=\"utf-8\"), encoding=\"utf-8\")\n",
    "    print(f\" baseline SLO saved  {BASELINE_SLO_PATH}\")\n",
    "    return BASELINE_SLO_PATH\n",
    "def canary_eval(\n",
    "    canary_path: Path = CANARY_SLO_PATH,\n",
    "    baseline_path: Path = BASELINE_SLO_PATH,\n",
    ") -> Dict[str, Any]:\n",
    "    pol = load_promotion_policy()\n",
    "    canary = _load_json(canary_path) or {}\n",
    "    base = _load_json(baseline_path) or {}\n",
    "    rel_p95 = pol.get(\"canary_rel_p95\", 1.10)\n",
    "    rel_err = pol.get(\"canary_rel_err\", 1.25)\n",
    "    def _pick(ns_dict: Dict[str, Any]) -> Tuple[float, float]:\n",
    "        if not ns_dict:\n",
    "            return 999999.0, 1.0\n",
    "        if \"default\" in ns_dict:\n",
    "            d = ns_dict[\"default\"]\n",
    "        else:\n",
    "            d = list(ns_dict.values())[0]\n",
    "        return d.get(\"actual_p95_ms\") or 999999.0, d.get(\"actual_error_rate\") or 1.0\n",
    "    can_ns = (canary.get(\"tenants\") or {})\n",
    "    base_ns = (base.get(\"tenants\") or {})\n",
    "    c_p95, c_err = _pick(can_ns)\n",
    "    b_p95, b_err = _pick(base_ns)\n",
    "    ok = True\n",
    "    reasons: List[str] = []\n",
    "    if c_p95 > b_p95 * rel_p95:\n",
    "        ok = False\n",
    "        reasons.append(f\"canary p95 {c_p95} > baseline {b_p95} * {rel_p95}\")\n",
    "    if c_err > b_err * rel_err:\n",
    "        ok = False\n",
    "        reasons.append(f\"canary err {c_err} > baseline {b_err} * {rel_err}\")\n",
    "    res = {\n",
    "        \"ts\": int(time.time()),\n",
    "        \"ok\": ok,\n",
    "        \"reasons\": reasons,\n",
    "        \"canary\": {\"p95\": c_p95, \"err\": c_err},\n",
    "        \"baseline\": {\"p95\": b_p95, \"err\": b_err},\n",
    "    }\n",
    "    (ART_DIR / \"gateway_canary_eval.json\").write_text(json.dumps(res, indent=2), encoding=\"utf-8\")\n",
    "    print(f\" canary-eval  {res}\")\n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 775,
   "id": "40f9f9a7-0924-42af-bc3b-7d1237c8d818",
   "metadata": {},
   "outputs": [],
   "source": [
    "RETENTION_POLICY = {\n",
    "    \"jsonl_max_mb\": 5.0,\n",
    "    \"jsonl_keep_lines\": 5000,\n",
    "    \"max_age_s\": 7 * 24 * 3600,  \n",
    "    \"targets\": [\n",
    "        \"gateway_admin_ops.log\",\n",
    "        \"gateway_chaos_audit.jsonl\",\n",
    "        \"gateway_oncall_inbox.jsonl\",\n",
    "        \"n8n-dlq.jsonl\",\n",
    "        \"otel_spans.jsonl\",\n",
    "    ],\n",
    "}\n",
    "def sweep_retention(policy: Dict[str, Any] = RETENTION_POLICY) -> Dict[str, Any]:\n",
    "    removed: List[str] = []\n",
    "    now = time.time()\n",
    "    for tgt in policy.get(\"targets\", []):\n",
    "        p = ART_DIR / tgt\n",
    "        if not p.exists():\n",
    "            continue\n",
    "        age_s = now - p.stat().st_mtime\n",
    "        if age_s > policy.get(\"max_age_s\", 999999999):\n",
    "            p.unlink(missing_ok=True)\n",
    "            removed.append(f\"{tgt}:age>{policy['max_age_s']}\")\n",
    "            continue\n",
    "        sz_mb = p.stat().st_size / (1024 * 1024)\n",
    "        if sz_mb > policy.get(\"jsonl_max_mb\", 5.0):\n",
    "            keep = policy.get(\"jsonl_keep_lines\", 5000)\n",
    "            lines = p.read_text(encoding=\"utf-8\").splitlines()\n",
    "            lines = lines[-keep:]\n",
    "            p.write_text(\"\\n\".join(lines) + (\"\\n\" if lines else \"\"), encoding=\"utf-8\")\n",
    "            removed.append(f\"{tgt}:shrink{keep}lines\")\n",
    "    rep = {\n",
    "        \"ts\": int(time.time()),\n",
    "        \"removed\": removed,\n",
    "    }\n",
    "    (ART_DIR / \"gateway_retention_report.json\").write_text(json.dumps(rep, indent=2), encoding=\"utf-8\")\n",
    "    print(f\" retention  {rep}\")\n",
    "    return rep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 776,
   "id": "b5636287-3f4e-40b4-a4bb-2d6b6c67fc24",
   "metadata": {},
   "outputs": [],
   "source": [
    "def otel_span(name: str, attrs: Optional[Dict[str, Any]] = None, status: str = \"OK\") -> None:\n",
    "    rec = {\n",
    "        \"ts\": int(time.time()),\n",
    "        \"name\": name,\n",
    "        \"attrs\": attrs or {},\n",
    "        \"status\": status,\n",
    "        \"host\": HOSTNAME,\n",
    "    }\n",
    "    with OTEL_SPANS_PATH.open(\"a\", encoding=\"utf-8\") as f:\n",
    "        f.write(json.dumps(rec) + \"\\n\")\n",
    "    if OTEL_SPANS_PATH.stat().st_size > 2 * 1024 * 1024:\n",
    "        sweep_retention()\n",
    "def write_sbom() -> Path:\n",
    "    items: List[Dict[str, Any]] = []\n",
    "    for p in ART_DIR.glob(\"**/*\"):\n",
    "        if p.is_file():\n",
    "            rel = p.relative_to(ART_DIR).as_posix()\n",
    "            h = hashlib.sha256(p.read_bytes()).hexdigest()\n",
    "            items.append({\n",
    "                \"path\": rel,\n",
    "                \"sha256\": h,\n",
    "                \"size\": p.stat().st_size,\n",
    "            })\n",
    "    sbom = {\n",
    "        \"ts\": int(time.time()),\n",
    "        \"schema\": CURRENT_SCHEMA,\n",
    "        \"host\": HOSTNAME,\n",
    "        \"items\": items,\n",
    "    }\n",
    "    SBOM_PATH.write_text(json.dumps(sbom, indent=2), encoding=\"utf-8\")\n",
    "    print(f\" SBOM  {SBOM_PATH}\")\n",
    "    sig = hashlib.sha256(SBOM_PATH.read_bytes()).hexdigest()\n",
    "    SBOM_SIG_PATH.write_text(json.dumps({\"ts\": int(time.time()), \"sig\": sig}, indent=2), encoding=\"utf-8\")\n",
    "    print(f\" SBOM sig  {SBOM_SIG_PATH}\")\n",
    "    return SBOM_PATH"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 777,
   "id": "e27017be-64cd-4eb9-8ea5-7a762e925c91",
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_k6_scenario(\n",
    "    base: str = os.getenv(\"GATEWAY_BASE\", \"http://127.0.0.1:9910\"),\n",
    "    out: Path = K6_SCENARIO_PATH,\n",
    ") -> Path:\n",
    "    js = f\"\"\"import http from 'k6/http';\n",
    "import {{ sleep }} from 'k6';\n",
    "\n",
    "export const options = {{\n",
    "  vus: 5,\n",
    "  duration: '1m',\n",
    "}};\n",
    "\n",
    "export default function () {{\n",
    "  const url = '{base}/answer';\n",
    "  const payload = JSON.stringify({{ query: 'smoke test', top_k: 2 }});\n",
    "  const params = {{\n",
    "    headers: {{\n",
    "      'Content-Type': 'application/json',\n",
    "      'x-admin-token': '{os.getenv(\"GATEWAY_ADMIN_TOKEN\", \"\")}',\n",
    "    }},\n",
    "  }};\n",
    "  const res = http.post(url, payload, params);\n",
    "  sleep(1);\n",
    "}}\n",
    "\"\"\"\n",
    "    out.write_text(js, encoding=\"utf-8\")\n",
    "    print(f\" k6 scenario  {out}\")\n",
    "    return out\n",
    "def run_golden_tests(\n",
    "    base: str = os.getenv(\"GATEWAY_BASE\", \"http://127.0.0.1:9910\"),\n",
    "    queries_path: Path = GOLDEN_QUERIES_PATH,\n",
    ") -> Path:\n",
    "    if queries_path.exists():\n",
    "        queries = json.loads(queries_path.read_text(encoding=\"utf-8\"))\n",
    "    else:\n",
    "        queries = [\n",
    "            {\"query\": \"what is the gateway topology?\"},\n",
    "            {\"query\": \"show me the latency profile\"},\n",
    "        ]\n",
    "        queries_path.write_text(json.dumps(queries, indent=2), encoding=\"utf-8\")\n",
    "    import requests\n",
    "    results: List[Dict[str, Any]] = []\n",
    "    for q in queries:\n",
    "        t0 = time.time()\n",
    "        try:\n",
    "            r = requests.post(\n",
    "                f\"{base}/answer\",\n",
    "                json=q,\n",
    "                headers={\"content-type\": \"application/json\"},\n",
    "                timeout=3.0,\n",
    "            )\n",
    "            dt = (time.time() - t0) * 1000\n",
    "            results.append({\n",
    "                \"query\": q,\n",
    "                \"status\": r.status_code,\n",
    "                \"latency_ms\": round(dt, 1),\n",
    "                \"body\": r.json() if r.headers.get(\"content-type\", \"\").startswith(\"application/json\") else r.text[:500],\n",
    "            })\n",
    "        except Exception as e:\n",
    "            results.append({\n",
    "                \"query\": q,\n",
    "                \"status\": 599,\n",
    "                \"error\": str(e),\n",
    "            })\n",
    "    out = {\n",
    "        \"ts\": int(time.time()),\n",
    "        \"results\": results,\n",
    "    }\n",
    "    GOLDEN_RESULTS_PATH.write_text(json.dumps(out, indent=2), encoding=\"utf-8\")\n",
    "    print(f\" golden-tests  {GOLDEN_RESULTS_PATH}\")\n",
    "    return GOLDEN_RESULTS_PATH\n",
    "def compare_golden_results(\n",
    "    current_path: Path = GOLDEN_RESULTS_PATH,\n",
    "    baseline_path: Path = ART_DIR / \"gateway_golden_results.baseline.json\",\n",
    ") -> Dict[str, Any]:\n",
    "    cur = _load_json(current_path) or {}\n",
    "    base = _load_json(baseline_path) or {}\n",
    "    diffs: List[str] = []\n",
    "    cur_map = {(r.get(\"query\") or {}).get(\"query\"): r for r in cur.get(\"results\") or []}\n",
    "    base_map = {(r.get(\"query\") or {}).get(\"query\"): r for r in base.get(\"results\") or []}\n",
    "    for q, r in cur_map.items():\n",
    "        br = base_map.get(q)\n",
    "        if not br:\n",
    "            diffs.append(f\"new query: {q}\")\n",
    "            continue\n",
    "        if r.get(\"status\") != br.get(\"status\"):\n",
    "            diffs.append(f\"status mismatch for {q}: {r.get('status')} vs {br.get('status')}\")\n",
    "    rep = {\n",
    "        \"ts\": int(time.time()),\n",
    "        \"diffs\": diffs,\n",
    "        \"ok\": not bool(diffs),\n",
    "    }\n",
    "    GOLDEN_DIFF_PATH.write_text(json.dumps(rep, indent=2), encoding=\"utf-8\")\n",
    "    print(f\" golden-diff  {GOLDEN_DIFF_PATH} (ok={rep['ok']})\")\n",
    "    return rep\n",
    "def ci_promotion_gate() -> Dict[str, Any]:\n",
    "    \"\"\"\n",
    "    This is the CI-style gate: schema ok? slo ok? burn ok? golden ok?\n",
    "    \"\"\"\n",
    "    prom = check_promotion_gates()\n",
    "    golden = compare_golden_results()\n",
    "    ok = prom.get(\"ok\") and golden.get(\"ok\")\n",
    "    res = {\n",
    "        \"ts\": int(time.time()),\n",
    "        \"ok\": ok,\n",
    "        \"promotion\": prom,\n",
    "        \"golden\": golden,\n",
    "    }\n",
    "    (ART_DIR / \"gateway_ci_gate.json\").write_text(json.dumps(res, indent=2), encoding=\"utf-8\")\n",
    "    print(f\" CI gate  {res}\")\n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 778,
   "id": "c95076b0-1c3e-44de-bfd2-5947ce7cca27",
   "metadata": {},
   "outputs": [],
   "source": [
    "import http.server\n",
    "import socketserver\n",
    "class N8NHandler(http.server.BaseHTTPRequestHandler):\n",
    "    def _json(self, code: int, payload: Dict[str, Any]) -> None:\n",
    "        body = json.dumps(payload).encode(\"utf-8\")\n",
    "        self.send_response(code)\n",
    "        self.send_header(\"Content-Type\", \"application/json\")\n",
    "        self.send_header(\"Content-Length\", str(len(body)))\n",
    "        self.end_headers()\n",
    "        self.wfile.write(body)\n",
    "    def do_POST(self):\n",
    "        length = int(self.headers.get(\"content-length\", \"0\") or \"0\")\n",
    "        raw = self.rfile.read(length) if length else b\"{}\"\n",
    "        try:\n",
    "            data = json.loads(raw.decode(\"utf-8\") or \"{}\")\n",
    "        except Exception:\n",
    "            data = {}\n",
    "        path = self.path\n",
    "        if path == \"/freeze\":\n",
    "            ttl = int(data.get(\"ttl_s\", 3600))\n",
    "            reason = data.get(\"reason\", \"n8n\")\n",
    "            set_freeze(reason=reason, ttl_s=ttl)\n",
    "            self._json(200, {\"ok\": True, \"frozen\": True})\n",
    "            return\n",
    "        if path == \"/unfreeze\":\n",
    "            clear_freeze()\n",
    "            self._json(200, {\"ok\": True, \"frozen\": False})\n",
    "            return\n",
    "        if path == \"/retention\":\n",
    "            rep = sweep_retention()\n",
    "            self._json(200, rep)\n",
    "            return\n",
    "        if path == \"/promote/check\":\n",
    "            rep = ci_promotion_gate()\n",
    "            self._json(200, rep)\n",
    "            return\n",
    "        if path == \"/dlq/list\":\n",
    "            items = dlq_iter(limit=int(data.get(\"limit\", 100)))\n",
    "            self._json(200, {\"items\": items})\n",
    "            return\n",
    "        self._json(404, {\"error\": \"not-found\"})\n",
    "    def do_GET(self):\n",
    "        if self.path == \"/healthz\":\n",
    "            self._json(200, {\"ok\": True, \"frozen\": is_frozen(), \"ts\": int(time.time())})\n",
    "            return\n",
    "        if self.path == \"/freeze\":\n",
    "            self._json(200, load_freeze())\n",
    "            return\n",
    "        if self.path == \"/otel\":\n",
    "            items = dlq_iter(0) \n",
    "            lines = []\n",
    "            if OTEL_SPANS_PATH.exists():\n",
    "                lines = OTEL_SPANS_PATH.read_text(encoding=\"utf-8\").splitlines()[-100:]\n",
    "            self._json(200, {\"spans\": [json.loads(l) for l in lines if l.strip()]})\n",
    "            return\n",
    "        self._json(404, {\"error\": \"not-found\"})\n",
    "def serve_n8n_http(port: int = 9929) -> None:\n",
    "    with socketserver.TCPServer((\"0.0.0.0\", port), N8NHandler) as httpd:\n",
    "        print(f\" n8n HTTP at http://0.0.0.0:{port}\")\n",
    "        try:\n",
    "            httpd.serve_forever()\n",
    "        except KeyboardInterrupt:\n",
    "            print(\" n8n HTTP stopped\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 779,
   "id": "f8e900d8-11a2-4c0f-acaf-b26f4b311c66",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " SBOM  \\tmp\\art\\gateway_sbom.json\n",
      " SBOM sig  \\tmp\\art\\gateway_sbom.sig\n",
      " k6 scenario  \\tmp\\art\\k6-gateway-smoke.js\n",
      " retention  {'ts': 1762115207, 'removed': []}\n",
      "\n",
      " continuation (freeze + promo-gates + canary + retention + dlq + otel + sbom + k6 + n8n-http) loaded.\n",
      "Try:\n",
      "  gw_cli9('freeze', reason='latency-spike')\n",
      "  gw_cli9('promotion.check')\n",
      "  gw_cli9('golden.run')\n",
      "  gw_cli9('ci.gate')\n",
      "  gw_cli9('n8n.http', port=9929)\n"
     ]
    }
   ],
   "source": [
    "def gw_cli9(cmd: str, **kwargs) -> None:\n",
    "    cmd = cmd.strip().lower()\n",
    "    if cmd == \"freeze\":\n",
    "        set_freeze(reason=kwargs.get(\"reason\", \"cli\"), ttl_s=int(kwargs.get(\"ttl_s\", 3600)))\n",
    "    elif cmd == \"unfreeze\":\n",
    "        clear_freeze()\n",
    "    elif cmd == \"promotion.check\":\n",
    "        check_promotion_gates()\n",
    "    elif cmd == \"canary.eval\":\n",
    "        canary_eval()\n",
    "    elif cmd == \"baseline.save\":\n",
    "        save_as_baseline()\n",
    "    elif cmd == \"retention\":\n",
    "        sweep_retention()\n",
    "    elif cmd == \"sbom\":\n",
    "        write_sbom()\n",
    "    elif cmd == \"k6\":\n",
    "        write_k6_scenario()\n",
    "    elif cmd == \"golden.run\":\n",
    "        run_golden_tests()\n",
    "    elif cmd == \"ci.gate\":\n",
    "        ci_promotion_gate()\n",
    "    elif cmd == \"n8n.http\":\n",
    "        port = int(kwargs.get(\"port\", 9929))\n",
    "        serve_n8n_http(port=port)\n",
    "    else:\n",
    "        print(\" unknown cmd. try:\")\n",
    "        print(\"  gw_cli9('freeze', reason='incident', ttl_s=1800)\")\n",
    "        print(\"  gw_cli9('unfreeze')\")\n",
    "        print(\"  gw_cli9('promotion.check')\")\n",
    "        print(\"  gw_cli9('canary.eval')\")\n",
    "        print(\"  gw_cli9('baseline.save')\")\n",
    "        print(\"  gw_cli9('retention')\")\n",
    "        print(\"  gw_cli9('sbom')\")\n",
    "        print(\"  gw_cli9('k6')\")\n",
    "        print(\"  gw_cli9('golden.run')\")\n",
    "        print(\"  gw_cli9('ci.gate')\")\n",
    "        print(\"  gw_cli9('n8n.http', port=9929)\")\n",
    "write_sbom()\n",
    "write_k6_scenario()\n",
    "sweep_retention()\n",
    "print(\"\\n continuation (freeze + promo-gates + canary + retention + dlq + otel + sbom + k6 + n8n-http) loaded.\")\n",
    "print(\"Try:\")\n",
    "print(\"  gw_cli9('freeze', reason='latency-spike')\")\n",
    "print(\"  gw_cli9('promotion.check')\")\n",
    "print(\"  gw_cli9('golden.run')\")\n",
    "print(\"  gw_cli9('ci.gate')\")\n",
    "print(\"  gw_cli9('n8n.http', port=9929)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 780,
   "id": "8e382f27-74bc-4b02-9621-a0b4caee582f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " continuation (secrets-backend + notify + dlq-replay + drift-detect + version-matrix + runbook + otel-export) using ART_DIR=\\tmp\\art\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "import time\n",
    "import socket\n",
    "from pathlib import Path\n",
    "from typing import Any, Dict, List, Optional, Tuple\n",
    "ART_DIR = Path(os.getenv(\"ART_DIR\", \"/tmp/art\"))\n",
    "ART_DIR.mkdir(parents=True, exist_ok=True)\n",
    "print(f\" continuation (secrets-backend + notify + dlq-replay + drift-detect + version-matrix + runbook + otel-export) using ART_DIR={ART_DIR}\")\n",
    "HOSTNAME = socket.gethostname()\n",
    "CURRENT_SCHEMA = \"1.2.1\"\n",
    "DLQ_PATH = ART_DIR / \"n8n-dlq.jsonl\"\n",
    "CONTROL_PATH = ART_DIR / \"gateway_control.json\"\n",
    "CONTROL_BLUE = ART_DIR / \"gateway_control.blue.json\"\n",
    "CONTROL_GREEN = ART_DIR / \"gateway_control.green.json\"\n",
    "ENV_PATH = ART_DIR / \"gateway_env.json\"\n",
    "VAULT_SIM_PATH = ART_DIR / \"vault-secrets.json\"          \n",
    "SECRET_MAP_PATH = ART_DIR / \"gateway_secret_map.json\"    \n",
    "NOTIFY_LOG_PATH = ART_DIR / \"gateway_notify.jsonl\"\n",
    "DRIFT_REPORT_PATH = ART_DIR / \"gateway_drift_report.json\"\n",
    "VERSION_MATRIX_PATH = ART_DIR / \"gateway_version_matrix.json\"\n",
    "RUNBOOK_PATH = ART_DIR / \"gateway_runbook.md\"\n",
    "OTEL_EXPORT_PATH = ART_DIR / \"otel_export.json\"\n",
    "N8N_PUSH_OUTBOX = ART_DIR / \"n8n-outbox.jsonl\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 781,
   "id": "72050b01-ce9a-4bb3-905f-e31e4886b80d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " default secret-map  \\tmp\\art\\gateway_secret_map.json\n"
     ]
    }
   ],
   "source": [
    "DEFAULT_SECRET_MAP = {\n",
    "    \"ts\": int(time.time()),\n",
    "    \"schema\": CURRENT_SCHEMA,\n",
    "    \"secrets\": {\n",
    "        \"admin_token\": {\"backend\": \"env\", \"key\": \"GATEWAY_ADMIN_TOKEN\"},\n",
    "        \"slack_webhook\": {\"backend\": \"vault\", \"key\": \"slack/webhook\"},\n",
    "    },\n",
    "}\n",
    "def _write_default_secret_map() -> None:\n",
    "    if not SECRET_MAP_PATH.exists():\n",
    "        SECRET_MAP_PATH.write_text(json.dumps(DEFAULT_SECRET_MAP, indent=2), encoding=\"utf-8\")\n",
    "        print(f\" default secret-map  {SECRET_MAP_PATH}\")\n",
    "_write_default_secret_map()\n",
    "def _load_json_safe(path: Path, default: Any = None) -> Any:\n",
    "    if not path.exists():\n",
    "        return default\n",
    "    try:\n",
    "        return json.loads(path.read_text(encoding=\"utf-8\"))\n",
    "    except Exception:\n",
    "        return default\n",
    "def secret_get(logical_name: str) -> Optional[str]:\n",
    "    \"\"\"\n",
    "    Resolve secret by logical name using secret-map.\n",
    "    Backends:\n",
    "      - env\n",
    "      - file (from vault-secrets.json)\n",
    "    \"\"\"\n",
    "    sm = _load_json_safe(SECRET_MAP_PATH, {})\n",
    "    spec = (sm.get(\"secrets\") or {}).get(logical_name)\n",
    "    if not spec:\n",
    "        return None\n",
    "    backend = spec.get(\"backend\", \"env\")\n",
    "    key = spec.get(\"key\")\n",
    "    if backend == \"env\":\n",
    "        return os.getenv(key or \"\", \"\")\n",
    "    if backend == \"vault\":\n",
    "        vault = _load_json_safe(VAULT_SIM_PATH, {})\n",
    "        return (vault or {}).get(key or \"\", \"\")\n",
    "    if backend == \"file\":\n",
    "        p = ART_DIR / (key or \"\")\n",
    "        if p.exists():\n",
    "            return p.read_text(encoding=\"utf-8\").strip()\n",
    "    return None\n",
    "def secret_put_vault(key: str, value: str) -> None:\n",
    "    \"\"\"\n",
    "    Fake vault writer (local JSON).\n",
    "    \"\"\"\n",
    "    vault = _load_json_safe(VAULT_SIM_PATH, {})\n",
    "    vault[key] = value\n",
    "    VAULT_SIM_PATH.write_text(json.dumps(vault, indent=2), encoding=\"utf-8\")\n",
    "    print(f\" vault secret set  {key}\")\n",
    "def _notify_write_local(kind: str, payload: Dict[str, Any]) -> None:\n",
    "    rec = {\n",
    "        \"ts\": int(time.time()),\n",
    "        \"kind\": kind,\n",
    "        \"payload\": payload,\n",
    "        \"host\": HOSTNAME,\n",
    "    }\n",
    "    with NOTIFY_LOG_PATH.open(\"a\", encoding=\"utf-8\") as f:\n",
    "        f.write(json.dumps(rec) + \"\\n\")\n",
    "    print(f\" notify-log  {kind}\")\n",
    "def notify_slack(text: str) -> Dict[str, Any]:\n",
    "    url = secret_get(\"slack_webhook\")\n",
    "    if not url:\n",
    "        _notify_write_local(\"slack-missing\", {\"text\": text})\n",
    "        return {\"ok\": False, \"error\": \"no-slack-webhook\"}\n",
    "    try:\n",
    "        import requests\n",
    "        r = requests.post(url, json={\"text\": text}, timeout=2.5)\n",
    "        _notify_write_local(\"slack\", {\"text\": text, \"status\": r.status_code})\n",
    "        return {\"ok\": 200 <= r.status_code < 300, \"status\": r.status_code}\n",
    "    except Exception as e:\n",
    "        _notify_write_local(\"slack-error\", {\"text\": text, \"error\": str(e)})\n",
    "        return {\"ok\": False, \"error\": str(e)}\n",
    "def notify_n8n(event: str, data: Dict[str, Any]) -> None:\n",
    "    \"\"\"\n",
    "    Cheaper than HTTP  drop in outbox, let n8n poll it.\n",
    "    \"\"\"\n",
    "    rec = {\n",
    "        \"ts\": int(time.time()),\n",
    "        \"event\": event,\n",
    "        \"data\": data,\n",
    "        \"host\": HOSTNAME,\n",
    "    }\n",
    "    with N8N_PUSH_OUTBOX.open(\"a\", encoding=\"utf-8\") as f:\n",
    "        f.write(json.dumps(rec) + \"\\n\")\n",
    "    print(f\" n8n-outbox  {event}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 782,
   "id": "666cd293-531a-473a-b8ed-f7550cccdd0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def dlq_iter(limit: int = 200) -> List[Dict[str, Any]]:\n",
    "    if not DLQ_PATH.exists():\n",
    "        return []\n",
    "    out: List[Dict[str, Any]] = []\n",
    "    with DLQ_PATH.open(\"r\", encoding=\"utf-8\") as f:\n",
    "        for i, line in enumerate(f):\n",
    "            line = line.strip()\n",
    "            if not line:\n",
    "                continue\n",
    "            try:\n",
    "                out.append(json.loads(line))\n",
    "            except Exception:\n",
    "                pass\n",
    "            if i + 1 >= limit:\n",
    "                break\n",
    "    return out\n",
    "def dlq_replay(target_base: Optional[str] = None, limit: int = 50) -> Dict[str, Any]:\n",
    "    \"\"\"\n",
    "    Best-effort replay: we just POST the payloads again.\n",
    "    Only for items we know how to replay (like /admin ops).\n",
    "    \"\"\"\n",
    "    rows = dlq_iter(limit=limit)\n",
    "    if not rows:\n",
    "        print(\" dlq empty\")\n",
    "        return {\"replayed\": 0}\n",
    "    target_base = target_base or os.getenv(\"GATEWAY_BASE\", \"http://127.0.0.1:9910\")\n",
    "    ok_count = 0\n",
    "    try:\n",
    "        import requests\n",
    "    except Exception as e:\n",
    "        print(f\" cannot replay: {e}\")\n",
    "        return {\"replayed\": 0, \"error\": str(e)}\n",
    "    for r in rows:\n",
    "        payload = r.get(\"payload\") or {}\n",
    "        kind = r.get(\"kind\", \"\")\n",
    "        if kind.startswith(\"frozen.\") and \"path\" in payload:\n",
    "            url = target_base.rstrip(\"/\") + payload[\"path\"]\n",
    "            try:\n",
    "                resp = requests.post(url, json=payload.get(\"body\") or {}, timeout=2.5)\n",
    "                if 200 <= resp.status_code < 300:\n",
    "                    ok_count += 1\n",
    "            except Exception as e:\n",
    "                print(\" replay error:\", e)\n",
    "    print(f\" dlq replayed  {ok_count}/{len(rows)}\")\n",
    "    return {\"replayed\": ok_count, \"total\": len(rows)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 783,
   "id": "797ee975-3ecd-46f6-8caf-adf312bea028",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _load_control_any() -> Dict[str, Any]:\n",
    "    if CONTROL_PATH.exists():\n",
    "        return _load_json_safe(CONTROL_PATH, {}) or {}\n",
    "    if CONTROL_BLUE.exists():\n",
    "        return _load_json_safe(CONTROL_BLUE, {}) or {}\n",
    "    if CONTROL_GREEN.exists():\n",
    "        return _load_json_safe(CONTROL_GREEN, {}) or {}\n",
    "    return {}\n",
    "def drift_detect(remote_path: Optional[Path] = None) -> Dict[str, Any]:\n",
    "    \"\"\"\n",
    "    Compare local control vs remote (if provided) OR vs blue/green twin.\n",
    "    \"\"\"\n",
    "    local = _load_control_any()\n",
    "    if remote_path is None:\n",
    "        blu = _load_json_safe(CONTROL_BLUE, {})\n",
    "        grn = _load_json_safe(CONTROL_GREEN, {})\n",
    "        drift = {\n",
    "            \"ts\": int(time.time()),\n",
    "            \"schema\": CURRENT_SCHEMA,\n",
    "            \"blue_only\": sorted(set((blu or {}).keys()) - set((grn or {}).keys())),\n",
    "            \"green_only\": sorted(set((grn or {}).keys()) - set((blu or {}).keys())),\n",
    "            \"size_blue\": len((blu or {}).keys()),\n",
    "            \"size_green\": len((grn or {}).keys()),\n",
    "        }\n",
    "    else:\n",
    "        remote = _load_json_safe(remote_path, {}) or {}\n",
    "        drift = {\n",
    "            \"ts\": int(time.time()),\n",
    "            \"schema\": CURRENT_SCHEMA,\n",
    "            \"local_keys\": sorted((local or {}).keys()),\n",
    "            \"remote_keys\": sorted((remote or {}).keys()),\n",
    "            \"extra_local\": sorted(set(local.keys()) - set(remote.keys())),\n",
    "            \"extra_remote\": sorted(set(remote.keys()) - set(local.keys())),\n",
    "        }\n",
    "    DRIFT_REPORT_PATH.write_text(json.dumps(drift, indent=2), encoding=\"utf-8\")\n",
    "    print(f\" drift-detect  {DRIFT_REPORT_PATH}\")\n",
    "    return drift"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 784,
   "id": "a88d5bb6-e94d-420d-852c-85e18c33e54a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_version_matrix() -> Path:\n",
    "    env = _load_json_safe(ENV_PATH, {})\n",
    "    ctrl_info = _load_json_safe(ART_DIR / \"gateway_controller_info.json\", {})\n",
    "    manifest = _load_json_safe(ART_DIR / \"gateway_manifest.json\", {})\n",
    "    mat = {\n",
    "        \"ts\": int(time.time()),\n",
    "        \"host\": HOSTNAME,\n",
    "        \"schema\": CURRENT_SCHEMA,\n",
    "        \"controller_version\": ctrl_info.get(\"controller_version\"),\n",
    "        \"gateway_base\": env.get(\"base\"),\n",
    "        \"active_schema\": manifest.get(\"current_schema\"),\n",
    "        \"next_schema\": manifest.get(\"next_schema\"),\n",
    "        \"artifacts\": manifest.get(\"generated_files\"),\n",
    "    }\n",
    "    VERSION_MATRIX_PATH.write_text(json.dumps(mat, indent=2), encoding=\"utf-8\")\n",
    "    print(f\" version-matrix  {VERSION_MATRIX_PATH}\")\n",
    "    return VERSION_MATRIX_PATH"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 785,
   "id": "540ccab7-794d-4ee7-accd-979cc6741432",
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_runbook() -> Path:\n",
    "    lines: List[str] = []\n",
    "    lines.append(\"# Gateway Runbook (auto-generated)\")\n",
    "    lines.append(\"\")\n",
    "    lines.append(f\"- Generated: {time.ctime()}\")\n",
    "    lines.append(f\"- Host: `{HOSTNAME}`\")\n",
    "    lines.append(\"\")\n",
    "    lines.append(\"## Freeze / Incident\")\n",
    "    lines.append(\"- Freeze gateway: `gw_cli9('freeze', reason='incident')`\")\n",
    "    lines.append(\"- Unfreeze gateway: `gw_cli9('unfreeze')`\")\n",
    "    lines.append(\"- Check: `GET /healthz` on n8n-http\")\n",
    "    lines.append(\"\")\n",
    "    lines.append(\"## Promotion\")\n",
    "    lines.append(\"- Run CI gate: `gw_cli9('ci.gate')`\")\n",
    "    lines.append(\"- If blocked  check `gateway_promotion_gate.json` and `gateway_canary_eval.json`\")\n",
    "    lines.append(\"- If OK  switch blue/green via controller HTTP\")\n",
    "    lines.append(\"\")\n",
    "    lines.append(\"## DLQ\")\n",
    "    lines.append(\"- Inspect: `cat /tmp/art/n8n-dlq.jsonl`\")\n",
    "    lines.append(\"- Replay: `gw_cli10('dlq.replay')` (this file)\")\n",
    "    lines.append(\"\")\n",
    "    lines.append(\"## Drift\")\n",
    "    lines.append(\"- Run: `gw_cli10('drift')`\")\n",
    "    lines.append(\"- See: `/tmp/art/gateway_drift_report.json`\")\n",
    "    lines.append(\"\")\n",
    "    lines.append(\"## Notifications\")\n",
    "    lines.append(\"- Slack webhook is read from `vault-secrets.json` key: `slack/webhook`\")\n",
    "    lines.append(\"- Fallback log: `/tmp/art/gateway_notify.jsonl`\")\n",
    "    RUNBOOK_PATH.write_text(\"\\n\".join(lines), encoding=\"utf-8\")\n",
    "    print(f\" runbook  {RUNBOOK_PATH}\")\n",
    "    return RUNBOOK_PATH"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 786,
   "id": "86ba499d-a0b2-4c38-8376-6906d18a81ec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " version-matrix  \\tmp\\art\\gateway_version_matrix.json\n",
      " runbook  \\tmp\\art\\gateway_runbook.md\n",
      " otel-export  \\tmp\\art\\otel_export.json\n",
      "\n",
      " continuation (secrets-backend + notify + dlq-replay + drift-detect + version-matrix + runbook + otel-export) loaded.\n",
      "Try:\n",
      "  gw_cli10('secret.get', name='admin_token')\n",
      "  gw_cli10('notify', text='gateway: promotion blocked')\n",
      "  gw_cli10('dlq.replay')\n",
      "  gw_cli10('drift')\n",
      "  gw_cli10('versions')\n",
      "  gw_cli10('runbook')\n",
      "  gw_cli10('otel.export')\n"
     ]
    }
   ],
   "source": [
    "OTEL_SPANS_PATH = ART_DIR / \"otel_spans.jsonl\"\n",
    "def otel_export(limit: int = 500) -> Path:\n",
    "    spans: List[Dict[str, Any]] = []\n",
    "    if OTEL_SPANS_PATH.exists():\n",
    "        with OTEL_SPANS_PATH.open(\"r\", encoding=\"utf-8\") as f:\n",
    "            for i, line in enumerate(f):\n",
    "                line = line.strip()\n",
    "                if not line:\n",
    "                    continue\n",
    "                try:\n",
    "                    spans.append(json.loads(line))\n",
    "                except Exception:\n",
    "                    pass\n",
    "                if i + 1 >= limit:\n",
    "                    break\n",
    "    payload = {\n",
    "        \"ts\": int(time.time()),\n",
    "        \"host\": HOSTNAME,\n",
    "        \"spans\": spans,\n",
    "        \"count\": len(spans),\n",
    "    }\n",
    "    OTEL_EXPORT_PATH.write_text(json.dumps(payload, indent=2), encoding=\"utf-8\")\n",
    "    print(f\" otel-export  {OTEL_EXPORT_PATH}\")\n",
    "    return OTEL_EXPORT_PATH\n",
    "def gw_cli10(cmd: str, **kwargs) -> None:\n",
    "    cmd = cmd.strip().lower()\n",
    "    if cmd == \"secret.get\":\n",
    "        name = kwargs.get(\"name\", \"admin_token\")\n",
    "        val = secret_get(name)\n",
    "        print(json.dumps({\"name\": name, \"value\": val}, indent=2))\n",
    "    elif cmd == \"secret.put\":\n",
    "        key = kwargs.get(\"key\")\n",
    "        val = kwargs.get(\"value\")\n",
    "        if not key or val is None:\n",
    "            print(\" need key= and value=\")\n",
    "            return\n",
    "        secret_put_vault(key, val)\n",
    "    elif cmd == \"notify\":\n",
    "        text = kwargs.get(\"text\", \"hello from gateway\")\n",
    "        notify_slack(text)\n",
    "        notify_n8n(\"manual.notify\", {\"text\": text})\n",
    "    elif cmd == \"dlq.replay\":\n",
    "        dlq_replay(target_base=kwargs.get(\"base\"))\n",
    "    elif cmd == \"drift\":\n",
    "        drift_detect()\n",
    "    elif cmd == \"versions\":\n",
    "        write_version_matrix()\n",
    "    elif cmd == \"runbook\":\n",
    "        write_runbook()\n",
    "    elif cmd == \"otel.export\":\n",
    "        otel_export()\n",
    "    else:\n",
    "        print(\" unknown cmd. try:\")\n",
    "        print(\"  gw_cli10('secret.get', name='admin_token')\")\n",
    "        print(\"  gw_cli10('secret.put', key='slack/webhook', value='https://hooks.slack.com/...')\")\n",
    "        print(\"  gw_cli10('notify', text='p95 > 1500ms, promotion blocked')\")\n",
    "        print(\"  gw_cli10('dlq.replay')\")\n",
    "        print(\"  gw_cli10('drift')\")\n",
    "        print(\"  gw_cli10('versions')\")\n",
    "        print(\"  gw_cli10('runbook')\")\n",
    "        print(\"  gw_cli10('otel.export')\")\n",
    "write_version_matrix()\n",
    "write_runbook()\n",
    "otel_export()\n",
    "print(\"\\n continuation (secrets-backend + notify + dlq-replay + drift-detect + version-matrix + runbook + otel-export) loaded.\")\n",
    "print(\"Try:\")\n",
    "print(\"  gw_cli10('secret.get', name='admin_token')\")\n",
    "print(\"  gw_cli10('notify', text='gateway: promotion blocked')\")\n",
    "print(\"  gw_cli10('dlq.replay')\")\n",
    "print(\"  gw_cli10('drift')\")\n",
    "print(\"  gw_cli10('versions')\")\n",
    "print(\"  gw_cli10('runbook')\")\n",
    "print(\"  gw_cli10('otel.export')\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 787,
   "id": "cb294a3c-3b64-46a4-878c-1792e044b51c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " continuation (authz + mTLS-stub + policy-as-code + integrity + jobs + html + upgrade) using ART_DIR=\\tmp\\art\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "import time\n",
    "import socket\n",
    "import hashlib\n",
    "from pathlib import Path\n",
    "from typing import Any, Dict, List, Optional, Tuple, Callable\n",
    "ART_DIR = Path(os.getenv(\"ART_DIR\", \"/tmp/art\"))\n",
    "ART_DIR.mkdir(parents=True, exist_ok=True)\n",
    "print(f\" continuation (authz + mTLS-stub + policy-as-code + integrity + jobs + html + upgrade) using ART_DIR={ART_DIR}\")\n",
    "HOSTNAME = socket.gethostname()\n",
    "CURRENT_SCHEMA = \"1.3.0\"\n",
    "AUTHZ_PATH = ART_DIR / \"gateway_admin_allowlist.json\"\n",
    "MTLS_DIR = ART_DIR / \"mtls\"\n",
    "MTLS_DIR.mkdir(parents=True, exist_ok=True)\n",
    "POLICY_CODE_PATH = ART_DIR / \"gateway_policy.json\"\n",
    "INTEGRITY_PATH = ART_DIR / \"gateway_integrity.json\"\n",
    "JOBS_DIR = ART_DIR / \"jobs\"\n",
    "JOBS_DIR.mkdir(parents=True, exist_ok=True)\n",
    "JOBS_STATE_PATH = ART_DIR / \"jobs_state.json\"\n",
    "HTML_SUMMARY_PATH = ART_DIR / \"gateway_state.html\"\n",
    "UPGRADE_REPORT_PATH = ART_DIR / \"gateway_upgrade_report.json\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 788,
   "id": "8af7f6e0-b13d-4694-9515-ccb4d07169d5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " authz default  \\tmp\\art\\gateway_admin_allowlist.json\n"
     ]
    }
   ],
   "source": [
    "DEFAULT_AUTHZ = {\n",
    "    \"ts\": int(time.time()),\n",
    "    \"schema\": CURRENT_SCHEMA,\n",
    "    \"allow_cidrs\": [\"127.0.0.1/32\", \"::1/128\"],\n",
    "    \"allow_tokens\": [],  \n",
    "    \"allow_users\": [\"n8n\", \"notebook\"],\n",
    "    \"deny_paths\": [\n",
    "        \"/admin/tenants/drop\",\n",
    "        \"/admin/wipe\",\n",
    "        \"/admin/chaos/set?flag=wipe_index\",\n",
    "    ],\n",
    "}\n",
    "def _write_default_authz() -> None:\n",
    "    if not AUTHZ_PATH.exists():\n",
    "        AUTHZ_PATH.write_text(json.dumps(DEFAULT_AUTHZ, indent=2), encoding=\"utf-8\")\n",
    "        print(f\" authz default  {AUTHZ_PATH}\")\n",
    "_write_default_authz()\n",
    "def load_authz() -> Dict[str, Any]:\n",
    "    try:\n",
    "        return json.loads(AUTHZ_PATH.read_text(encoding=\"utf-8\"))\n",
    "    except Exception:\n",
    "        return DEFAULT_AUTHZ\n",
    "def _ip_in_cidrs(ip: str, cidrs: List[str]) -> bool:\n",
    "    if ip in (\"127.0.0.1\", \"::1\", \"localhost\"):\n",
    "        return True\n",
    "    for c in cidrs:\n",
    "        if c.startswith(ip):\n",
    "            return True\n",
    "    return False\n",
    "def admin_allowed(ip: str, token: Optional[str], path: str, actor: str = \"notebook\") -> Tuple[bool, str]:\n",
    "    az = load_authz()\n",
    "    if path in (az.get(\"deny_paths\") or []):\n",
    "        return False, \"path-denied\"\n",
    "    if _ip_in_cidrs(ip, az.get(\"allow_cidrs\") or []):\n",
    "        return True, \"cidr-ok\"\n",
    "    if token and token in (az.get(\"allow_tokens\") or []):\n",
    "        return True, \"token-ok\"\n",
    "    if actor in (az.get(\"allow_users\") or []):\n",
    "        return True, \"actor-ok\"\n",
    "    return False, \"no-match\"\n",
    "MTLS_CA_PATH = MTLS_DIR / \"ca.pem\"\n",
    "MTLS_SRV_PATH = MTLS_DIR / \"server.pem\"\n",
    "MTLS_CLI_PATH = MTLS_DIR / \"client.pem\"\n",
    "MTLS_META_PATH = MTLS_DIR / \"mtls_meta.json\"\n",
    "def write_mtls_stub() -> Dict[str, Any]:\n",
    "    \"\"\"\n",
    "    We just drop placeholder PEMs so ops knows where to mount real ones.\n",
    "    \"\"\"\n",
    "    if not MTLS_CA_PATH.exists():\n",
    "        MTLS_CA_PATH.write_text(\"-----BEGIN CERTIFICATE-----\\nFAKE-CA\\n-----END CERTIFICATE-----\\n\", encoding=\"utf-8\")\n",
    "    if not MTLS_SRV_PATH.exists():\n",
    "        MTLS_SRV_PATH.write_text(\"-----BEGIN CERTIFICATE-----\\nFAKE-SERVER\\n-----END CERTIFICATE-----\\n\", encoding=\"utf-8\")\n",
    "    if not MTLS_CLI_PATH.exists():\n",
    "        MTLS_CLI_PATH.write_text(\"-----BEGIN CERTIFICATE-----\\nFAKE-CLIENT\\n-----END CERTIFICATE-----\\n\", encoding=\"utf-8\")\n",
    "    meta = {\n",
    "        \"ts\": int(time.time()),\n",
    "        \"schema\": CURRENT_SCHEMA,\n",
    "        \"ca\": MTLS_CA_PATH.as_posix(),\n",
    "        \"server\": MTLS_SRV_PATH.as_posix(),\n",
    "        \"client\": MTLS_CLI_PATH.as_posix(),\n",
    "        \"note\": \"replace with real certs or k8s secrets; notebook only writes placeholders.\",\n",
    "    }\n",
    "    MTLS_META_PATH.write_text(json.dumps(meta, indent=2), encoding=\"utf-8\")\n",
    "    print(f\" mTLS stub  {MTLS_META_PATH}\")\n",
    "    return meta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 789,
   "id": "2ccc27e7-aeea-435f-b304-4538dce48e95",
   "metadata": {},
   "outputs": [],
   "source": [
    "DEFAULT_POLICY = {\n",
    "    \"ts\": int(time.time()),\n",
    "    \"schema\": CURRENT_SCHEMA,\n",
    "    \"rules\": [\n",
    "        {\n",
    "            \"id\": \"no-chaos-when-frozen\",\n",
    "            \"if\": {\"frozen\": True},\n",
    "            \"deny\": [\"chaos.apply\", \"control.import\"],\n",
    "        },\n",
    "        {\n",
    "            \"id\": \"must-pass-ci-gate-before-promote\",\n",
    "            \"if\": {\"ci_gate\": False},\n",
    "            \"deny\": [\"control.promote\", \"rl.promote\"],\n",
    "        },\n",
    "    ],\n",
    "    \"effects\": {\n",
    "        \"on-deny\": \"log+dlq\",  \n",
    "    },\n",
    "}\n",
    "def load_policy() -> Dict[str, Any]:\n",
    "    if POLICY_CODE_PATH.exists():\n",
    "        try:\n",
    "            return json.loads(POLICY_CODE_PATH.read_text(encoding=\"utf-8\"))\n",
    "        except Exception:\n",
    "            pass\n",
    "    POLICY_CODE_PATH.write_text(json.dumps(DEFAULT_POLICY, indent=2), encoding=\"utf-8\")\n",
    "    print(f\" policy-as-code  {POLICY_CODE_PATH}\")\n",
    "    return DEFAULT_POLICY\n",
    "def policy_enforce(action: str, ctx: Dict[str, Any]) -> Tuple[bool, str]:\n",
    "    \"\"\"\n",
    "    action: e.g. 'chaos.apply', 'control.promote'\n",
    "    ctx: {'frozen': bool, 'ci_gate': bool, ...}\n",
    "    \"\"\"\n",
    "    pol = load_policy()\n",
    "    for rule in pol.get(\"rules\") or []:\n",
    "        cond = rule.get(\"if\") or {}\n",
    "        matched = True\n",
    "        for k, v in cond.items():\n",
    "            if ctx.get(k) != v:\n",
    "                matched = False\n",
    "                break\n",
    "        if not matched:\n",
    "            continue\n",
    "        if action in (rule.get(\"deny\") or []):\n",
    "            eff = pol.get(\"effects\", {}).get(\"on-deny\", \"log\")\n",
    "            msg = f\"denied by policy {rule.get('id')}\"\n",
    "            if eff in (\"log+dlq\", \"dlq\"):\n",
    "                dlq = ART_DIR / \"n8n-dlq.jsonl\"\n",
    "                with dlq.open(\"a\", encoding=\"utf-8\") as f:\n",
    "                    f.write(json.dumps({\"ts\": int(time.time()), \"kind\": f\"policy.{action}\", \"ctx\": ctx, \"msg\": msg}) + \"\\n\")\n",
    "            print(f\" policy  {msg}\")\n",
    "            return False, msg\n",
    "    return True, \"ok\"\n",
    "CRITICAL_FILES = [\n",
    "    \"gateway_control.json\",\n",
    "    \"gateway_control.blue.json\",\n",
    "    \"gateway_control.green.json\",\n",
    "    \"gateway_slo.json\",\n",
    "    \"gateway_slo_eval.json\",\n",
    "    \"gateway_promotion_gate.json\",\n",
    "    \"gateway_canary_eval.json\",\n",
    "    \"gateway_freeze.json\",\n",
    "]\n",
    "def compute_integrity() -> Path:\n",
    "    items: List[Dict[str, Any]] = []\n",
    "    for rel in CRITICAL_FILES:\n",
    "        p = ART_DIR / rel\n",
    "        if not p.exists():\n",
    "            items.append({\"path\": rel, \"exists\": False})\n",
    "            continue\n",
    "        h = hashlib.sha256(p.read_bytes()).hexdigest()\n",
    "        items.append({\n",
    "            \"path\": rel,\n",
    "            \"exists\": True,\n",
    "            \"sha256\": h,\n",
    "            \"size\": p.stat().st_size,\n",
    "            \"mtime\": int(p.stat().st_mtime),\n",
    "        })\n",
    "    rep = {\n",
    "        \"ts\": int(time.time()),\n",
    "        \"host\": HOSTNAME,\n",
    "        \"schema\": CURRENT_SCHEMA,\n",
    "        \"items\": items,\n",
    "    }\n",
    "    INTEGRITY_PATH.write_text(json.dumps(rep, indent=2), encoding=\"utf-8\")\n",
    "    print(f\" integrity  {INTEGRITY_PATH}\")\n",
    "    return INTEGRITY_PATH"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 790,
   "id": "943abc57-62ce-408d-982f-ff4d01c405f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _jobs_state() -> Dict[str, Any]:\n",
    "    if JOBS_STATE_PATH.exists():\n",
    "        try:\n",
    "            return json.loads(JOBS_STATE_PATH.read_text(encoding=\"utf-8\"))\n",
    "        except Exception:\n",
    "            pass\n",
    "    return {\"ts\": int(time.time()), \"jobs\": {}}\n",
    "\n",
    "def _write_jobs_state(st: Dict[str, Any]) -> None:\n",
    "    st[\"ts\"] = int(time.time())\n",
    "    JOBS_STATE_PATH.write_text(json.dumps(st, indent=2), encoding=\"utf-8\")\n",
    "def jobs_submit(kind: str, args: Dict[str, Any]) -> str:\n",
    "    st = _jobs_state()\n",
    "    job_id = f\"job-{int(time.time())}-{len(st['jobs'])+1}\"\n",
    "    st[\"jobs\"][job_id] = {\n",
    "        \"id\": job_id,\n",
    "        \"kind\": kind,\n",
    "        \"args\": args,\n",
    "        \"status\": \"queued\",\n",
    "        \"created_at\": int(time.time()),\n",
    "    }\n",
    "    _write_jobs_state(st)\n",
    "    print(f\" job queued  {job_id} ({kind})\")\n",
    "    return job_id\n",
    "JOB_FUNCS: Dict[str, Callable[..., Any]] = {\n",
    "    \"integrity\": lambda **kw: compute_integrity(),\n",
    "    \"sbom\": lambda **kw: (ART_DIR / \"gateway_sbom.json\").as_posix(),\n",
    "    \"slo.eval\": lambda **kw: (ART_DIR / \"gateway_slo_eval.json\").as_posix(),\n",
    "    \"backup\": lambda **kw: (ART_DIR / \"backups\").as_posix(),\n",
    "}\n",
    "def jobs_run_once() -> Dict[str, Any]:\n",
    "    st = _jobs_state()\n",
    "    ran = []\n",
    "    for jid, meta in list(st[\"jobs\"].items()):\n",
    "        if meta.get(\"status\") != \"queued\":\n",
    "            continue\n",
    "        kind = meta.get(\"kind\")\n",
    "        fn = JOB_FUNCS.get(kind)\n",
    "        if not fn:\n",
    "            meta[\"status\"] = \"error\"\n",
    "            meta[\"error\"] = \"unknown-kind\"\n",
    "            continue\n",
    "        try:\n",
    "            res = fn(**(meta.get(\"args\") or {}))\n",
    "            meta[\"status\"] = \"done\"\n",
    "            meta[\"result\"] = res\n",
    "        except Exception as e:\n",
    "            meta[\"status\"] = \"error\"\n",
    "            meta[\"error\"] = str(e)\n",
    "        ran.append(jid)\n",
    "        break\n",
    "    _write_jobs_state(st)\n",
    "    rep = {\"ts\": int(time.time()), \"ran\": ran}\n",
    "    print(f\" jobs tick  {rep}\")\n",
    "    return rep\n",
    "def write_html_summary() -> Path:\n",
    "    env = {}\n",
    "    if (ART_DIR / \"gateway_env.json\").exists():\n",
    "        env = json.loads((ART_DIR / \"gateway_env.json\").read_text(encoding=\"utf-8\"))\n",
    "    freeze = {}\n",
    "    if (ART_DIR / \"gateway_freeze.json\").exists():\n",
    "        freeze = json.loads((ART_DIR / \"gateway_freeze.json\").read_text(encoding=\"utf-8\"))\n",
    "    prom_gate = {}\n",
    "    if (ART_DIR / \"gateway_promotion_gate.json\").exists():\n",
    "        prom_gate = json.loads((ART_DIR / \"gateway_promotion_gate.json\").read_text(encoding=\"utf-8\"))\n",
    "    slo_eval = {}\n",
    "    if (ART_DIR / \"gateway_slo_eval.json\").exists():\n",
    "        slo_eval = json.loads((ART_DIR / \"gateway_slo_eval.json\").read_text(encoding=\"utf-8\"))\n",
    "    html = []\n",
    "    html.append(\"<html><head><title>Gateway State</title></head><body>\")\n",
    "    html.append(f\"<h1>Gateway State @ {time.ctime()}</h1>\")\n",
    "    html.append(f\"<p><b>Host:</b> {HOSTNAME}</p>\")\n",
    "    html.append(\"<h2>Env</h2>\")\n",
    "    html.append(f\"<pre>{json.dumps(env, indent=2)}</pre>\")\n",
    "    html.append(\"<h2>Freeze</h2>\")\n",
    "    html.append(f\"<pre>{json.dumps(freeze, indent=2)}</pre>\")\n",
    "    html.append(\"<h2>Promotion Gate</h2>\")\n",
    "    html.append(f\"<pre>{json.dumps(prom_gate, indent=2)}</pre>\")\n",
    "    html.append(\"<h2>SLO Eval</h2>\")\n",
    "    html.append(f\"<pre>{json.dumps(slo_eval, indent=2)}</pre>\")\n",
    "    html.append(\"</body></html>\")\n",
    "    HTML_SUMMARY_PATH.write_text(\"\\n\".join(html), encoding=\"utf-8\")\n",
    "    print(f\" html summary  {HTML_SUMMARY_PATH}\")\n",
    "    return HTML_SUMMARY_PATH"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 791,
   "id": "65339109-2be4-4a52-af3f-c6d08b9c1a71",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " mTLS stub  \\tmp\\art\\mtls\\mtls_meta.json\n",
      " integrity  \\tmp\\art\\gateway_integrity.json\n",
      " html summary  \\tmp\\art\\gateway_state.html\n",
      "\n",
      " continuation (authz + mTLS-stub + policy-as-code + integrity + jobs + html + upgrade) loaded.\n",
      "Try:\n",
      "  gw_cli11('authz.show')\n",
      "  gw_cli11('mtls.write')\n",
      "  gw_cli11('integrity')\n",
      "  gw_cli11('jobs.submit', kind='integrity'); gw_cli11('jobs.tick')\n",
      "  gw_cli11('html')\n",
      "  gw_cli11('upgrade')\n"
     ]
    }
   ],
   "source": [
    "def upgrade_to_1_3_0() -> Dict[str, Any]:\n",
    "    touched: List[str] = []\n",
    "    if not AUTHZ_PATH.exists():\n",
    "        _write_default_authz()\n",
    "        touched.append(\"authz\")\n",
    "    if not POLICY_CODE_PATH.exists():\n",
    "        load_policy()\n",
    "        touched.append(\"policy\")\n",
    "    mts = write_mtls_stub()\n",
    "    touched.append(\"mtls\")\n",
    "    compute_integrity()\n",
    "    touched.append(\"integrity\")\n",
    "    rep = {\n",
    "        \"ts\": int(time.time()),\n",
    "        \"from\": \"1.2.x\",\n",
    "        \"to\": CURRENT_SCHEMA,\n",
    "        \"touched\": touched,\n",
    "        \"mtls\": mts,\n",
    "    }\n",
    "    UPGRADE_REPORT_PATH.write_text(json.dumps(rep, indent=2), encoding=\"utf-8\")\n",
    "    print(f\" upgrade  {UPGRADE_REPORT_PATH}\")\n",
    "    return rep\n",
    "def gw_cli11(cmd: str, **kwargs) -> None:\n",
    "    cmd = cmd.strip().lower()\n",
    "    if cmd == \"authz.show\":\n",
    "        print(json.dumps(load_authz(), indent=2))\n",
    "    elif cmd == \"mtls.write\":\n",
    "        write_mtls_stub()\n",
    "    elif cmd == \"policy.show\":\n",
    "        print(json.dumps(load_policy(), indent=2))\n",
    "    elif cmd == \"integrity\":\n",
    "        compute_integrity()\n",
    "    elif cmd == \"jobs.submit\":\n",
    "        kind = kwargs.get(\"kind\", \"integrity\")\n",
    "        args = kwargs.get(\"args\", {})\n",
    "        jobs_submit(kind, args)\n",
    "    elif cmd == \"jobs.tick\":\n",
    "        jobs_run_once()\n",
    "    elif cmd == \"html\":\n",
    "        write_html_summary()\n",
    "    elif cmd == \"upgrade\":\n",
    "        upgrade_to_1_3_0()\n",
    "    else:\n",
    "        print(\" unknown cmd. try:\")\n",
    "        print(\"  gw_cli11('authz.show')\")\n",
    "        print(\"  gw_cli11('mtls.write')\")\n",
    "        print(\"  gw_cli11('policy.show')\")\n",
    "        print(\"  gw_cli11('integrity')\")\n",
    "        print(\"  gw_cli11('jobs.submit', kind='integrity')\")\n",
    "        print(\"  gw_cli11('jobs.tick')\")\n",
    "        print(\"  gw_cli11('html')\")\n",
    "        print(\"  gw_cli11('upgrade')\")\n",
    "write_mtls_stub()\n",
    "compute_integrity()\n",
    "write_html_summary()\n",
    "print(\"\\n continuation (authz + mTLS-stub + policy-as-code + integrity + jobs + html + upgrade) loaded.\")\n",
    "print(\"Try:\")\n",
    "print(\"  gw_cli11('authz.show')\")\n",
    "print(\"  gw_cli11('mtls.write')\")\n",
    "print(\"  gw_cli11('integrity')\")\n",
    "print(\"  gw_cli11('jobs.submit', kind='integrity'); gw_cli11('jobs.tick')\")\n",
    "print(\"  gw_cli11('html')\")\n",
    "print(\"  gw_cli11('upgrade')\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 797,
   "id": "a0712296-1944-4800-aa96-5853a6191d6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import tarfile\n",
    "from pathlib import Path\n",
    "import time\n",
    "import os\n",
    "ART_DIR = Path(os.getenv(\"ART_DIR\", r\"C:\\tmp\\art\" if os.name == \"nt\" else \"/tmp/art\"))\n",
    "ART_DIR.mkdir(parents=True, exist_ok=True)\n",
    "CI_REPORT_PATH = ART_DIR / \"gateway_ci_full.json\"\n",
    "RELEASE_PATH = ART_DIR / \"gateway_release_bundle.tar.gz\"\n",
    "N8N_WORKFLOW_PATH = ART_DIR / \"n8n-workflow-gateway.json\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 798,
   "id": "182dd7de-0f51-439f-8a45-29ac8902a030",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _json_safe(obj):\n",
    "    \"\"\"recursively make stuff JSON-serializable\"\"\"\n",
    "    if isinstance(obj, Path):\n",
    "        return str(obj)\n",
    "    if isinstance(obj, (list, tuple)):\n",
    "        return [_json_safe(x) for x in obj]\n",
    "    if isinstance(obj, dict):\n",
    "        return {k: _json_safe(v) for k, v in obj.items()}\n",
    "    return obj\n",
    "def run_full_ci() -> dict:\n",
    "    \"\"\"\n",
    "    CI runner:\n",
    "      - artifact index\n",
    "      - lint\n",
    "      - promotion gate (this can fail  ok=False)\n",
    "      - integrity\n",
    "      - sbom\n",
    "      - html\n",
    "      - version matrix\n",
    "    writes: /tmp/art/gateway_ci_full.json\n",
    "    \"\"\"\n",
    "    steps = []\n",
    "    ok = True\n",
    "    if \"build_artifact_index\" in globals():\n",
    "        idx_path = globals()[\"build_artifact_index\"]()\n",
    "        steps.append({\"step\": \"artifact-index\", \"output\": str(idx_path)})\n",
    "    else:\n",
    "        steps.append({\"step\": \"artifact-index\", \"output\": None})\n",
    "    lint_res = None\n",
    "    if \"lint_gateway_state\" in globals():\n",
    "        lint_path = globals()[\"lint_gateway_state\"]()\n",
    "        try:\n",
    "            lint_res = json.loads(lint_path.read_text(encoding=\"utf-8\"))\n",
    "        except Exception:\n",
    "            lint_res = {\"ok\": False, \"error\": \"cannot read lint file\"}\n",
    "        steps.append({\"step\": \"lint\", \"output\": str(lint_path), \"result\": lint_res})\n",
    "        ok = ok and bool(lint_res.get(\"ok\", False))\n",
    "    else:\n",
    "        steps.append({\"step\": \"lint\", \"output\": None})\n",
    "    promo = None\n",
    "    if \"ci_promotion_gate\" in globals():\n",
    "        promo = globals()[\"ci_promotion_gate\"]()\n",
    "        steps.append({\"step\": \"ci-promotion-gate\", \"result\": promo})\n",
    "        ok = ok and bool(promo.get(\"ok\", False))\n",
    "    elif \"check_promotion_gates\" in globals():\n",
    "        promo = globals()[\"check_promotion_gates\"]()\n",
    "        steps.append({\"step\": \"promotion-gate\", \"result\": promo})\n",
    "        ok = ok and bool(promo.get(\"ok\", False))\n",
    "    else:\n",
    "        steps.append({\"step\": \"promotion-gate\", \"result\": None})\n",
    "    integ_path = None\n",
    "    if \"compute_integrity\" in globals():\n",
    "        integ_path = globals()[\"compute_integrity\"]()\n",
    "        steps.append({\"step\": \"integrity\", \"output\": str(integ_path)})\n",
    "    if \"write_sbom\" in globals():\n",
    "        sbom_path = globals()[\"write_sbom\"]()\n",
    "        steps.append({\"step\": \"sbom\", \"output\": str(sbom_path)})\n",
    "    if \"write_html_summary\" in globals():\n",
    "        html_path = globals()[\"write_html_summary\"]()\n",
    "        steps.append({\"step\": \"html\", \"output\": str(html_path)})\n",
    "    if \"write_version_matrix\" in globals():\n",
    "        vm_path = globals()[\"write_version_matrix\"]()\n",
    "        steps.append({\"step\": \"version-matrix\", \"output\": str(vm_path)})\n",
    "    rep = {\n",
    "        \"ts\": int(time.time()),\n",
    "        \"ok\": ok,\n",
    "        \"steps\": steps,\n",
    "    }\n",
    "    rep_safe = _json_safe(rep)\n",
    "    CI_REPORT_PATH.write_text(json.dumps(rep_safe, indent=2), encoding=\"utf-8\")\n",
    "    print(f\" CI full  {CI_REPORT_PATH} (ok={ok})\")\n",
    "    return rep_safe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 799,
   "id": "f390fe8f-43da-4bfc-9539-4a6667e4ca7c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " artifact index  \\tmp\\art\\gateway_artifact_index.json\n",
      " lint ok  \\tmp\\art\\gateway_state_lint.json\n",
      " oncall item  promotion blocked: error_rate 1.0 > 0.03; fast burn 20.0 > 1.0; slow burn 20.0 > 1.0\n",
      " promotion-gate  {'ts': 1762118093, 'ok': False, 'reasons': ['error_rate 1.0 > 0.03', 'fast burn 20.0 > 1.0', 'slow burn 20.0 > 1.0'], 'slo_ns': {'tenant': 'default', 'target_p95_ms': 1800.0, 'actual_p95_ms': 3.7, 'target_error_rate': 0.05, 'actual_error_rate': 1.0, 'total_probes': 2, 'breach_p95': False, 'breach_error': True}, 'burn': {'ts': 1762114750, 'schema': '1.1.0', 'error_budget': 0.05, 'fast': {'window_s': 300, 'errors': 6, 'total': 6, 'burn_rate': 20.0}, 'slow': {'window_s': 1800, 'errors': 6, 'total': 6, 'burn_rate': 20.0}}}\n",
      " golden-diff  \\tmp\\art\\gateway_golden_diff.json (ok=False)\n",
      " CI gate  {'ts': 1762118093, 'ok': False, 'promotion': {'ts': 1762118093, 'ok': False, 'reasons': ['error_rate 1.0 > 0.03', 'fast burn 20.0 > 1.0', 'slow burn 20.0 > 1.0'], 'slo_ns': {'tenant': 'default', 'target_p95_ms': 1800.0, 'actual_p95_ms': 3.7, 'target_error_rate': 0.05, 'actual_error_rate': 1.0, 'total_probes': 2, 'breach_p95': False, 'breach_error': True}, 'burn': {'ts': 1762114750, 'schema': '1.1.0', 'error_budget': 0.05, 'fast': {'window_s': 300, 'errors': 6, 'total': 6, 'burn_rate': 20.0}, 'slow': {'window_s': 1800, 'errors': 6, 'total': 6, 'burn_rate': 20.0}}}, 'golden': {'ts': 1762118093, 'diffs': ['new query: what is the gateway topology?', 'new query: show me the latency profile'], 'ok': False}}\n",
      " integrity  \\tmp\\art\\gateway_integrity.json\n",
      " SBOM  \\tmp\\art\\gateway_sbom.json\n",
      " SBOM sig  \\tmp\\art\\gateway_sbom.sig\n",
      " html summary  \\tmp\\art\\gateway_state.html\n",
      " version-matrix  \\tmp\\art\\gateway_version_matrix.json\n",
      " CI full  C:\\tmp\\art\\gateway_ci_full.json (ok=False)\n",
      " release bundle  C:\\tmp\\art\\gateway_release_bundle.tar.gz\n",
      " done. CI ok? False  bundle at C:\\tmp\\art\\gateway_release_bundle.tar.gz\n"
     ]
    }
   ],
   "source": [
    "def build_release_bundle() -> Path:\n",
    "    \"\"\"\n",
    "    Pack important artifacts into a single tar.gz for ops.\n",
    "    \"\"\"\n",
    "    wanted = [\n",
    "        \"n8n-workflow-gateway.json\",\n",
    "        \"gateway_ci_full.json\",\n",
    "        \"gateway_manifest.json\",\n",
    "        \"gateway_runbook.md\",\n",
    "        \"gateway_sbom.json\",\n",
    "        \"gateway_sbom.sig\",\n",
    "        \"gateway_state.html\",\n",
    "        \"helm-gateway-values.yaml\",\n",
    "        \"gateway_slo.json\",\n",
    "        \"gateway_slo_eval.json\",\n",
    "        \"gateway_promotion_gate.json\",\n",
    "        \"gateway_canary_eval.json\",\n",
    "        \"gateway_integrity.json\",\n",
    "        \"deploy/README.md\",\n",
    "        \"deploy/k8s-gateway-sidecar.yaml\",\n",
    "        \"deploy/k8s-gateway-agent-cronjob.yaml\",\n",
    "        \"deploy/prometheus-gateway-rules.yml\",\n",
    "    ]\n",
    "    with tarfile.open(RELEASE_PATH, \"w:gz\") as tar:\n",
    "        for rel in wanted:\n",
    "            p = ART_DIR / rel\n",
    "            if p.exists():\n",
    "                tar.add(p, arcname=rel)\n",
    "    print(f\" release bundle  {RELEASE_PATH}\")\n",
    "    return RELEASE_PATH\n",
    "ci_res = run_full_ci()\n",
    "bundle_path = build_release_bundle()\n",
    "print(\" done. CI ok?\", ci_res.get(\"ok\"), \" bundle at\", bundle_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 800,
   "id": "02f64ba7-d318-4982-91b5-34f652eacffc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " continuation (safe-promote + n8n-workflow emit + otel-push + outbox-pull + cli12) using ART_DIR=C:\\tmp\\art\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "import time\n",
    "import socket\n",
    "from pathlib import Path\n",
    "from typing import Any, Dict, List, Optional, Tuple\n",
    "ART_DIR = Path(os.getenv(\"ART_DIR\", r\"C:\\tmp\\art\" if os.name == \"nt\" else \"/tmp/art\"))\n",
    "ART_DIR.mkdir(parents=True, exist_ok=True)\n",
    "print(f\" continuation (safe-promote + n8n-workflow emit + otel-push + outbox-pull + cli12) using ART_DIR={ART_DIR}\")\n",
    "HOSTNAME = socket.gethostname()\n",
    "PROMOTE_LOG_PATH = ART_DIR / \"gateway_promotions.jsonl\"\n",
    "N8N_WORKFLOW_PATH = ART_DIR / \"n8n-workflow-gateway.json\"  \n",
    "OUTBOX_PATH = ART_DIR / \"n8n-outbox.jsonl\"\n",
    "OUTBOX_CURSOR_PATH = ART_DIR / \"n8n-outbox.cursor\"\n",
    "OTEL_SPANS_PATH = ART_DIR / \"otel_spans.jsonl\"\n",
    "OTEL_PUSH_REPORT = ART_DIR / \"otel_push_report.json\"\n",
    "PROMOTE_LAST_PATH = ART_DIR / \"gateway_last_promotion.json\"\n",
    "REMOTE_PUSH_URL = os.getenv(\"GATEWAY_REMOTE_PUSH_URL\", \"\").strip()\n",
    "DEFAULT_OTEL_ENDPOINT = os.getenv(\"OTEL_HTTP_COLLECTOR\", \"\").strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 801,
   "id": "dfde76d8-1223-4cf5-8a7a-026d92162a04",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _load_json_safe(path: Path, default: Any = None) -> Any:\n",
    "    if not path.exists():\n",
    "        return default\n",
    "    try:\n",
    "        return json.loads(path.read_text(encoding=\"utf-8\"))\n",
    "    except Exception:\n",
    "        return default\n",
    "def _append_jsonl(path: Path, row: Dict[str, Any]) -> None:\n",
    "    with path.open(\"a\", encoding=\"utf-8\") as f:\n",
    "        f.write(json.dumps(row) + \"\\n\")\n",
    "def _redact(d: Any) -> Any:\n",
    "    SENSITIVE = {\"token\", \"authorization\", \"x-admin-token\", \"password\", \"secret\"}\n",
    "    if isinstance(d, dict):\n",
    "        out = {}\n",
    "        for k, v in d.items():\n",
    "            if k.lower() in SENSITIVE:\n",
    "                out[k] = \"***REDACTED***\"\n",
    "            else:\n",
    "                out[k] = _redact(v)\n",
    "        return out\n",
    "    if isinstance(d, list):\n",
    "        return [_redact(x) for x in d]\n",
    "    return d\n",
    "def _is_frozen_local() -> bool:\n",
    "    f = ART_DIR / \"gateway_freeze.json\"\n",
    "    if not f.exists():\n",
    "        return False\n",
    "    try:\n",
    "        js = json.loads(f.read_text(encoding=\"utf-8\"))\n",
    "    except Exception:\n",
    "        return False\n",
    "    if not js.get(\"frozen\"):\n",
    "        return False\n",
    "    until = js.get(\"until_ts\")\n",
    "    if until and time.time() > until:\n",
    "        try:\n",
    "            f.unlink()\n",
    "        except Exception:\n",
    "            pass\n",
    "        return False\n",
    "    return True\n",
    "def _ci_gate_ok() -> Tuple[bool, Dict[str, Any]]:\n",
    "    \"\"\"\n",
    "    try newest one first\n",
    "    \"\"\"\n",
    "    if \"ci_promotion_gate\" in globals():\n",
    "        res = globals()[\"ci_promotion_gate\"]()\n",
    "        return bool(res.get(\"ok\")), res\n",
    "    if \"check_promotion_gates\" in globals():\n",
    "        res = globals()[\"check_promotion_gates\"]()\n",
    "        return bool(res.get(\"ok\")), res\n",
    "    return True, {\"note\": \"no ci gate in scope\"}\n",
    "def _policy_allow(action: str, ctx: Dict[str, Any]) -> Tuple[bool, str]:\n",
    "    if \"policy_enforce\" in globals():\n",
    "        return globals()[\"policy_enforce\"](action, ctx)\n",
    "    return True, \"no-policy\"\n",
    "def _switch_blue_green(target: Optional[str] = None) -> str:\n",
    "    \"\"\"\n",
    "    best-effort: use switch_control() or write to gateway_control.active\n",
    "    \"\"\"\n",
    "    if \"switch_control\" in globals() and target is None:\n",
    "        nxt = globals()[\"switch_control\"]()\n",
    "        return nxt\n",
    "    active_path = ART_DIR / \"gateway_control.active\"\n",
    "    if target:\n",
    "        active_path.write_text(target, encoding=\"utf-8\")\n",
    "        return target\n",
    "    cur = \"blue\"\n",
    "    if active_path.exists():\n",
    "        cur = active_path.read_text(encoding=\"utf-8\").strip() or \"blue\"\n",
    "    nxt = \"green\" if cur == \"blue\" else \"blue\"\n",
    "    active_path.write_text(nxt, encoding=\"utf-8\")\n",
    "    return nxt\n",
    "def _notify_local(kind: str, payload: Dict[str, Any]) -> None:\n",
    "    if \"notify_slack\" in globals():\n",
    "        try:\n",
    "            globals()[\"notify_slack\"](f\"{kind}: {payload.get('summary', '') or payload}\")\n",
    "        except Exception:\n",
    "            pass\n",
    "    if \"notify_n8n\" in globals():\n",
    "        try:\n",
    "            globals()[\"notify_n8n\"](kind, payload)\n",
    "        except Exception:\n",
    "            pass\n",
    "    log_path = ART_DIR / \"gateway_notify.jsonl\"\n",
    "    _append_jsonl(log_path, {\"ts\": int(time.time()), \"kind\": kind, \"payload\": _redact(payload)})\n",
    "def promote_control_safe(\n",
    "    target: Optional[str] = None,\n",
    "    reason: str = \"manual\",\n",
    "    actor: str = \"notebook\",\n",
    ") -> Dict[str, Any]:\n",
    "    \"\"\"\n",
    "    1) freeze check\n",
    "    2) CI / SLO / golden gates\n",
    "    3) policy-as-code\n",
    "    4) switch control\n",
    "    5) notify + log\n",
    "    \"\"\"\n",
    "    ts = int(time.time())\n",
    "    if _is_frozen_local():\n",
    "        row = {\n",
    "            \"ts\": ts,\n",
    "            \"ok\": False,\n",
    "            \"error\": \"frozen\",\n",
    "            \"reason\": reason,\n",
    "            \"actor\": actor,\n",
    "        }\n",
    "        _append_jsonl(PROMOTE_LOG_PATH, row)\n",
    "        _notify_local(\"promote.blocked\", {\"summary\": \"promotion blocked (frozen)\", **row})\n",
    "        return row\n",
    "    ci_ok, ci_res = _ci_gate_ok()\n",
    "    ctx = {\n",
    "        \"frozen\": False,\n",
    "        \"ci_gate\": ci_ok,\n",
    "        \"actor\": actor,\n",
    "        \"reason\": reason,\n",
    "    }\n",
    "    pol_ok, pol_msg = _policy_allow(\"control.promote\", ctx)\n",
    "    if not ci_ok or not pol_ok:\n",
    "        row = {\n",
    "            \"ts\": ts,\n",
    "            \"ok\": False,\n",
    "            \"error\": \"gate-failed\" if not ci_ok else f\"policy:{pol_msg}\",\n",
    "            \"ci\": ci_res,\n",
    "            \"policy\": pol_msg,\n",
    "            \"actor\": actor,\n",
    "            \"reason\": reason,\n",
    "        }\n",
    "        _append_jsonl(PROMOTE_LOG_PATH, row)\n",
    "        _notify_local(\"promote.blocked\", {\"summary\": \"promotion blocked (gate/policy)\", **row})\n",
    "        return row\n",
    "    active = _switch_blue_green(target)\n",
    "    final = {\n",
    "        \"ts\": ts,\n",
    "        \"ok\": True,\n",
    "        \"switched_to\": active,\n",
    "        \"actor\": actor,\n",
    "        \"reason\": reason,\n",
    "        \"ci\": ci_res,\n",
    "        \"policy\": pol_msg,\n",
    "    }\n",
    "    _append_jsonl(PROMOTE_LOG_PATH, final)\n",
    "    PROMOTE_LAST_PATH.write_text(json.dumps(final, indent=2), encoding=\"utf-8\")\n",
    "    _notify_local(\"promote.ok\", {\"summary\": f\"promoted to {active}\", **final})\n",
    "    if \"push_state_bundle\" in globals():\n",
    "        try:\n",
    "            globals()[\"push_state_bundle\"](url=REMOTE_PUSH_URL or None)\n",
    "        except Exception:\n",
    "            pass\n",
    "    print(f\" promoted  {active}\")\n",
    "    return final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 802,
   "id": "737a983a-383a-4ddf-8573-bf991786be79",
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_n8n_workflow_gateway() -> Path:\n",
    "    \"\"\"\n",
    "    emit a minimal n8n workflow JSON that:\n",
    "      - cron (5m)\n",
    "      - HTTP Node  POST /promote/check\n",
    "      - IF  promotion ok?\n",
    "      - HTTP Node  POST /freeze (if not ok)\n",
    "      - HTTP Node  POST /promote (custom endpoint)  we will call our notebook's promote endpoint indirectly\n",
    "    This is a *template*, user can import into n8n.\n",
    "    \"\"\"\n",
    "    wf = {\n",
    "        \"name\": \"Gateway SLO / Promotion Orchestration\",\n",
    "        \"nodes\": [\n",
    "            {\n",
    "                \"id\": \"1\",\n",
    "                \"name\": \"Cron 5m\",\n",
    "                \"type\": \"n8n-nodes-base.cron\",\n",
    "                \"typeVersion\": 2,\n",
    "                \"position\": [240, 300],\n",
    "                \"parameters\": {\n",
    "                    \"rule\": {\n",
    "                        \"interval\": 5\n",
    "                    }\n",
    "                }\n",
    "            },\n",
    "            {\n",
    "                \"id\": \"2\",\n",
    "                \"name\": \"Check Promotion Gate\",\n",
    "                \"type\": \"n8n-nodes-base.httpRequest\",\n",
    "                \"typeVersion\": 3,\n",
    "                \"position\": [520, 300],\n",
    "                \"parameters\": {\n",
    "                    \"url\": \"http://127.0.0.1:9929/promote/check\",\n",
    "                    \"method\": \"POST\",\n",
    "                    \"jsonParameters\": True,\n",
    "                }\n",
    "            },\n",
    "            {\n",
    "                \"id\": \"3\",\n",
    "                \"name\": \"IF Gate OK?\",\n",
    "                \"type\": \"n8n-nodes-base.if\",\n",
    "                \"typeVersion\": 2,\n",
    "                \"position\": [780, 300],\n",
    "                \"parameters\": {\n",
    "                    \"conditions\": {\n",
    "                        \"boolean\": [\n",
    "                            {\n",
    "                                \"value1\": \"={{ $json[\\\"ok\\\"] }}\",\n",
    "                                \"value2\": True\n",
    "                            }\n",
    "                        ]\n",
    "                    }\n",
    "                }\n",
    "            },\n",
    "            {\n",
    "                \"id\": \"4\",\n",
    "                \"name\": \"Promote (Notebook)\",\n",
    "                \"type\": \"n8n-nodes-base.httpRequest\",\n",
    "                \"typeVersion\": 3,\n",
    "                \"position\": [1040, 180],\n",
    "                \"parameters\": {\n",
    "                    \"url\": \"http://127.0.0.1:9929/promote/run\",  \n",
    "                    \"method\": \"POST\",\n",
    "                    \"jsonParameters\": True,\n",
    "                    \"options\": {},\n",
    "                }\n",
    "            },\n",
    "            {\n",
    "                \"id\": \"5\",\n",
    "                \"name\": \"Freeze (bad SLO)\",\n",
    "                \"type\": \"n8n-nodes-base.httpRequest\",\n",
    "                \"typeVersion\": 3,\n",
    "                \"position\": [1040, 420],\n",
    "                \"parameters\": {\n",
    "                    \"url\": \"http://127.0.0.1:9929/freeze\",\n",
    "                    \"method\": \"POST\",\n",
    "                    \"jsonParameters\": True,\n",
    "                    \"options\": {},\n",
    "                    \"bodyParametersJson\": \"{\\\"reason\\\": \\\"n8n-slo-breach\\\", \\\"ttl_s\\\": 1800}\"\n",
    "                }\n",
    "            },\n",
    "        ],\n",
    "        \"connections\": {\n",
    "            \"Cron 5m\": {\n",
    "                \"main\": [\n",
    "                    [\n",
    "                        {\n",
    "                            \"node\": \"Check Promotion Gate\",\n",
    "                            \"type\": \"main\",\n",
    "                            \"index\": 0\n",
    "                        }\n",
    "                    ]\n",
    "                ]\n",
    "            },\n",
    "            \"Check Promotion Gate\": {\n",
    "                \"main\": [\n",
    "                    [\n",
    "                        {\n",
    "                            \"node\": \"IF Gate OK?\",\n",
    "                            \"type\": \"main\",\n",
    "                            \"index\": 0\n",
    "                        }\n",
    "                    ]\n",
    "                ]\n",
    "            },\n",
    "            \"IF Gate OK?\": {\n",
    "                \"main\": [\n",
    "                    [\n",
    "                        {\n",
    "                            \"node\": \"Promote (Notebook)\",\n",
    "                            \"type\": \"main\",\n",
    "                            \"index\": 0\n",
    "                        }\n",
    "                    ],\n",
    "                    [\n",
    "                        {\n",
    "                            \"node\": \"Freeze (bad SLO)\",\n",
    "                            \"type\": \"main\",\n",
    "                            \"index\": 0\n",
    "                        }\n",
    "                    ]\n",
    "                ]\n",
    "            }\n",
    "        },\n",
    "        \"active\": False,\n",
    "        \"settings\": {},\n",
    "        \"versionId\": f\"gw-{int(time.time())}\"\n",
    "    }\n",
    "    N8N_WORKFLOW_PATH.write_text(json.dumps(wf, indent=2), encoding=\"utf-8\")\n",
    "    print(f\" n8n workflow  {N8N_WORKFLOW_PATH}\")\n",
    "    return N8N_WORKFLOW_PATH\n",
    "def otel_push(endpoint: Optional[str] = None, limit: int = 200) -> Dict[str, Any]:\n",
    "    \"\"\"\n",
    "    push OTEL spans to a remote HTTP collector (OTLP/HTTP-json-ish)\n",
    "    we stay generic: POST JSON array of spans\n",
    "    \"\"\"\n",
    "    endpoint = endpoint or DEFAULT_OTEL_ENDPOINT\n",
    "    spans: List[Dict[str, Any]] = []\n",
    "    if OTEL_SPANS_PATH.exists():\n",
    "        with OTEL_SPANS_PATH.open(\"r\", encoding=\"utf-8\") as f:\n",
    "            for i, line in enumerate(f):\n",
    "                if not line.strip():\n",
    "                    continue\n",
    "                try:\n",
    "                    spans.append(json.loads(line))\n",
    "                except Exception:\n",
    "                    pass\n",
    "                if i + 1 >= limit:\n",
    "                    break\n",
    "    payload = {\n",
    "        \"ts\": int(time.time()),\n",
    "        \"host\": HOSTNAME,\n",
    "        \"spans\": spans,\n",
    "    }\n",
    "    if not endpoint:\n",
    "        OTEL_PUSH_REPORT.write_text(json.dumps(payload, indent=2), encoding=\"utf-8\")\n",
    "        print(f\" otel-push (local only)  {OTEL_PUSH_REPORT}\")\n",
    "        return {\"ok\": True, \"local\": True, \"count\": len(spans)}\n",
    "    try:\n",
    "        import requests\n",
    "        r = requests.post(endpoint, json=payload, timeout=3.0)\n",
    "        out = {\"ok\": 200 <= r.status_code < 300, \"status\": r.status_code, \"count\": len(spans)}\n",
    "    except Exception as e:\n",
    "        out = {\"ok\": False, \"error\": str(e), \"count\": len(spans)}\n",
    "    OTEL_PUSH_REPORT.write_text(json.dumps(out, indent=2), encoding=\"utf-8\")\n",
    "    print(f\" otel-push  {OTEL_PUSH_REPORT} ({out})\")\n",
    "    return out\n",
    "def n8n_outbox_pull(max_items: int = 50) -> Dict[str, Any]:\n",
    "    \"\"\"\n",
    "    let n8n poll messages without re-sending old ones.\n",
    "    we keep a cursor = line offset.\n",
    "    \"\"\"\n",
    "    if not OUTBOX_PATH.exists():\n",
    "        return {\"items\": [], \"cursor\": 0}\n",
    "    cursor = 0\n",
    "    if OUTBOX_CURSOR_PATH.exists():\n",
    "        try:\n",
    "            cursor = int(OUTBOX_CURSOR_PATH.read_text(encoding=\"utf-8\").strip() or \"0\")\n",
    "        except Exception:\n",
    "            cursor = 0\n",
    "    lines = OUTBOX_PATH.read_text(encoding=\"utf-8\").splitlines()\n",
    "    new_lines = lines[cursor:cursor + max_items]\n",
    "    items = []\n",
    "    for ln in new_lines:\n",
    "        try:\n",
    "            items.append(json.loads(ln))\n",
    "        except Exception:\n",
    "            items.append({\"raw\": ln})\n",
    "    new_cursor = cursor + len(new_lines)\n",
    "    OUTBOX_CURSOR_PATH.write_text(str(new_cursor), encoding=\"utf-8\")\n",
    "    rep = {\"items\": items, \"cursor\": new_cursor, \"total\": len(lines)}\n",
    "    print(f\" n8n-outbox pull  {len(items)} item(s), cursor={new_cursor}/{len(lines)}\")\n",
    "    return rep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 803,
   "id": "9896723a-cf07-4109-bf9c-809a504df949",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " n8n workflow  C:\\tmp\\art\\n8n-workflow-gateway.json\n",
      "\n",
      " continuation (safe-promote + n8n-workflow emit + otel-push + outbox-pull + cli12) loaded.\n",
      "Try next:\n",
      "  gw_cli12('promote', target='green', reason='slo-ok')\n",
      "  gw_cli12('n8n.workflow')\n",
      "  gw_cli12('otel.push')\n",
      "  gw_cli12('outbox.pull')\n",
      "  gw_cli12('n8n.http', port=9930)\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    import http.server\n",
    "    import socketserver\n",
    "    class N8NHandlerV2(http.server.BaseHTTPRequestHandler):\n",
    "        def _json(self, code: int, payload: Dict[str, Any]) -> None:\n",
    "            body = json.dumps(payload).encode(\"utf-8\")\n",
    "            self.send_response(code)\n",
    "            self.send_header(\"Content-Type\", \"application/json\")\n",
    "            self.send_header(\"Content-Length\", str(len(body)))\n",
    "            self.end_headers()\n",
    "            self.wfile.write(body)\n",
    "        def do_POST(self):\n",
    "            length = int(self.headers.get(\"content-length\", \"0\") or \"0\")\n",
    "            raw = self.rfile.read(length) if length else b\"{}\"\n",
    "            try:\n",
    "                data = json.loads(raw.decode(\"utf-8\") or \"{}\")\n",
    "            except Exception:\n",
    "                data = {}\n",
    "            if self.path == \"/promote/run\":\n",
    "                res = promote_control_safe(reason=data.get(\"reason\", \"n8n-http\"), actor=\"n8n-http\")\n",
    "                self._json(200, res)\n",
    "                return\n",
    "            if self.path == \"/otel/push\":\n",
    "                res = otel_push(endpoint=data.get(\"endpoint\"))\n",
    "                self._json(200, res)\n",
    "                return\n",
    "            if self.path == \"/outbox/pull\":\n",
    "                res = n8n_outbox_pull(max_items=int(data.get(\"max_items\", 50)))\n",
    "                self._json(200, res)\n",
    "                return\n",
    "            self._json(404, {\"error\": \"not-found\"})\n",
    "        def do_GET(self):\n",
    "            if self.path == \"/outbox/pull\":\n",
    "                res = n8n_outbox_pull(max_items=50)\n",
    "                self._json(200, res)\n",
    "                return\n",
    "            if self.path == \"/promote/last\":\n",
    "                if PROMOTE_LAST_PATH.exists():\n",
    "                    self._json(200, json.loads(PROMOTE_LAST_PATH.read_text(encoding=\"utf-8\")))\n",
    "                else:\n",
    "                    self._json(200, {\"ok\": True, \"note\": \"no promotions yet\"})\n",
    "                return\n",
    "            self._json(404, {\"error\": \"not-found\"})\n",
    "    def serve_n8n_http_v2(port: int = 9930) -> None:\n",
    "        with socketserver.TCPServer((\"0.0.0.0\", port), N8NHandlerV2) as httpd:\n",
    "            print(f\" n8n HTTP v2 at http://0.0.0.0:{port}\")\n",
    "            try:\n",
    "                httpd.serve_forever()\n",
    "            except KeyboardInterrupt:\n",
    "                print(\" n8n HTTP v2 stopped\")\n",
    "except Exception:\n",
    "    pass\n",
    "def gw_cli12(cmd: str, **kwargs) -> None:\n",
    "    cmd = cmd.strip().lower()\n",
    "    if cmd == \"promote\":\n",
    "        promote_control_safe(target=kwargs.get(\"target\"), reason=kwargs.get(\"reason\", \"cli12\"), actor=\"cli12\")\n",
    "    elif cmd == \"n8n.workflow\":\n",
    "        write_n8n_workflow_gateway()\n",
    "    elif cmd == \"otel.push\":\n",
    "        otel_push(endpoint=kwargs.get(\"endpoint\"))\n",
    "    elif cmd == \"outbox.pull\":\n",
    "        n8n_outbox_pull(max_items=int(kwargs.get(\"max_items\", 50)))\n",
    "    elif cmd == \"n8n.http\":\n",
    "        port = int(kwargs.get(\"port\", 9930))\n",
    "        serve_n8n_http_v2(port=port)\n",
    "    else:\n",
    "        print(\" unknown cmd. try:\")\n",
    "        print(\"  gw_cli12('promote', target='green', reason='deploy-123')\")\n",
    "        print(\"  gw_cli12('n8n.workflow')\")\n",
    "        print(\"  gw_cli12('otel.push', endpoint='http://127.0.0.1:4318/v1/traces')\")\n",
    "        print(\"  gw_cli12('outbox.pull', max_items=20)\")\n",
    "        print(\"  gw_cli12('n8n.http', port=9930)\")\n",
    "write_n8n_workflow_gateway()\n",
    "print(\"\\n continuation (safe-promote + n8n-workflow emit + otel-push + outbox-pull + cli12) loaded.\")\n",
    "print(\"Try next:\")\n",
    "print(\"  gw_cli12('promote', target='green', reason='slo-ok')\")\n",
    "print(\"  gw_cli12('n8n.workflow')\")\n",
    "print(\"  gw_cli12('otel.push')\")\n",
    "print(\"  gw_cli12('outbox.pull')\")\n",
    "print(\"  gw_cli12('n8n.http', port=9930)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 804,
   "id": "5103e965-6eeb-4550-a6b9-4058917f1228",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " continuation (prom-export + audit-rollup + policy-edit + tenant-sim + metrics-http + cli13) using ART_DIR=C:\\tmp\\art\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "import time\n",
    "import socket\n",
    "from pathlib import Path\n",
    "from typing import Any, Dict, List, Optional, Tuple\n",
    "import http.server\n",
    "import socketserver\n",
    "ART_DIR = Path(os.getenv(\"ART_DIR\", r\"C:\\tmp\\art\" if os.name == \"nt\" else \"/tmp/art\"))\n",
    "ART_DIR.mkdir(parents=True, exist_ok=True)\n",
    "print(f\" continuation (prom-export + audit-rollup + policy-edit + tenant-sim + metrics-http + cli13) using ART_DIR={ART_DIR}\")\n",
    "HOSTNAME = socket.gethostname()\n",
    "PROM_TEXT_PATH = ART_DIR / \"gateway_prometheus.txt\"\n",
    "AUDIT_REPORT_PATH = ART_DIR / \"gateway_audit_report.json\"\n",
    "TENANT_SIM_PATH = ART_DIR / \"gateway_tenant_sim.json\"\n",
    "POLICY_CODE_PATH = ART_DIR / \"gateway_policy.json\"  \n",
    "METRICS_HTTP_PORT = int(os.getenv(\"GATEWAY_METRICS_PORT\", \"9940\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 805,
   "id": "2f5c021f-27c3-4963-be82-cee097cb4479",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _load_json_safe(p: Path, default: Any = None) -> Any:\n",
    "    if not p.exists():\n",
    "        return default\n",
    "    try:\n",
    "        return json.loads(p.read_text(encoding=\"utf-8\"))\n",
    "    except Exception:\n",
    "        return default\n",
    "def write_prometheus_export(out: Path = PROM_TEXT_PATH) -> Path:\n",
    "    lines: List[str] = []\n",
    "    now = int(time.time())\n",
    "    lines.append(f'# scraped_at {now}')\n",
    "    lines.append(f'gateway_info{{host=\"{HOSTNAME}\"}} 1')\n",
    "    slo = _load_json_safe(ART_DIR / \"gateway_slo_eval.json\", {}) or {}\n",
    "    burn = _load_json_safe(ART_DIR / \"gateway_slo_burn.json\", {}) or {}\n",
    "    prom_gate = _load_json_safe(ART_DIR / \"gateway_promotion_gate.json\", {}) or {}\n",
    "    tenants = slo.get(\"tenants\") or {}\n",
    "    for name, spec in tenants.items():\n",
    "        p95 = spec.get(\"actual_p95_ms\", 0)\n",
    "        err = spec.get(\"actual_error_rate\", 0)\n",
    "        lines.append(f'gateway_slo_p95_ms{{tenant=\"{name}\"}} {p95}')\n",
    "        lines.append(f'gateway_slo_error_rate{{tenant=\"{name}\"}} {err}')\n",
    "    if burn.get(\"fast\"):\n",
    "        lines.append(f'gateway_burn_rate{{window=\"fast\"}} {burn[\"fast\"].get(\"burn_rate\", 0)}')\n",
    "    if burn.get(\"slow\"):\n",
    "        lines.append(f'gateway_burn_rate{{window=\"slow\"}} {burn[\"slow\"].get(\"burn_rate\", 0)}')\n",
    "    gate_ok = 1 if prom_gate.get(\"ok\", False) else 0\n",
    "    lines.append(f'gateway_promotion_gate_ok  {gate_ok}')\n",
    "    fr = _load_json_safe(ART_DIR / \"gateway_freeze.json\", {}) or {}\n",
    "    frozen = 1 if fr.get(\"frozen\") else 0\n",
    "    lines.append(f'gateway_frozen  {frozen}')\n",
    "    dlq = ART_DIR / \"n8n-dlq.jsonl\"\n",
    "    dlq_sz = 0\n",
    "    if dlq.exists():\n",
    "        dlq_sz = dlq.stat().st_size\n",
    "    lines.append(f'gateway_dlq_size_bytes  {dlq_sz}')\n",
    "    nlog = ART_DIR / \"gateway_notify.jsonl\"\n",
    "    nlog_sz = 0\n",
    "    if nlog.exists():\n",
    "        nlog_sz = nlog.stat().st_size\n",
    "    lines.append(f'gateway_notify_log_size_bytes  {nlog_sz}')\n",
    "    out.write_text(\"\\n\".join(lines) + \"\\n\", encoding=\"utf-8\")\n",
    "    print(f\" prom-export  {out}\")\n",
    "    return out\n",
    "def build_audit_report(out: Path = AUDIT_REPORT_PATH) -> Path:\n",
    "    chaos_path = ART_DIR / \"gateway_chaos_audit.jsonl\"\n",
    "    promote_path = ART_DIR / \"gateway_promotions.jsonl\"\n",
    "    dlq_path = ART_DIR / \"n8n-dlq.jsonl\"\n",
    "    notify_path = ART_DIR / \"gateway_notify.jsonl\"\n",
    "    def _read_lines(p: Path, limit: int = 500) -> List[Dict[str, Any]]:\n",
    "        items: List[Dict[str, Any]] = []\n",
    "        if not p.exists():\n",
    "            return items\n",
    "        with p.open(\"r\", encoding=\"utf-8\") as f:\n",
    "            for i, line in enumerate(f):\n",
    "                line = line.strip()\n",
    "                if not line:\n",
    "                    continue\n",
    "                try:\n",
    "                    items.append(json.loads(line))\n",
    "                except Exception:\n",
    "                    items.append({\"raw\": line})\n",
    "                if i + 1 >= limit:\n",
    "                    break\n",
    "        return items\n",
    "    rep = {\n",
    "        \"ts\": int(time.time()),\n",
    "        \"host\": HOSTNAME,\n",
    "        \"chaos\": _read_lines(chaos_path),\n",
    "        \"promotions\": _read_lines(promote_path),\n",
    "        \"dlq\": _read_lines(dlq_path),\n",
    "        \"notify\": _read_lines(notify_path),\n",
    "    }\n",
    "    out.write_text(json.dumps(rep, indent=2), encoding=\"utf-8\")\n",
    "    print(f\" audit-report  {out}\")\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 808,
   "id": "d8dd0176-4429-48a1-814d-e0d3fd64fbda",
   "metadata": {},
   "outputs": [],
   "source": [
    "def policy_add_deny(action: str, when_frozen: bool = True) -> Path:\n",
    "    pol = _load_json_safe(POLICY_CODE_PATH, {}) or {\"rules\": []}\n",
    "    rules = pol.get(\"rules\") or []\n",
    "    cond = {\"frozen\": True} if when_frozen else {}\n",
    "    rules.append({\n",
    "        \"id\": f\"auto-{action}-{int(time.time())}\",\n",
    "        \"if\": cond,\n",
    "        \"deny\": [action],\n",
    "    })\n",
    "    pol[\"rules\"] = rules\n",
    "    pol[\"ts\"] = int(time.time())\n",
    "    POLICY_CODE_PATH.write_text(json.dumps(pol, indent=2), encoding=\"utf-8\")\n",
    "    print(f\" policy updated  {POLICY_CODE_PATH}\")\n",
    "    return POLICY_CODE_PATH\n",
    "def write_tenant_sim(out: Path = TENANT_SIM_PATH) -> Path:\n",
    "    slo = _load_json_safe(ART_DIR / \"gateway_slo_eval.json\", {}) or {}\n",
    "    tenants = slo.get(\"tenants\") or {}\n",
    "    rows: List[Dict[str, Any]] = []\n",
    "    for name, spec in tenants.items():\n",
    "        rows.append({\n",
    "            \"tenant\": name,\n",
    "            \"p95_ms\": spec.get(\"actual_p95_ms\", 0),\n",
    "            \"error_rate\": spec.get(\"actual_error_rate\", 0),\n",
    "            \"burn_fast\": 0.0,\n",
    "        })\n",
    "    burn = _load_json_safe(ART_DIR / \"gateway_slo_burn.json\", {}) or {}\n",
    "    for r in rows:\n",
    "        r[\"burn_fast\"] = (burn.get(\"fast\") or {}).get(\"burn_rate\", 0.0)\n",
    "    snap = {\n",
    "        \"ts\": int(time.time()),\n",
    "        \"host\": HOSTNAME,\n",
    "        \"tenants\": rows,\n",
    "    }\n",
    "    out.write_text(json.dumps(snap, indent=2), encoding=\"utf-8\")\n",
    "    print(f\" tenant-sim  {out}\")\n",
    "    return out\n",
    "class MetricsHandler(http.server.BaseHTTPRequestHandler):\n",
    "    def _send_json(self, code: int, payload: Dict[str, Any]):\n",
    "        body = json.dumps(payload).encode(\"utf-8\")\n",
    "        self.send_response(code)\n",
    "        self.send_header(\"Content-Type\", \"application/json\")\n",
    "        self.send_header(\"Content-Length\", str(len(body)))\n",
    "        self.end_headers()\n",
    "        self.wfile.write(body)\n",
    "    def _send_file(self, path: Path, ctype: str = \"text/plain\"):\n",
    "        if not path.exists():\n",
    "            self.send_response(404)\n",
    "            self.end_headers()\n",
    "            self.wfile.write(b\"not found\")\n",
    "            return\n",
    "        data = path.read_bytes()\n",
    "        self.send_response(200)\n",
    "        self.send_header(\"Content-Type\", ctype)\n",
    "        self.send_header(\"Content-Length\", str(len(data)))\n",
    "        self.end_headers()\n",
    "        self.wfile.write(data)\n",
    "    def do_GET(self):\n",
    "        if self.path == \"/metrics\":\n",
    "            p = write_prometheus_export()\n",
    "            self._send_file(p, \"text/plain; version=0.0.4\")\n",
    "            return\n",
    "        if self.path == \"/audit\":\n",
    "            p = build_audit_report()\n",
    "            self._send_file(p, \"application/json\")\n",
    "            return\n",
    "        if self.path == \"/tenant-sim\":\n",
    "            p = write_tenant_sim()\n",
    "            self._send_file(p, \"application/json\")\n",
    "            return\n",
    "        self._send_json(404, {\"error\": \"not-found\"})\n",
    "def serve_metrics_http(port: int = METRICS_HTTP_PORT) -> None:\n",
    "    with socketserver.TCPServer((\"0.0.0.0\", port), MetricsHandler) as httpd:\n",
    "        print(f\" metrics HTTP at http://0.0.0.0:{port}\")\n",
    "        try:\n",
    "            httpd.serve_forever()\n",
    "        except KeyboardInterrupt:\n",
    "            print(\" metrics HTTP stopped\")\n",
    "def gw_cli13(cmd: str, **kwargs) -> None:\n",
    "    cmd = cmd.strip().lower()\n",
    "    if cmd == \"metrics\":\n",
    "        write_prometheus_export()\n",
    "    elif cmd == \"audit\":\n",
    "        build_audit_report()\n",
    "    elif cmd == \"policy.deny\":\n",
    "        act = kwargs.get(\"action\", \"chaos.apply\")\n",
    "        policy_add_deny(act, when_frozen=bool(kwargs.get(\"when_frozen\", True)))\n",
    "    elif cmd == \"tenant.sim\":\n",
    "        write_tenant_sim()\n",
    "    elif cmd == \"serve.metrics\":\n",
    "        port = int(kwargs.get(\"port\", METRICS_HTTP_PORT))\n",
    "        serve_metrics_http(port=port)\n",
    "    else:\n",
    "        print(\" unknown cmd. try:\")\n",
    "        print(\"  gw_cli13('metrics')\")\n",
    "        print(\"  gw_cli13('audit')\")\n",
    "        print(\"  gw_cli13('policy.deny', action='control.import')\")\n",
    "        print(\"  gw_cli13('tenant.sim')\")\n",
    "        print(\"  gw_cli13('serve.metrics', port=9940)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 809,
   "id": "8982d0db-29a4-4826-a5d2-c14f72d63b67",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " prom-export  C:\\tmp\\art\\gateway_prometheus.txt\n",
      " audit-report  C:\\tmp\\art\\gateway_audit_report.json\n",
      " tenant-sim  C:\\tmp\\art\\gateway_tenant_sim.json\n",
      "\n",
      " continuation (prom-export + audit-rollup + policy-edit + tenant-sim + metrics-http + cli13) loaded.\n",
      "Try:\n",
      "  gw_cli13('metrics')\n",
      "  gw_cli13('audit')\n",
      "  gw_cli13('policy.deny', action='chaos.apply')\n",
      "  gw_cli13('tenant.sim')\n",
      "  gw_cli13('serve.metrics', port=9940')\n"
     ]
    }
   ],
   "source": [
    "write_prometheus_export()\n",
    "build_audit_report()\n",
    "write_tenant_sim()\n",
    "print(\"\\n continuation (prom-export + audit-rollup + policy-edit + tenant-sim + metrics-http + cli13) loaded.\")\n",
    "print(\"Try:\")\n",
    "print(\"  gw_cli13('metrics')\")\n",
    "print(\"  gw_cli13('audit')\")\n",
    "print(\"  gw_cli13('policy.deny', action='chaos.apply')\")\n",
    "print(\"  gw_cli13('tenant.sim')\")\n",
    "print(\"  gw_cli13('serve.metrics', port=9940')\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 810,
   "id": "70ff788f-3d79-4691-9b74-2354cd587a9d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " continuation (timeline + anomaly-scan + remediation-hints + archive + webhook-stub + cli14) using ART_DIR=C:\\tmp\\art\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "import time\n",
    "import math\n",
    "import gzip\n",
    "from pathlib import Path\n",
    "from typing import Any, Dict, List, Optional, Tuple\n",
    "ART_DIR = Path(os.getenv(\"ART_DIR\", r\"C:\\tmp\\art\" if os.name == \"nt\" else \"/tmp/art\"))\n",
    "ART_DIR.mkdir(parents=True, exist_ok=True)\n",
    "print(f\" continuation (timeline + anomaly-scan + remediation-hints + archive + webhook-stub + cli14) using ART_DIR={ART_DIR}\")\n",
    "TIMELINE_PATH = ART_DIR / \"gateway_timeline.json\"\n",
    "TIMELINE_HTML_PATH = ART_DIR / \"gateway_timeline.html\"\n",
    "ANOMALY_REPORT_PATH = ART_DIR / \"gateway_anomalies.json\"\n",
    "REMEDIATION_PATH = ART_DIR / \"gateway_remediation_plan.json\"\n",
    "ARCHIVE_DIR = ART_DIR / \"archive\"\n",
    "ARCHIVE_DIR.mkdir(parents=True, exist_ok=True)\n",
    "WEBHOOK_INBOX = ART_DIR / \"gateway_webhook_inbox.jsonl\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 811,
   "id": "8a8b51ee-4a24-4c56-a79a-3fa3308984ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _load_json_safe(path: Path, default: Any = None) -> Any:\n",
    "    if not path.exists():\n",
    "        return default\n",
    "    try:\n",
    "        return json.loads(path.read_text(encoding=\"utf-8\"))\n",
    "    except Exception:\n",
    "        return default\n",
    "def _read_jsonl(path: Path, limit: int = 1000) -> List[Dict[str, Any]]:\n",
    "    if not path.exists():\n",
    "        return []\n",
    "    out: List[Dict[str, Any]] = []\n",
    "    with path.open(\"r\", encoding=\"utf-8\") as f:\n",
    "        for i, line in enumerate(f):\n",
    "            line = line.strip()\n",
    "            if not line:\n",
    "                continue\n",
    "            try:\n",
    "                out.append(json.loads(line))\n",
    "            except Exception:\n",
    "                out.append({\"raw\": line})\n",
    "            if i + 1 >= limit:\n",
    "                break\n",
    "    return out\n",
    "def _append_jsonl(path: Path, row: Dict[str, Any]) -> None:\n",
    "    with path.open(\"a\", encoding=\"utf-8\") as f:\n",
    "        f.write(json.dumps(row) + \"\\n\")\n",
    "def build_timeline() -> Path:\n",
    "    \"\"\"\n",
    "    Build a single time-ordered view out of:\n",
    "      - chaos audit\n",
    "      - promotions\n",
    "      - DLQ\n",
    "      - notify\n",
    "      - SLO evals\n",
    "      - freeze/unfreeze\n",
    "    Render as JSON and also to a tiny HTML list.\n",
    "    \"\"\"\n",
    "    events: List[Dict[str, Any]] = []\n",
    "    now = int(time.time())\n",
    "    chaos = _read_jsonl(ART_DIR / \"gateway_chaos_audit.jsonl\", limit=500)\n",
    "    for c in chaos:\n",
    "        events.append({\n",
    "            \"ts\": c.get(\"ts\", now),\n",
    "            \"kind\": \"chaos.apply\",\n",
    "            \"data\": c,\n",
    "        })\n",
    "    promos = _read_jsonl(ART_DIR / \"gateway_promotions.jsonl\", limit=200)\n",
    "    for p in promos:\n",
    "        events.append({\n",
    "            \"ts\": p.get(\"ts\", now),\n",
    "            \"kind\": \"promotion\",\n",
    "            \"data\": p,\n",
    "        })\n",
    "    dlq = _read_jsonl(ART_DIR / \"n8n-dlq.jsonl\", limit=200)\n",
    "    for d in dlq:\n",
    "        events.append({\n",
    "            \"ts\": d.get(\"ts\", now),\n",
    "            \"kind\": \"dlq\",\n",
    "            \"data\": d,\n",
    "        })\n",
    "    noti = _read_jsonl(ART_DIR / \"gateway_notify.jsonl\", limit=200)\n",
    "    for n in noti:\n",
    "        events.append({\n",
    "            \"ts\": n.get(\"ts\", now),\n",
    "            \"kind\": \"notify\",\n",
    "            \"data\": n,\n",
    "        })\n",
    "    fr = _load_json_safe(ART_DIR / \"gateway_freeze.json\", {})\n",
    "    if fr:\n",
    "        events.append({\n",
    "            \"ts\": fr.get(\"ts\", now),\n",
    "            \"kind\": \"freeze-state\",\n",
    "            \"data\": fr,\n",
    "        })\n",
    "    slo_eval = _load_json_safe(ART_DIR / \"gateway_slo_eval.json\", {})\n",
    "    if slo_eval:\n",
    "        events.append({\n",
    "            \"ts\": slo_eval.get(\"ts\", now),\n",
    "            \"kind\": \"slo-eval\",\n",
    "            \"data\": slo_eval,\n",
    "        })\n",
    "    events_sorted = sorted(events, key=lambda x: x.get(\"ts\", 0), reverse=False)\n",
    "    timeline = {\n",
    "        \"ts\": now,\n",
    "        \"count\": len(events_sorted),\n",
    "        \"events\": events_sorted,\n",
    "    }\n",
    "    TIMELINE_PATH.write_text(json.dumps(timeline, indent=2), encoding=\"utf-8\")\n",
    "    print(f\" timeline  {TIMELINE_PATH}\")\n",
    "    html: List[str] = []\n",
    "    html.append(\"<html><head><title>Gateway Timeline</title></head><body>\")\n",
    "    html.append(f\"<h1>Gateway Timeline ({len(events_sorted)} events)</h1>\")\n",
    "    html.append(\"<ul>\")\n",
    "    for ev in events_sorted[-200:]:   \n",
    "        ts_s = time.strftime(\"%Y-%m-%d %H:%M:%S\", time.localtime(ev.get(\"ts\", now)))\n",
    "        kind = ev.get(\"kind\", \"event\")\n",
    "        data = ev.get(\"data\", {})\n",
    "        summary = data.get(\"summary\") or data.get(\"reason\") or \"\"\n",
    "        html.append(f\"<li><b>{ts_s}</b>  <code>{kind}</code> {summary}</li>\")\n",
    "    html.append(\"</ul>\")\n",
    "    html.append(\"</body></html>\")\n",
    "    TIMELINE_HTML_PATH.write_text(\"\\n\".join(html), encoding=\"utf-8\")\n",
    "    print(f\" timeline HTML  {TIMELINE_HTML_PATH}\")\n",
    "    return TIMELINE_PATH"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 812,
   "id": "09fb2adf-002c-4410-8696-0585ba36cd6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _zscore(val: float, mean: float, std: float) -> float:\n",
    "    if std == 0:\n",
    "        return 0.0\n",
    "    return (val - mean) / std\n",
    "def scan_anomalies(window: int = 20, z_thresh: float = 2.2) -> Dict[str, Any]:\n",
    "    \"\"\"\n",
    "    Look through *recent* metrics and flag weird spikes.\n",
    "    We'll use:\n",
    "      - gateway_slo_burn.json (fast/slow)\n",
    "      - audit/promotions for high frequency promotions\n",
    "    This is deliberately dumb  it's enough for notebook.\n",
    "    \"\"\"\n",
    "    now = int(time.time())\n",
    "    burn = _load_json_safe(ART_DIR / \"gateway_slo_burn.json\", {}) or {}\n",
    "    fast = (burn.get(\"fast\") or {}).get(\"burn_rate\", 0.0)\n",
    "    slow = (burn.get(\"slow\") or {}).get(\"burn_rate\", 0.0)\n",
    "    hist_path = ART_DIR / \"gateway_anomaly_hist.jsonl\"\n",
    "    _append_jsonl(hist_path, {\n",
    "        \"ts\": now,\n",
    "        \"fast\": fast,\n",
    "        \"slow\": slow,\n",
    "    })\n",
    "    hist = _read_jsonl(hist_path, limit=window)\n",
    "    fast_vals = [h.get(\"fast\", 0.0) for h in hist]\n",
    "    slow_vals = [h.get(\"slow\", 0.0) for h in hist]\n",
    "    def _mean_std(xs: List[float]) -> Tuple[float, float]:\n",
    "        if not xs:\n",
    "            return 0.0, 0.0\n",
    "        m = sum(xs) / len(xs)\n",
    "        var = sum((x - m) ** 2 for x in xs) / max(len(xs) - 1, 1)\n",
    "        return m, math.sqrt(var)\n",
    "    f_mean, f_std = _mean_std(fast_vals)\n",
    "    s_mean, s_std = _mean_std(slow_vals)\n",
    "    f_z = _zscore(fast, f_std and f_mean or 0.0, f_std)  \n",
    "    s_z = _zscore(slow, s_std and s_mean or 0.0, s_std)\n",
    "    anomalies: List[str] = []\n",
    "    if fast > 1.0:\n",
    "        anomalies.append(f\"fast burn > 1.0 ({fast})\")\n",
    "    if slow > 1.0:\n",
    "        anomalies.append(f\"slow burn > 1.0 ({slow})\")\n",
    "    if abs(f_z) > z_thresh:\n",
    "        anomalies.append(f\"fast burn z-score {round(f_z, 2)} > {z_thresh}\")\n",
    "    if abs(s_z) > z_thresh:\n",
    "        anomalies.append(f\"slow burn z-score {round(s_z, 2)} > {z_thresh}\")\n",
    "    rep = {\n",
    "        \"ts\": now,\n",
    "        \"fast\": fast,\n",
    "        \"slow\": slow,\n",
    "        \"fast_mean\": f_mean,\n",
    "        \"fast_std\": f_std,\n",
    "        \"slow_mean\": s_mean,\n",
    "        \"slow_std\": s_std,\n",
    "        \"fast_z\": f_z,\n",
    "        \"slow_z\": s_z,\n",
    "        \"anomalies\": anomalies,\n",
    "        \"ok\": not anomalies,\n",
    "    }\n",
    "    ANOMALY_REPORT_PATH.write_text(json.dumps(rep, indent=2), encoding=\"utf-8\")\n",
    "    print(f\" anomaly-scan  {ANOMALY_REPORT_PATH} (ok={rep['ok']})\")\n",
    "    if anomalies:\n",
    "        alert = {\n",
    "            \"ts\": now,\n",
    "            \"event\": \"anomaly.detected\",\n",
    "            \"data\": rep,\n",
    "            \"host\": os.getenv(\"HOSTNAME\") or \"notebook\",\n",
    "        }\n",
    "        outbox = ART_DIR / \"n8n-outbox.jsonl\"\n",
    "        _append_jsonl(outbox, alert)\n",
    "    return rep\n",
    "def build_remediation_plan() -> Path:\n",
    "    \"\"\"\n",
    "    Look at latest:\n",
    "      - promotion gate\n",
    "      - anomalies\n",
    "      - freeze\n",
    "    and output a human-readable \"do this next\".\n",
    "    \"\"\"\n",
    "    gate = _load_json_safe(ART_DIR / \"gateway_promotion_gate.json\", {}) or {}\n",
    "    anom = _load_json_safe(ANOMALY_REPORT_PATH, {}) or {}\n",
    "    fr = _load_json_safe(ART_DIR / \"gateway_freeze.json\", {}) or {}\n",
    "    steps: List[str] = []\n",
    "    if fr.get(\"frozen\"):\n",
    "        steps.append(\"1. Gateway is FROZEN  unfreeze only after investigation.\")\n",
    "    if not gate.get(\"ok\", True):\n",
    "        steps.append(\"2. Promotion gate FAILED  check gateway_promotion_gate.json for reasons.\")\n",
    "    if anom.get(\"anomalies\"):\n",
    "        steps.append(\"3. Anomalies detected  check burn / p95 and re-run gw_cli9('retention').\")\n",
    "    if not steps:\n",
    "        steps.append(\"1. System looks healthy. You can proceed with promote: gw_cli12('promote').\")\n",
    "    plan = {\n",
    "        \"ts\": int(time.time()),\n",
    "        \"summary\": steps[0] if steps else \"healthy\",\n",
    "        \"steps\": steps,\n",
    "        \"gate\": gate,\n",
    "        \"anomalies\": anom,\n",
    "        \"frozen\": fr,\n",
    "    }\n",
    "    REMEDIATION_PATH.write_text(json.dumps(plan, indent=2), encoding=\"utf-8\")\n",
    "    print(f\" remediation  {REMEDIATION_PATH}\")\n",
    "    return REMEDIATION_PATH\n",
    "def archive_old_artifacts(max_age_s: int = 3 * 24 * 3600) -> Dict[str, Any]:\n",
    "    \"\"\"\n",
    "    Find large/old JSON/JSONL/HTML and gzip them into /tmp/art/archive\n",
    "    \"\"\"\n",
    "    now = time.time()\n",
    "    archived: List[str] = []\n",
    "    for p in ART_DIR.glob(\"**/*\"):\n",
    "        if not p.is_file():\n",
    "            continue\n",
    "        if p.parent == ARCHIVE_DIR:\n",
    "            continue\n",
    "        if p.suffix.lower() not in (\".json\", \".jsonl\", \".html\", \".txt\"):\n",
    "            continue\n",
    "        age = now - p.stat().st_mtime\n",
    "        if age < max_age_s:\n",
    "            continue\n",
    "        gz = ARCHIVE_DIR / f\"{p.name}.gz\"\n",
    "        with gzip.open(gz, \"wb\") as f:\n",
    "            f.write(p.read_bytes())\n",
    "        archived.append(p.name)\n",
    "    rep = {\"ts\": int(now), \"archived\": archived}\n",
    "    print(f\" archive  {rep}\")\n",
    "    return rep\n",
    "def webhook_ingest(payload: Dict[str, Any]) -> None:\n",
    "    \"\"\"\n",
    "    Drop external events (e.g. from CI, GH, PagerDuty) into inbox.\n",
    "    \"\"\"\n",
    "    safe = {\n",
    "        \"ts\": int(time.time()),\n",
    "        \"payload\": payload,\n",
    "    }\n",
    "    _append_jsonl(WEBHOOK_INBOX, safe)\n",
    "    print(f\" webhook-ingest  {WEBHOOK_INBOX}\")\n",
    "try:\n",
    "    import http.server\n",
    "    import socketserver\n",
    "    class WebhookHandler(http.server.BaseHTTPRequestHandler):\n",
    "        def _json(self, code: int, payload: Dict[str, Any]) -> None:\n",
    "            body = json.dumps(payload).encode(\"utf-8\")\n",
    "            self.send_response(code)\n",
    "            self.send_header(\"Content-Type\", \"application/json\")\n",
    "            self.send_header(\"Content-Length\", str(len(body)))\n",
    "            self.end_headers()\n",
    "            self.wfile.write(body)\n",
    "        def do_POST(self):\n",
    "            ln = int(self.headers.get(\"content-length\") or \"0\")\n",
    "            raw = self.rfile.read(ln) if ln else b\"{}\"\n",
    "            try:\n",
    "                data = json.loads(raw.decode(\"utf-8\") or \"{}\")\n",
    "            except Exception:\n",
    "                data = {\"raw\": raw.decode(\"utf-8\", \"ignore\")}\n",
    "            webhook_ingest(data)\n",
    "            self._json(200, {\"ok\": True})\n",
    "    def serve_webhook(port: int = 9945) -> None:\n",
    "        with socketserver.TCPServer((\"0.0.0.0\", port), WebhookHandler) as httpd:\n",
    "            print(f\" webhook HTTP at http://0.0.0.0:{port}\")\n",
    "            try:\n",
    "                httpd.serve_forever()\n",
    "            except KeyboardInterrupt:\n",
    "                print(\" webhook HTTP stopped\")\n",
    "except Exception:\n",
    "    def serve_webhook(port: int = 9945) -> None:\n",
    "        print(\" http.server not available in this env\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 813,
   "id": "97ab2ae5-23fa-4ef2-bc50-1bc98046a3ad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " timeline  C:\\tmp\\art\\gateway_timeline.json\n",
      " timeline HTML  C:\\tmp\\art\\gateway_timeline.html\n",
      " anomaly-scan  C:\\tmp\\art\\gateway_anomalies.json (ok=False)\n",
      " remediation  C:\\tmp\\art\\gateway_remediation_plan.json\n",
      "\n",
      " continuation (timeline + anomaly-scan + remediation-hints + archive + webhook-stub + cli14) loaded.\n",
      "Try next:\n",
      "  gw_cli14('timeline')\n",
      "  gw_cli14('anomaly.scan')\n",
      "  gw_cli14('remediation')\n",
      "  gw_cli14('archive')\n",
      "  gw_cli14('webhook.serve', port=9945)\n"
     ]
    }
   ],
   "source": [
    "def gw_cli14(cmd: str, **kwargs) -> None:\n",
    "    cmd = cmd.strip().lower()\n",
    "    if cmd == \"timeline\":\n",
    "        build_timeline()\n",
    "    elif cmd == \"anomaly.scan\":\n",
    "        scan_anomalies()\n",
    "    elif cmd == \"remediation\":\n",
    "        build_remediation_plan()\n",
    "    elif cmd == \"archive\":\n",
    "        archive_old_artifacts()\n",
    "    elif cmd == \"webhook.serve\":\n",
    "        port = int(kwargs.get(\"port\", 9945))\n",
    "        serve_webhook(port=port)\n",
    "    elif cmd == \"webhook.ingest\":\n",
    "        payload = kwargs.get(\"payload\") or {\"test\": True}\n",
    "        webhook_ingest(payload)\n",
    "    else:\n",
    "        print(\" unknown cmd. try:\")\n",
    "        print(\"  gw_cli14('timeline')\")\n",
    "        print(\"  gw_cli14('anomaly.scan')\")\n",
    "        print(\"  gw_cli14('remediation')\")\n",
    "        print(\"  gw_cli14('archive')\")\n",
    "        print(\"  gw_cli14('webhook.serve', port=9945)\")\n",
    "        print(\"  gw_cli14('webhook.ingest', payload={'source': 'ci', 'status': 'fail'})\")\n",
    "build_timeline()\n",
    "scan_anomalies()\n",
    "build_remediation_plan()\n",
    "print(\"\\n continuation (timeline + anomaly-scan + remediation-hints + archive + webhook-stub + cli14) loaded.\")\n",
    "print(\"Try next:\")\n",
    "print(\"  gw_cli14('timeline')\")\n",
    "print(\"  gw_cli14('anomaly.scan')\")\n",
    "print(\"  gw_cli14('remediation')\")\n",
    "print(\"  gw_cli14('archive')\")\n",
    "print(\"  gw_cli14('webhook.serve', port=9945)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 814,
   "id": "69a70ee9-9345-44c9-bc1b-0697cd5101c3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " continuation (config + eval_slos + per-tenant gates + golden-promote + secure-http + boot) using ART_DIR=C:\\tmp\\art\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "import time\n",
    "import threading\n",
    "import socket\n",
    "from http.server import HTTPServer\n",
    "from pathlib import Path\n",
    "from typing import Any, Dict, List, Optional, Tuple, Callable\n",
    "ART_DIR = Path(os.getenv(\"ART_DIR\", r\"C:\\tmp\\art\" if os.name == \"nt\" else \"/tmp/art\"))\n",
    "ART_DIR.mkdir(parents=True, exist_ok=True)\n",
    "print(f\" continuation (config + eval_slos + per-tenant gates + golden-promote + secure-http + boot) using ART_DIR={ART_DIR}\")\n",
    "HOSTNAME = socket.gethostname()\n",
    "CONFIG_PATH = ART_DIR / \"gateway_config.json\"\n",
    "SLO_DEF_PATH = ART_DIR / \"gateway_slo.json\"               \n",
    "SLA_RUNS_PATH = ART_DIR / \"gateway_sla_runs.json\"         \n",
    "PROM_TS_PATH = ART_DIR / \"gateway_prom_timeseries.json\"   \n",
    "SLO_EVAL_PATH = ART_DIR / \"gateway_slo_eval.json\"         \n",
    "GOLDEN_BASELINE_PATH = ART_DIR / \"gateway_golden_results.baseline.json\"\n",
    "BOOT_REPORT_PATH = ART_DIR / \"gateway_boot_report.json\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 815,
   "id": "b45c5ceb-679b-4ce0-92e4-fd0ffac84b7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "DEFAULT_CONFIG = {\n",
    "    \"ts\": int(time.time()),\n",
    "    \"schema\": \"1.0.0\",\n",
    "    \"env\": \"dev\",\n",
    "    \"http\": {\n",
    "        \"serve_n8n_v1\": True,\n",
    "        \"serve_n8n_v2\": True,\n",
    "        \"serve_metrics\": True,\n",
    "        \"serve_webhook\": True,\n",
    "        \"serve_admin\": False,  \n",
    "        \"n8n_v1_port\": 9929,\n",
    "        \"n8n_v2_port\": 9930,\n",
    "        \"metrics_port\": 9940,\n",
    "        \"webhook_port\": 9945,\n",
    "    },\n",
    "    \"security\": {\n",
    "        \"require_admin_token\": False,\n",
    "        \"require_mtls\": False,\n",
    "    },\n",
    "    \"tenants\": [\n",
    "        \"default\"\n",
    "    ],\n",
    "    \"promotion\": {\n",
    "        \"default_tenant\": \"default\"\n",
    "    }\n",
    "}\n",
    "def load_config() -> Dict[str, Any]:\n",
    "    if CONFIG_PATH.exists():\n",
    "        try:\n",
    "            cfg = json.loads(CONFIG_PATH.read_text(encoding=\"utf-8\"))\n",
    "            return cfg\n",
    "        except Exception as e:\n",
    "            print(f\" failed to read config: {e}\")\n",
    "    CONFIG_PATH.write_text(json.dumps(DEFAULT_CONFIG, indent=2), encoding=\"utf-8\")\n",
    "    return DEFAULT_CONFIG\n",
    "def _load_json_safe(p: Path, default: Any = None) -> Any:\n",
    "    if not p.exists():\n",
    "        return default\n",
    "    try:\n",
    "        return json.loads(p.read_text(encoding=\"utf-8\"))\n",
    "    except Exception:\n",
    "        return default\n",
    "def eval_slos(\n",
    "    slo_def_path: Path = SLO_DEF_PATH,\n",
    "    sla_runs_path: Path = SLA_RUNS_PATH,\n",
    "    prom_ts_path: Path = PROM_TS_PATH,\n",
    "    out_path: Path = SLO_EVAL_PATH,\n",
    ") -> Path:\n",
    "    \"\"\"\n",
    "    take desired SLOs (what we want) + observed metrics (what we got)\n",
    "     write gateway_slo_eval.json in the shape the rest of the notebook expects\n",
    "\n",
    "    shape:\n",
    "    {\n",
    "      \"ts\": ...,\n",
    "      \"tenants\": {\n",
    "        \"default\": {\n",
    "          \"target_p95_ms\": ...,\n",
    "          \"actual_p95_ms\": ...,\n",
    "          \"target_error_rate\": ...,\n",
    "          \"actual_error_rate\": ...\n",
    "        },\n",
    "        ...\n",
    "      }\n",
    "    }\n",
    "    \"\"\"\n",
    "    slo_def = _load_json_safe(slo_def_path, {}) or {}\n",
    "    wanted_tenants = slo_def.get(\"tenants\") or {\"default\": slo_def}  \n",
    "    runs = _load_json_safe(sla_runs_path, {}) or {}\n",
    "    prom = _load_json_safe(prom_ts_path, {}) or {}\n",
    "    now = int(time.time())\n",
    "    tenants_eval: Dict[str, Dict[str, Any]] = {}\n",
    "    per_tenant_runs: Dict[str, List[Dict[str, Any]]] = {}\n",
    "    for r in runs.get(\"runs\", []):\n",
    "        tname = r.get(\"tenant\") or \"default\"\n",
    "        per_tenant_runs.setdefault(tname, []).append(r)\n",
    "    for tname, tdef in wanted_tenants.items():\n",
    "        tgt_p95 = tdef.get(\"target_p95_ms\") or tdef.get(\"p95_ms\") or 1500\n",
    "        tgt_err = tdef.get(\"target_error_rate\") or tdef.get(\"error_rate\") or 0.03\n",
    "        obs_p95 = None\n",
    "        obs_err = None\n",
    "        if tname in per_tenant_runs and per_tenant_runs[tname]:\n",
    "            rs = per_tenant_runs[tname][-20:]\n",
    "            obs_p95 = max(r.get(\"p95_ms\", tgt_p95 * 2) for r in rs)\n",
    "            total = sum(r.get(\"count\", 0) for r in rs)\n",
    "            errs = sum(r.get(\"errors\", 0) for r in rs)\n",
    "            obs_err = (errs / total) if total else 0.0\n",
    "        else:\n",
    "            mlist = prom.get(\"metrics\") or []\n",
    "            obs_p95 = tgt_p95 * 0.9  \n",
    "            obs_err = 0.0\n",
    "            for m in mlist:\n",
    "                if m.get(\"name\") == \"gateway_answer_latency_ms\":\n",
    "                    obs_p95 = max(obs_p95, float(m.get(\"p95\") or tgt_p95))\n",
    "        tenants_eval[tname] = {\n",
    "            \"target_p95_ms\": tgt_p95,\n",
    "            \"actual_p95_ms\": obs_p95,\n",
    "            \"target_error_rate\": tgt_err,\n",
    "            \"actual_error_rate\": obs_err,\n",
    "            \"ok\": (obs_p95 is not None and obs_p95 <= tgt_p95) and (obs_err is not None and obs_err <= tgt_err),\n",
    "        }\n",
    "    out = {\n",
    "        \"ts\": now,\n",
    "        \"schema\": \"1.0.0\",\n",
    "        \"tenants\": tenants_eval,\n",
    "    }\n",
    "    out_path.write_text(json.dumps(out, indent=2), encoding=\"utf-8\")\n",
    "    print(f\" SLO eval  {out_path}\")\n",
    "    return out_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 816,
   "id": "e9c50956-48f7-41d6-b020-095a95ce244f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_promotion_gates_tenant(tenant: str) -> Dict[str, Any]:\n",
    "    \"\"\"\n",
    "    wrap the existing check_promotion_gates() but force tenant\n",
    "    we assume check_promotion_gates is already defined in the notebook\n",
    "    \"\"\"\n",
    "    base_path = ART_DIR / \"gateway_slo_eval.json\"\n",
    "    if not base_path.exists():\n",
    "        eval_slos()\n",
    "    slo = _load_json_safe(base_path, {}) or {}\n",
    "    burn = _load_json_safe(ART_DIR / \"gateway_slo_burn.json\", {}) or {}\n",
    "    pol = _load_json_safe(ART_DIR / \"gateway_promotion_policy.json\", {}) or {\n",
    "        \"required\": {\n",
    "            \"p95_ms\": 1500,\n",
    "            \"error_rate\": 0.03,\n",
    "            \"burn_fast\": 1.0,\n",
    "            \"burn_slow\": 1.0,\n",
    "        }\n",
    "    }\n",
    "    tenants = slo.get(\"tenants\") or {}\n",
    "    ns = tenants.get(tenant) or tenants.get(\"default\") or {}\n",
    "    actual_p95 = ns.get(\"actual_p95_ms\") or 999999\n",
    "    actual_err = ns.get(\"actual_error_rate\") or 1.0\n",
    "    fast_br = (burn.get(\"fast\") or {}).get(\"burn_rate\", 0.0)\n",
    "    slow_br = (burn.get(\"slow\") or {}).get(\"burn_rate\", 0.0)\n",
    "    req = pol.get(\"required\") or {}\n",
    "    ok = True\n",
    "    reasons = []\n",
    "    if actual_p95 > req.get(\"p95_ms\", 1500):\n",
    "        ok = False\n",
    "        reasons.append(f\"{tenant}: p95 {actual_p95} > {req.get('p95_ms')}\")\n",
    "    if actual_err > req.get(\"error_rate\", 0.03):\n",
    "        ok = False\n",
    "        reasons.append(f\"{tenant}: err {actual_err} > {req.get('error_rate')}\")\n",
    "    if fast_br > req.get(\"burn_fast\", 1.0):\n",
    "        ok = False\n",
    "        reasons.append(f\"{tenant}: fast burn {fast_br} > {req.get('burn_fast')}\")\n",
    "    if slow_br > req.get(\"burn_slow\", 1.0):\n",
    "        ok = False\n",
    "        reasons.append(f\"{tenant}: slow burn {slow_br} > {req.get('burn_slow')}\")\n",
    "    res = {\n",
    "        \"ts\": int(time.time()),\n",
    "        \"ok\": ok,\n",
    "        \"tenant\": tenant,\n",
    "        \"reasons\": reasons,\n",
    "    }\n",
    "    (ART_DIR / f\"gateway_promotion_gate.{tenant}.json\").write_text(json.dumps(res, indent=2), encoding=\"utf-8\")\n",
    "    print(f\" per-tenant gate ({tenant})  {res}\")\n",
    "    return res\n",
    "def golden_promote(\n",
    "    current_path: Path = ART_DIR / \"gateway_golden_results.json\",\n",
    "    baseline_path: Path = GOLDEN_BASELINE_PATH,\n",
    "    reason: str = \"manual\",\n",
    ") -> Path:\n",
    "    \"\"\"\n",
    "    accept the current golden results as the new baseline\n",
    "    useful after legit behavior change\n",
    "    \"\"\"\n",
    "    if not current_path.exists():\n",
    "        raise FileNotFoundError(str(current_path))\n",
    "    data = current_path.read_text(encoding=\"utf-8\")\n",
    "    baseline_path.write_text(data, encoding=\"utf-8\")\n",
    "    audit_line = {\n",
    "        \"ts\": int(time.time()),\n",
    "        \"kind\": \"golden.promote\",\n",
    "        \"reason\": reason,\n",
    "        \"baseline\": baseline_path.as_posix(),\n",
    "    }\n",
    "    (ART_DIR / \"gateway_goldens_audit.jsonl\").open(\"a\", encoding=\"utf-8\").write(json.dumps(audit_line) + \"\\n\")\n",
    "    print(f\" golden promoted  {baseline_path}\")\n",
    "    return baseline_path\n",
    "def http_guard(handler_cls, *, require_token: bool = False, allowed_tokens: Optional[List[str]] = None):\n",
    "    \"\"\"\n",
    "    wrap an existing BaseHTTPRequestHandler to check token & policy.\n",
    "    we reuse admin_allowed(...) if it's in globals()\n",
    "    \"\"\"\n",
    "    allowed_tokens = allowed_tokens or []\n",
    "    class Secured(handler_cls):\n",
    "        def _is_allowed(self_inner) -> Tuple[bool, str]:\n",
    "            ip = self_inner.client_address[0] if self_inner.client_address else \"unknown\"\n",
    "            tok = self_inner.headers.get(\"x-admin-token\", \"\")\n",
    "            path = self_inner.path\n",
    "            if require_token:\n",
    "                if tok and (tok in allowed_tokens or tok == os.getenv(\"GATEWAY_ADMIN_TOKEN\", \"\").strip()):\n",
    "                    return True, \"token\"\n",
    "            if \"admin_allowed\" in globals():\n",
    "                ok, why = globals()[\"admin_allowed\"](ip, tok, path, actor=\"http\")\n",
    "                return ok, why\n",
    "            if ip in (\"127.0.0.1\", \"::1\"):\n",
    "                return True, \"localhost\"\n",
    "            return False, \"denied\"\n",
    "        def do_GET(self_inner):\n",
    "            ok, why = self_inner._is_allowed()\n",
    "            if not ok:\n",
    "                self_inner.send_response(403)\n",
    "                self_inner.end_headers()\n",
    "                self_inner.wfile.write(b'{\"error\":\"forbidden\"}')\n",
    "                return\n",
    "            return super(Secured, self_inner).do_GET()\n",
    "        def do_POST(self_inner):\n",
    "            ok, why = self_inner._is_allowed()\n",
    "            if not ok:\n",
    "                self_inner.send_response(403)\n",
    "                self_inner.end_headers()\n",
    "                self_inner.wfile.write(b'{\"error\":\"forbidden\"}')\n",
    "                return\n",
    "            return super(Secured, self_inner).do_POST()\n",
    "    return Secured\n",
    "def _try_spawn(target: Callable, name: str) -> None:\n",
    "    th = threading.Thread(target=target, name=name, daemon=True)\n",
    "    th.start()\n",
    "    print(f\" started {name}\")\n",
    "def boot_all() -> Path:\n",
    "    \"\"\"\n",
    "    start all HTTP endpoints we defined earlier **if** their functions exist\n",
    "    this is the missing glue\n",
    "    \"\"\"\n",
    "    cfg = load_config()\n",
    "    http_cfg = (cfg.get(\"http\") or {})\n",
    "    started = []\n",
    "    if http_cfg.get(\"serve_n8n_v1\", False) and \"serve_n8n_http\" in globals():\n",
    "        port = int(http_cfg.get(\"n8n_v1_port\", 9929))\n",
    "        _try_spawn(lambda: globals()[\"serve_n8n_http\"](port=port), f\"n8n-v1:{port}\")\n",
    "        started.append(f\"n8n-v1:{port}\")\n",
    "    if http_cfg.get(\"serve_n8n_v2\", False) and \"serve_n8n_http_v2\" in globals():\n",
    "        port = int(http_cfg.get(\"n8n_v2_port\", 9930))\n",
    "        _try_spawn(lambda: globals()[\"serve_n8n_http_v2\"](port=port), f\"n8n-v2:{port}\")\n",
    "        started.append(f\"n8n-v2:{port}\")\n",
    "    if http_cfg.get(\"serve_metrics\", False) and \"serve_metrics_http\" in globals():\n",
    "        port = int(http_cfg.get(\"metrics_port\", 9940))\n",
    "        _try_spawn(lambda: globals()[\"serve_metrics_http\"](port=port), f\"metrics:{port}\")\n",
    "        started.append(f\"metrics:{port}\")\n",
    "    if http_cfg.get(\"serve_webhook\", False) and \"serve_webhook\" in globals():\n",
    "        port = int(http_cfg.get(\"webhook_port\", 9945))\n",
    "        _try_spawn(lambda: globals()[\"serve_webhook\"](port=port), f\"webhook:{port}\")\n",
    "        started.append(f\"webhook:{port}\")\n",
    "    def _scheduler_loop():\n",
    "        while True:\n",
    "            if \"run_scheduler_once\" in globals():\n",
    "                globals()[\"run_scheduler_once\"]()\n",
    "            if \"scan_anomalies\" in globals():\n",
    "                globals()[\"scan_anomalies\"]()\n",
    "            if \"build_timeline\" in globals():\n",
    "                globals()[\"build_timeline\"]()\n",
    "            time.sleep(60)\n",
    "    _try_spawn(_scheduler_loop, \"scheduler\")\n",
    "    rep = {\n",
    "        \"ts\": int(time.time()),\n",
    "        \"host\": HOSTNAME,\n",
    "        \"started\": started,\n",
    "    }\n",
    "    BOOT_REPORT_PATH.write_text(json.dumps(rep, indent=2), encoding=\"utf-8\")\n",
    "    print(f\" boot-all  {BOOT_REPORT_PATH}\")\n",
    "    return BOOT_REPORT_PATH\n",
    "def smoke_tests() -> Dict[str, Any]:\n",
    "    problems = []\n",
    "    if not CONFIG_PATH.exists():\n",
    "        problems.append(\"config-missing\")\n",
    "    try:\n",
    "        eval_slos()\n",
    "    except Exception as e:\n",
    "        problems.append(f\"slo-eval:{e}\")\n",
    "    if not (ART_DIR / \"gateway_ci_full.json\").exists():\n",
    "        problems.append(\"ci-full-missing\")\n",
    "    ok = not problems\n",
    "    rep = {\n",
    "        \"ts\": int(time.time()),\n",
    "        \"ok\": ok,\n",
    "        \"problems\": problems,\n",
    "    }\n",
    "    (ART_DIR / \"gateway_smoke_report.json\").write_text(json.dumps(rep, indent=2), encoding=\"utf-8\")\n",
    "    print(f\" smoke  {rep}\")\n",
    "    return rep\n",
    "def gw_cli15(cmd: str, **kwargs) -> None:\n",
    "    cmd = cmd.strip().lower()\n",
    "    if cmd == \"config.show\":\n",
    "        print(json.dumps(load_config(), indent=2))\n",
    "    elif cmd == \"config.write\":\n",
    "        cfg = kwargs.get(\"config\") or load_config()\n",
    "        cfg[\"ts\"] = int(time.time())\n",
    "        CONFIG_PATH.write_text(json.dumps(cfg, indent=2), encoding=\"utf-8\")\n",
    "        print(f\" config  {CONFIG_PATH}\")\n",
    "    elif cmd == \"slo.eval\":\n",
    "        eval_slos()\n",
    "    elif cmd == \"gate.tenant\":\n",
    "        t = kwargs.get(\"tenant\", \"default\")\n",
    "        check_promotion_gates_tenant(t)\n",
    "    elif cmd == \"golden.promote\":\n",
    "        golden_promote(reason=kwargs.get(\"reason\", \"cli15\"))\n",
    "    elif cmd == \"boot\":\n",
    "        boot_all()\n",
    "    elif cmd == \"smoke\":\n",
    "        smoke_tests()\n",
    "    else:\n",
    "        print(\" unknown cmd. try:\")\n",
    "        print(\"  gw_cli15('config.show')\")\n",
    "        print(\"  gw_cli15('slo.eval')\")\n",
    "        print(\"  gw_cli15('gate.tenant', tenant='acme')\")\n",
    "        print(\"  gw_cli15('golden.promote', reason='new-behavior')\")\n",
    "        print(\"  gw_cli15('boot')\")\n",
    "        print(\"  gw_cli15('smoke')\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 817,
   "id": "329773f0-8196-4d5c-a059-a7dde5477aba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " SLO eval  C:\\tmp\\art\\gateway_slo_eval.json\n",
      " SLO eval  C:\\tmp\\art\\gateway_slo_eval.json\n",
      " smoke  {'ts': 1762119875, 'ok': True, 'problems': []}\n",
      "\n",
      " continuation (config + eval_slos + per-tenant + golden-promote + boot + cli15) loaded.\n",
      "Try next:\n",
      "  gw_cli15('config.show')\n",
      "  gw_cli15('boot')\n",
      "  gw_cli15('gate.tenant', tenant='default')\n",
      "  gw_cli15('golden.promote')\n"
     ]
    }
   ],
   "source": [
    "eval_slos()\n",
    "smoke_tests()\n",
    "print(\"\\n continuation (config + eval_slos + per-tenant + golden-promote + boot + cli15) loaded.\")\n",
    "print(\"Try next:\")\n",
    "print(\"  gw_cli15('config.show')\")\n",
    "print(\"  gw_cli15('boot')\")\n",
    "print(\"  gw_cli15('gate.tenant', tenant='default')\")\n",
    "print(\"  gw_cli15('golden.promote')\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 818,
   "id": "45ba7e2d-22d5-4180-b4d9-9399ec5d5669",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " continuation (profiles + secure-boot + state-snapshot + readme + tests + cli16) using ART_DIR=C:\\tmp\\art\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "import time\n",
    "import socket\n",
    "import threading\n",
    "from pathlib import Path\n",
    "from typing import Any, Dict, List, Optional, Tuple, Callable\n",
    "ART_DIR = Path(os.getenv(\"ART_DIR\", r\"C:\\tmp\\art\" if os.name == \"nt\" else \"/tmp/art\"))\n",
    "ART_DIR.mkdir(parents=True, exist_ok=True)\n",
    "print(f\" continuation (profiles + secure-boot + state-snapshot + readme + tests + cli16) using ART_DIR={ART_DIR}\")\n",
    "HOSTNAME = socket.gethostname()\n",
    "CONFIG_PATH = ART_DIR / \"gateway_config.json\"\n",
    "PROFILES_DIR = ART_DIR / \"profiles\"\n",
    "PROFILES_DIR.mkdir(parents=True, exist_ok=True)\n",
    "STATE_SNAPSHOT_PATH = ART_DIR / \"gateway_state_snapshot.json\"\n",
    "README_PATH = ART_DIR / \"gateway_README.md\"\n",
    "TEST_REPORT_PATH = ART_DIR / \"gateway_test_report.json\"\n",
    "ADMIN_TOKEN_FILE = ART_DIR / \"admin_token.txt\"   \n",
    "DEFAULT_ADMIN_TOKEN = os.getenv(\"GATEWAY_ADMIN_TOKEN\", \"\").strip()\n",
    "DEFAULT_PROFILES = {\n",
    "    \"dev\": {\n",
    "        \"http\": {\n",
    "            \"serve_n8n_v1\": True,\n",
    "            \"serve_n8n_v2\": True,\n",
    "            \"serve_metrics\": True,\n",
    "            \"serve_webhook\": True,\n",
    "            \"serve_admin\": False,\n",
    "            \"n8n_v1_port\": 9929,\n",
    "            \"n8n_v2_port\": 9930,\n",
    "            \"metrics_port\": 9940,\n",
    "            \"webhook_port\": 9945,\n",
    "        },\n",
    "        \"security\": {\n",
    "            \"require_admin_token\": False,\n",
    "            \"require_mtls\": False,\n",
    "        },\n",
    "    },\n",
    "    \"stage\": {\n",
    "        \"http\": {\n",
    "            \"serve_n8n_v1\": True,\n",
    "            \"serve_n8n_v2\": True,\n",
    "            \"serve_metrics\": True,\n",
    "            \"serve_webhook\": True,\n",
    "            \"serve_admin\": False,\n",
    "            \"n8n_v1_port\": 10929,\n",
    "            \"n8n_v2_port\": 10930,\n",
    "            \"metrics_port\": 10940,\n",
    "            \"webhook_port\": 10945,\n",
    "        },\n",
    "        \"security\": {\n",
    "            \"require_admin_token\": True,\n",
    "            \"require_mtls\": False,\n",
    "        },\n",
    "    },\n",
    "    \"prod\": {\n",
    "        \"http\": {\n",
    "            \"serve_n8n_v1\": True,\n",
    "            \"serve_n8n_v2\": True,\n",
    "            \"serve_metrics\": True,\n",
    "            \"serve_webhook\": False,  \n",
    "            \"serve_admin\": False,\n",
    "            \"n8n_v1_port\": 20929,\n",
    "            \"n8n_v2_port\": 20930,\n",
    "            \"metrics_port\": 20940,\n",
    "            \"webhook_port\": 20945,\n",
    "        },\n",
    "        \"security\": {\n",
    "            \"require_admin_token\": True,\n",
    "            \"require_mtls\": True,   \n",
    "        },\n",
    "    },\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 819,
   "id": "643dc4f6-2b49-44e7-9eb7-4e1061cd546f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " profile template  C:\\tmp\\art\\profiles\\dev.json\n",
      " profile template  C:\\tmp\\art\\profiles\\stage.json\n",
      " profile template  C:\\tmp\\art\\profiles\\prod.json\n"
     ]
    }
   ],
   "source": [
    "def _json_safe(obj):\n",
    "    if isinstance(obj, Path):\n",
    "        return str(obj)\n",
    "    if isinstance(obj, dict):\n",
    "        return {k: _json_safe(v) for k, v in obj.items()}\n",
    "    if isinstance(obj, (list, tuple)):\n",
    "        return [_json_safe(x) for x in obj]\n",
    "    return obj\n",
    "def load_config() -> Dict[str, Any]:\n",
    "    if CONFIG_PATH.exists():\n",
    "        try:\n",
    "            return json.loads(CONFIG_PATH.read_text(encoding=\"utf-8\"))\n",
    "        except Exception:\n",
    "            pass\n",
    "    if \"DEFAULT_CONFIG\" in globals():\n",
    "        cfg = globals()[\"DEFAULT_CONFIG\"]\n",
    "    else:\n",
    "        cfg = {\n",
    "            \"ts\": int(time.time()),\n",
    "            \"schema\": \"1.0.0\",\n",
    "            \"env\": \"dev\",\n",
    "            \"http\": {},\n",
    "            \"security\": {},\n",
    "        }\n",
    "    CONFIG_PATH.write_text(json.dumps(cfg, indent=2), encoding=\"utf-8\")\n",
    "    return cfg\n",
    "def _profile_path(name: str) -> Path:\n",
    "    return PROFILES_DIR / f\"{name}.json\"\n",
    "def ensure_profiles() -> None:\n",
    "    \"\"\"write dev / stage / prod templates if missing\"\"\"\n",
    "    for name, data in DEFAULT_PROFILES.items():\n",
    "        p = _profile_path(name)\n",
    "        if not p.exists():\n",
    "            data_out = {\n",
    "                \"ts\": int(time.time()),\n",
    "                \"name\": name,\n",
    "                **data,\n",
    "            }\n",
    "            p.write_text(json.dumps(data_out, indent=2), encoding=\"utf-8\")\n",
    "            print(f\" profile template  {p}\")\n",
    "ensure_profiles()\n",
    "def load_profile(name: str) -> Optional[Dict[str, Any]]:\n",
    "    p = _profile_path(name)\n",
    "    if not p.exists():\n",
    "        return None\n",
    "    try:\n",
    "        return json.loads(p.read_text(encoding=\"utf-8\"))\n",
    "    except Exception:\n",
    "        return None\n",
    "def merge_dict(base: Dict[str, Any], over: Dict[str, Any]) -> Dict[str, Any]:\n",
    "    out = dict(base)\n",
    "    for k, v in over.items():\n",
    "        if isinstance(v, dict) and isinstance(out.get(k), dict):\n",
    "            out[k] = merge_dict(out[k], v)\n",
    "        else:\n",
    "            out[k] = v\n",
    "    return out\n",
    "def apply_profile_to_config(profile_name: str) -> Dict[str, Any]:\n",
    "    cfg = load_config()\n",
    "    prof = load_profile(profile_name)\n",
    "    if not prof:\n",
    "        raise ValueError(f\"profile {profile_name} not found\")\n",
    "    merged = merge_dict(cfg, prof)\n",
    "    merged[\"env\"] = profile_name\n",
    "    merged[\"ts\"] = int(time.time())\n",
    "    CONFIG_PATH.write_text(json.dumps(merged, indent=2), encoding=\"utf-8\")\n",
    "    print(f\" config updated from profile='{profile_name}'  {CONFIG_PATH}\")\n",
    "    return merged\n",
    "def _load_json_safe(path: Path, default: Any = None) -> Any:\n",
    "    if not path.exists():\n",
    "        return default\n",
    "    try:\n",
    "        return json.loads(path.read_text(encoding=\"utf-8\"))\n",
    "    except Exception:\n",
    "        return default\n",
    "def load_admin_tokens() -> List[str]:\n",
    "    toks: List[str] = []\n",
    "    env_tok = os.getenv(\"GATEWAY_ADMIN_TOKEN\", \"\").strip()\n",
    "    if env_tok:\n",
    "        toks.append(env_tok)\n",
    "    if ADMIN_TOKEN_FILE.exists():\n",
    "        try:\n",
    "            t = ADMIN_TOKEN_FILE.read_text(encoding=\"utf-8\").strip()\n",
    "            if t:\n",
    "                toks.append(t)\n",
    "        except Exception:\n",
    "            pass\n",
    "    az_path = ART_DIR / \"gateway_admin_allowlist.json\"\n",
    "    if az_path.exists():\n",
    "        try:\n",
    "            az = json.loads(az_path.read_text(encoding=\"utf-8\"))\n",
    "            toks.extend(az.get(\"allow_tokens\") or [])\n",
    "        except Exception:\n",
    "            pass\n",
    "    return list(dict.fromkeys(toks))  \n",
    "def wrap_existing_servers_with_security() -> None:\n",
    "    \"\"\"\n",
    "    Some of earlier handlers were defined without token check.\n",
    "    If they are in globals(), rebind them to secured versions.\n",
    "    \"\"\"\n",
    "    cfg = load_config()\n",
    "    sec = cfg.get(\"security\") or {}\n",
    "    require_tok = bool(sec.get(\"require_admin_token\", False))\n",
    "    toks = load_admin_tokens()\n",
    "    if \"N8NHandler\" in globals():\n",
    "        globals()[\"N8NHandlerSecured\"] = globals()[\"http_guard\"](globals()[\"N8NHandler\"],\n",
    "                                                                 require_token=require_tok,\n",
    "                                                                 allowed_tokens=toks)\n",
    "        print(\" secured  N8NHandler  N8NHandlerSecured\")\n",
    "    if \"N8NHandlerV2\" in globals():\n",
    "        globals()[\"N8NHandlerV2Secured\"] = globals()[\"http_guard\"](globals()[\"N8NHandlerV2\"],\n",
    "                                                                   require_token=require_tok,\n",
    "                                                                   allowed_tokens=toks)\n",
    "        print(\" secured  N8NHandlerV2  N8NHandlerV2Secured\")\n",
    "    if \"MetricsHandler\" in globals():\n",
    "        globals()[\"MetricsHandlerSecured\"] = globals()[\"http_guard\"](globals()[\"MetricsHandler\"],\n",
    "                                                                     require_token=require_tok,\n",
    "                                                                     allowed_tokens=toks)\n",
    "        print(\" secured  MetricsHandler  MetricsHandlerSecured\")\n",
    "    if \"WebhookHandler\" in globals():\n",
    "        globals()[\"WebhookHandlerSecured\"] = globals()[\"http_guard\"](globals()[\"WebhookHandler\"],\n",
    "                                                                     require_token=require_tok,\n",
    "                                                                     allowed_tokens=toks)\n",
    "        print(\" secured  WebhookHandler  WebhookHandlerSecured\")\n",
    "def boot_all_secure() -> Path:\n",
    "    \"\"\"\n",
    "    Same as boot_all() but uses secured handlers if available.\n",
    "    If boot_all() exists already  call it after ensuring security.\n",
    "    \"\"\"\n",
    "    wrap_existing_servers_with_security()\n",
    "    started = []\n",
    "    cfg = load_config()\n",
    "    http_cfg = cfg.get(\"http\") or {}\n",
    "    def _spawn(fn: Callable, name: str):\n",
    "        th = threading.Thread(target=fn, name=name, daemon=True)\n",
    "        th.start()\n",
    "        print(f\" started {name}\")\n",
    "        started.append(name)\n",
    "    if http_cfg.get(\"serve_n8n_v1\", False):\n",
    "        if \"N8NHandlerSecured\" in globals():\n",
    "            import socketserver, http.server\n",
    "            def _run_v1():\n",
    "                port = int(http_cfg.get(\"n8n_v1_port\", 9929))\n",
    "                with socketserver.TCPServer((\"0.0.0.0\", port), globals()[\"N8NHandlerSecured\"]) as httpd:\n",
    "                    print(f\" secure n8n v1 at http://0.0.0.0:{port}\")\n",
    "                    httpd.serve_forever()\n",
    "            _spawn(_run_v1, f\"n8n-v1-secure:{http_cfg.get('n8n_v1_port', 9929)}\")\n",
    "        elif \"serve_n8n_http\" in globals():\n",
    "            _spawn(lambda: globals()[\"serve_n8n_http\"](port=int(http_cfg.get(\"n8n_v1_port\", 9929))),\n",
    "                   f\"n8n-v1:{http_cfg.get('n8n_v1_port', 9929)}\")\n",
    "    if http_cfg.get(\"serve_n8n_v2\", False):\n",
    "        if \"N8NHandlerV2Secured\" in globals():\n",
    "            import socketserver\n",
    "            def _run_v2():\n",
    "                port = int(http_cfg.get(\"n8n_v2_port\", 9930))\n",
    "                with socketserver.TCPServer((\"0.0.0.0\", port), globals()[\"N8NHandlerV2Secured\"]) as httpd:\n",
    "                    print(f\" secure n8n v2 at http://0.0.0.0:{port}\")\n",
    "                    httpd.serve_forever()\n",
    "            _spawn(_run_v2, f\"n8n-v2-secure:{http_cfg.get('n8n_v2_port', 9930)}\")\n",
    "        elif \"serve_n8n_http_v2\" in globals():\n",
    "            _spawn(lambda: globals()[\"serve_n8n_http_v2\"](port=int(http_cfg.get(\"n8n_v2_port\", 9930))),\n",
    "                   f\"n8n-v2:{http_cfg.get('n8n_v2_port', 9930)}\")\n",
    "    if http_cfg.get(\"serve_metrics\", False):\n",
    "        if \"MetricsHandlerSecured\" in globals():\n",
    "            import socketserver\n",
    "            def _run_metrics():\n",
    "                port = int(http_cfg.get(\"metrics_port\", 9940))\n",
    "                with socketserver.TCPServer((\"0.0.0.0\", port), globals()[\"MetricsHandlerSecured\"]) as httpd:\n",
    "                    print(f\" secure metrics at http://0.0.0.0:{port}/metrics\")\n",
    "                    httpd.serve_forever()\n",
    "            _spawn(_run_metrics, f\"metrics-secure:{http_cfg.get('metrics_port', 9940)}\")\n",
    "        elif \"serve_metrics_http\" in globals():\n",
    "            _spawn(lambda: globals()[\"serve_metrics_http\"](port=int(http_cfg.get(\"metrics_port\", 9940))),\n",
    "                   f\"metrics:{http_cfg.get('metrics_port', 9940)}\")\n",
    "    if http_cfg.get(\"serve_webhook\", False):\n",
    "        if \"WebhookHandlerSecured\" in globals():\n",
    "            import socketserver\n",
    "            def _run_webhook():\n",
    "                port = int(http_cfg.get(\"webhook_port\", 9945))\n",
    "                with socketserver.TCPServer((\"0.0.0.0\", port), globals()[\"WebhookHandlerSecured\"]) as httpd:\n",
    "                    print(f\" secure webhook at http://0.0.0.0:{port}\")\n",
    "                    httpd.serve_forever()\n",
    "            _spawn(_run_webhook, f\"webhook-secure:{http_cfg.get('webhook_port', 9945)}\")\n",
    "        elif \"serve_webhook\" in globals():\n",
    "            _spawn(lambda: globals()[\"serve_webhook\"](port=int(http_cfg.get(\"webhook_port\", 9945))),\n",
    "                   f\"webhook:{http_cfg.get('webhook_port', 9945)}\")\n",
    "    def _scheduler_loop():\n",
    "        while True:\n",
    "            if \"run_scheduler_once\" in globals():\n",
    "                globals()[\"run_scheduler_once\"]()\n",
    "            if \"scan_anomalies\" in globals():\n",
    "                globals()[\"scan_anomalies\"]()\n",
    "            if \"build_timeline\" in globals():\n",
    "                globals()[\"build_timeline\"]()\n",
    "            time.sleep(60)\n",
    "    _spawn(_scheduler_loop, \"scheduler\")\n",
    "    rep = {\n",
    "        \"ts\": int(time.time()),\n",
    "        \"host\": HOSTNAME,\n",
    "        \"started\": started,\n",
    "        \"secure\": True,\n",
    "    }\n",
    "    (ART_DIR / \"gateway_boot_secure.json\").write_text(json.dumps(rep, indent=2), encoding=\"utf-8\")\n",
    "    print(f\" boot-all-secure  {ART_DIR / 'gateway_boot_secure.json'}\")\n",
    "    return ART_DIR / \"gateway_boot_secure.json\"\n",
    "def state_snapshot() -> Path:\n",
    "    \"\"\"\n",
    "    one-file view of everything important so far\n",
    "    \"\"\"\n",
    "    snap: Dict[str, Any] = {\n",
    "        \"ts\": int(time.time()),\n",
    "        \"host\": HOSTNAME,\n",
    "        \"config\": _load_json_safe(CONFIG_PATH, {}),\n",
    "        \"slo_eval\": _load_json_safe(ART_DIR / \"gateway_slo_eval.json\", {}),\n",
    "        \"promotion_gate\": _load_json_safe(ART_DIR / \"gateway_promotion_gate.json\", {}),\n",
    "        \"promotion_last\": _load_json_safe(ART_DIR / \"gateway_last_promotion.json\", {}),\n",
    "        \"freeze\": _load_json_safe(ART_DIR / \"gateway_freeze.json\", {}),\n",
    "        \"audit\": _load_json_safe(ART_DIR / \"gateway_audit_report.json\", {}),\n",
    "        \"timeline\": _load_json_safe(ART_DIR / \"gateway_timeline.json\", {}),\n",
    "        \"ci_full\": _load_json_safe(ART_DIR / \"gateway_ci_full.json\", {}),\n",
    "        \"tests\": _load_json_safe(ART_DIR / \"gateway_test_report.json\", {}),\n",
    "    }\n",
    "    STATE_SNAPSHOT_PATH.write_text(json.dumps(_json_safe(snap), indent=2), encoding=\"utf-8\")\n",
    "    print(f\" state-snapshot  {STATE_SNAPSHOT_PATH}\")\n",
    "    return STATE_SNAPSHOT_PATH\n",
    "def write_readme() -> Path:\n",
    "    cfg = load_config()\n",
    "    lines: List[str] = []\n",
    "    lines.append(\"# Gateway Control Plane (Notebook Build)\")\n",
    "    lines.append(\"\")\n",
    "    lines.append(f\"- Host: `{HOSTNAME}`\")\n",
    "    lines.append(f\"- Env: `{cfg.get('env', 'dev')}`\")\n",
    "    lines.append(f\"- Generated: {time.ctime()}\")\n",
    "    lines.append(\"\")\n",
    "    lines.append(\"## Quickstart\")\n",
    "    lines.append(\"```bash\")\n",
    "    lines.append(\"python notebook.py  # your jupyter-run\")\n",
    "    lines.append(\"gw_cli15('boot')     # or gw_cli16('boot.secure')\")\n",
    "    lines.append(\"```\")\n",
    "    lines.append(\"\")\n",
    "    lines.append(\"## HTTP Ports\")\n",
    "    http_cfg = cfg.get(\"http\") or {}\n",
    "    for k in (\"n8n_v1_port\", \"n8n_v2_port\", \"metrics_port\", \"webhook_port\"):\n",
    "        if k in http_cfg:\n",
    "            lines.append(f\"- {k}: `{http_cfg[k]}`\")\n",
    "    lines.append(\"\")\n",
    "    lines.append(\"## Security\")\n",
    "    sec = cfg.get(\"security\") or {}\n",
    "    lines.append(f\"- require_admin_token: {sec.get('require_admin_token', False)}\")\n",
    "    lines.append(f\"- require_mtls: {sec.get('require_mtls', False)}\")\n",
    "    lines.append(\"\")\n",
    "    lines.append(\"## Useful CLI\")\n",
    "    lines.append(\"- gw_cli9('freeze', reason='incident')\")\n",
    "    lines.append(\"- gw_cli12('promote', reason='deploy')\")\n",
    "    lines.append(\"- gw_cli13('metrics')\")\n",
    "    lines.append(\"- gw_cli14('timeline')\")\n",
    "    lines.append(\"- gw_cli15('slo.eval')\")\n",
    "    lines.append(\"- gw_cli16('snapshot')\")\n",
    "    README_PATH.write_text(\"\\n\".join(lines), encoding=\"utf-8\")\n",
    "    print(f\" README  {README_PATH}\")\n",
    "    return README_PATH\n",
    "def run_notebook_tests() -> Dict[str, Any]:\n",
    "    \"\"\"\n",
    "    light-weight \"am I broken?\" tests\n",
    "    \"\"\"\n",
    "    problems: List[str] = []\n",
    "    try:\n",
    "        if \"eval_slos\" in globals():\n",
    "            globals()[\"eval_slos\"]()\n",
    "        else:\n",
    "            problems.append(\"eval_slos-missing\")\n",
    "    except Exception as e:\n",
    "        problems.append(f\"eval_slos:{e}\")\n",
    "    try:\n",
    "        if \"check_promotion_gates_tenant\" in globals():\n",
    "            globals()[\"check_promotion_gates_tenant\"](\"default\")\n",
    "    except Exception as e:\n",
    "        problems.append(f\"tenant-gate:{e}\")\n",
    "    try:\n",
    "        state_snapshot()\n",
    "    except Exception as e:\n",
    "        problems.append(f\"snapshot:{e}\")\n",
    "    rep = {\n",
    "        \"ts\": int(time.time()),\n",
    "        \"ok\": not problems,\n",
    "        \"problems\": problems,\n",
    "    }\n",
    "    TEST_REPORT_PATH.write_text(json.dumps(rep, indent=2), encoding=\"utf-8\")\n",
    "    print(f\" tests  {TEST_REPORT_PATH} (ok={rep['ok']})\")\n",
    "    return rep\n",
    "def gw_cli16(cmd: str, **kwargs) -> None:\n",
    "    cmd = cmd.strip().lower()\n",
    "    if cmd == \"profile.apply\":\n",
    "        name = kwargs.get(\"name\", \"dev\")\n",
    "        apply_profile_to_config(name)\n",
    "    elif cmd == \"boot.secure\":\n",
    "        boot_all_secure()\n",
    "    elif cmd == \"snapshot\":\n",
    "        state_snapshot()\n",
    "    elif cmd == \"readme\":\n",
    "        write_readme()\n",
    "    elif cmd == \"tests\":\n",
    "        run_notebook_tests()\n",
    "    elif cmd == \"tokens\":\n",
    "        print(json.dumps({\"tokens\": load_admin_tokens()}, indent=2))\n",
    "    else:\n",
    "        print(\" unknown cmd. try:\")\n",
    "        print(\"  gw_cli16('profile.apply', name='dev')\")\n",
    "        print(\"  gw_cli16('boot.secure')\")\n",
    "        print(\"  gw_cli16('snapshot')\")\n",
    "        print(\"  gw_cli16('readme')\")\n",
    "        print(\"  gw_cli16('tests')\")\n",
    "        print(\"  gw_cli16('tokens')\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 820,
   "id": "f3a7c48d-c37b-4e66-8ed6-5891f71b1fad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " README  C:\\tmp\\art\\gateway_README.md\n",
      " state-snapshot  C:\\tmp\\art\\gateway_state_snapshot.json\n",
      " SLO eval  C:\\tmp\\art\\gateway_slo_eval.json\n",
      " per-tenant gate (default)  {'ts': 1762120345, 'ok': False, 'tenant': 'default', 'reasons': ['default: p95 1620.0 > 1500.0', 'default: err 1.0 > 0.03', 'default: fast burn 20.0 > 1.0', 'default: slow burn 20.0 > 1.0']}\n",
      " state-snapshot  C:\\tmp\\art\\gateway_state_snapshot.json\n",
      " tests  C:\\tmp\\art\\gateway_test_report.json (ok=True)\n",
      "\n",
      " continuation (profiles + secure-boot + snapshot + readme + tests + cli16) loaded.\n",
      "Try next:\n",
      "  gw_cli16('profile.apply', name='prod')\n",
      "  gw_cli16('boot.secure')\n",
      "  gw_cli16('snapshot')\n",
      "  gw_cli16('readme')\n",
      "  gw_cli16('tests')\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    write_readme()\n",
    "except Exception:\n",
    "    pass\n",
    "try:\n",
    "    state_snapshot()\n",
    "except Exception:\n",
    "    pass\n",
    "try:\n",
    "    run_notebook_tests()\n",
    "except Exception:\n",
    "    pass\n",
    "print(\"\\n continuation (profiles + secure-boot + snapshot + readme + tests + cli16) loaded.\")\n",
    "print(\"Try next:\")\n",
    "print(\"  gw_cli16('profile.apply', name='prod')\")\n",
    "print(\"  gw_cli16('boot.secure')\")\n",
    "print(\"  gw_cli16('snapshot')\")\n",
    "print(\"  gw_cli16('readme')\")\n",
    "print(\"  gw_cli16('tests')\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 821,
   "id": "d3b05aa0-d13e-470b-88b0-b54b05bc5811",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " continuation (push-state + labeled-metrics + mtls-enforce + ghci + cli17) using ART_DIR=C:\\tmp\\art\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "import time\n",
    "import tarfile\n",
    "import socket\n",
    "import hashlib\n",
    "from pathlib import Path\n",
    "from typing import Any, Dict, List, Optional, Tuple\n",
    "ART_DIR = Path(os.getenv(\"ART_DIR\", r\"C:\\tmp\\art\" if os.name == \"nt\" else \"/tmp/art\"))\n",
    "ART_DIR.mkdir(parents=True, exist_ok=True)\n",
    "print(f\" continuation (push-state + labeled-metrics + mtls-enforce + ghci + cli17) using ART_DIR={ART_DIR}\")\n",
    "HOSTNAME = socket.gethostname()\n",
    "CONFIG_PATH = ART_DIR / \"gateway_config.json\"\n",
    "PROM_LABELED_PATH = ART_DIR / \"gateway_prometheus_labeled.txt\"\n",
    "STATE_BUNDLE_PATH = ART_DIR / \"gateway_state_bundle.tar.gz\"\n",
    "GHA_WORKFLOW_PATH = ART_DIR / \"gateway_ci_workflow.yml\"\n",
    "MTLS_DIR = ART_DIR / \"mtls\"\n",
    "MTLS_CA_PATH = MTLS_DIR / \"ca.pem\"\n",
    "MTLS_SRV_PATH = MTLS_DIR / \"server.pem\"\n",
    "MTLS_CLI_PATH = MTLS_DIR / \"client.pem\"\n",
    "def _load_json_safe(p: Path, default: Any = None) -> Any:\n",
    "    if not p.exists():\n",
    "        return default\n",
    "    try:\n",
    "        return json.loads(p.read_text(encoding=\"utf-8\"))\n",
    "    except Exception:\n",
    "        return default\n",
    "def load_config() -> Dict[str, Any]:\n",
    "    if CONFIG_PATH.exists():\n",
    "        try:\n",
    "            return json.loads(CONFIG_PATH.read_text(encoding=\"utf-8\"))\n",
    "        except Exception:\n",
    "            pass\n",
    "    cfg = {\n",
    "        \"ts\": int(time.time()),\n",
    "        \"schema\": \"1.0.0\",\n",
    "        \"env\": \"dev\",\n",
    "        \"http\": {},\n",
    "        \"security\": {},\n",
    "    }\n",
    "    CONFIG_PATH.write_text(json.dumps(cfg, indent=2), encoding=\"utf-8\")\n",
    "    return cfg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 822,
   "id": "c30aea0d-fc89-4981-a2a2-e1a73ca097b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def mtls_enforce_if_required() -> Dict[str, Any]:\n",
    "    \"\"\"\n",
    "    If config.security.require_mtls == True  fail fast if any of the PEMs are missing.\n",
    "    This is the 'real' version, not just 'we wrote placeholder files'.\n",
    "    \"\"\"\n",
    "    cfg = load_config()\n",
    "    sec = cfg.get(\"security\") or {}\n",
    "    need = bool(sec.get(\"require_mtls\", False))\n",
    "    status = {\n",
    "        \"ts\": int(time.time()),\n",
    "        \"required\": need,\n",
    "        \"ok\": True,\n",
    "        \"missing\": [],\n",
    "    }\n",
    "    if not need:\n",
    "        (ART_DIR / \"gateway_mtls_check.json\").write_text(json.dumps(status, indent=2), encoding=\"utf-8\")\n",
    "        print(\" mTLS not required by config\")\n",
    "        return status\n",
    "    missing = []\n",
    "    if not MTLS_CA_PATH.exists():\n",
    "        missing.append(MTLS_CA_PATH.as_posix())\n",
    "    if not MTLS_SRV_PATH.exists():\n",
    "        missing.append(MTLS_SRV_PATH.as_posix())\n",
    "    if not MTLS_CLI_PATH.exists():\n",
    "        missing.append(MTLS_CLI_PATH.as_posix())\n",
    "    if missing:\n",
    "        status[\"ok\"] = False\n",
    "        status[\"missing\"] = missing\n",
    "        (ART_DIR / \"gateway_mtls_check.json\").write_text(json.dumps(status, indent=2), encoding=\"utf-8\")\n",
    "        print(f\" mTLS required but missing: {missing}\")\n",
    "        return status\n",
    "    (ART_DIR / \"gateway_mtls_check.json\").write_text(json.dumps(status, indent=2), encoding=\"utf-8\")\n",
    "    print(\" mTLS enforced  all certs present\")\n",
    "    return status\n",
    "def write_prometheus_labeled(out: Path = PROM_LABELED_PATH) -> Path:\n",
    "    \"\"\"\n",
    "    We already have write_prometheus_export() from earlier.\n",
    "    This adds labels: env, host, active_control\n",
    "    \"\"\"\n",
    "    base_prom = \"\"\n",
    "    base_path = ART_DIR / \"gateway_prometheus.txt\"\n",
    "    if base_path.exists():\n",
    "        base_prom = base_path.read_text(encoding=\"utf-8\")\n",
    "    cfg = load_config()\n",
    "    env = cfg.get(\"env\", \"dev\")\n",
    "    active_control = \"\"\n",
    "    act_path = ART_DIR / \"gateway_control.active\"\n",
    "    if act_path.exists():\n",
    "        active_control = act_path.read_text(encoding=\"utf-8\").strip()\n",
    "    lines_out: List[str] = []\n",
    "    for line in base_prom.splitlines():\n",
    "        line = line.rstrip()\n",
    "        if not line or line.startswith(\"#\"):\n",
    "            lines_out.append(line)\n",
    "            continue\n",
    "        if \"{\" in line:\n",
    "            metric, rest = line.split(\"{\", 1)\n",
    "            rest = rest.rstrip()\n",
    "            if rest.endswith(\"}\"):\n",
    "                rest = rest[:-1]\n",
    "            rest_labels = rest\n",
    "            if rest_labels:\n",
    "                rest_labels += f',env=\"{env}\",host=\"{HOSTNAME}\",control=\"{active_control}\"'\n",
    "            else:\n",
    "                rest_labels = f'env=\"{env}\",host=\"{HOSTNAME}\",control=\"{active_control}\"'\n",
    "            lines_out.append(f'{metric}{{{rest_labels}}}')\n",
    "        else:\n",
    "            lines_out.append(f'{line}{{env=\"{env}\",host=\"{HOSTNAME}\",control=\"{active_control}\"}}')\n",
    "    out.write_text(\"\\n\".join(lines_out) + \"\\n\", encoding=\"utf-8\")\n",
    "    print(f\" labeled prom-export  {out}\")\n",
    "    return out\n",
    "def build_state_bundle(out: Path = STATE_BUNDLE_PATH) -> Path:\n",
    "    \"\"\"\n",
    "    Tar all the 'important' .json/.jsonl/.txt/.html into one bundle.\n",
    "    This is what remote push or GitOps can pick up.\n",
    "    \"\"\"\n",
    "    wanted_ext = {\".json\", \".jsonl\", \".txt\", \".html\", \".yml\", \".yaml\", \".md\"}\n",
    "    with tarfile.open(out, \"w:gz\") as tar:\n",
    "        for p in ART_DIR.glob(\"**/*\"):\n",
    "            if not p.is_file():\n",
    "                continue\n",
    "            if p.suffix.lower() not in wanted_ext:\n",
    "                continue\n",
    "            rel = p.relative_to(ART_DIR)\n",
    "            tar.add(p, arcname=rel.as_posix())\n",
    "    print(f\" state bundle  {out}\")\n",
    "    return out\n",
    "def push_state_bundle(url: Optional[str] = None, out: Optional[Path] = None) -> Dict[str, Any]:\n",
    "    \"\"\"\n",
    "    If url is None  just build and return.\n",
    "    If url is set  HTTP POST the tarball (content-type: application/gzip)\n",
    "    \"\"\"\n",
    "    out = out or STATE_BUNDLE_PATH\n",
    "    build_state_bundle(out)\n",
    "    res = {\n",
    "        \"ts\": int(time.time()),\n",
    "        \"ok\": True,\n",
    "        \"path\": str(out),\n",
    "        \"pushed\": False,\n",
    "    }\n",
    "    if not url:\n",
    "        return res\n",
    "    try:\n",
    "        import requests\n",
    "        data = out.read_bytes()\n",
    "        r = requests.post(url, data=data, headers={\"content-type\": \"application/gzip\"}, timeout=4.0)\n",
    "        res[\"pushed\"] = 200 <= r.status_code < 300\n",
    "        res[\"status\"] = r.status_code\n",
    "        print(f\" state bundle pushed  {url} ({r.status_code})\")\n",
    "    except Exception as e:\n",
    "        res[\"ok\"] = False\n",
    "        res[\"error\"] = str(e)\n",
    "        print(f\" push_state_bundle error: {e}\")\n",
    "    return res\n",
    "def write_ci_workflow(out: Path = GHA_WORKFLOW_PATH) -> Path:\n",
    "    \"\"\"\n",
    "    Emit a GH Actions-style workflow that:\n",
    "      - checks out\n",
    "      - runs `python gateway.py --ci` (your run_full_ci())\n",
    "      - uploads /tmp/art/*\n",
    "    This is just a template.\n",
    "    \"\"\"\n",
    "    yml = f\"\"\"name: gateway-ci\n",
    "on:\n",
    "  push:\n",
    "    paths:\n",
    "      - '**.py'\n",
    "      - '**.ipynb'\n",
    "      - '.github/workflows/**'\n",
    "  workflow_dispatch:\n",
    "\n",
    "jobs:\n",
    "  build-and-test:\n",
    "    runs-on: ubuntu-latest\n",
    "    steps:\n",
    "      - name: Checkout\n",
    "        uses: actions/checkout@v4\n",
    "\n",
    "      - name: Setup Python\n",
    "        uses: actions/setup-python@v5\n",
    "        with:\n",
    "          python-version: '3.11'\n",
    "\n",
    "      - name: Install deps\n",
    "        run: |\n",
    "          python -m pip install --upgrade pip\n",
    "          # add your deps here\n",
    "          # pip install -r requirements.txt\n",
    "\n",
    "      - name: Run CI inside notebook code\n",
    "        env:\n",
    "          ART_DIR: /tmp/art\n",
    "        run: |\n",
    "          python - <<'PYCODE'\n",
    "import json, os, time, pathlib\n",
    "from pathlib import Path\n",
    "ART_DIR = Path(os.getenv(\"ART_DIR\", \"/tmp/art\"))\n",
    "ART_DIR.mkdir(parents=True, exist_ok=True)\n",
    "# try to call run_full_ci if present\n",
    "try:\n",
    "    from notebook import run_full_ci  # adapt to your file\n",
    "except Exception:\n",
    "    run_full_ci = None\n",
    "if run_full_ci:\n",
    "    rep = run_full_ci()\n",
    "    (ART_DIR / \"gha_ci_report.json\").write_text(json.dumps(rep, indent=2), encoding=\"utf-8\")\n",
    "else:\n",
    "    (ART_DIR / \"gha_ci_report.json\").write_text(json.dumps({{\"ok\": False, \"error\": \"run_full_ci not found\"}}, indent=2), encoding=\"utf-8\")\n",
    "PYCODE\n",
    "\n",
    "      - name: Upload artifacts\n",
    "        uses: actions/upload-artifact@v4\n",
    "        with:\n",
    "          name: gateway-art\n",
    "          path: /tmp/art/\n",
    "\"\"\"\n",
    "    out.write_text(yml, encoding=\"utf-8\")\n",
    "    print(f\" CI workflow  {out}\")\n",
    "    return out\n",
    "def gw_cli17(cmd: str, **kwargs) -> None:\n",
    "    cmd = cmd.strip().lower()\n",
    "    if cmd == \"mtls.check\":\n",
    "        mtls_enforce_if_required()\n",
    "    elif cmd == \"metrics.labeled\":\n",
    "        write_prometheus_labeled()\n",
    "    elif cmd == \"state.bundle\":\n",
    "        url = kwargs.get(\"url\")\n",
    "        push_state_bundle(url=url)\n",
    "    elif cmd == \"ci.workflow\":\n",
    "        write_ci_workflow()\n",
    "    elif cmd == \"all\":\n",
    "        mtls_enforce_if_required()\n",
    "        write_prometheus_labeled()\n",
    "        push_state_bundle()\n",
    "        write_ci_workflow()\n",
    "    else:\n",
    "        print(\" unknown cmd. try:\")\n",
    "        print(\"  gw_cli17('mtls.check')\")\n",
    "        print(\"  gw_cli17('metrics.labeled')\")\n",
    "        print(\"  gw_cli17('state.bundle')\")\n",
    "        print(\"  gw_cli17('state.bundle', url='http://127.0.0.1:8080/upload')\")\n",
    "        print(\"  gw_cli17('ci.workflow')\")\n",
    "        print(\"  gw_cli17('all')\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 823,
   "id": "f56dfcf4-9706-4b0a-95e9-74b56dba2e7b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " mTLS not required by config\n",
      " labeled prom-export  C:\\tmp\\art\\gateway_prometheus_labeled.txt\n",
      " state bundle  C:\\tmp\\art\\gateway_state_bundle.tar.gz\n",
      " CI workflow  C:\\tmp\\art\\gateway_ci_workflow.yml\n",
      "\n",
      " continuation (push-state + labeled-metrics + mtls-enforce + ghci + cli17) loaded.\n",
      "Try next:\n",
      "  gw_cli17('mtls.check')\n",
      "  gw_cli17('metrics.labeled')\n",
      "  gw_cli17('state.bundle')\n",
      "  gw_cli17('ci.workflow')\n",
      "  gw_cli17('all')\n"
     ]
    }
   ],
   "source": [
    "mtls_enforce_if_required()\n",
    "write_prometheus_labeled()\n",
    "build_state_bundle()\n",
    "write_ci_workflow()\n",
    "print(\"\\n continuation (push-state + labeled-metrics + mtls-enforce + ghci + cli17) loaded.\")\n",
    "print(\"Try next:\")\n",
    "print(\"  gw_cli17('mtls.check')\")\n",
    "print(\"  gw_cli17('metrics.labeled')\")\n",
    "print(\"  gw_cli17('state.bundle')\")\n",
    "print(\"  gw_cli17('ci.workflow')\")\n",
    "print(\"  gw_cli17('all')\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 824,
   "id": "ae9d0584-9b41-4069-8047-f41b9620875e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " continuation (final extras: n8n-full, secrets-iface, prom-pull, slo-from-prom, http-v3, fastapi-stub, gw_cli18) using ART_DIR=C:\\tmp\\art\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "import time\n",
    "import socket\n",
    "import threading\n",
    "from pathlib import Path\n",
    "from typing import Any, Dict, List, Optional, Tuple, Callable\n",
    "ART_DIR = Path(os.getenv(\"ART_DIR\", r\"C:\\tmp\\art\" if os.name == \"nt\" else \"/tmp/art\"))\n",
    "ART_DIR.mkdir(parents=True, exist_ok=True)\n",
    "print(f\" continuation (final extras: n8n-full, secrets-iface, prom-pull, slo-from-prom, http-v3, fastapi-stub, gw_cli18) using ART_DIR={ART_DIR}\")\n",
    "HOSTNAME = socket.gethostname()\n",
    "OTEL_SPANS_PATH = ART_DIR / \"otel_spans.jsonl\"\n",
    "PROM_PULL_PATH = ART_DIR / \"gateway_prom_pull.json\"\n",
    "SLO_EVAL_PATH = ART_DIR / \"gateway_slo_eval.json\"\n",
    "N8N_WORKFLOW_FULL_PATH = ART_DIR / \"n8n-workflow-gateway-full.json\"\n",
    "STATE_BUNDLE_PATH = ART_DIR / \"gateway_state_bundle.tar.gz\"\n",
    "N8N_V3_PORT = int(os.getenv(\"N8N_V3_PORT\", \"9935\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 825,
   "id": "398fc401-8dd9-4375-a7d6-971989508f7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SecretProvider:\n",
    "    \"\"\"\n",
    "    Unified secret provider:\n",
    "      - env: GATEWAY_ADMIN_TOKEN\n",
    "      - file: /tmp/art/secret-*.txt\n",
    "      - vault-json: vault-secrets.json (existing)\n",
    "      - http: POST to remote secret service\n",
    "    \"\"\"\n",
    "    def __init__(self, art_dir: Path):\n",
    "        self.art_dir = art_dir\n",
    "        self.vault_path = art_dir / \"vault-secrets.json\"\n",
    "        self.map_path = art_dir / \"gateway_secret_map.json\"\n",
    "        self.http_endpoint = os.getenv(\"SECRET_HTTP_ENDPOINT\", \"\").strip()\n",
    "    def _load_json(self, p: Path, default: Any = None) -> Any:\n",
    "        if not p.exists():\n",
    "            return default\n",
    "        try:\n",
    "            return json.loads(p.read_text(encoding=\"utf-8\"))\n",
    "        except Exception:\n",
    "            return default\n",
    "    def get(self, logical: str) -> Optional[str]:\n",
    "        sm = self._load_json(self.map_path, {})\n",
    "        spec = (sm.get(\"secrets\") or {}).get(logical)\n",
    "        if not spec:\n",
    "            return os.getenv(logical.upper(), \"\")\n",
    "        backend = spec.get(\"backend\", \"env\")\n",
    "        key = spec.get(\"key\")\n",
    "        if backend == \"env\":\n",
    "            return os.getenv(key or \"\", \"\")\n",
    "        if backend == \"vault\":\n",
    "            vault = self._load_json(self.vault_path, {})\n",
    "            return (vault or {}).get(key or \"\", \"\")\n",
    "        if backend == \"file\":\n",
    "            p = self.art_dir / (key or \"\")\n",
    "            return p.read_text(encoding=\"utf-8\").strip() if p.exists() else \"\"\n",
    "        if backend == \"http\" and self.http_endpoint:\n",
    "            try:\n",
    "                import requests\n",
    "                r = requests.post(self.http_endpoint, json={\"name\": key}, timeout=3.0)\n",
    "                if 200 <= r.status_code < 300:\n",
    "                    js = r.json()\n",
    "                    return js.get(\"value\") or \"\"\n",
    "            except Exception as e:\n",
    "                print(f\" secret http error: {e}\")\n",
    "                return \"\"\n",
    "        return \"\"\n",
    "    def put_vault(self, key: str, value: str) -> None:\n",
    "        vault = self._load_json(self.vault_path, {}) or {}\n",
    "        vault[key] = value\n",
    "        self.vault_path.write_text(json.dumps(vault, indent=2), encoding=\"utf-8\")\n",
    "        print(f\" secret stored in local vault  {self.vault_path}\")\n",
    "SECRET_PROVIDER = SecretProvider(ART_DIR)\n",
    "def prom_pull_to_json(url: str, out: Path = PROM_PULL_PATH) -> Path:\n",
    "    \"\"\"\n",
    "    Pull Prometheus text exposition from `url` and convert to naive JSON.\n",
    "    Works for small, local Prom.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        import requests\n",
    "    except Exception as e:\n",
    "        raise RuntimeError(f\"requests not available: {e}\")\n",
    "    r = requests.get(url, timeout=3.0)\n",
    "    r.raise_for_status()\n",
    "    lines = r.text.splitlines()\n",
    "    metrics: List[Dict[str, Any]] = []\n",
    "    for ln in lines:\n",
    "        ln = ln.strip()\n",
    "        if not ln or ln.startswith(\"#\"):\n",
    "            continue\n",
    "        name = ln\n",
    "        value = None\n",
    "        labels = {}\n",
    "        if \"{\" in ln:\n",
    "            name, rest = ln.split(\"{\", 1)\n",
    "            rest = rest.strip()\n",
    "            if \"}\" in rest:\n",
    "                lbls, val = rest.split(\"}\", 1)\n",
    "                val = val.strip()\n",
    "                for part in lbls.split(\",\"):\n",
    "                    if not part:\n",
    "                        continue\n",
    "                    if \"=\" in part:\n",
    "                        k, v = part.split(\"=\", 1)\n",
    "                        labels[k.strip()] = v.strip().strip('\"')\n",
    "                value = float(val)\n",
    "        else:\n",
    "            parts = ln.split()\n",
    "            if len(parts) == 2:\n",
    "                name = parts[0]\n",
    "                value = float(parts[1])\n",
    "        metrics.append({\n",
    "            \"name\": name.strip(),\n",
    "            \"labels\": labels,\n",
    "            \"value\": value,\n",
    "        })\n",
    "    payload = {\n",
    "        \"ts\": int(time.time()),\n",
    "        \"host\": HOSTNAME,\n",
    "        \"metrics\": metrics,\n",
    "        \"source\": url,\n",
    "    }\n",
    "    out.write_text(json.dumps(payload, indent=2), encoding=\"utf-8\")\n",
    "    print(f\" prom-pull  {out} ({len(metrics)} metrics)\")\n",
    "    return out\n",
    "def slo_from_prom(prom_json: Path = PROM_PULL_PATH, out: Path = SLO_EVAL_PATH) -> Path:\n",
    "    \"\"\"\n",
    "    Take prom-pulled JSON and infer p95/error for tenants.\n",
    "    Expect metrics like:\n",
    "      gateway_slo_p95_ms{tenant=\"default\"}\n",
    "      gateway_slo_error_rate{tenant=\"default\"}\n",
    "    \"\"\"\n",
    "    if not prom_json.exists():\n",
    "        raise FileNotFoundError(str(prom_json))\n",
    "    data = json.loads(prom_json.read_text(encoding=\"utf-8\"))\n",
    "    tenants: Dict[str, Dict[str, Any]] = {}\n",
    "    for m in data.get(\"metrics\", []):\n",
    "        name = m.get(\"name\")\n",
    "        val = m.get(\"value\")\n",
    "        lbls = m.get(\"labels\") or {}\n",
    "        tenant = lbls.get(\"tenant\", \"default\")\n",
    "        t = tenants.setdefault(tenant, {\n",
    "            \"target_p95_ms\": 1500.0,\n",
    "            \"actual_p95_ms\": 1500.0,\n",
    "            \"target_error_rate\": 0.03,\n",
    "            \"actual_error_rate\": 0.0,\n",
    "            \"ok\": True,\n",
    "        })\n",
    "        if name.startswith(\"gateway_slo_p95_ms\"):\n",
    "            t[\"actual_p95_ms\"] = float(val or 0.0)\n",
    "        elif name.startswith(\"gateway_slo_error_rate\"):\n",
    "            t[\"actual_error_rate\"] = float(val or 0.0)\n",
    "    for tname, spec in tenants.items():\n",
    "        spec[\"ok\"] = (spec[\"actual_p95_ms\"] <= spec[\"target_p95_ms\"]) and (\n",
    "            spec[\"actual_error_rate\"] <= spec[\"target_error_rate\"]\n",
    "        )\n",
    "    out_data = {\n",
    "        \"ts\": int(time.time()),\n",
    "        \"schema\": \"1.0.0\",\n",
    "        \"tenants\": tenants,\n",
    "        \"source\": prom_json.as_posix(),\n",
    "    }\n",
    "    out.write_text(json.dumps(out_data, indent=2), encoding=\"utf-8\")\n",
    "    print(f\" SLO from prom  {out}\")\n",
    "    return out\n",
    "def write_n8n_workflow_full() -> Path:\n",
    "    \"\"\"\n",
    "    Bigger variant than the earlier minimal workflow.\n",
    "    Calls:\n",
    "      - 9930 /outbox/pull\n",
    "      - 9929 /promote/check\n",
    "      - promote or freeze\n",
    "      - 9929 /retention\n",
    "      - 9930 /otel/push\n",
    "      - 9930 /promote/last (for heartbeat)\n",
    "    \"\"\"\n",
    "    wf = {\n",
    "        \"name\": \"Gateway Full Orchestration (cron  SLO  gate  promote/freeze  retention  notify)\",\n",
    "        \"nodes\": [\n",
    "            {\n",
    "                \"id\": \"1\",\n",
    "                \"name\": \"Cron 5m\",\n",
    "                \"type\": \"n8n-nodes-base.cron\",\n",
    "                \"typeVersion\": 2,\n",
    "                \"position\": [240, 300],\n",
    "                \"parameters\": {\n",
    "                    \"rule\": {\n",
    "                        \"interval\": 5\n",
    "                    }\n",
    "                }\n",
    "            },\n",
    "            {\n",
    "                \"id\": \"2\",\n",
    "                \"name\": \"Pull Events\",\n",
    "                \"type\": \"n8n-nodes-base.httpRequest\",\n",
    "                \"typeVersion\": 3,\n",
    "                \"position\": [480, 140],\n",
    "                \"parameters\": {\n",
    "                    \"url\": \"http://127.0.0.1:9930/outbox/pull\",\n",
    "                    \"method\": \"GET\",\n",
    "                    \"jsonParameters\": True,\n",
    "                }\n",
    "            },\n",
    "            {\n",
    "                \"id\": \"3\",\n",
    "                \"name\": \"Check Promotion Gate\",\n",
    "                \"type\": \"n8n-nodes-base.httpRequest\",\n",
    "                \"typeVersion\": 3,\n",
    "                \"position\": [480, 300],\n",
    "                \"parameters\": {\n",
    "                    \"url\": \"http://127.0.0.1:9929/promote/check\",\n",
    "                    \"method\": \"POST\",\n",
    "                    \"jsonParameters\": True,\n",
    "                }\n",
    "            },\n",
    "            {\n",
    "                \"id\": \"4\",\n",
    "                \"name\": \"IF Gate OK?\",\n",
    "                \"type\": \"n8n-nodes-base.if\",\n",
    "                \"typeVersion\": 2,\n",
    "                \"position\": [720, 300],\n",
    "                \"parameters\": {\n",
    "                    \"conditions\": {\n",
    "                        \"boolean\": [\n",
    "                            {\n",
    "                                \"value1\": \"={{ $json[\\\"ok\\\"] }}\",\n",
    "                                \"value2\": True\n",
    "                            }\n",
    "                        ]\n",
    "                    }\n",
    "                }\n",
    "            },\n",
    "            {\n",
    "                \"id\": \"5\",\n",
    "                \"name\": \"Promote (Notebook)\",\n",
    "                \"type\": \"n8n-nodes-base.httpRequest\",\n",
    "                \"typeVersion\": 3,\n",
    "                \"position\": [960, 180],\n",
    "                \"parameters\": {\n",
    "                    \"url\": \"http://127.0.0.1:9930/promote/run\",\n",
    "                    \"method\": \"POST\",\n",
    "                    \"jsonParameters\": True,\n",
    "                    \"options\": {},\n",
    "                    \"bodyParametersJson\": \"{\\\"reason\\\": \\\"n8n-auto\\\"}\"\n",
    "                }\n",
    "            },\n",
    "            {\n",
    "                \"id\": \"6\",\n",
    "                \"name\": \"Freeze (bad SLO)\",\n",
    "                \"type\": \"n8n-nodes-base.httpRequest\",\n",
    "                \"typeVersion\": 3,\n",
    "                \"position\": [960, 420],\n",
    "                \"parameters\": {\n",
    "                    \"url\": \"http://127.0.0.1:9929/freeze\",\n",
    "                    \"method\": \"POST\",\n",
    "                    \"jsonParameters\": True,\n",
    "                    \"bodyParametersJson\": \"{\\\"reason\\\": \\\"n8n-slo-breach\\\", \\\"ttl_s\\\": 1800}\"\n",
    "                }\n",
    "            },\n",
    "            {\n",
    "                \"id\": \"7\",\n",
    "                \"name\": \"Retention Sweep\",\n",
    "                \"type\": \"n8n-nodes-base.httpRequest\",\n",
    "                \"typeVersion\": 3,\n",
    "                \"position\": [1200, 420],\n",
    "                \"parameters\": {\n",
    "                    \"url\": \"http://127.0.0.1:9929/retention\",\n",
    "                    \"method\": \"POST\",\n",
    "                    \"jsonParameters\": True,\n",
    "                }\n",
    "            },\n",
    "            {\n",
    "                \"id\": \"8\",\n",
    "                \"name\": \"Push OTEL (optional)\",\n",
    "                \"type\": \"n8n-nodes-base.httpRequest\",\n",
    "                \"typeVersion\": 3,\n",
    "                \"position\": [1200, 180],\n",
    "                \"parameters\": {\n",
    "                    \"url\": \"http://127.0.0.1:9930/otel/push\",\n",
    "                    \"method\": \"POST\",\n",
    "                    \"jsonParameters\": True,\n",
    "                    \"bodyParametersJson\": \"{}\"\n",
    "                }\n",
    "            },\n",
    "            {\n",
    "                \"id\": \"9\",\n",
    "                \"name\": \"Heartbeat (get last promotion)\",\n",
    "                \"type\": \"n8n-nodes-base.httpRequest\",\n",
    "                \"typeVersion\": 3,\n",
    "                \"position\": [1440, 300],\n",
    "                \"parameters\": {\n",
    "                    \"url\": \"http://127.0.0.1:9930/promote/last\",\n",
    "                    \"method\": \"GET\",\n",
    "                    \"jsonParameters\": True,\n",
    "                }\n",
    "            },\n",
    "        ],\n",
    "        \"connections\": {\n",
    "            \"Cron 5m\": {\n",
    "                \"main\": [\n",
    "                    [\n",
    "                        {\n",
    "                            \"node\": \"Pull Events\",\n",
    "                            \"type\": \"main\",\n",
    "                            \"index\": 0\n",
    "                        },\n",
    "                        {\n",
    "                            \"node\": \"Check Promotion Gate\",\n",
    "                            \"type\": \"main\",\n",
    "                            \"index\": 0\n",
    "                        }\n",
    "                    ]\n",
    "                ]\n",
    "            },\n",
    "            \"Check Promotion Gate\": {\n",
    "                \"main\": [\n",
    "                    [\n",
    "                        {\n",
    "                            \"node\": \"IF Gate OK?\",\n",
    "                            \"type\": \"main\",\n",
    "                            \"index\": 0\n",
    "                        }\n",
    "                    ]\n",
    "                ]\n",
    "            },\n",
    "            \"IF Gate OK?\": {\n",
    "                \"main\": [\n",
    "                    [\n",
    "                        {\n",
    "                            \"node\": \"Promote (Notebook)\",\n",
    "                            \"type\": \"main\",\n",
    "                            \"index\": 0\n",
    "                        },\n",
    "                        {\n",
    "                            \"node\": \"Push OTEL (optional)\",\n",
    "                            \"type\": \"main\",\n",
    "                            \"index\": 0\n",
    "                        },\n",
    "                        {\n",
    "                            \"node\": \"Heartbeat (get last promotion)\",\n",
    "                            \"type\": \"main\",\n",
    "                            \"index\": 0\n",
    "                        }\n",
    "                    ],\n",
    "                    [\n",
    "                        {\n",
    "                            \"node\": \"Freeze (bad SLO)\",\n",
    "                            \"type\": \"main\",\n",
    "                            \"index\": 0\n",
    "                        },\n",
    "                        {\n",
    "                            \"node\": \"Retention Sweep\",\n",
    "                            \"type\": \"main\",\n",
    "                            \"index\": 0\n",
    "                        }\n",
    "                    ]\n",
    "                ]\n",
    "            }\n",
    "        },\n",
    "        \"active\": False,\n",
    "        \"settings\": {},\n",
    "        \"versionId\": f\"gw-full-{int(time.time())}\"\n",
    "    }\n",
    "    N8N_WORKFLOW_FULL_PATH.write_text(json.dumps(wf, indent=2), encoding=\"utf-8\")\n",
    "    print(f\" n8n full workflow  {N8N_WORKFLOW_FULL_PATH}\")\n",
    "    return N8N_WORKFLOW_FULL_PATH\n",
    "def serve_n8n_http_v3(port: int = N8N_V3_PORT) -> None:\n",
    "    import http.server\n",
    "    import socketserver\n",
    "    class N8NHandlerV3(http.server.BaseHTTPRequestHandler):\n",
    "        def _json(self, code: int, payload: Dict[str, Any]) -> None:\n",
    "            body = json.dumps(payload).encode(\"utf-8\")\n",
    "            self.send_response(code)\n",
    "            self.send_header(\"Content-Type\", \"application/json\")\n",
    "            self.send_header(\"Content-Length\", str(len(body)))\n",
    "            self.end_headers()\n",
    "            self.wfile.write(body)\n",
    "        def do_POST(self):\n",
    "            ln = int(self.headers.get(\"content-length\") or \"0\")\n",
    "            raw = self.rfile.read(ln) if ln else b\"{}\"\n",
    "            try:\n",
    "                data = json.loads(raw.decode(\"utf-8\") or \"{}\")\n",
    "            except Exception:\n",
    "                data = {}\n",
    "            if self.path == \"/archive\":\n",
    "                if \"archive_old_artifacts\" in globals():\n",
    "                    rep = globals()[\"archive_old_artifacts\"]()\n",
    "                else:\n",
    "                    rep = {\"error\": \"archive_old_artifacts not found\"}\n",
    "                self._json(200, rep)\n",
    "                return\n",
    "            if self.path == \"/snapshot\":\n",
    "                if \"state_snapshot\" in globals():\n",
    "                    p = globals()[\"state_snapshot\"]()\n",
    "                    self._json(200, {\"ok\": True, \"path\": str(p)})\n",
    "                else:\n",
    "                    self._json(500, {\"error\": \"state_snapshot not found\"})\n",
    "                return\n",
    "            if self.path == \"/state/bundle\":\n",
    "                if \"push_state_bundle\" in globals():\n",
    "                    url = data.get(\"url\")\n",
    "                    rep = globals()[\"push_state_bundle\"](url=url)\n",
    "                    self._json(200, rep)\n",
    "                else:\n",
    "                    self._json(500, {\"error\": \"push_state_bundle not found\"})\n",
    "                return\n",
    "            self._json(404, {\"error\": \"not-found\"})\n",
    "        def do_GET(self):\n",
    "            if self.path == \"/workflow/full\":\n",
    "                if N8N_WORKFLOW_FULL_PATH.exists():\n",
    "                    body = N8N_WORKFLOW_FULL_PATH.read_bytes()\n",
    "                    self.send_response(200)\n",
    "                    self.send_header(\"Content-Type\", \"application/json\")\n",
    "                    self.send_header(\"Content-Length\", str(len(body)))\n",
    "                    self.end_headers()\n",
    "                    self.wfile.write(body)\n",
    "                else:\n",
    "                    self._json(404, {\"error\": \"workflow-not-found\"})\n",
    "                return\n",
    "            self._json(404, {\"error\": \"not-found\"})\n",
    "    with socketserver.TCPServer((\"0.0.0.0\", port), N8NHandlerV3) as httpd:\n",
    "        print(f\" n8n HTTP v3 at http://0.0.0.0:{port}\")\n",
    "        try:\n",
    "            httpd.serve_forever()\n",
    "        except KeyboardInterrupt:\n",
    "            print(\" n8n HTTP v3 stopped\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 826,
   "id": "afceaa8b-47da-45b2-84c9-b62015dfecd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_fastapi_gateway_stub(host: str = \"0.0.0.0\", port: int = 9910) -> None:\n",
    "    \"\"\"\n",
    "    Very small FastAPI app that simulates /answer and emits OTEL spans to our file.\n",
    "    Only runs if FastAPI is available.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        from fastapi import FastAPI, Request\n",
    "        import uvicorn\n",
    "    except Exception as e:\n",
    "        print(f\" FastAPI not available: {e}\")\n",
    "        return\n",
    "    app = FastAPI(title=\"Gateway Stub\")\n",
    "    def _write_span(name: str, attrs: Dict[str, Any]):\n",
    "        rec = {\n",
    "            \"ts\": int(time.time()),\n",
    "            \"name\": name,\n",
    "            \"attrs\": attrs,\n",
    "            \"host\": HOSTNAME,\n",
    "            \"status\": \"OK\",\n",
    "        }\n",
    "        with OTEL_SPANS_PATH.open(\"a\", encoding=\"utf-8\") as f:\n",
    "            f.write(json.dumps(rec) + \"\\n\")\n",
    "    @app.post(\"/answer\")\n",
    "    async def answer(req: Request):\n",
    "        body = await req.json()\n",
    "        q = body.get(\"query\", \"none\")\n",
    "        t0 = time.time()\n",
    "        time.sleep(0.05)\n",
    "        dt_ms = round((time.time() - t0) * 1000, 2)\n",
    "        _write_span(\"gateway.answer\", {\"query\": q, \"latency_ms\": dt_ms})\n",
    "        return {\n",
    "            \"query\": q,\n",
    "            \"answer\": f\"stubbed answer for: {q}\",\n",
    "            \"latency_ms\": dt_ms,\n",
    "        }\n",
    "    print(f\" FastAPI gateway stub on http://{host}:{port}\")\n",
    "    uvicorn.run(app, host=host, port=port)\n",
    "def gw_cli18(cmd: str, **kwargs) -> None:\n",
    "    cmd = cmd.strip().lower()\n",
    "    if cmd == \"n8n.full\":\n",
    "        write_n8n_workflow_full()\n",
    "    elif cmd == \"prom.pull\":\n",
    "        url = kwargs.get(\"url\", \"http://127.0.0.1:9940/metrics\")\n",
    "        prom_pull_to_json(url)\n",
    "    elif cmd == \"slo.from.prom\":\n",
    "        slo_from_prom()\n",
    "    elif cmd == \"secrets.test\":\n",
    "        name = kwargs.get(\"name\", \"admin_token\")\n",
    "        val = SECRET_PROVIDER.get(name)\n",
    "        print(json.dumps({\"name\": name, \"value\": val}, indent=2))\n",
    "    elif cmd == \"http.v3\":\n",
    "        port = int(kwargs.get(\"port\", N8N_V3_PORT))\n",
    "        t = threading.Thread(target=serve_n8n_http_v3, kwargs={\"port\": port}, daemon=True)\n",
    "        t.start()\n",
    "        print(f\" n8n v3 started on {port}\")\n",
    "    elif cmd == \"gateway.fastapi\":\n",
    "        host = kwargs.get(\"host\", \"0.0.0.0\")\n",
    "        port = int(kwargs.get(\"port\", 9910))\n",
    "        run_fastapi_gateway_stub(host=host, port=port)\n",
    "    else:\n",
    "        print(\" unknown cmd. try:\")\n",
    "        print(\"  gw_cli18('n8n.full')\")\n",
    "        print(\"  gw_cli18('prom.pull', url='http://127.0.0.1:9940/metrics')\")\n",
    "        print(\"  gw_cli18('slo.from.prom')\")\n",
    "        print(\"  gw_cli18('secrets.test', name='slack_webhook')\")\n",
    "        print(\"  gw_cli18('http.v3', port=9935)\")\n",
    "        print(\"  gw_cli18('gateway.fastapi', port=9910)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 827,
   "id": "1b9a5c58-7f5b-4d34-bfc6-8b9be83c9532",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " n8n full workflow  C:\\tmp\\art\\n8n-workflow-gateway-full.json\n",
      "\n",
      " final continuation loaded. PRD Phase-3 notebook side is effectively done.\n",
      "Try:\n",
      "  gw_cli18('n8n.full')\n",
      "  gw_cli18('prom.pull', url='http://127.0.0.1:9940/metrics')\n",
      "  gw_cli18('slo.from.prom')\n",
      "  gw_cli18('http.v3', port=9935)\n",
      "  gw_cli18('gateway.fastapi', port=9910)\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    write_n8n_workflow_full()\n",
    "except Exception as e:\n",
    "    print(f\" cannot write full n8n workflow now: {e}\")\n",
    "print(\"\\n final continuation loaded. PRD Phase-3 notebook side is effectively done.\")\n",
    "print(\"Try:\")\n",
    "print(\"  gw_cli18('n8n.full')\")\n",
    "print(\"  gw_cli18('prom.pull', url='http://127.0.0.1:9940/metrics')\")\n",
    "print(\"  gw_cli18('slo.from.prom')\")\n",
    "print(\"  gw_cli18('http.v3', port=9935)\")\n",
    "print(\"  gw_cli18('gateway.fastapi', port=9910)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99348e28-a573-48d1-a211-b0321a13b43b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
